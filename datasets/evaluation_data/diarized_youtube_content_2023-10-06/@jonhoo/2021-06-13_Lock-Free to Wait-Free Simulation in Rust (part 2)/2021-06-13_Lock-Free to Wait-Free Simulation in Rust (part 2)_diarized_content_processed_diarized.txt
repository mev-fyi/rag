00:00:01.040 - 00:00:54.603, Speaker A: Hello folks, welcome back to part two of our video series if you will, on lock free to wait Free simulation in Rust. The idea here is to take this paper someone sent to me a while back and I think it's just really cool. On a practical weight free simulation for lock free data structures. That's basically a way to turn a lock free data structure into a weight free data structure. I'm not going to rehash all of the, all the definitions and what that all means and how we get there. All of that is covered in part one, which I will make sure that if you look at this video after the fact, there'll be a little pop up like up here somewhere that you can click. And if you're watching live, hopefully in chat, there'll be a video to the first part video and you should go watch that.
00:00:54.603 - 00:02:19.627, Speaker A: Like I don't think it's likely that you will get much from this video if you haven't watched part one. So go watch part one instead and you can come watch the video on demand version of this afterwards. I'll also just take a quick aside before we dive into the actual paper and the implementation where we last left off to mention that I wrote a book. I wrote a book called Rust for Rustations and it sort of is what the title tries to say based basically it is a book that tries to cover the Rust language and the ways to use it and the sort of idiomatic programming techniques and the mechanisms and housed of work sort of under the hood, how things fit together, best practices, sort of everything that I've built up of sort of knowledge and experience with the language up through the years in book form and is specifically written for those who already know Rust. So this is not a report replacement for the Rust book and not at all. This is specifically a book for those who already know Rust and want to sort of deepen that understanding. Make sure that you actually like understand what's really going on like under the hood or understand good like programming patterns, how code should fit together, what idiomatic Rust looks like.
00:02:19.627 - 00:02:54.115, Speaker A: Or if you just want more exposure to more parts of Rust, the more cool features of Rust. It's available for like early access now. Ooh, wrong book for early access now. So you can by the early access and then you also get all the chapters as they're released and the final book at the end and you can also order the print book once it's actually released. I'll leave the link in YouTube in the chat. All right, sweet. Now let's go over to the paper.
00:02:54.115 - 00:04:18.945, Speaker A: All right, so last time we went through mostly the construction of the sort of abstraction that we want to provide. So the abstraction here is like, remember there's a sort of a translation from something that's lock free to something that's weight free, but only if the lock free data structure is expressed in this normalized form. As what we did last time was mostly try to capture what that normalized form is through the use of a trait. So specifically we wrote this trait there is it this trait normalize lock free. It has some associated types and then it has these two methods, generator and wrap up, where generator is stuff that happens before the critical section, the sort of commit point for the lock free algorithm and it generates a list of compare and swap operations that are the actual commit points. And then a wrap up method that executes after the commit point has happened and then tries to sort of complete the operation and give some final output of the computation. And then what we did was we wrote this, no, not the help queue, but this weight free simulator struct which is generic over any type that implements the trait, which is basically the lock free data structure.
00:04:18.945 - 00:05:24.425, Speaker A: And then it does all of this sort of logic around when you call the generator, when you call wrap up. How do multiple threads help each other out to make progress and sort of deal with all the concurrency aspects of it? And where we left off was basically the CASS execute method. We only have like a very, very basic implementation of. And then there's also the help queue which is this like wait free queue of pending tasks or pending actions that threads might help each other perform. And this wait free queue, it can't itself use the wait free simulation we're using because that would be a recursive dependency. So instead we're going to have to implement this wait free queue from scratch. And luckily the paper includes a sort of tailor made wait free queue implementation in its appendix towards the bottom that we're going to be implementing here.
00:05:24.425 - 00:06:34.371, Speaker A: I think just to sort of get back into things before we implement the wave free queue. Let's go ahead and finish up CAS execute because that's sort of where we were last time before we do sort of go take sort of one step up just to see what the higher level sort of call graph looks like here. So we have the this is all in the wait free simulator struct. We have a run method that takes an operation that some thread wants to execute and returns an output for that operation. And if you remember, this was something that made us a little Bit sad, right, that the technically there's like one output type for each input type and we can't easily express that the way it's currently set up. There are ways for us to get around this, but I think for now what we're going to do is just say that both of these are enums and then if you send a particular input ENUM variant, you should expect the output enum variant is the corresponding one. We can try to find ways to enforce that with the type system later on.
00:06:34.371 - 00:07:33.211, Speaker A: But for now let's leave it the way it was. And you'll remember that in run the sort of algorithm we're following is that first we're going to try to help another thread. If another thread needs help, then we're going to try this fast path of the lock free algorithm, which is basically we're going to assume that there's not that much contention. We're going to just run the lock free algorithm, start to finish and then we're going to look for signs of contention. And that's what this contention measure struct is for. Where we run the generator and if we detect that there's a bunch of contention by virtue of for example compare and swap operations failing, then we switch to using the slow path, Then we do the CASS execute step on the compare and swap operations generator generated. If we detect contention there, we switch to the slow path and then we do the wrap up and if it completes, we're done.
00:07:33.211 - 00:08:29.675, Speaker A: Otherwise we either do the slow path if there was contention, you'll see a pattern here, otherwise we'll sort of go around again. So the idea is that first help in case there is someone who needs help, then do the fast path a couple of times. And then if we detect contention or if the fast path doesn't succeed in a certain number of tries, which is equivalent to saying there's probably contention, then we switch to the slow path. And the slow path, if you remember, is we sort of enqueue this encoding of what operation we want to execute. So that's this operation record which we stick in operation operation record box because we need to be able to do RCU on it. Like we need to atomically replace this operation. As we execute, we enqueue the help operation and then we keep looking at whether our operation has completed and if it has, we return.
00:08:29.675 - 00:09:20.775, Speaker A: Otherwise we keep helping. So the idea is the slow path is stick my operation on the queue and then just help everything that's in the queue until my thing completes. And that's how we sort of guarantee that over time, as long as some threads execute, all threads make progress. And then if we look at this sort of helping method, which is the core of what makes this algorithm weight free rather than lock free. Remember the basketball analogy from the previous stream? Help first just looks at the front of the queue, the front of the help queue, and if there's something there, it tries to help it. And the actual helping, if you recall, is look at what the current state of the operational record is. If it's completed, then we just remove it from the help queue if it's in a precast state.
00:09:20.775 - 00:10:21.115, Speaker A: Right. If it, if it hasn't executed any of the comparison swaps yet, then we execute the comparison swaps and then we try to do basically rcu, right? So we, we read the current operation record, we modify our local copy of that operational record, and then we try to atomically swap back in the updated version. And even if we fail, that just indicates that some other thread helped instead. And so progress is still made is sort of the general idea here. And so this is really just every operational record is really a state machine of trying to move the operational record through multiple different stages of execution, where each stage is the computation made it a little bit farther than last time. And it does this by read, copy, update, and then atomically write back using a compare and swap. And so each of these help operations is really just calling into the underlying algorithm repeatedly.
00:10:21.115 - 00:11:15.545, Speaker A: Right. So remember, the generator can be called concurrently for multiple threads. Cass execute can be called concurrently for multiple threads, and wrap up can also be called concurrently for multiple threads. And the idea is that overall, as even if lots of threads are trying to help a single operation, you'll still make progress over time. Yeah, so this is that compare exchange where we take the our RCU copy and actually write it back or try to write it back into the operation record box. Great. So all of that sort of depends on this machinery of the generator, which is dictated by the algorithm, the wrap up which is dictated by the algorithm, and the CAS execute operation, which is dictated by the, the implementation provided by the paper.
00:11:15.545 - 00:12:01.845, Speaker A: Right. So CAS execute is really, it takes a list of command and swap operations and then it's going to execute each one in turn until it finds one that fails. And if one fails, it's going to return the index of the one that failed, because subsequently we need to make sure that we execute from there on is the basic idea. So if we go back to the paper, what does this say? Figure 5 is what we want. Let's see if we can find Figure 5, it's probably further up is my guess. Actually I should have written the page number. That was silly of me.
00:12:01.845 - 00:12:29.083, Speaker A: Figure five, the execute CASUS method. So if you remember the casses. So if we backtrack a little bit. Right. The normalized lock free trait has an associated type called cases or casses is really. This really should be casses. But Russ doesn't like that as a convention.
00:12:29.083 - 00:13:08.345, Speaker A: We might try to find a better name here like cast list maybe. But the idea is that the implement, the implementer of the trait can choose the representation of the list of CAS operations to execute. And the reason we wanted to do that is because most lock free algorithms will just have a single cas. And so we don't want to enforce that. It's like a vector or something. But some operations might actually want to have multiple casses that need to be executed. And so we want to support that case again without making that be a cost for any algorithm that doesn't need more than one.
00:13:08.345 - 00:13:59.235, Speaker A: In addition, the casses is also passed to the. Where is it? Further up here. I really need to reorganize this file to the wrap up method which also receives the casses. So the casses are sort of a way for the generator to both convey which caches have to be executed, but also as a way to communicate to the wrap up method any additional meta information. So in some sense like this shouldn't really be called casus, it should sort of be called like generator output. And it just so happens that we require that the generator output type is something that we can index into in order to get out the cast descriptors. Currently we did this with index.
00:13:59.235 - 00:15:08.525, Speaker A: It's like a question of whether that's actually a good idea, but we can figure that out later on. I think what we might want here is actually just to call this. Actually no, I think what we want here is call this generator or M. What's a good word for this? Like this is the. It's sort of the commit meta, right? Like this is the information that the generator produces that informs how to commit and what information to convey to wrap up. So maybe it's Maybe it's like commit sequence. Commit doesn't have to be a sequence Commit state.
00:15:08.525 - 00:16:08.745, Speaker A: Maybe really just is commit state. That seems like at least a less confusing name than CASSES cases. So we get this commit state. Descriptor is also pretty good. Staged is not bad actually. Commit descriptor isn't bad. Commit descriptor is good like that.
00:16:08.745 - 00:17:00.485, Speaker A: So for the cast I mean they call it cast list. What we've really done here is that the commit descriptor is a type that has to implement cast descriptors which is part of what makes it a list. But the commit descriptor itself we don't actually require to be a list. And, but, but this trait bound requires that it's indexable and can give its length, which gets us pretty far. So in that case it says iterate through the length of the list, right? Get out the ith element. Do we ever need. I guess we need I and we need each individual element.
00:17:00.485 - 00:18:32.245, Speaker A: So maybe all we need here is really iterator. Like maybe the length isn't important. Right? Like I'm thinking here we iterate over it and we do need to get like we, we only use the eye for this eye here. I wonder what this I here is even used for, right? Like why is that? I return where does it go? The failed index goes into failed CAS index and where is failed CAS index even used? In fact we don't even where's our run method? So that should be in help which does this. That's the outcome which we stick in post cas. And what do we use the outcome for? The outcome goes into wrap up. Does it go anywhere else? Seems like it only goes to wrap up.
00:18:32.245 - 00:19:12.763, Speaker A: Wrap up gets the output of execute casses. So what does wrap up even do? Post casses. All right, so let's look at post casses which is down here. I want to see what it does with the record with the failed index. It seems like it doesn't do anything with that failed CAS index which is kind of interesting. Like maybe it's not even important which one it is. I guess maybe.
00:19:12.763 - 00:19:51.895, Speaker A: Okay, so let's imagine that we have a lock free algorithm that does two casses to commit. Maybe the wrap up method needs to know which one failed in order to know like what kind of recovery it has to do. So it needs to know like the index. That might be the case, but it sort of feels like it shouldn't necessarily have to be an index. But I guess let's, let's stick with index for now. I think then what we actually want here is that we don't care that it's indexable. I think all we care about is that it implements.
00:19:51.895 - 00:21:37.675, Speaker A: I think all we care about is that it implements like into iterator. It's a little awkward, right? Because we want, we want a reference to the commit descriptor to implement iterator and I think you can technically do that. Like I think you can add a where bound here but I think what we will actually want to do here is this and then I think we can say here where commit descriptor. Where self commit descriptor implement were not even that, but where for any reference that implements into Iterator where the item is that's real ugly. But self cast. Right, like that's sort of what we want to say, but I kind of don't want to express it that way. I think actually what we're going to do is not even include it there and instead say that we're going to stick that on the impulse block down here where.
00:21:37.675 - 00:22:23.109, Speaker A: Oh yeah, the for syntax. So this is higher. Higher kind of lifetime bounds. So basically I'm saying for any lifetime tick a I want a reference with tick a of the. Of a commit descriptor to implement into iterator item of that same lifetime ticket reference to a CAS higher ranked trade bounds. Yeah, and yeah, I think that's good because I don't think we need the length. Right.
00:22:23.109 - 00:23:04.835, Speaker A: Like I think all we need here is for I and casual in descriptors.intoiter. enumerate right. And now this will be a CAS that we can execute. That looks nicer. Right. This also gives us a couple of things. Like it might eliminate a bounce check in the generated code.
00:23:04.835 - 00:23:37.343, Speaker A: It also means that if people use an option for example it work pretty nicely. I forget it's if tuples. No, probably not. It'd be nice about one tuple implemented into iterator of its inner type. I feel it's like a cool implementation to add. Yeah, I like this. That looks nice.
00:23:37.343 - 00:24:26.957, Speaker A: Because now we're no longer requiring the implementer to like implement the in the index trait, which is a little weird anyway. Like it feels like all it should require is iterator anyway, so I think this is fine. All right, so back to this. So we get the CAs and then we look at the state of the cast, right? So here's one thing that we're currently missing which is currently we just say that a. That the cast type has to be a cast descriptor and the only thing cast descriptor has to do is execute. But in reality what should actually be in a CAS is it also needs to have state. It needs to be able to do like clear bit.
00:24:26.957 - 00:25:18.307, Speaker A: So it looks like there's actually some more stuff that goes in the cast than just execute. So I wonder if cast is actually a type that we want to control. Like I think cap descriptor maybe should be a struct instead, maybe backed by a type that implements, I guess, cas. Right. So here we can say that CAS descriptor should contain, I guess a state which we don't know what type is going to have yet. Whatever this like bit stuff is. Cast state field, write state field, modified bit set.
00:25:18.307 - 00:26:16.945, Speaker A: Okay, so there's going to be some like bit set in here too, which we don't know what is yet. And it looks like there's like a. An enum of cast state which is going to be either success or failure or pending. Right. So the success, there's failure, there's pending, and that looks like all of them. Okay, so state is going to be a casual state and it looks like we have to be able to do a compare and swap on the state field. So what we're going to do here is actually a, I guess Repper U8 and then we're going to have this be a.
00:26:16.945 - 00:27:14.661, Speaker A: Oh, it's a little awkward. I was thinking this should be like an atomic U8. Tommy Q8 but that's A. It's a little awkward because we want to sort of say that this is really a cast state, but I think it's going to have to be an atomic U8 because otherwise we can't do like a compare and swap on it, for example. But this repper U8 means that we should be able to just trivially cast between a U8 and a cast 8 and the bit set. This is like clear bit and modified bit set. Oh, modified bit set.
00:27:14.661 - 00:27:36.673, Speaker A: I see. So we need to figure out what this like modified bit is. There are no atomic enums. No. I guess. Let's maybe actually read this text. The execute Caf CAS method receives as its input a list of cast descriptors to be executed.
00:27:36.673 - 00:28:14.043, Speaker A: Each cast descriptor is also associated with a state field which describes the execution state of this cast succeeded, failed, or still pending. Great. So we got those three. Controlled execution of these critical castes requires care to ensure that each cast is executed exactly. The success of the CAS gets published even if one of the threads stop responding. And an ABA problem is not created by letting several threads execute the sensitive CAs instead of the single thread that was supposed to execute it. In the original lock free algorithm, the ABA problem is introduced because a thread may be inactive for a while and then successfully execute a CAS that had been executed before.
00:28:14.043 - 00:29:04.627, Speaker A: If after its execution the target address was restored back to its hold value. I see. Okay, so what they're trying to solve for here is. So we talked about the ABA problem on the previous stream. But the idea is that the compare and swap is really an operation that's trying to like do a compare and swap of some other value in the system, right? Like it's trying to swap, let's say, something that currently has the value A to something that has the value B, right? And imagine that we have all these threads trying to help and one of those threads just like goes to sleep for a long time and eventually that operation succeeds. Like A does get changed to B, the operation succeeds and the whole rest of the program keeps executing. And then at some point for some, because of some other operation, that B gets turned back into an A again.
00:29:04.627 - 00:30:09.655, Speaker A: And now that that helping thread that was just asleep comes back and it sort of resumes trying to redo that old cast, those trying to change A to B. And normally that would have failed because the value had changed from beyond A, but since it's just looking at the value, it's going to now see, oh, the value is A, so I'm going to change it to B again, even though that's the wrong thing to do. And so what they're trying to get had is, let's see. Ideally we would have liked to. We would have liked to execute three instructions atomically, read the state, attempt the CAs if state is pending, and update the CAST state. Unfortunately, since these three instructions work on two different locations, the CAS's target address and the descriptor state field, we cannot run this atomically without using a heavy mutual exclusion machinery that foils weight freedom and is also costly. To solve this atomicity problem, we introduce both a versioning mechanism to the fields being cast and an additional bit named modification bit to each CAST field.
00:30:09.655 - 00:30:56.855, Speaker A: In a practical implementation, the modified bit is on the same memory word as the version number. The modified bit will signify that a successful CAST has been executed by a helping thread, but possibly not yet reported. So when a CAST is executed in the slow path, a successful execution will put the new value together with a modified bit set. As a result, further attempts to modify this field must fail, since the expected value of any CAST never has his bit set. I see. So the idea is that we're going to have a modified bit that is like in the value that gets cast that we're going to turn. That's always going to be off when you do the casts.
00:30:56.855 - 00:31:31.325, Speaker A: When a field has the modified bit set, it can only be modified and this prevents the aba because if the value is. Let's see. So the CAS is going to try to modify A to B. But it's only going to be allowed to do that if the modified bit is unset. There's something weird here. I think this is going to come back to. I think what we're missing here is the versioning, which they talked about a little bit earlier.
00:31:31.325 - 00:32:24.545, Speaker A: I think we're going to end up having to implement a way to do versioning and the modified bit here. The modified bit will signify that a successful CAST has been executed by helping thread, but possibly not yet reported. So when a CAST is executed in a slow path, a successful execution will put the new value together with a modified bit set. As a result, further attempt to modify this field must fail, since the expected value of any cast never has bit set. When a field has the modified bit set, it can only be modified by a special CAST primitive designed to clear the modified bit. This cas, which we refer to as clear bit, cas, is the only cast that is executed without incrementing the version number. Yeah, so there's actually a combination here of a version number and a modified bit.
00:32:24.545 - 00:33:10.471, Speaker A: Yeah, it only clears the modified bit and nothing more. Interesting. Okay, so what we really need here is we need to implement the versioning. That's a little awkward. So to recap the versioning, let me go back up here to find where they talk about that. Finally, we. So this is for the requirements for how to do the generation.
00:33:10.471 - 00:33:52.705, Speaker A: We require that the CAS is that the generator method outputs before fields that employ versioning. That is, a counter is associated with the field to avoid the ABA problem. The version number in the expected value field of a CAS that the generator method outputs cannot be greater than the version number currently stored in the target address. The version number in the expected value field of a CAS generator method outputs cannot be greater than the value already stored in the target address. Right. So every CAS has to end up incrementing the version number. This requirement guarantees that if the target address is modified after the generator method is complete, then the cast will fail.
00:33:52.705 - 00:34:32.525, Speaker A: Okay, so we're going to have to do a little bit of drawing here, I think, to explain what's going on. So let's see here. The basic idea is. So I think we drew a little bit about the ABA problem last time, but essentially this cable is in the way. Go away cable. Essentially what we have is, let's say there's some place in memory over here. This is foo.
00:34:32.525 - 00:35:22.725, Speaker A: Foo is some address in memory, currently has the value A. Right. And now imagine we have some thread that wants to do a cas. They want to cast foo From A to B, right? And let's say that we have multiple threads that are all trying to do this. So we have T1 is trying to do this and T2 is also trying to do this cas of foo from A to be. And now imagine that T, apparently my writing is really bad. Let's say that this goes to sleep, but this one succeeds.
00:35:22.725 - 00:36:42.625, Speaker A: So now this is no longer A, this is now B. And then some T3 comes along. This is what I described earlier, right? It tries to change foo from B to A, it ends up succeeding. So this now gets changed to A. This T2 then now resumes and now executes that operation and it succeeds. So this becomes B again, even though these two were sort of supposed to be a single operation and only be applied once, but we ended up executing like one two, one, when really we should have just done one, two, right? So because this, this operation here succeeded even though it was an earlier update that has already happened. And so the, the versioning that we're going to be introducing here is basically to say, or what the paper is asking us to do, is to instead say that any value that you want to be castable needs to not just be like.
00:36:42.625 - 00:37:16.475, Speaker A: You need to have versioning embedded in the value that gets cast. So if this is A, it actually needs to be like a zero. And then the way the casus will look, see if I can actually. Let's do this over here. So with versioning, it's going to look like foo is going to be a box over here and it's going to have the value A zero. And then let's walk through what happens with the same thing. So we have T1 and T2 both trying to help each other.
00:37:16.475 - 00:38:17.792, Speaker A: This is a casual of Foo from A0 to B1. And this is the same, right? It's the exact same operation. This goes to sleep, this completes. So this is now like foo is now going to be B1. And notice that this, this CAS incremented the version number, right? The second thing here is the version number. And now T3 comes along and it wants to do a CAS of foo from B1 to A2, right? And you'll notice the difference here, right, is that now that the new value here is no longer the same as the old value here because the version is being incremented again. So even if this now succeeds and then T2 runs, the value that's in here is a two.
00:38:17.792 - 00:39:47.765, Speaker A: And so the, if we go back here, T2 the CAS of foo from a 00, that writing was bad to be one will now fail because a zero, right? Remember, compare and swap is if this first argument is equal to the current value, then and only then update. Well, this value which is the current value is not equal to or this, or if you prefer math, this, they're not equal and therefore the compare and swap will fail. So versioning sort of solves that ABA problem. Of course, the challenge with ABA here or the challenge with versioning is that now that the value that you have to compare and swap is no longer like just a value. Let's take the example here of this type that we originally wanted to cast being a pointer, right? So we're using atomic pointer. Well, atomic pointer only holds a thing that's of pointer size, right? So where would the version number go? Where does this go? Right, because the A the value A fills the pointer value. So where does the version number go? We can't just make it a tuple because then we couldn't do a compare and swap on it in the first place.
00:39:47.765 - 00:40:57.765, Speaker A: So there are a couple of ways to deal with this. One is by doing bit fiddling. So one observation you can make is that a pointer is something like 0x on 64 bit, that is. And then it's usually like. I forget exactly what the. I think generally the high bits are ones and then it's like 2, 4B, a C 0, 1, 00 or whatever some address, right? But if you know that the high bits are always ones, then you can sort of like leech off some of the high bits. Or if you know that the low bits are always zero because for example, the value needs to be aligned, then you can use, you can like stick some secret bits in here, right? So we can take the pointer that we were supposed to use and just here like stick in like a version number of, let's say 1, 0 or something, right? Like, or I guess 02.
00:40:57.765 - 00:41:44.285, Speaker A: If we wanted to store a two here, where this is a, right? What I, what I wrote out here was the value of the a pointer, then a comma 2. So a versioned as 2 would be the same thing for all the lead digits, but then would be 02 at the end instead. And then we just need to remember to like zero out the low bits before we actually try to dereference in the value ever. This works. In fact, there are a decent number of libraries and tools and programs that do this kind of magic. There are a couple of downsides to it. So the first one is that you are relying on particular features of the address space.
00:41:44.285 - 00:43:27.525, Speaker A: So in particular you're, if you use the high bits, then you're assuming that the, the high bits are, can all be set to 0 or 1 to recover the original pointer, right? Like, imagine that you're running on like an ARM platform or something where the high bits are actually like ACBA 24 or whatever, right? Then if you tried to store your version number up here and then just flip them all back to once to all Fs in order to recover the original pointer, that wouldn't be the original pointer anymore because the original pointer started with this value instead. And so that wouldn't work. So if you use the high bits, it often ends up being architecture dependent. If you use the low bits, you can always say that we require your target addresses to be like 64 byte aligned or something to say, basically dictate how many, how many bits at the end are going to be zeros. But the problem is that you don't end up with too many bits to spare, right? Like, let's say we require that someone uses M. Let's say that we require that their values are 256 byte aligned. So anything that they point to has to start at a, an address that's aligned to 256 bytes, which basically means that the last eight bits of the address have to be zero.
00:43:27.525 - 00:44:08.827, Speaker A: Well, all we then have to store the version number is 8 bits, right? Which is just 256. Which means that you can't have more than 256 versions, right? Or you wrap around. So the moment like at some point you do. Like if we imagine continuing the sequence, right, eventually you get to like x comma255. And at this point you've set all the bits you can. So your only option if someone modifies to Y is going to be y comma 0. But now you're exposing yourself to the ABA problem again, because imagine, I mean, you're making it less likely.
00:44:08.827 - 00:44:48.379, Speaker A: But imagine that like T2. Like, let's imagine someone changed X to a right at zero because it wrapped around. So it's really 256, but it has to be zero because we only have so many bits then. Now T2 might end up succeeding in this operation, even though lots of operations have happened because we wrapped around. So you really kind of need the version number space to be large enough that you're not going to wrap. Otherwise you reintroduce the ABA problem. If you use the high bits, usually you have more bits to play with and so you're less likely to wrap around.
00:44:48.379 - 00:45:25.717, Speaker A: But even then you need a decent number of bits before you can actually reliably assume that you won't wrap around. Like with 64 bits you're just not going to wrap around. Like it's like the total lifetime of the universe or something. Like you're just not going to wrap around. But if you only have 10 bits, like that's not. Realistically, you're not going to get there. Okay, so then what do we do if we don't want to do this bit twiddling? So there are a couple of options.
00:45:25.717 - 00:46:00.799, Speaker A: For example, here's another option. If you have access to Atomic U128 and you're on a 64 bit platform, then voila, your problem is solved. You just have your atomic pointer be an atomic U128 where the left 64. It doesn't really matter whether it's left or right. This is the pointer and the right 64. This is the version. Tada.
00:46:00.799 - 00:47:04.343, Speaker A: Okay, great. Now let's go and see whether we can actually get atomic U128. There's no atomic U128 here, right? Like what's in here? If we look down. Yeah, it only goes to 64. Now I'm fairly sure that there is intrinsics. If we go to intrinsics Atomic exchange week, I'm pretty sure that this, unlike x86 64 on newer CPUs I think you can do. Why does this not.
00:47:04.343 - 00:47:52.215, Speaker A: I guess it's because an intrinsics. Let's go ahead and look at. Maybe C specs will tell us. Yeah, C reference exchange. That's not very helpful. Atomic exchange. It doesn't actually say which types.
00:47:52.215 - 00:48:42.695, Speaker A: Well, it doesn't really matter. We're not going to start using intrinsics anyway. But like clearly in the standard Rust library, we don't have access to an atomic 128bit value. I think some CPU support it. If you have that, that's great, you can just go with that. But for all of us simpletons who only have 64 bit processors that can do 64 bit operations atomically, what do we do? Well, there's a little trick we can play which is the following. So what we're going to do is we're still going to have foo and you're not going to like this, but it's sort of a forced reality.
00:48:42.695 - 00:49:51.285, Speaker A: Now instead of saying A, which is what we really want to say we're going to do is we're going to make foo be a pointer and it's going to be a pointer to a new object and that on the heap we're going to heap, allocate an object and we're going to say that it's going to have a value which is going to be A and it's going to have a version which is going to be zero. Okay? And then what we're going to do is the compare and swap is going to swap this pointer for a pointer to a newly allocated object. This is going to be a value of B and a version of one. That's a B right there. So the swap is going to be to this pointer, ew, the heap. Yeah, I know, right. And then it's going to be another heap object.
00:49:51.285 - 00:50:38.169, Speaker A: I'm just going to use V for both of these because at this point, you know what they mean A and 2. And now the reason this works is because let's do a compare and swap, right? To use this pointer instead, right? And now T2. So let's see, let's say this is 0x1, this is 0x2, 0x3. I mean, that's not actually what their pointers are going to be, but let's say it is. And remember our good old friend T2 who's doing the old CAS. Well, its CAS is going to be foo from 0x1 to 0x2. But when we're in this state, right, which is where it ends up after.
00:50:38.169 - 00:51:37.275, Speaker A: This was like T1 executing, this was T3 executing. If T2 comes along down here, it's trying to Compare and swap 0x1. But that's not the current value, right? These are not the same and therefore the compare and swap fails, which is what we wanted. It doesn't happen because of a version number in the compare and swap thing, but it happens because of a version number in the, in the heap allocation. So you might wonder, well, like, okay, John, why can't the ABA problem happen now? And you're right, it totally can. So the trick here is to make sure to basically make use of the way that we do garbage collection. So the idea here is that in order for T2 to be able to name this value, it must still have a reference to this value.
00:51:37.275 - 00:52:07.141, Speaker A: Therefore, this value cannot have been deallocated. It must still be allocated, otherwise T2 wouldn't have a reference to it. Okay? So when this is allocated, it cannot have the same address as this, because this one is still allocated. It hasn't been freed yet. Therefore the two addresses must be different. Therefore the compare and swap must fail. Therefore you're not subject to ABA.
00:52:07.141 - 00:53:07.345, Speaker A: Now, later on, when T2 no longer holds a reference to the original, like a zero, right, then a zero is deallocated and that address can be used again. But that's okay because there's no longer anyone who has that address and is trying to do a compare and swap, because if they did, we wouldn't deallocate a zero. So this works. The big downside, of course, is we have to do an extra allocation for any compare and swap value. And we have to do this like, I mean, we're going to have to deal with memory reclamation at some point and that also now has to sort of feed down into the compare and swap operations. But that's sort of where we get to what's the point of having a version? So that's another good question. Do we even need the versioning at this point? Or is the versioning sort of embedded in the fact that we're doing allocations? You know, it's a good question.
00:53:07.345 - 00:53:55.509, Speaker A: We might actually not need the version. Why would we not need the version? I think we still need the version to do. Because we can do a sort of. We can do a proactive check for whether the CAS is going to fail if we store the version. But you're right, we probably don't need the version, but it's also not very costly for us to keep the version at the point. Does that immediately ref counting? No, not necessarily. We could reference count these, but that wasn't my plan actually.
00:53:55.509 - 00:55:01.225, Speaker A: Maybe these are a good candidate for reference counting. Is that true? No. This is going to be using the same memory reclamation scheme that we end up using for the rest of the data structure, which is probably going to be hazard pointers, which I haven't talked too much about. We'll talk about those in a separate stream where we implement hazard pointers. But the reason you don't want to use reference counting here, and I talked a little bit about this in the previous video, is that with reference counting you still have a race condition between when you read the value, when you read the pointer to the reference counted value out of the atomic, and when you try to increment the reference count for the copy you just got, there's a race condition there and hazard pointers deal with that race condition. Can you use an arena allocator? Yeah, I mean, you can use whatever allocator you want here and it should work just fine. I don't know if you really need an arena allocator here, but it might be reasonable to associate an arena allocator with the whole data structure, especially because all of these allocations are fixed size.
00:55:01.225 - 00:55:49.333, Speaker A: Virtually all modern X8664 processors support 16 byte compare and exchange. Yeah, that's what I thought too. Let's add it to the Rust standard library. This is all general to the simulation, but specifically the thing running under it. So you could also have a data structure which uses the pointer manipulated version or whatever. Yeah, I'm imagining that we could, we could totally make this generic so that if someone wanted to do the bit fiddling, they could. At the same time, I kind of don't want to do that because people are going to get it wrong.
00:55:49.333 - 00:56:28.569, Speaker A: They're not going to keep into account the fact that you will still potentially encounter the ABA problem. I would rather just give a correct solution and then have that be a little bit less efficient for now. And then what we can do is always do the like swap out the implementation behind the scenes when we get something like 16 byte or 128 bit compare and swap. And one additional benefit actually of doing the scheme this way is let's say that we're willing to bite the bullet and just do it this way. Well, now we have a great place to stuff other stuff. Like for example, this modified bit. Previously we'd had to figure out like how do we stuff that into the pointer and stuff.
00:56:28.569 - 00:57:12.181, Speaker A: Well, now we can just have a bool right here in the struct. This is a struct, we're just heap allocating it. So now we can add all this like additional information that we want just straight in that cast descriptor, which is pretty nice. Sweet. Okay, so now that we have an idea of what the scheme is going to look like, let's go back to the code. So if we think about this now for a second, the commit descriptor is going to be a list of casses and. Interesting, interesting, interesting.
00:57:12.181 - 00:58:15.783, Speaker A: So rather than have the state be an atomic U8, maybe we do still want that. But I'm thinking we basically end up doing like RCU here again of like we're gonna have a. Okay, we're still gonna have a cast descriptor. It's like a weird problem here where we sort of want. We want the user's data structure to wrap a data structure of R type. Right? So we're going to require that the thing that the user does a CAS against the value that they do a CAS against has to embed the version and stuff. Right.
00:58:15.783 - 00:59:03.975, Speaker A: So if we go back to this for a bit, we require that the casses of the generator method outputs before fields that employ versioning. Right. So this, the CAST descriptor, trying to figure out what the right way to model this is. So each descriptor they produce is going to be. I guess this, this sort of has to be a trait. It's gonna be a little bit awkward. I think you'll see why in a second.
00:59:03.975 - 00:59:31.365, Speaker A: But basically it's. That's gonna be a type that they provide. Sort of don't want it to be I. Because. Okay, let me. Let me try to articulate my thoughts here. What I want is for the user to not have to think about this.
00:59:31.365 - 01:00:47.045, Speaker A: I want them to be able to just think in terms of their values, right? Like the values that their data structure is going to use, rather than have to think about this like versioning and additional bits and stuff. Right. So I sort of want their commit descriptors to be of a type that I control that does this like heap wrapping and stuff, and that they can just express it in terms of what value they end up replacing out. So I think what I'm going to do is say that they have to provide something that implements cast descriptor. And our cast descriptor is really just going to be an atomic pointer to also a type that we control, which is going to be a cast descript. I guess this is going to be a cast descriptor box to follow the same nomenclature, the same terminology that we used for the operational records earlier. Right.
01:00:47.045 - 01:01:28.985, Speaker A: So this is going to hold an atomic pointer to a cast descriptor of T and a cast descriptor of T. Sure, let's have it still be. CAS is going to contain the state. It's going to contain the version and I think we want the version to specifically be a U64. We don't want to be U32 even on a 32 bit platform because wrapping around would still be really bad. And then we're probably going to end up with this like modified bit, right? And my guess is that these may have to be atomic as well. And I'll get into why that is in a second.
01:01:28.985 - 01:02:35.565, Speaker A: And then what we'll say is we're also going to store. I'm like, I'm trying to think ahead here, right? So what did they have to be able to do with the cast? I think their casts. It's almost like the cast just has to tell us what the new value is. Like they're not actually going to do the cast. We're going to do the cast by swapping out this like box instead Comment in the paper. It should all fit in a word. I don't think it said that, did it? Where is.
01:02:35.565 - 01:03:10.849, Speaker A: I'm just trying to find the. To the physical cast named modification bit to each cast field. It's on the same memory. Oh, same memory word as the version number. I don't think that's important. That's more of an optimization that they did. The reason I want the box type is because I don't want to expose the fact that we're using an atomic pointer.
01:03:10.849 - 01:03:42.655, Speaker A: That's not important to the consumer. But I'm trying to think ahead here of like the. The implementing algorithm might also want additional. Like you could totally imagine that they also have some additional bookkeeping they want to like stick in this descriptor. So I almost sort of want to say that like there's a. Like a meta which is t meta so that they have a way to stick additional information in here. Right.
01:03:42.655 - 01:04:34.007, Speaker A: But then there also has to be. What is the actual CAS going to be like? I think what's actually going to happen is we're going to even. Okay, so imagine that the user actually just wants to do a compare and swap of like a boolean. There's like. There's something. Something's fuzzing in my head here of like something is not quite right. Let me look at the actual implementation they give.
01:04:34.007 - 01:05:37.465, Speaker A: So they have to be able to get the state separately, clear the bit separately. That's all fine. And I sort of want to see what the execute CAS is like. The execute CAS is the thing that has to take into account the versioning. So it's almost like the ExecuTech that they do has to be over this type itself. It would actually help to see how a given data structure uses this actually. Let's go and look at the.
01:05:37.465 - 01:06:37.035, Speaker A: Let's go and look at what the real implementation does. So yeah, so as I mentioned, I managed to reach out to the original author infrastructure to get the original source code. And let's see what we get here. Is there a diff between. This is one thing you'll see with like research code bases is that there's a lot of just copies of files for various iterations of the same thing. Okay, so they have for a CAST descriptor they just have it be a trait that's interesting that just does execute CAS modify. Okay, so now I want to see an implementation of this trait.
01:06:37.035 - 01:07:44.505, Speaker A: I cast desk. That's fine. Oh, that's probably in like one of the implementations. So let's go Ahead and look at I guess skip list or folk doesn't really matter. So FSCAST desk Holder dot oh, this is. This is a linked list and the compare and swap successor compare and set. I see what's going on.
01:07:44.505 - 01:08:22.565, Speaker A: I see what's going on. Okay, let me try to find the right way to articulate this. So holder here is reference to like the current node in a linked list or something. You see FR node here is like the actual type for any given node. And you see that basically all of these methods end up forwarding into the sucks or the next pointer, if you will of some. Whatever this node type is. And you see it has all of these implementations.
01:08:22.565 - 01:09:12.854, Speaker A: And you see compare and set here on that takes like old version, old ref, new ref, old mark, new mark, old flag, new flag. So a bunch of like arguments to make to differentiate which version essentially we're talking about, which makes me think that it's actually. We're actually requiring the data structure is going to use our provided type, the one that does the heap in direction anywhere where it normally would use just a pointer type so that it gets to embed this versioning. So my guess is that if we go look at FR node. FR node. Yeah. So you see the successor, the next thing has a versioned tripled mark reference.
01:09:12.854 - 01:09:59.725, Speaker A: Aha. And my guess is that's the heap indirection thing. So version triple mark reference is a atomic reference to a reference quintet and the reference quintet has the actual reference, a mark bit, a flag bit, a help bit and a version. Yes. You see, this is the thing that gets stored on the heap. They just allocate a new one. And what does the compare and swap look like for this Compare and set is get the current value, check that all are the same and then compare and set the oh, current.
01:09:59.725 - 01:10:18.725, Speaker A: I see. And then, then. So it's basically doing rcu. It's doing RCU on. On the type itself. Okay, so the. We're not actually.
01:10:18.725 - 01:10:48.381, Speaker A: When you want to do a compare and swap of one of these, you're not actually doing a compare and swap. You're. Or I mean you kind of are, but you're really doing a RCU of like you allocate a new of these heap objects and then you do a compare and swap between them, which is exactly what we explained in the diagram. But this helps. Okay, so the question is how do we translate this into rust. Sorry. Actually, let me stop there and see.
01:10:48.381 - 01:11:37.395, Speaker A: Like we've been talking a lot around this topic. Let's do some Q and A about this to see that we all have like a shared understanding of roughly what's going on and if we don't, which is pretty reasonable, try to talk through what that is. Let's see. Surely more efficient to leak the versioning to the consumer. So it shouldn't be. Because in general, like, I mean it could be if they like happen to know that they will never go through more than say 256 iterations before they hit ABA, but in general they just won't know and will get it wrong. It seems better to just like provide a safe implementation.
01:11:37.395 - 01:12:27.565, Speaker A: Yeah. This source code, I'm working with the author to find a good way to like publish it beyond just me showing it on stream. Part of it is like it has a bunch of other code that's not quite related, so it needs to be tidied up a little before it can be posted anywhere. My plan is basically to post this alongside the code when I, when I put that in like a GitHub repo. All right, but let's sort of look at whether, whether the architecture we're sort of planning here sort of makes sense. Does the heap allocation make sense? That makes sense. Why we need to have this extra indirection and the sort of latest observation that really what we're going to do is have.
01:12:27.565 - 01:13:31.617, Speaker A: We're going to implement basically compare and swap for the user. Like this isn't going to be a tcas, it's just going to be the value that they want to compare and swap. And then we're going to do RCU on this atomic pointer by allocating new instances of this. Why can't Wrap up prevent future repeats of the cas? Can you say more about what you mean? Wrap up is called concurrently from multiple different threads and will be called regardless of whether a given cast succeeded or failed. It just always called at the end. Part of the challenge here, right, is that if the cast fails, we may have to do a bunch of cleanup, which is what Wrap up is for. And if you have multiple compare and swaps, like I think I don't know whether any of the data structures in the paper require this, but the reason they constructed it this way is because there are lock free data structures that have multiple commit points.
01:13:31.617 - 01:14:13.555, Speaker A: And so you have to deal with the fact that like the first CAs succeeded but the second CAs failed. And then when people help, you need to deal with that situation. And part of that might be in Wrap up you go, we're now in a weird state. We need to redo generation as well in order to make Progress. Part of the reason I'm stockbringing for Q and A is because I don't believe that no one has questions. So, like, if you have a question, you should ask it, because I'm pretty sure other people will have similar questions, even if it's just I don't know what's going on. Like, explain it again and I will try.
01:14:13.555 - 01:14:57.495, Speaker A: The basic thought here is that I don't think. In fact, I think we may not even need this. The basic idea here is that we're not going to do compare and swaps directly on a user's type. Instead we're going to require that the user is using our atomic type, basically. So. So really this should be called like atomic. And we're going to require that the user is using our implementation of atomic.
01:14:57.495 - 01:15:58.675, Speaker A: I guess we could make this a trait, right? We could say version datomic, right? And we're going to require. And this is sort of the way the Java code was going, right? We're going to require that they implement like clear bit and what were the other ones like? Modified bit and version and execute. And then we could implement version datomic for atomic T. Right, we could do that. And then we just require that the user uses some type that implements versioned atomic and then they could choose to implement their own scheme. But I'm sort of like, is that better? I'm not entirely sure. It sort of feels like, when would you ever use a different implementation? I feel like you're likely to use one that's just wrong.
01:15:58.675 - 01:17:09.937, Speaker A: I mean, maybe it'd be nice for testing. It's a good question. Maybe we should go this way if just because it's sort of easier to present this way. Can you post the rust so far to the playground? Yes, I can share. What if we take ownership of the user data and store it as heap allocated? That's basically what we're doing here, right, is that we're telling the user like, if you want some atomic reference type, you have to use our atomic reference type so that we can add versioning and this modified bit and all the other bits, like stuff that we need. And at that point, because we're controlling the wrapping structure, you can use any T. Like, one benefit here is that you don't need to use one of the standard atomic types.
01:17:09.937 - 01:18:02.247, Speaker A: Like it doesn't have to be like one of the things that the CPU can do atomic operations on, because we're basically going to do RCU for you. So it can be any T here. One thing that's a little Awkward is we're currently requiring that they use the same T, which is kind of silly. That might be one reason why we want versioned atomic here is because I think maybe we want. Maybe be nice if the commit descriptors were actually like DIN versioned atomic so that a data searcher could admit. Could emit like. So let's say that you want to.
01:18:02.247 - 01:18:45.247, Speaker A: Do you have like an atomic. I don't know, root pointer in a B tree but you also have an atomic like you want to. Your commit operations are do a cast on the root pointer and do a cast on a value. Right. Those aren't the same type, but we still want you to be able to express that those are your two commit points. And so I do think actually we do need this trait because we need this to be a DIN atomic version datomic so that you can have different values for it. I think that's actually going to be necessary.
01:18:45.247 - 01:19:25.585, Speaker A: And at that point this does need to be a trait. And then if we. I guess at this point we need to look at infrastructure icast desk and then we basically just need these operations. Oops. Which is going to be fn executecast bool. This is going to be execute. Let's take a ref self modified bit set.
01:19:25.585 - 01:20:27.625, Speaker A: Wait, this. Why does clear bit returnable? So this is going to take a. Itself. This has to return. I guess a CAS state and set state also has to be able to run atomically I think. Right. And then we can provide sort of a standard implementation in the form of our I guess cast descriptors.
01:20:27.625 - 01:21:14.093, Speaker A: Maybe a bad name here. This is really a cast by rcu. Maybe a better name and it doesn't have to be public. And then we can implement versioned atomic for our atomic type where execute is going to be. I guess here we can actually look at what this one did. Right. So actually yeah, so they.
01:21:14.093 - 01:21:49.711, Speaker A: But they. There's like more information around this which is what makes me hesitant that maybe we actually want to. Maybe our wrapper doesn't even make sense in the first place. Let's see. Yeah, you see here like execute CAS is executing it specifically for a particular like. Like it. This holds context as well.
01:21:49.711 - 01:22:36.323, Speaker A: Right. So maybe we. Maybe what we need to do is we're going to expect that the user uses our atomic type and that they sort of. Oh, I see what's going on. The cast descriptor they give us is something that wraps our atomic with additional information that that atomic might need in order to execute. So our atomic is going to implement. I guess it needs to implement all of these operations.
01:22:36.323 - 01:23:41.325, Speaker A: Is that even true? Yeah, it has to implement. Really what we're implementing is this like triple marked reference and it, it has methods like compare and swap. So we're going to say compare and I guess set is what they're using here. And it takes expected, which I'm guessing is a reference to a T and a new, which is going to be a T. It's like something weird here. I don't know what this mark is. I'm getting that guessing that's something we're going to get to later.
01:23:41.325 - 01:24:40.265, Speaker A: Where does it actually check the version is what I want to know. So it checks all this like associated metadata checks with the reference is equal to the current one. But where does it actually. All right, all it has to do is increment the version. I see. So really without any additional information, this is just if. So this is self zero load and this is where we're going to get into this business of the unsafe load again.
01:24:40.265 - 01:25:53.615, Speaker A: And then if this dot, I guess this should maybe be value or referent or something. If this old value equals expected, then like self0 dot store, then we're going to replace the whole thing. This is the RCU part by RCU with some updated stuff down here. My being silly ordering sec cst then return true, otherwise return false. And I guess I'm being kind of silly because we just replaced all these methods with ones with different names. Oh, I see. No, that's what I did.
01:25:53.615 - 01:26:30.933, Speaker A: Right. So I actually think atomic shouldn't implement this trait. Atomic should provide methods that the user then uses to implement versioned Atomic for their CAST descriptors. So I'm talking a little, a little bit around this because I'm like grappling with it in my head too. But I think what we want here is the user is going to implement versioned Atomic for. Or maybe version CAS is a better name for this. They're going to implement versioned CAS for every CAS descriptor they have.
01:26:30.933 - 01:27:26.025, Speaker A: So remember, a CAS descriptor isn't just an atomic. It's like a descriptor of what CAs should you do, which includes the current value, the expected value, the expected value, the new value, and any associated metadata that it might need to actually execute the cas. And ultimately we require that they implement versioned cas. That is, they have to provide these methods. And the way they're going to do that is their cast descriptor will probably internally contain one of our provided atomic types and just call into its methods, which is what we Provide down here. But atomic itself doesn't implement like cast descriptor, right? It just implements like a versioned cas. So like really maybe this is like versioned cast descriptor, but really it's well versioned.
01:27:26.025 - 01:28:31.495, Speaker A: It's like a versioned cas. It's like a pre filled versioned CAS or a. No, I think version CAS is right. This is a ready to execute compare and swap that is itself versioned. And what it's probably going to use internally is one of our atomic types which provides you with the methods that you will need in order to implement versioned cas. I think that's the way this sort of thing is going to go around. Okay, so in our case, what do we want this to provide? I guess we can continue to take inspiration here from the version triple mark reference from Java because presumably this is all that you really need.
01:28:31.495 - 01:29:34.165, Speaker A: So you can obviously make a new one that sounds like a pub of N. New certainly seems like something that we need and a new. What does new take? In this case it takes an initial ref which is a T. We don't know what mark flag and help is. My guess is like this, and this is presumably why it's called a versioned triple mark reference is because it has three of these additional meta information. So really it sounds like what we want here is our atomic should hold like T and M maybe where M is like metal. That is not a part of the value but rather is something that's considered part of the current state and something that should be considered for whether the compare and swap should should happen.
01:29:34.165 - 01:30:40.705, Speaker A: So I think that's actually what we're going for here. This is a meta which is M and you can think of like these are conditions for doing the swap, which means that state probably shouldn't be in there actually is my guess. I don't think this talks about state at all and it probably doesn't talk about the modified bit at all either. It just has version and meta and then this is the value that will actually be cast. I think that's what we want. And then the user's cast descriptor is going to implement the modified bit and the and the state. But that's not something that we have to provide in our atomic type.
01:30:40.705 - 01:31:49.621, Speaker A: And we probably here then are going to do something like acquire that M implements partial E and probably also e. And then what we'll do is. Right, so you see it has these like is marked flag and is help which are really just accessors for these like three Booleans but really what we're saying is you can say, you can include whatever meta information you want. So maybe we have a method that's like the equivalent for us is meta, which you takes in itself and gives you an M back and that just self dot meta. And for compare and set, it's like if this dot value is equals expected and this dot meta equals. So this is going to also take a meta, not meet. Actually, yes.
01:31:49.621 - 01:32:35.961, Speaker A: You see, the compare and set takes in both the old meta and the new meta, so takes in the expected. The expected sort of value and the expected meta. And it says. And it takes in a new and new meta and it checks that the current value matches the expected value and the current meta matches the expected meta. And if that is the case, then it stores the updated value, right? So it checks that the bits are all the same, it's just that the current value is still the same. I'm not quite sure why they're separated because it can just be a tuple. Like why does it need to be separated? Not entirely clear yet.
01:32:35.961 - 01:33:33.573, Speaker A: It might be that we just make this T and that's good enough and. Oh, I see. And then it's like if the new values are equal to the old values, then we don't need to do the compare and set. So this is like if this dot if I guess expected equals new and expected meta equals new meta. Or to invert, if that's not the case, then we do actually have to do the store and that's going to be not a store, but a compare and set. So we're going to have. We don't have to actually read out the current value so that we know what to like.
01:33:33.573 - 01:34:13.655, Speaker A: There's like a race condition here, right in the way that I wrote this so far. If we had this be a store, right? We read out the current. The current state of the cas, right? The current state of the thing in the heap, the meta descriptor, and then we check it, and then we just store the updated one. But while we're checking, someone else could go in and swap it out behind us. So we need to do. Make this be a compare and swap instead. And we need to make sure that we only do the store if this is still the same, if the thing in the heap is still the same one.
01:34:13.655 - 01:35:26.715, Speaker A: So we're going to do that by doing this. So this is going to be, I guess, this reference or pointer rather. Then we're going to do compare and swap this pointer with the updated one. And this is like the RCU bit And the version is going to be, I guess this dot version plus one meta is going to be new meta and value is going to be new. Who. Why can't user directly just use our atomic and not necessarily have to implement version cas? So the reason we want the user to implement version cast, why they can't use atomic directly is because there's additional information in a CAST descriptor, right? Like a CAST descriptor also includes the address of the thing to do the CAs on what the expected value should be, what the new value should be. It's a descriptor of a CAS that you haven't run yet.
01:35:26.715 - 01:36:12.581, Speaker A: So an atomic is really just a castable thing, right? Like in some sense, like atomic implements casable. But what we want is something that implements versioned cast, like a thing that describes the cast that we haven't done yet. So the user's type is a CAST descriptor that's going to ultimately invoke a CAS on an atomic. So this is like a. It's like a version, a prepared version CAS or something, Right? But it's not actually the result of a cas. It's not. It's not like Atomic here isn't a cas, it's a CASA bowl type.
01:36:12.581 - 01:37:08.195, Speaker A: And that's why we need that intermediate layer, which is the actual descriptor. Does that make sense? Like why, why we need the distinction, and this is going to be clear with some documentation. Right, but. But I think once we actually write an implementation, you'll also see why this distinction makes sense. Which is basically all the stuff that needs to go in a descriptor has to go somewhere, and it can't go in atomic itself. Instead of relying on the user to invoke methods of Atomic, would it be better to make them implement a function that returns an atomic? No, because if this trait had a function that returned an atomic, then we wouldn't know what arguments to invoke that atomic with. That's the problem.
01:37:08.195 - 01:37:34.075, Speaker A: Right. Again, an atomic is just something that is casable. And then the descriptor contains basically all the arguments. And because the argument is going to necessarily depend on whatever the data structure wanted to do, we don't have them. Like we, as in the executing library, don't have them. They're part of the descriptor, which is part of the user's type. So I think ultimately they have to call into the atomic.
01:37:34.075 - 01:38:14.207, Speaker A: And this also means that if they have some other way for them to represent the same things, then that's great, all good for them. Okay. So new takes an initial T and I guess an initial meta. I still feel like meta and T can probably just be combined. That looks like all it takes. And they presumably start the version out at zero. Yeah, it starts out at zero, so that makes a lot of sense.
01:38:14.207 - 01:39:00.945, Speaker A: So this is going to just say self and it's going to have I guess a, a version which is going to be zero, a meta which is going to be meta and a value that's going to be initial. What do you mean? No such field. Right. Self Atomic pointer. New box into raw box. New CAS by RCU of this. Foolish me thinking that that would work.
01:39:00.945 - 01:39:51.595, Speaker A: Yeah, this is probably needs box new. Great. So creating a new atomic is pretty straightforward. You give the initial value, which is sort of the thing that you will be doing compare and swaps on. So this might be a pointer type, it might be a number, like it doesn't really matter. And then meta is the other information you need to compare. Okay, and what do we need to implement for this? We need to implement clone, which doesn't clone version.
01:39:51.595 - 01:41:21.675, Speaker A: Okay, we'll figure out whether clone is necessary later. There's a get reference which I think we don't want to implement for now. Oh, I see. So the tricky part here, right, is that I don't want to use the word reference because it implies that the only type you can use this with are references, which isn't true. So it's like value, right, which takes self and returns T. And the challenge here is that this is really sort of what this used to be, right? Like this is a, an unsafe dereference of this dot value. And so this is definitely another place where we're going to need that whatever that guarding for memory reclamation ends up being, that has to be a part of this too, because otherwise if you had an atomic, like what happens if the target of the atomic has been deallocated? Maybe the atomic actually just owns this value.
01:41:21.675 - 01:42:28.185, Speaker A: That might be the answer here. No, the specifically the problem here is imagine someone calls value at the same time as some other thread calls compare and set, right? Then the compare and set might remove the thing that the value is referencing. And so if that gets dropped, then this reference would be invalid. So this is where, why we need this like concurrent memory reclamation scheme, which we don't currently have, and why all of these unsafes are currently just like the. All of the unsafes here are currently safe because we don't deallocate ever. But the moment we start deallocating, these unsafes would no longer be fine. So like, yeah, we could, we could totally say like, safety, this is safe because we never deallocate, right? Which like, is not a good safety guarantee.
01:42:28.185 - 01:43:18.475, Speaker A: Like that's not where we want to go. We want to do better than that. But we don't currently have the mechanism to do so. And like the same here. And I guess if we wanted to, and we probably should do this, we should have like unsafe FN get, which returns you a reference. Actually it can be safe and it returns you a reference to the ARC CAS by rcutm. But at that point it's like unclear whether it really buys you that much.
01:43:18.475 - 01:44:33.695, Speaker A: But it does mean that this can now be self get. Right, so the compare and set is really just going to compare the old value and then do a compare and swap of the pointer. Right? So this is implementing the. The scheme we drew earlier of the heap allocation being sort of the differentiating factor here. What else does it provide? So these are all just to get the value and the meta, there's a get that gives you the V, gives you the V and the mark in an array get with all bits. See, this is just weird. I feel like realistically what we would do here is like maybe we require that M is copyrighted.
01:44:33.695 - 01:45:18.685, Speaker A: That might be a good way to do this. But like, I don't know why this meta is just. Isn't just a tuple is help inversion. I see. So there's some like. I see. So really what's going on here is like if the user has an atomic, one thing they might want to do is like look at all of the values in the heap box at the same point in time.
01:45:18.685 - 01:46:06.795, Speaker A: And so what we're going to do here. Ah, here's what we're going to do. We're going to go the more of a rusty way and say with current or maybe just width, it takes an F, takes a self and a F. We can be helpful and say it also takes an R. It returns R where F is an fn once from a T and an M. I really feel like this should just be T. Like fairly tempted to just make this T, but that's fine.
01:46:06.795 - 01:47:15.575, Speaker A: And what this is going to do is it's going to do this pointer itself get. And then we're going to do this bit and then it's going to invoke F with this dot value and this dot meta. Right? The important bit is that the like, the reason why it has all of these like get with all bits get is help. Like the reason it has all of these is because it needs to make sure that if it looks at like the V, it gets back is at the same instant as the mark it gets back. This is really the Java way of saying return two values, right? Really this is like v comma boolean. But you can't write that in Java, presumably, right? So this is a way to get both of them at the same time. And the way we're going to do that is to say you just give us a closure and we give you the references at the same time.
01:47:15.575 - 01:48:05.885, Speaker A: Like this could alternatively just return a. A reference to a T and a reference to the M is sort of equivalent. Doing it with a closure seems kind of nice. Like maybe this should just be a. Like the alternative to width, right? Is let me do get to for now, right? And it just returns the T, the M and the version at some. Some single point in time. Maybe that's just nicer.
01:48:05.885 - 01:49:11.915, Speaker A: The reason the width is nice is because it makes it a little bit easier for us in the future to do like guarding of the value, right? Like once we want to do memory deallocation, once you hand out references, you need to deal with keeping track of those references so that you don't deallocate anything while the references are alive. If you do with a closure, it's a lot easier because the moment the closure returns, you know that they're no longer holding onto the reference. At least if you require that like R is static or something, right? So I think we're going to stick with the closure way and that way I think these accessors aren't even going to be relevant anymore. Get with all bits is is help inversion. Can so is help inversion. For example, here, right? Is get and then check whether version is equal to the value and then whether the help bit is set. And you could implement that using our rust with, right? By doing something like.
01:49:11.915 - 01:49:58.175, Speaker A: I just want to demonstrate the rust version of this self.with and I guess actually this should also be given the version is important. You would write it as I don't care what the value is. I care that the. Let's say that I've defined that by meta as these three booleans, right? So this. What were they called? Like mark flag help. So let's say that's a struct that holds three bools, right? So MFH for short and version this was like is.
01:49:58.175 - 01:51:01.205, Speaker A: What's it called Is help inversion and it takes a V or version I guess, which is us and this is V and then this is just like V equals version and mfh.help. right. That is the way you would write the equivalent of this method using just the width. And it has the same guarantees about both the both of these reads accessing the same instant in time by virtue of this just doing one read of the underlying atomic pointer. Great. So because we have with. I don't think we need all of these like helper methods.
01:51:01.205 - 01:51:47.841, Speaker A: The helper methods are lame and we don't want them. Do we want to have meta behind a trait? How will users know that they're required to have version help and mark? We're not going to require. That is the thing like whether the user needs like help and mark and flag and stuff I think is going to depend on the data structure. It's not going to depend on like we're not going to depend on them in our simulator, in our executor. This is why there's like version triple mark reference is a thing that existed inside of a particular data structure implementation, not in the implementation of the simulator itself. Because for example, like this might be needed for the linked list, but it might not be needed for the B tree. The B tree might need.
01:51:47.841 - 01:52:25.685, Speaker A: Who knows, like other. In fact, we could look at this. Right? So Fomachev here, I think is a linked list and it includes this version triple mark reference that has mark, flag and help. If we go into bst, I guess what does it do for its BST CAS desk? Okay. It also uses a version triple mark reference. What about the skip list? Where do we have. Ooh, who knows.
01:52:25.685 - 01:53:10.113, Speaker A: Skip combined two implements iList and C list. That doesn't seem right. Aha. Skip CAS desk. It uses a version double mark reference. Where is version version double. So in source there's a version double mark reference and that has only mark and help.
01:53:10.113 - 01:53:31.859, Speaker A: So the skip list doesn't need the flag, but it needs mark and help. Right. So I think this is going to come down to whatever additional meta information any given data structure might need. If you. If we look through this right, My guess is that this looks exactly like the other one. Except the helper methods are a little different. Yeah, like it has get with both bits instead of get with all bits is help.
01:53:31.859 - 01:53:59.535, Speaker A: Inversion is still there. It has weak compare and set and compare and set. That's interesting. Yeah, set. But ultimately the methods are sort of the same. Why is there different compare and set? There are multiple compare and sets expect so that one takes all the arguments, that one also takes a version. Interesting.
01:53:59.535 - 01:54:39.187, Speaker A: We'll have to figure out which ones of these are actually Needed but I think like. Oh yeah, so this one has the same. So it has compare and set. It has just a straight up set and it has a compare and set that takes a version as well get state. So this can be implemented with our. With get with both bits can also be implemented with our. With our.
01:54:39.187 - 01:55:18.675, Speaker A: With. Okay, so I think what we want to provide I guess is probably also. I don't think we need version because that can also be expressed with just with. So there's sort of a compare and set and then it looks like they also want to compare and set with version and just a straight up set method. So I guess we can implement set. So set doesn't take expected. It just I guess sets the value if new reference.
01:55:18.675 - 01:55:58.175, Speaker A: I see, so. So the set really just does. 15 just does. If. If this dot value not equals new and this dot meta not equals new meta, then store or I guess it still has to be a compare and set actually. Oh, it is not. It just does a store.
01:55:58.175 - 01:56:33.981, Speaker A: That's interesting. So set really is a just set but all of them do increment the version, which I guess is the important part. And then this one that increments the version as well, I guess should be. Well, it's a little silly. Like in some sense it should be like I guess version can be an option. Bool. Right.
01:56:33.981 - 01:57:20.535, Speaker A: This is where function overloading in Java is a little nice where we want to say here this just also includes the current version as this thing to compare. So if let sum V equals version and if V is not equal to this dot version then we return false. Right. So this is like an additional guard. If version is specified. Why isn't version part of meta? Version isn't part of meta. Because we need to be able to directly manipulate it and touch it.
01:57:20.535 - 01:58:17.615, Speaker A: The stuff that's inside of meta is stuff that we don't care about. Like as in the atomic. The implementation of atomic doesn't need to access any of this stuff inside of meta. It just cares about meta as a whole. But looking at this like, I still don't think there's a reason to have meta be separate. You know, I think that it can just be a T and then we just ask the user to have a more structured value in there. I'm just gonna get rid of the meta man and then we can.
01:58:17.615 - 01:59:27.085, Speaker A: It's certainly then very clear what cast by RCU does. Right, so this no longer has M, but we are going to require that T implements partial eek and eek because otherwise we can't Check whether the expected value is the same. This is going to return just this. This is just going to be given a reference which I feel like is just going to be nicer. This now just takes a like a value which seems nicer. This just stores the new value. This takes expected and value and just does that.
01:59:27.085 - 01:59:57.739, Speaker A: If expected not equal value, no need to check the meta. This is just nice. That seems much nicer. Oh, did I also remove T? That was silly. I think that's going to be. I think that's going to be good enough. And then I think the.
01:59:57.739 - 02:01:21.695, Speaker A: The implementation version casual like now in fact we can even provide our own like, what was it called? Like triple versioned tripled mark reference. So a versioned triple mark reference just to like see that we can do it using what we have so far, right? It's going to have a reference which is going to be a T. It's going to have a mark which is going to be a bool. It's going to have a flag which is going to be a bool and help which is going to be a bool and then it's going to have the reference actually is going to be an atomic of T. Oh, actually it's not even going to be that. It's going to be a triplet triple marked. This is actually just going to be an atomic of it's just going to hold an atomic triple mark T which is going to hold these things right? And then implement.
02:01:21.695 - 02:02:31.037, Speaker A: If we now wanted to implement this just to sort of have the analog to the Java code. The question is can we write all of the methods that are in the Java code? And I'm pretty sure we can, so. Oops. So this is just pub offend new which takes a initial which is a t mark. I guess it's obvious that they're all initial because this is a constructor. So it takes a flag which is a bool and it takes a help which is a bool and it returns a self and that's really just going to be a self atomic new value and then this is going to be a triple marked. So this is where I mean that like there's no reason to differentiate between the meta and the type because we can just use a new type right? Of I guess this can just be value to make this a little nicer.
02:02:31.037 - 02:03:47.315, Speaker A: So value mark flag help. So that's easy enough. Let's ignore clone for now because it seems complicated or seems annoying rather. So what can we do about these Puff? This is supposed to return a reference to the V. So get reference self that's just going to be self dot zero dot with V and we don't care about the version and we're just going to return a V dot value is marked. You see the pattern here, right? Like all of these can be expressed through the use of with mark. I guess flag is going to return.
02:03:47.315 - 02:04:16.457, Speaker A: Flag is help. It's going to return help. So I think this is a pretty good helper type, no pun intended. And all of these we could also trivially implement with help. Great. I guess we can look at these just to make sure we can. But yeah, it's the same thing.
02:04:16.457 - 02:04:45.495, Speaker A: This just looks at the mark bit and the flag bit and this could also just be a closure. So this like version triple mark reference is trivial to implement in terms of our new atomic type, which means our atomic type seems pretty. Seems pretty general. And then we might actually want this implementation. Right, because it seems like that's a thing that many of these data structures use. So maybe it's worthwhile to include this. I feel like ultimately like all you really need is this.
02:04:45.495 - 02:05:41.795, Speaker A: So I'm going to go ahead and just remove this. All right, so we have this atomic type of ours and we have the versioned cas and now we don't need this trait anymore. Instead what we're going to require is that this implements versioned CAs. So the commit descriptor we get needs to be able to produce a sequence of versioned CAS operations. Descriptors like things that are versioned casses that have yet to be executed. Maybe pending versioned CAS is a good name. Hi cat.
02:05:41.795 - 02:06:05.263, Speaker A: Hello. All right, let the cat say hi. Let's Cat, do you want to say hi? Oh, the light turned on. She had to leave. Light turned on, she's out of here. Alright, so back to this. So now I think we're in a pretty good shape to just like implement this very straightforwardly.
02:06:05.263 - 02:06:47.271, Speaker A: So we loop over them. If the cast dot get state, which I guess can just be state if that if let. Oh my. This is just if it's success, if it's failure and otherwise. So this is a match, popularly known as a match state. So cast eight success. So if we get a cast state success, then what do we want to do? Then we would do cast clear bit and continue.
02:06:47.271 - 02:07:26.645, Speaker A: That's fine. That's implied cast state failure. I'm going to guess it's just return I. Yep. And cast a pending means that it still needs to be executed. So if it's pending, that's the remainder here Then we're going to cast execute. And it looks like we don't do an error, which is kind of interesting.
02:07:26.645 - 02:08:20.885, Speaker A: We just do cast dot execute, and then if cast dot has modified bit, then then cast dot CAS date field. That's interesting. Cast state field. I don't remember us adding that as a method. Is that a method we missed? There's just set state, which then sounds like it should be something else. Right, so if we go back to infrastructure here. Icast desk.
02:08:20.885 - 02:09:00.333, Speaker A: See, this one just has get state and set state. So it seems like something changed between the paper and the code. That's interesting. Is there even a CAS state field? There isn't. All right, so let's look at the equivalent of execute cassis. So line 53. All right, so clear bit.
02:09:00.333 - 02:09:26.175, Speaker A: This is the same. Yeah. So see this one? It's actually different in the paper. The paper says if modified bit set, then do CAS in the code. It's if modified bit set, then just set state. In fact, this is a cas. And if it turned to success, then clear.
02:09:26.175 - 02:09:52.835, Speaker A: This does a set to success and clear. That's interesting. I wonder which one is right. That's problematic. These seem different. All right, we're gonna have to think. And while we're thinking, we're gonna say hi to the cat, right? Hi.
02:09:52.835 - 02:10:38.615, Speaker A: Do you want to say hi? Do you want to say hi? No. They're up there. They're up there. Hey, where are you going? You leaving Meow? Do you want to smell the microphone? Does it smell weird? Do you want to go say hi all the way up to the camera? All right, I'll let you go. I'll let you go. You're the one who came here to meow at me. You came to meow at me and now you just want to leave.
02:10:38.615 - 02:11:17.173, Speaker A: All right, all right, back to the code. Where were we? All right, let's see. So the different. You can see my screen again, Right. I think I switched it back. The difference between these is that in one, we. In one we compare and swap and then see whether we succeeded in the other one.
02:11:17.173 - 02:12:48.335, Speaker A: We just blindly overwrite it with success and then clear the bit. You know, that's very interesting. I'm going to go with. What the code does is probably right, because the code was presumably run, although it is a little worrying, but all right, let's go with what the code does, which is set state, castate success, paper and code diverge here. Cast8 should definitely not take reference. And then cast clear bit. And then if cast get cast state not equal to cast state failure.
02:12:48.335 - 02:13:23.745, Speaker A: Oops. Not equal to success. Then cast set state. This like also feels weird. And return. See here it doesn't even return the index. Remember how we talked about in the paper, like it like returns the index of the thing, but the code version doesn't actually return the index.
02:13:23.745 - 02:14:19.535, Speaker A: This is real weird. I mean it's easy enough for us to just return I here. But like why doesn't it. It's like something real fishy going on here. Clone, copy, partial E debug. Yeah, it's real weird that this doesn't return anything anymore. Maybe it's because now that's handled by the caller or something.
02:14:19.535 - 02:14:48.877, Speaker A: Or maybe post casses like walks the list. Nope. Well, I guess the paper had more things than the code does. So maybe. Maybe what happened was the paper gave a more general description and then in the cause of implementation they were like, this seems like it wasn't needed. That's a little disturbing. There is no git.
02:14:48.877 - 02:15:23.185, Speaker A: No, no. This is just a source dump. Tarball. Does set state internally compare and swap? That's a good question. Let's take a look at. I guess the fomichev fr cast des 45. Nope.
02:15:23.185 - 02:16:47.795, Speaker A: Set state is just. It's not even. It just changes the value in the descriptor, which in and of itself is bizarre because. Because how can it even do that? Like there's concurrent access to these descriptors. So how can it mutate this in place anyway? Yeah, like it's something. There's something weird. Like this set state doesn't seem like it'll be possible to implement because in Rust, like this is going to take an immutable reference to self.
02:16:47.795 - 02:17:34.124, Speaker A: Right. Like if we look back at where this gets called from, which is in base combiner. Right. Like CAS here is just a. Is just a shared list of descriptors, so you can't call set state on it. I guess maybe this ends up working in Java because volatile, but it's not volatile. Yeah, this seems super sketchy.
02:17:34.124 - 02:18:10.565, Speaker A: I mean maybe it really just is. Like it's just an atomic like U8 that it's fine to just store to. Like maybe what we'll end up doing is like when we implement the descriptor, this will just be a. Like an atomic U8 internally. But like this certainly seems kind of weird. Yeah, not sure where that that's going. Okay, that's fine.
02:18:10.565 - 02:19:05.294, Speaker A: So what does the paper say? It says if the modified bit is set, then set it to success and if it's now success, then clear the bit and what we did was always set it to success regardless of whether it's been changed since we set it to pending and then always clear the bit regardless of whether we succeeded at setting to success. So I can some sense this change. I don't want to say simplification, but this change is like accurate because like we set it to success and then we clear the bit because it is success. In the old one it would conditionally be success. So this, this conditional was required. I don't see why it's okay to do this unconditionally, but I'm more inclined to trust the code that was actually run. This might be something I can check with the author about too.
02:19:05.294 - 02:19:52.915, Speaker A: And then the reason of course we have to check here is in case some like for example, if the modified bit is not set, then we might then some other thread might still have put us in the success state or in a non success state. This definitely a weird. There's something weird here. Why does. Why is the capital I that's also a big problem. Like this should really not be capitalized because this is the lowercase I which is the variable name here, I assume. Okay, that's fine.
02:19:52.915 - 02:20:34.085, Speaker A: That is at least now that now at least matches what the code does. So now I guess let's go down here. What is this complaining about result commit descriptor and contention? Oh right. We changed generator to return like an error on contention. Oh, so that people could use question mark. That's right. So that makes it a little bit more awkward to do here though.
02:20:34.085 - 02:21:33.725, Speaker A: We can do this really nicely with a try block actually here. If we did like try but try blocks aren't stable. But basically we sort of want like a question mark here to break rather than return. I mean we can always do that by like if I guess match. Fine. So okay, cases is going to be cases and error is going to be a contention which is going to just break and then this is going to be the same which is going to be match this. Oh Cass.
02:21:33.725 - 02:22:15.455, Speaker A: Except the CAS execute does not do that and I guess we don't check the contention here. In fact, it looks like there isn't even a check for the contention counter in there, which is a little odd. Like it feels like there should be. It feels like Cass execute should also like cat CAS execute to do. Should this also return on contention? I feel like it probably should. In which case we. We would want to sort of do the same kind of structure here.
02:22:15.455 - 02:23:05.565, Speaker A: Why is this an option? Right? We changed wrap up so the wrap up could Succeed by resolving. Or it could succeed in just cleaning up. Or it could error in the case of contention, in which case we break. Right. And index is no longer used. That's great. Atomic U8 might be used later, but not used right now.
02:23:05.565 - 02:23:36.675, Speaker A: And now it's complaining about this. That's fine. And this version is going to be a U64. That's right. We specifically chose for it to be a U64. This is because Rust is annoying about references. Version that should be a U64.
02:23:36.675 - 02:24:12.633, Speaker A: Of course, if you want to compare the version, this should be a compare exchange. Should arguably be a compare exchange week actually. But that's fine for now. This same thing, the. This pointer. That's fine. This can be a mute.
02:24:12.633 - 02:24:50.235, Speaker A: That's what we get from load anyway. And then here, I guess what we want to return is if these are already equal, then true. Otherwise we want to return if this dot is okay. And here too there's like a. If. Like if. If this fails immediately deallocate the box because we never shared it.
02:24:50.235 - 02:25:26.675, Speaker A: Right. So we don't actually need the fancy garbage collection. In this particular case, the capital return eyes is a comment that wrapped to the next line. Yeah, but then there's not a code line there also. No, because it has its own line number, whereas this wrapped comment does not. It's like something we're also. Why is this typeset literal and this one's not? This is like something weird about the typesetting.
02:25:26.675 - 02:25:58.525, Speaker A: Okay. I just want to get rid of the errors. Right, Cass? Execute does not currently deal with the contention, which seems maybe problematic. I feel like maybe Execute should take care of the. If we go back up to look at where they talked about the contention, I feel like there was a. Hi, cat. Hi, cat.
02:25:58.525 - 02:26:18.889, Speaker A: Hi, cat. Hi, cat. Hi, cat. Hi, cat. You wanna come say hi? All right. Come on. Do you want to meow again? Why are you sitting so weird? Sit normal.
02:26:18.889 - 02:26:48.535, Speaker A: There you go. That's better. You happy now? Is that better? Wait, let me make you full screen again. What? You want to leave again? Hey. What? What now? You come in meowing and then you just leave. That's very rude. Very rude.
02:26:48.535 - 02:28:34.725, Speaker A: I'm just looking for the place where they talk about the content Normalized representation contention failure counter. Yep, I expect to find this address because I'm pretty sure we're just going to want to pass that to execute Casses and have atomic. The atomic compare and set method takes a mutable reference to a contention as well. Is that helping Threads must synchronize the critical Points, normal state, normal representation. I feel like it was. It must have been further down somewhere. All right, they talked about this in terms of like monitored run, but there's no monitored run here, so maybe it specifically shouldn't be monitored here then.
02:28:34.725 - 02:29:41.815, Speaker A: It's weird. I feel like. I feel like there's almost certainly you should track contention and execute CAST too, because you think about it like if the commit operation cast fails, that is definitely a sign of contention and you may want to back off to the slow path. Right, But I wonder whether they talk about this a little later. Counting a contention failure counter for all the methods in the linked list can be implemented by counting the number of failed CATS generator. The wrap up. Okay, where's the part where they talk about original algorithm for fast path memory management comparison appendix for the whitefree queue Contention failure counter.
02:29:41.815 - 02:30:38.943, Speaker A: Yeah, I'm just gonna assume that that's the case. One thing that's nice about the contention counter is that it's not a, it's not a correction. It's not a. It's not a correctness measure or a correctness risk. Because if you, if you over approximate contention, what that means is you're just going to take the slow path more often than you otherwise would need to. So your performance goes down, but correctness is still right. So this is okay, this is contention, and then we're going to do this, and then this is going to be okay.
02:30:38.943 - 02:31:48.635, Speaker A: Of error and then this is going to be okay. Of okay, okay, okay. So execute is going to take a contention measure and is going to return a result of bool or contention is what I'm thinking. Right. And then what we'll do is we'll say that this one also takes a contention which is going to be a mute contentionmeasure. So the expectation is that you're going to pass this into the atomic compare and set. And if it fails, then contention detected.
02:31:48.635 - 02:33:06.675, Speaker A: This should now return a result bool contention, which like, is a little weird, I'm not gonna lie. Like an okay false is also contention, but you don't have to measure it yourself, I guess. And this is okay true. And this is. I guess what we'll do is we'll match on this because we have to, we sort of have to match on it anyway because we want to deal with the case where, where we can immediately deallocate. So if it's okay, what does that even mean? We don't care about the value in the case of an okay, we can just return okay true. But if it returned an Error I guess we'll, we'll probably care about the error type, but if it returned an error, then contention was detected and we want to return okay, false.
02:33:06.675 - 02:34:12.225, Speaker A: And this returns the current, which we don't actually care about the current value, but we do care about the value we tried to stick in there, which is going to be new. So this is going to try to con compare exchange in the new I guess new pointer maybe. So in this case we want to do box from raw new pointer safety. The box was never shared. So this way we guarantee that that deallocation that allocation ends up going away. But we still detect contention. And so now I guess this will still be false.
02:34:12.225 - 02:35:20.285, Speaker A: Ooh, what does help do though if it detects contention? I guess if, if we detect contention when executing a cas then we just continue, right? This is just going to be a match on this. If outcome is just going to be outcome and error contention, contention is going to be continue. Same here. Let's format this nicely. There's probably another one of these is my guess right up here. Right. So now this one can be the same thing where if there was contention then we break.
02:35:20.285 - 02:36:29.019, Speaker A: So the observation here is we're basically trying to make it so that the user doesn't have to manually deal with the contention counter. As long as they use our atomic type, they may want to increment the contention counter themselves, which is sort of why we provide it to them in certain other cases and this is described in like appendix B. I think of the paper, like there are certain cases where you might want to say that there's contention, even if it wasn't because a particular CAS failed. But it really does feel like CASS execute also needs to sort of participate in observing contention. This does need to be pub. That's true. Huh? Does it compile? Is it possible? It does compile.
02:36:29.019 - 02:37:25.265, Speaker A: Great. So I think now we have. If we, if we look down at figure five, right, we've now encoded all of execute CAS and now let's look at the post casses, which I think we did last time, but let's just make sure that that is the case. So post casses. So this CAS execute and there's post casses. Where does post casses get executed from? Is it only from help? I think it's only from help. Real question is on the fast path, do we also need to call post casses? I don't think we do.
02:37:25.265 - 02:38:21.305, Speaker A: I think it's only used by help. I mean this is easy enough to find out if we go to base combiner. I guess which is the main one. So where is like the main operation? There is no main base. Oh, I guess that it's all done we extension because of course it is fr combined 3. Let's go with the highest number extends base combiner. So look at like some random operation like I guess delete.
02:38:21.305 - 02:39:15.325, Speaker A: Interesting. So here there certainly seems to be like a bunch of stuff that happens in Delete beyond ask for help. So this is what we've tried to encode in our run method, right? Is that if help, then ask for help. Except here that's like encoded in the. In the consumer. And then we call generator. Oh, I see what's going on in our implementation of run our fast path calls like generator, then cast, execute and then wrap up.
02:39:15.325 - 02:40:49.517, Speaker A: But for them they actually call like the original algorithm, right? And then only if they detect contention there do they end up asking for help. That's interesting. This is probably so that the fast path is even faster, right? Like rather than go through the generator and stuff, which is what you end up doing if you do this like ask for help business, right? They just have the fast path directly encoded in the method and then sort of fall back to the slow path as appropriate. So maybe what we want to do here is like have a where's. Where's our trait. I have too many things in this file now like maybe what we want is like a fast path which takes a self and an op and a contention and it returns a result of either self output or contention output. And then I feel like we probably want to encode this first bit.
02:40:49.517 - 02:41:54.115, Speaker A: But what we'll do is what does help first. So help first calls help, which is the one that ends up calling generator and cast, execute and wrap up and then run. I think we're just gonna say rather than it doing the same thing, it's going to do match self algorithm, fast path of the operation and of course a mute to the. To the contention counter. And if that just gives you the result, then we return the result directly. And if it returns contention, then we. Then we do nothing.
02:41:54.115 - 02:42:42.625, Speaker A: I think that's what we want because then we retry the fast path and otherwise we fall back to the slow path. Right? So that's something that seems to be like it's encoded directly in here, right? It's like this is the first bit for helping. I see. This is their encoding of like there's like a setting where you always go through the helping loop rather than do the fast path. This is basically always use the slow path. So first help Then the normal fast path operation and if it turns null, I guess that means contention. That means ask for help.
02:42:42.625 - 02:43:29.125, Speaker A: And this is like the retry loop where they retry the fast path and on contention go to the slow path and otherwise fall back to ask for help. So is that generally the case for all of them? So delete is. This is the always slow path. This is the contention. This is generally the fast path. And again contention then go to the slow path. Who knows what this does? Yeah, there's like.
02:43:29.125 - 02:44:13.695, Speaker A: I guess there's a question here of how much do you leave up to the implementer of the data structure to like choose when to go slow or fast? Right? Like, okay, for do they have a contains? That's not helpful. Interesting. Here it even gets to override the postcases method. I was like, there's definitely a leaky abstraction here which is a little worrying. Even it overrides precases. Oh no, this is just the override is the implementation of the method for the trait. Got it.
02:44:13.695 - 02:45:09.309, Speaker A: Although shouldn't that just be generator? Like I feel like something's off here, but maybe I'm just like, why aren't these called generator and wrap up is what I want to know. Like this definitely differs from the way that the paper is structured, which is that help manages the state machine. And then there's not a precast and a post cas. There is a generator and a wrap up. And precast and postcas are like dictated implementations that call wrapup and generator. So here I sort of trust the paper more. Or at least the paper has a cleaner abstraction for this rather than having each one implement like precastes itself.
02:45:09.309 - 02:46:46.039, Speaker A: I mean, I guess one way to look at this is like if we combine, if we compare like this to I don't know, source, I don't know combined list2.java. If that was probably not very helpful. Let me just open up this and go to precast pre cass and then open up, I guess source combined list two and search for it. Precast. Yeah, like they're the same. And you see really what they do is they call the appropriated the appropriate generator method for the operation and then just stick that in the stick that compare and set into the box. And what about the post cast? So the post cast here and the post cast here is the postcards here just like loops through and doesn't do any of the other things that our postcards.
02:46:46.039 - 02:47:22.775, Speaker A: Like the postcards from the paper has this like monitored run should restart. I guess this does have a restart. But this post cast has like other bits in it. So there's a. Hmm. It's a little disturbing that the implementation varies from the paper's model here. It just makes it hard to figure out what the right abstraction should be.
02:47:22.775 - 02:48:16.285, Speaker A: I think what we should do is stick to the paper and then when we implement the data structures later, we sort of then figure out whether we can make it fit in the model that the paper sets. It does raise the question of like, should the fast path be. I do think it makes sense for the fast path to be its own implementation so that the implementer can choose to have its own, like, good fast path rather than have to go through the generator and stuff. That I think makes sense. I think we're going to keep. We're going to keep our run method here and just see whether that this structure, enforcing the structure makes sense. I'm guessing it probably does.
02:48:16.285 - 02:48:46.625, Speaker A: One option, right, is that we don't have a retry loop. We just say that the retry loop is something that should be in the fast path specifically for the. Again, the fast path implementation in the implementer. But we can look at that later. So I think Postcast then is one thing we need to adjust because it needs to match this postcases. Because currently we just call wrap up, which sort of is what this does. Right.
02:48:46.625 - 02:49:17.335, Speaker A: Should restart an operation result. Oh, we've actually already encoded this. I remember us doing this. It was just a question of the monitored run and monitored run we do with a contention measure and then matching on contention. So I think we're good there. I think that means that we actually now have an encoding of the simulator that these in theory is Right. And I think the big thing that's missing now is the actual wait free queue.
02:49:17.335 - 02:49:47.975, Speaker A: That's going to be like the help queue, the underlying thing, and then trying to implement a data structure on top of it. Let's pause there. We've been talking for a long time. We've gone through a lot of different bits. I think this is a great place to like pause, collect our thoughts before we dive into this help queue, which is really just going to be its own separate implementation. So let's do some questions about how far we've gotten so far. Then we'll take a little bit of a break to like go to the bathroom or make tea or whatever and then we'll keep going with this.
02:49:47.975 - 02:53:17.675, Speaker A: Who ship it? Yeah, it compiles. I know, right? You can use ternary operator in precases. Where in precases. I'm not sure what you're referring to. I mean I know what the ternary operator is, but I don't see how it applies here. All right, does the general structure, what we built so far makes sense and why it was a little weird that these, that the paper and the implementation differed and also sort of what we have and what we don't have so far. Actually, how about I go away quickly and come back shortly and then you discuss amongst yourselves whether you have questions it I'm back, can you, I assume you can hear me and see my screen.
02:53:17.675 - 02:54:08.147, Speaker A: I'm always paranoid that I like come back and then forget to either unmute myself or forget to switch to my screen. Someone's like, I can't see anything. No, yeah. This is very much like it's research. Here we dragons like the only person who's successfully run the code is probably the person who wrote the paper, which is like unfortunate, but it is nice that we have the code for reference at least. What I'll probably do is chat to the original author and sort of ask him a little bit. One thing that's sort of unfortunate, right, is that this paper was Originally published in 2014, 2015 and then there was a revision of it in 2017.
02:54:08.147 - 02:55:34.035, Speaker A: That's like the longer version that we're working on now. But the author hasn't worked on this for like 5 years and as far as I've understood it's worked on completely different things since. And so it's unclear that anyone really knows how this works at this point or how the code works at least. So it might be we're sort of trying to reverse engineer understanding, not fully grasping the paper as a whole, but it seems like a pretty big deviation. What deviation? We haven't really deviated from anything currently. Oh, you mean the deviation between the Java code from the author and the paper? It's not really a deviation as much as it is a like leaky abstraction. Like the paper presents a clean abstraction between post casses and precases which is implemented by the simulator and the generator and the wrap up methods that are implemented by the data structure and that clean separation doesn't seem to quite be there in the, in the code.
02:55:34.035 - 02:56:29.855, Speaker A: What could be the case actually is that maybe they realized later that they can do that clean separation, but they didn't update all of the data structure to use that clean separation or that that's in like not in the version of the code I have. It's not entirely clear. I guess we dealt with this to do Result, result unit use. Size contention is a Real weird return type. I think Clippy is going to yell at me. But like it sort of is accurate, right? Like, I mean we could have a type alias for like maybe contention. But like it really is A.
02:56:29.855 - 02:58:25.875, Speaker A: Either the CAS execute failed to do its job in that there was contention so it returned early, or the CAS execute ran and what we get is the result of the casses. But the other way to represent this, right, is to unify the error types and say like CAS execute returns either unit or an error type that is an enum of CAS failed and contention. Maybe that's nicer. Maybe we should just do that. It's only really used internally anyway, right? So we could do like enum CAS execute failure is either A and we're also going to implement from contention for that. So the question mark will work and we'll do I can either be a CAS failed of a use size or contention contention. And then we say this is going to return a result of CAS execute failure and this is going to be a error of CAS execute failure CAS failed.
02:58:25.875 - 02:59:54.995, Speaker A: And it's like the question of is this nicer? I think maybe it is, I suppose. And this question mark still works because of the from implementation then anywhere that calls this now needs to deal with, right? So this now we get into a slightly weird case where this now becomes okay of outcome and this becomes if this is execute CAS execute failure cast failed I, then it's error of I and if it's CAs execute content contention then we continue maybe that's nicer. Not entirely sure. All right, still haven't done a git commit? Maybe I should do a git commit? Nah, I'll do it later. Okay, I think the big next question now is implementing. Unless there are other questions about what we've done so far. I think next question now is how we implement the wait free queue.
02:59:54.995 - 03:00:55.441, Speaker A: So as a reminder, this is where anytime we go to the slow path, we need to sort of put up a description of the work that we want to do and stick it on that queue. And every time we are about to do an operation we want to take, we want to see whether the queue of people looking for help is non empty and if so, try to help them make progress. And because we want the whole thing to be weight free, any operation on the help queue has to be weight free. That means that we need to implement a weight free queue. Now luckily the paper authors already wrote one which is an adaptation of another wait free queue that someone else implemented. This is adapted specifically for this use case and the code is down here. I love reading code, Java code in literal wrapped PDFs, like literally typed PDFs with line wrapping and page wrapping.
03:00:55.441 - 03:01:23.375, Speaker A: It's great. Okay, so a help queue. It's a help queue. Do atomic reference. All right, so there's a node type. It's pretty common, I think. Let's probably make this its own module.
03:01:23.375 - 03:02:09.095, Speaker A: I really need to like organize this code. The biggest reason I haven't split this into multiple files and such is because as you've seen over the course of the stream we've moved so much stuff around because we're still sort of coming to grips with what the interface should be, what the abstraction points should be and the layer should be. So it's nice to have it all in one file where you can just like rejig it around. But the help queue is sort of unique in that we know what its interface is going to be and we know that it's just going to be a self contained thing. So it makes a lot of sense for it to just be in its own module. Yeah, so that's, that's my plan. So we're gonna have our first module.
03:02:09.095 - 03:02:58.055, Speaker A: I don't think we're actually gonna need phantom data, but you know, we need operation record box and normalized lock free. We may actually not need these types, but we'll have them there for now. Right. And this needs to use help queue. Help queue. And this need to be published create because we're going to use it outside of here. And same goes for enqueue peek and try remove front.
03:02:58.055 - 03:03:22.517, Speaker A: Great. And the phantom data use up here can go away. All right, so now we need to figure out how to implement a help queue. Let's see what we got. So we have a node type. There's a value that makes sense. There's a next which is an atomic reference.
03:03:22.517 - 03:04:01.911, Speaker A: This is an atomic pointer. But it also includes the. It's going to eventually have to include the logic around garbage collection. It has TID is probably the thread id. So it's like the enqueueing threads ID and the dequeuing threads id. They're a little weird. There's I guess operation description, phase pending and queue.
03:04:01.911 - 03:05:01.833, Speaker A: So this is probably like. Remember how the way you make something weight free is you make people help each other or threads help each other. Which means that this weight free queue which we're using for the queue of things to help needs to have its own way to describe things to be helped internally, which is probably what this OP desk is going to Be atomic reference. Head, tail. Atomic reference array. What is an atomic reference array? Oh, it's an array of atomics. But okay, good.
03:05:01.833 - 03:05:42.569, Speaker A: You need to give a length. But what's the length? Oh no, that's awkward. The array whole, it's an array of. That's like bounded by the total number of threads, which is a value we don't really know. And in general you don't really know. Like what. Where does this test? Num threads.
03:05:42.569 - 03:06:50.675, Speaker A: Sounds an awful lot like the wrong value. I want to look at what the code does here. Base combiner. No, that doesn't look right. This doesn't look right at all. This is. Okay, this is not giving me the results that I need.
03:06:50.675 - 03:08:10.115, Speaker A: I'm guessing this is like. It seems like this is like a junit value or something. But this seems really problematic because like, okay, what if someone spawns another thread that wants to access this thing? What happens then? I guess this tid will just be out of bounds and then the whole thing will just crash. Like what happens if I call get with a value that's too large? I mean, I. I assume it just throws or something like throws an exception. Like this seems sketchy. The test Java file in source.
03:08:10.115 - 03:09:51.885, Speaker A: Well, that's awful. Well, that's interesting. Not quite sure what we do here. I mean, it seems like this implementation needs to know the number of threads. Yeah, because you see, what it does is when you enqueue, you basically set the state of this thread in that array, which means that. And the idea of course is that because every thread has its own index, it can like atomically operate on that index all at once. And other threads can help on its index by just walking the list of things.
03:09:51.885 - 03:10:51.965, Speaker A: But if you don't know the number of threads, then you have no way of doing this. I mean, we could say that every time you want to clone a new handle to it will like take a mutex and like, the problem is this can't be a vector, right? Because if the vector needs to grow, who grows the vector when there are lots of threads we can concurrent access. That's why it's an array here. How does it even get the thread id? What does it use as the thread ID here? That's what I want to know. Ask for help just takes the thread id. Okay, Ask for help. Delete is just pass the thread id.
03:10:51.965 - 03:12:24.355, Speaker A: Well, I see. So basically you need to say in advance so it doesn't have to be thread IDs. If we, if we think about this for a second, what this is saying Is that every handle to the data structure needs to have a distinct identifier and they're sort of proxying that by thread ID here. But it doesn't need to be and you need to declare in advance how many handles you want there to be. So one option. Well, it's like a question of whether it should be send or not. But one option here, right, is to use const generics and say oops there.
03:12:24.355 - 03:14:01.265, Speaker A: So our help queue is going to be generic over that and that means our generator, sorry, our simulator rather also needs to be generic over that. And then what we can do is we can say like how's this going to work? Like, all right, let's say there's not a tri clone, is there? But, but if we say why does. Why do we need this trait bound all the way down? Oh, it's because this needs to name an associated type. That's really stupid. Okay, so, so here's what we can do. Here's what I'm thinking. We make it so that if you have a wait free simulator like you can always create your first one.
03:14:01.265 - 03:16:05.125, Speaker A: What am I doing there? Pubfnu, right? There's gonna be some new that gives you a new one and then we can say pub offend like fork which gives you a result self and too many handles. Right, and what fork will do is what will fork do? So let's say that the help queue, there's going to have to be a sort of shared struct that holds the actual help queue and holds the algorithm and holds identifiers. And this is going to be an atomic you size. Maybe you already see what I'm getting at here. And then we're going to say that this is going to have shared, which is going to be an arc to shared of LF and N. So I want atomic use size and I want standard sync arc. This needs to be generic over all of this as well.
03:16:05.125 - 03:16:46.615, Speaker A: That's fine. Two. Okay, that's fine. This is const and us and then this is self shared help. Self shared help. So all of these are just going to have to access through shared. So the idea here, right, is that it's really unfortunate that this is the case that we need to have this like handle.
03:16:46.615 - 03:17:45.479, Speaker A: But basically what we're saying is that rather than allow people just have like a weight free simulator that they themselves stick in a, in an arc, they actually need to explicitly have a handle to one. So we have to do the, the arcing for them. It's really unfortunate. It means that you can't like you can't stick it in a static, for example. But we may just have to live with that. This might actually mean that we can use EPIC based garbage collection too, because we can store the guards in the handles, right? So we could store a like a cross beam EPIC guard in here. But what I'm imagining right, is I isself do shared.
03:17:45.479 - 03:19:18.505, Speaker A: This should really be handles. Handles do fetch, add one and then if I is greater than or equal to N, then return error of too many handles. Otherwise okay of self of shared itself dot shared and I is I and this then needs to do self share.handles.fetch sub because it needs to make sure that in the future if someone gives one up, we don't just keep incrementing, incrementing it. And then we'll do like a drop implement drop for this mute self, which is going to just return the. It can't return the I either because it needs to return its I, not just any I. Handles are a pain.
03:19:18.505 - 03:19:57.425, Speaker A: All right, my thinking here was that this way we're like ensuring that there are only ever N handles. The problem is that when I drop, like imagine that I create eight handles and I have the maximum number of eight. I create eight handles, then I drop the fifth one. Then. Now if someone tries to create a new handle, it needs to get an ID of 5, right? Or 4. It needs to get the ID of the one that was last or of one that has been dropped. But this scheme won't do that.
03:19:57.425 - 03:21:06.751, Speaker A: Okay, so the other alternative here, and this is even stupider, we're just gonna do this. Screw it. So you, you can't give back candles. Like the moment you reserve a handle, it's yours forever. And then we'll just leave users to don't fork if you don't have to. This is dumb, right? I'm not saying this is a good API, but in the interest of making progress, I think we can do this. Because behind the scenes you could always improve Fork so that it knows how to manage these like multiple IDs.
03:21:06.751 - 03:22:09.555, Speaker A: Ooh, here's an option. This is even stupider, but maybe we're going to do it free. Handles is going to be a vector of usize. In fact, it's going to not even be that. It's going to be a. It's really going to be a. Some sense is going to be a bool of N, but I don't want to make it a bool of N.
03:22:09.555 - 03:23:09.385, Speaker A: So my, my thinking here is I want this to be a mutex because I'm fine with fork not being weight free. If you have to take a lock in order to get one, I'm okay with that. That way we can at least fixes the silliness. So what we'll do is say free handles is going to be just going to make it a ve for now and it's. It's dumb for so many reasons. But we're going to lock, unwrap pop and then we're going to say if let some I is that then you can get your handle. Otherwise there are too many handles.
03:23:09.385 - 03:24:05.975, Speaker A: And now we can implement drop for this which is going to be self.share.free handles.lock.unwrap.push self.I and then new is going to be self of. First of all it's going to assert that N is not zero. Arc is going to be an arc new of a shared and we're going to start out with I being zero. This is going to have.
03:24:05.975 - 03:24:57.835, Speaker A: I guess this is going to take an algorithm which is going to be the algorithm. It's going to take a help q new and free handles. And this is going to be real dumb is going to be 1/not.n. collect Mutex new and we don't have a new for help queue yet. And this is fine, right? Like it's just a vec of you sizes. It's not actually a vec of like thread descriptors. It's just an integer.
03:24:57.835 - 03:25:28.375, Speaker A: It's really just a handle ID if you will. And so now when you fork you take a lock. So it's not wait free to get a new handle. But once you have a handle all the operations are wait free and when you drop you just return whichever I you originally got when you created the handle. It is really unfortunate that this n has to be hard coded. Maybe one day we can get rid of that. But now the help queue.
03:25:28.375 - 03:26:16.585, Speaker A: So now in the help queue this is going to be generic over N pub fn new which is going to return a self is going to be a to do because of course now will this compile great. And I can get rid of the atomic use size. Nice. I don't understand why we popped away can fork. We popped the vacant fork because we need to get an identifier. So the free handles is a list of handle IDs that aren't currently used. And so we just take one of them and pop is one of them.
03:26:16.585 - 03:26:53.765, Speaker A: Like any of them is just as good as any other. That's why. So now I guess we can actually Implement this if we go back to look at the actual code, because that seems nicer. All right, so node is just a node in linked list, right next to an atomic reference, these identifiers. That's fine. Operational descriptor. That's fine.
03:26:53.765 - 03:27:49.625, Speaker A: That's basically the help queue. I see. So the observation here, what this is doing, right, is saying in this other larger data structure, we don't want to encode. We use the linked list as a way to encode all of the possible help things or things that might need help. Here we're not using a linked list, we're using an array in some sense. Like once you have this requirement, why not just use an array in the other one as well? Like something is weird, but it's fine. So the constructor here for the wait free queue is you create a sentinel which is going to be the indicator for the empty list.
03:27:49.625 - 03:28:33.975, Speaker A: You create a new thing that has like a operational descriptor per thread. So this is where the observation comes in that you don't actually need a, like a linked list of help operations because, you know, the maximum number of help operations, which is the number of threads or number of handles rather. And I guess it's maybe the reason we do that here is because this is going to allocate N of this size. Like if there are n handles n times size of opdesk, we. That's probably fine here because opdesk is kind of small. But for other data structures, the help structure might itself be pretty large. And so maybe that's why they, they want to use a link list here.
03:28:33.975 - 03:29:28.905, Speaker A: Interesting. Okay, I guess we just start encoding this. Shouldn't 1 to n collect be a 0 to n collect? No, that's very intentional. It's a good catch. But no, it's because the, the self we return has already taken the identifier zero turn, has already claimed zero, therefore one. So let's see, we should just start encoding this and then see. Actually now let's read a little bit more.
03:29:28.905 - 03:30:01.645, Speaker A: Head and tail pointer. That's pretty standard. Initializes the state array to be. I don't need help, I suppose. What is the actual operational descriptor? Face pending and queue. And a node I see. In some senses could be a thread local.
03:30:01.645 - 03:30:56.293, Speaker A: Except that you need some way of iterating all of the threads thread locals thinking whether we can make this const. But I don't think we can because of the node. Oh, maybe, maybe actually because that node is an option. This is really an option node because it's allowed to be null which really suggests that opdesk should be an enum or maybe should just be. Maybe the whole opdesk should be an option like this should be an array of option optesk. That's probably really what's going on. Interesting.
03:30:56.293 - 03:31:57.835, Speaker A: Q gets the phase and sets his opdesk be new op desk. All right, are these heap allocated because they need to? Probably because it's an atomic reference. So that means it also needs to be handled by the memory reclamation scheme we use. I don't know if it's going to need to do. Probably doesn't need a compare and swap because any given index will only ever be updated by that handle and be read by others. So there's no. There's no chance of an ABA problem happening, I think because I don't think any other thread will ever hi Cat, I don't think any other thread will ever modify it.
03:31:57.835 - 03:32:54.245, Speaker A: Oh, this help. What does finish ink do? Get state.get tid.face false true next. Also there is a compare and swap on Help Finish. I'm just trying to figure out whether there's an ABA problem here, and it looks like there might be, which maybe means this needs to be versioned as well. Well, there's no versioning here.
03:32:54.245 - 03:34:10.355, Speaker A: This just declares a new one. Although because it's using the. Because it's using the pointer you don't have. Like this is doing RCU again, which means you don't have the ABA problem for the same reason why we don't have the ABA problem when we're using the heap allocation for the thing to do like versioning in meta and stuff. That's a little awkward if each of these is a heap allocation as well, but it seems like it might have to be because this updates. Let's see, so when you in queue, what do you put there? Phase true true and a new node and when it gets updated it's phase falls true and next why isn't this curdesk dot phase? This feels like it could be curedesk, which just means it's updating that in place. Maybe it's only this one that gets updated.
03:34:10.355 - 03:34:57.967, Speaker A: Next is last Next get this just feels super racy, but I guess like I'm assuming this is a reasonable implementation. All right, let's go with the. Let's try to not optimize too much and go with the way that this is implemented in the first place. Just do the heap allocations and then like do RCU and heap allocations and then if necessary change it back so I Guess peak head is just you look at the head. That's easy enough. And Q is set the state for your thread. Then help.
03:34:57.967 - 03:35:48.755, Speaker A: I'm guessing it's like help everyone in the same phase as your operation is in and then finish the encoding of the help action you inserted. So really here all the work is happening in help. Unlike the other one where we had like a fast path, here it's just only slow path. Like doing an operation means sticking it on the help queue and then helping until your thing completes. Conditionally remove head. I wonder why this has to help. All right, let's try this out.
03:35:48.755 - 03:36:56.965, Speaker A: So we're going to need a class. Oh, John. We're gonna need a node which is gonna hold a value V. It's a good question what's even gonna be in these? I think it's gonna be these things, which means node is going to be generic over lf and we're going to have to require this, which means we're going to require this all so that we can name this type. So that is the value. There's a next which is an atomic. Oops.
03:36:56.965 - 03:37:37.005, Speaker A: I guess we should also grab these, which is an atomic pointer to a node. To a node lF. Sorry about that. Actually let's be even better and say to a self there is the ink. I'm gonna go inc I because we've used I instead of tid. Although maybe that's gonna be confusing because that might be the index. So let's have all of these be ID instead of just I.
03:37:37.005 - 03:38:45.425, Speaker A: And this is going to be free IDs, free IDs. And this is going to be ID and this is going to be IDs and this is going to be ID and it's going to be IDs and this is Going to be ID. So I guess actually this also means that probably all of the enqueue operations are going to take that identifier, which sort of suggests that actually the fork method should be on help queue rather than on wait free simulator. But let's leave it up there for now and just say that these. This is all going to take an id, which is going to be a use. This probably doesn't need to. In fact is it only in queue that needs it? Peak head doesn't and conditionally remove head doesn't.
03:38:45.425 - 03:39:46.461, Speaker A: So only this one takes an id and then over here this is then going to take self.id. okay, so the ank id is going to be a use size. What else we got up here we have the deck ID which is an atomic use size. Not entirely clear why that is yet, but I guess we will soon find out. And there's a constructor for node that's easy enough. Which takes a value which is one of these. I guess we could actually make a.
03:39:46.461 - 03:41:35.625, Speaker A: A fully generic help queue and then say that help queue is a variant that takes this. Maybe that'll make this a little bit nicer so we don't have to carry all these around. So we're going to say that a help queue is really just a weight of. Is really just a wait free help queue of this and then we can do that way we can just be fully generic down here, which is going to be a little bit nicer. So this is just going to be a T and we don't necessarily know what that's going to look like yet but at least it means we don't have to carry these long bounds around everywhere. And just say this holds T takes a T etc. And then what we'll do is we'll have these methods all be on wait free help qt right? And say that they all just take a T return a reference to a T or something like there's something odd here that we need to figure out.
03:41:35.625 - 03:42:19.951, Speaker A: This may end up operating on like raw pointers or something, I'm not quite sure. And then this is really just going to be self.the wait free queue.nq ID and help. So this is just a. Essentially this is just a forwarding. In fact this can just be a type alias and that way these can all just go away and we don't actually know what new node is.
03:42:19.951 - 03:43:13.135, Speaker A: Oh, I guess new node is pretty easy. It is just going to be a self of where the value is the value next is atomic pointer. Is there a null new standard pointer null pointer the enqueueing id. I see. So this needs to take an inc ID which is going to be a usize and stick that in there and this is going to be an atomic integer. So it needs to be able to be minus one and that's just because we don't have atomic options. But that's fine, we can use.
03:43:13.135 - 03:44:27.247, Speaker A: We can live with this being a minus one. Technically that means that half of the enqueue ID space can't be used which is a little awkward, but such as life and then what's this Op desk? This is going to be a struct. I still feel like this is probably going to end up being a an enum but there's going to be. Let's see face which is a i64 pending which is a bool and Q which is a bool node which is a node which means this has to be generic over T and there's a constructor for that too. Opdesk tffnu and it takes all of the fields so it doesn't have a constructor. In other words, that's fine. And I guess now we start to look at what the actual help queue contains.
03:44:27.247 - 03:45:03.055, Speaker A: So it has. The actual help queue has a atomic pointer to a node of T. It has a head which is this. It has a tail which is the same. And then it has this like state array which is going to be a atomic pointer. An array of atomic pointer to opdesk of length N. This is where this const and use size is going to come in.
03:45:03.055 - 03:46:01.267, Speaker A: Right? And then there's also enqueued and dequeued which feel like they're probably just there for debugging maybe. Yeah, they're not actually necessary. So we could have. They're basically for metrics collection like for stats. Like this is probably handling handy and debugging. But like in fact we can do this here too, right? We can say an cued or anked atomic view size. But I'm not going to do that now.
03:46:01.267 - 03:46:44.875, Speaker A: We're going to keep it simple for now. Is that constant an example of refinement type? No, this is a const generics type. There's no refinement here. Alright, so we have a head and a tail and then this is gonna have a new. A new is going to take no arguments apparently and return a self. What's it gonna do? It's gonna create a sentinel. Just going to be node new.
03:46:44.875 - 03:47:27.649, Speaker A: It's going to be null which. So that means that's an option T. So node new of none and minus one. So I guess this has. This does have to be an eye size. That's fine. And then we're going to say oops.
03:47:27.649 - 03:48:26.725, Speaker A: Then we're going to say that the head is going to be atomic pointer new of I guess box new box into raw. We've done this dance before. This is going to be pointing to the sentinel and the tail is also going to point to the sentinel. State is going to be. Well, so state is going to be a little bit weird because I guess we want like a const empty state. Let me write to this in the long form and then see why we have to change it first. So this is going to be a op op desk of length N.
03:48:26.725 - 03:49:26.625, Speaker A: Right? Where the phase is minus one pending is false, right? Second yeah, this one is true, apparently. And node is null, which means node is none, which means node here is an option node and I forgot a comma. So this won't actually compile or this eventually won't compile. Because you can't. You can't create erase this way just by repetition. You have. You can only do that if this is const.
03:49:26.625 - 03:50:12.425, Speaker A: If this is specifically a const value. So this is like an empty desk is this. And then you can do it here. But this is going to complain because you're not allowed to have one that's generated. Can't use generic parameters from outer function in the inner one. So this is going to be a little bit of a pain. Yeah, it's going to be a little bit of a pin.
03:50:12.425 - 03:50:42.943, Speaker A: Basically the problem here is we need. We need to construct the array. But to construct array you have to have all of the values at the same time, but they're all the same, which you can only express if their type is copy. Like you can put a type in here if you can put a value directly in there. If the values type is copy. But opDesk isn't copy because it contains. It may contain a node even though it doesn't currently contain a no node.
03:50:42.943 - 03:51:19.601, Speaker A: So we try to make it cons to indicate that it doesn't contain anything that can't be copied. But we can't use the T in that name. Maybe we can type erase the node here because it's just going to be a raw pointer anyway. I'm going to have to think about this a little. That's going to be a little annoying to deal with the. You can always work around this by creating a a vector of the appropriate type. Like it's real dumb.
03:51:19.601 - 03:53:12.779, Speaker A: Like, I mean, this is it. The restriction is there for a good reason. But the way we have to work around it is real dumb, which is basically we say state is like 0 to n dot map ignore the I give me an op desk for each one dot collect dot try into dot expect gave n elements and then we say this has to be of type atomic pointer opdesk T lengthen. It's going to complain probably here that this has to be atomic pointer new box, new box into raw this so now that state. But this is a restriction that's going to be ultimately relaxed that you are allowed to name the T or just use a value where the constructor happens to be constant, but it's just not something that you can currently do like to do once const can depend on T make this constant instead of via vec Instead of going via vec could also make it maybe on init. That's the other way to do it. But if I can avoid writing on safe code, I will avoid writing on safe code.
03:53:12.779 - 03:53:41.145, Speaker A: And like N here is likely to be small anyway. And this is one time construction so I'm not too worried about it. And eventually there'll be a fast safe way to do it. So I'm okay with it. And then it just constructs I guess, self from these. So head tail state. So that's great.
03:53:41.145 - 03:54:32.057, Speaker A: Let's look at. So enqueue is the most complicated operation. So let's just skip over that for now and look at peak head. So peak is just real straightforward, right? Which is node is self.head. load just going to make all of these be sequentially consistent, even though that's not necessarily necessary. So we know that. We know that this is always okay to dereference because we never deallocate such a.
03:54:32.057 - 03:55:19.145, Speaker A: Such a lame. Such a lame reason why. So we load why does this get the next. This doesn't peak. Oh, I see. The head is a whathead.nexthead. compare and set.
03:55:19.145 - 03:56:58.685, Speaker A: Doesn't it skip the thing that's actually at the head? It gets what's at the head like what the head pointer is pointing at. Then it looks at the next element and gets that and then returns the value. So this should return the second element, not the first, right? I was wondering like maybe the head is always like a sentinel value or something. But if you look at conditionally remove head it does head get to get the current head gets the next and then replaces the thing that's in head like it places what head points to. It doesn't replace head's next pointer, but it does compare expected value to the. There's like something weird going on here. No peak head is supposed to give you a reference to the first element of the list, but this seems like it gives you the second element of the list.
03:56:58.685 - 03:58:06.075, Speaker A: Unless the head is always a sentinel value, right? Like head is always just a node that doesn't hold a value in and of itself and only the next one holds the value. But that doesn't seem to be true because in conditionally remove head we truly remove the thing that's at the head, not its next conditionally remove head gets the current head, finds the next pointer. It shouldn't be possible for the next pointer to be null unless the head pointer is the sentinel, right? So this is checking for the sentinel. That's fine. Or if the next value equals the expect. If the next value does not equal the expected value, then return false. Okay, that's fine.
03:58:06.075 - 03:58:49.815, Speaker A: And then we replace. Then we replace the current head with its next pointer. So what ends up getting removed is the thing that's at the head. So the thing at the head can't be the sentinel pointer because then conditional remove head would only ever remove the sentinel pointer, which makes no sense. Also, why does this set. That's the thing that was removed. I see.
03:58:49.815 - 03:59:28.551, Speaker A: Yeah. Head.th. this head.get gets the first node right. Like head is a atomic reference to a node that initially starts out being the sentinel. But when you enqueue. See, here's what's weird.
03:59:28.551 - 04:00:24.871, Speaker A: And Q pushes to the end like it uses the tail pointer, right? Which makes me think that the head pointer really is always the head node is always the sentinel. The feeling I get. And if the head pointer is always the sentinel, then peak head makes sense because first element is the sentinel. So you always want to return whatever is immediately after the sentinel. But conditionally remove head doesn't make sense. Specifically this line. Why would that not end up removing the sentinel? Because this is checking the value of the next thing.
04:00:24.871 - 04:01:44.065, Speaker A: Like shouldn't this be head dot next? Or like there's something real fishy about this. It feels like conditionally remove head will just never succeed. Or rather no, it will succeed, but it won't do. See? Okay, well, what. What I thought this was going to do right. Is that this would. If head is indeed the sentinel always, then this should compare and set heads next pointer to be the following pointer.
04:01:44.065 - 04:02:35.033, Speaker A: But that's not what this does. This changes head itself to point directly to next, which would bypass the sentinel. And from that point forward, PK would be wrong because the first element wouldn't be the sentinel anymore. What does the paper say here? Well, like what's the paper code? No, paper code is the same thing. And what's. What's particularly what makes me think that this is wrong is that this is comparing the value of the next. So it's clearly they expect that the expected value that's passed in is going to be contained in next, not in head.
04:02:35.033 - 04:03:17.095, Speaker A: Right. So it's intending to remove next. But. But that's not what this does. Unless like. No, the compare and set method is on atomic reference. Like my expectation here would be that this would be head.
04:03:17.095 - 04:04:06.795, Speaker A: Or I guess curr head.next compare and set next I guess Kerr head.next next.next no next and next next. Get right, like that's how you remove next. If that is indeed your goal and that this would be next.next.set n.
04:04:06.795 - 04:04:47.935, Speaker A: That feels like what the code should be doing. The reason I'm. The reason I'm hesitating, the reason I'm doubting myself here is because this code has been run right. Like they did all sorts of experiments with it that their results in the paper. So clearly it. It's not wrong. Or if it's wrong, it's not wrong in a way that matters.
04:04:47.935 - 04:06:05.095, Speaker A: So what would happen with this code as is conditionally remove head would end up removing the sentinel, which is fine, although the. Yeah, I think removing the sentinel is probably fine, although it'll make the next. Okay, this breaks if you do a. Okay, you can't do a conditionally remove head until their sentinel is no longer the head of the list. So you need to do an enqueue first. Otherwise you can't do conditionally remove head anyway. So someone doesn't enqueue, which means that now from this point forward.
04:06:05.095 - 04:06:52.855, Speaker A: Okay, so now the tail pointer points to the thing we just enqueued. Then you conditionally remove the head, which is going to remove the sentinel, but not the value. Now the value is the first. Now you call peak and you get null when really there is a value there. I'm pretty sure this is just wrong. But the way that it'll manifest is just that. You'll end up leaving a completed operation at the start of the help queue.
04:06:52.855 - 04:07:33.017, Speaker A: I think what will happen is you will. Yeah, sorry. Yeah, I'm sorry, I'm lost in thought. So. So let me try to explain what's going on. So this data structure has a head pointer and a tail pointer, right? And initially it constructs a node that calls the sentinel. That's terribly written.
04:07:33.017 - 04:08:25.225, Speaker A: But it says sentinel and head and tail both point to this. What happens when you do an enqueue mean we haven't gone through it, but basically the end result after an enqueue, at least as far as I assume is you end up with head pointing to still the sentinel. And the sentinel has a next pointer. So let's say I cue this little box. Then there's going to be a second box that's going to have a pointer into this value and a next pointer that points to nothing. There's nothing there. And the tail is now going to point to this node.
04:08:25.225 - 04:09:16.551, Speaker A: Great. So far so good. Now if you look at the code for peak head, which is supposed to give you the first element in the list. So peak says head dot get so head dot get that. Let me maybe switch colors. So head dot get which is this node, right? Head dot get dot next dot get so that is dot next dot get so this so next here is this node which is indeed the next node. If next is null, then return null.
04:09:16.551 - 04:09:26.951, Speaker A: That's fine. Next is not null. Then return next dot value. So this is next dot value. So peak will return that. Great. That is indeed the first element of the list.
04:09:26.951 - 04:10:01.301, Speaker A: Everything is fine. Okay, no problem. Similarly, if another enqueue happens, notice that none of this touched anything past here. Oops, what did I do? What did I do? What on earth did I do? Ah, I have done something weird. Oh, that's because I have the eraser on. Nothing in the peak touched past here. So if there are more in cues, it doesn't matter.
04:10:01.301 - 04:10:30.791, Speaker A: Still does the right thing. Okay, so now we look at conditionally remove head. So conditionally remove head or conditionally Oops, let's do third color. So I guess this is peak. Let's do I guess orange type color. So conditionally remove head. So conditional remove head takes a expected value and will only remove the head if that is its value.
04:10:30.791 - 04:10:49.525, Speaker A: Okay, so it does head.get so head.get is this guy. Then it does. So this is curr head, Right? Then it does. Next is kerhead.next.get so that again is this node.
04:10:49.525 - 04:11:15.419, Speaker A: So that's going to be next. Great. Then it says if next is null, it's not. Or IfNext.value does not equal the expected value. So it compares, right? Is this V. It compares this v to this value, right? So it compares this v to this value, which is what we wanted it to.
04:11:15.419 - 04:11:34.899, Speaker A: Right? Like this is checking whether the heads value is equal to the provided argument. Great. So far so good. If they are equal, then it won't return false. If they're unequal it will. So it'll go down to this line. Okay? And then it says if head dot compare and set.
04:11:34.899 - 04:12:15.861, Speaker A: So head is this guy dot compare and set current head to next. So current head to next. So it's saying head used to point to here. Now stop pointing here and point to here instead. Right? That's what this line of code says. And then it modifies the current heads next to be null. So it takes this node, right? Which is now disconnected and like removes this little arrow, which is so that this node can ultimately be both.
04:12:15.861 - 04:12:53.115, Speaker A: So that this node can be deallocated and so that this next node might eventually Be deallocated too. At least that's my read of it. Okay, but now we have a graph to sort of tidy this up a little bit. We now have a graph that is head points to this node whose next pointer is null and whose value is still what was in Inc. Right. If you, if you sort of ignore this node which is the one that was just deleted. This is now the head of the list.
04:12:53.115 - 04:13:32.055, Speaker A: But conditionally remove head was supposed to remove the head of the list whose value is the one we gave in. But that's not the one that was actually removed. What was removed was sort of the sentinel node that was here. And in fact if we now do a peak again new color purple pink. So if we now do a peak here, right? Peak remember is going to look at head.next.value okay, head. Get next which is nothing.
04:13:32.055 - 04:14:11.965, Speaker A: So peak is going to return null. But there is a thing with a value here. So I guess peak does return the right thing after conditionally remove head. So maybe this is why it works because even though it's messed up like even though it leaves the node in place, Peek ends up skipping it anyway. I mean maybe it does work. It's just really strange, right? Because we did conditionally remove head of this value, but that value wasn't removed. It's just going to be skipped over by peek.
04:14:11.965 - 04:14:56.425, Speaker A: I guess what happens now if we do an enqueue, I guess the enqueue will still work because it's going to operate on this node which used to be the tail. It's just really weird to me that the remove leaves the node with the value we said to remove in place as the head. It removes the node before the thing we told it to remove. It's just really weird. But now that I've seen like this, Peek does return null. So I guess maybe it's right. If there was a node here, then Peek would return that node.
04:14:56.425 - 04:15:43.885, Speaker A: And if you do another conditionally remove head, it'll skip past the head, look at this node's value and then remove this one. I guess it's just like always one node behind. This is really weird. I mean it's. It seems to maybe do the right thing, which I guess means it's not wrong. Yeah, like next is made the new sentinel. That's weird.
04:15:43.885 - 04:16:15.935, Speaker A: That's weird. But O. Okay, fine. I. I guess so. Next is node next load. And then if next is.
04:16:15.935 - 04:17:30.503, Speaker A: If next is null, then none else unsafe Next dot value. That's a weird ass algorithm. I'm not quite sure what it's complaining about here now. All right, all right, well we'll, we'll try it. And then this is curr head is self.head.load and then next is kerhead next.load this is a standard pattern.
04:17:30.503 - 04:18:38.655, Speaker A: I suppose if next is null or I'm going to go ahead and make this a little more explicit then return false next is then going to be unsafe next. If next dot value is not equal to front then return false. I'm just splitting this up because I kind of want. Oh actually I guess I don't use next for anything else but like the alternative is to have this read like unsafe next dot value not equal to front. It just like looks a little less nice but I suppose is technically correct. That's fine. And then I guess self.head.
04:18:38.655 - 04:19:55.053, Speaker A: compare exchange curhead with next ordering ordering and this one I think yeah if it succeeds then we do some stuff. If it errors then we return false. And what we do on success is self help finish enc and then we do curhead.next.store pointer null. I think we probably don't need this extra store. Is this needed? The reason I think we might not need the store is because I think that's only there for like Java garbage collection. Oh no, it actually does need to be here.
04:19:55.053 - 04:20:33.665, Speaker A: This is the way that we turn it back into a sentinel. Turn the. Or is that true? It's not clear actually why this is necessary because Ker head isn't the sentinel. Next is the sentinel. Now next is the thing that we actually removed and is now the sentinel. Not sure why setting that to null is necessary. It could be for garbage collection in Java in which case we wouldn't need it in rust but I'm not sure.
04:20:33.665 - 04:21:25.161, Speaker A: Okay. And we're gonna. Oh, it does not like this. Why does it not like this? Right. So we're going to need this like help finish inc method which we don't actually know what does yet but that's fine. And curr head. Yeah it's a little frowned upon to.
04:21:25.161 - 04:21:48.615, Speaker A: Well we'll just do this the old fashioned way. Curr head is unsafe of currency. So now we have this and we can say take the pointer please. This should return error. That's right. This should return. Okay.
04:21:48.615 - 04:22:42.955, Speaker A: And this should return error. Right. And we need to require that. I guess really this should be by one. One question here is whether this value should be like a comparison by like partial E or whether it should be by pointer. I feel like maybe it should actually be by pointer because we want to check whether it's like whether it's the same T. It's not just whether they compare equal.
04:22:42.955 - 04:24:09.281, Speaker A: Yeah. So I think we actually want sort of like this. Why is the value an option T? It's an option T because dot as reference. Oh, it's we had to stick an option T there because for the sentinel node is not a T which sort of means that the enum this should really be an enum that it's either sentinel or node. But that probably gets annoying right around down here where we want to access fields like next anyway. So I think we're just going to keep this be option T. That's fine.
04:24:09.281 - 04:25:15.745, Speaker A: And then we'll just have to here do like as ref expectations Not a sentinel sentinel node, no field value on mute node. That's false. I just made that be not the case. And then same thing here. This should probably then be unsafe value as ref. Expect not a sentinel node because the only node that ever has a value of null is the original head node. Why is this complaining? Oh, that's why.
04:25:15.745 - 04:25:58.425, Speaker A: Okay. And this collect is complaining because. Oh, I wonder, can I just collect directly into an array? I don't think so. I think this does need to be a collect into a vac and then I'm pretty sure vec implements try into for array concert. That's good. Convert try into. Yeah, great.
04:25:58.425 - 04:27:44.775, Speaker A: What do we not need? We don't need phantom data anymore. Great. Right. And now we just need enqueue and all the helpers of course. So NQ is phase is self.max face plus one and then self self.state ID and then this looks like it's just like a store of box into raw box new off desk of something where the phase is that we're pending is true, where enqueue is true and where node is new node of this I guess should be value a value and oops a value and tid which is ID and this should be sum and this should be sum and ID should be eyesize.
04:27:44.775 - 04:29:06.735, Speaker A: Feels wrong for ID to be eye size here. Feels like this should really just be phase should be an option u64 u32 is probably fine, but u64 let's not make this silly. Ank id should be an option usize so this should be an option use size. This is where it might be nice to make note an actual enum because otherwise you get into this business of like every field has to be an option but it just feels nicer to use an actual none here. To use an actual none here and to use a sum here, really what we should do is say sentinel which returns a self and that is just going to be value is none and ANK ID is none. This should take an actual owned one of this. And value should be sum of value and ANK ID should be sum of ankid.
04:29:06.735 - 04:29:33.225, Speaker A: We want to. We don't want people to do this. I mean this is only internal code anyway. But it just like feels wrong to have to pass in all these sums. And then what does it do? Help and help finish. Okay, so does self.help finish ank and self.help
04:29:33.225 - 04:30:13.661, Speaker A: self help. Nice. And I guess we're going to need whatever this max face is, which I guess is like going to be a U64. Maybe it's going to end up being an option U64 because it might. The phase might not be known and who knows what FN help is going to be. But I do know that it's going to take a phase which is going to be U64 and this is going to be a to do. And where did I mess up? Cannot add integers.
04:30:13.661 - 04:31:49.225, Speaker A: So this is going to be an unwrap or it's going to be a map of p plus that unwrap or oh, it's actually even going to use the fancy map or what, right? Some face I can't spell anymore, which is a good indication that I've been programming for too long. And this is going to take the phase that we chose, right. And then so it's really all the helper methods which are going to move all down here. Who could start numbering IDs at one also true. It just like option is like the right thing to do here. I feel like the other thing we could do right is we can if we wanted, if we're worried about the overhead of the option, we could do like option non0usize. So I don't really want to unwrap options all over the place.
04:31:49.225 - 04:32:29.245, Speaker A: At the very least it should be expect and you should document why the expect is accurate. Like none of these are unwraps. Like unwrap or is fine because it's not actually an unwrap, but the places where you have to use expect, like here is like really unfortunate. What I really want to do is encode it away so that this just can't happen. It's a little harder with concurrency because like given one of these, it's like you can't atomically change the variant of an enum. Right? So this is why we're sort of forced into this pattern of. It has to be an option that we didn't always know has some value.
04:32:29.245 - 04:32:47.587, Speaker A: Just a little awkward. Maybe it could be union. Like we could manually implement a tagged union. But I don't know if it seems. It doesn't seem quite worth it. All right, let's see if we can power through these helper things. So we have.
04:32:47.587 - 04:33:19.249, Speaker A: Let's do max phase first. So max phase. I'm guessing there's like some construction here where like every. And it looks like this is the case, right. That every operation is given a phase and when you help, you help everything in your phase, no matter which thread or in our case, which handle instantiated that operation. But you had help all of the ones in that phase before you move on to the next one. I don't know why that's necessary.
04:33:19.249 - 04:34:02.277, Speaker A: But wait, this is a manually implemented max loop. We can do better than this, which is self state.iter.map s.load I guess unsafe. S.load no s load order ordering sequentially consistent. In fact, doesn't even need to.
04:34:02.277 - 04:34:34.355, Speaker A: This can just be an unsafe. That's fine. S. Load. Phase max. Of course, the problem here is that this returns an option. So what we're going to do is we're going to do a filter map.
04:34:34.355 - 04:34:59.153, Speaker A: Great. So that way any opdesk that hasn't been given a phase yet is just going to be ignored. We're going to take the max. If all of them don't have one set yet, then max will be none as well. The Iterator will be empty. Great. So that's Mac phase.
04:34:59.153 - 04:35:50.541, Speaker A: That was easy. Is still pending. This seems like a helper method that we might as well just add straight away. FN is still pending. Takes a self, takes an ID and takes a phase. I'm going to guess that that's what that is supposed to be and returns a boolean and that's going to be. Why does that not need to do a load? Oh, I see.
04:35:50.541 - 04:36:11.550, Speaker A: That's just. That's fine, that's fine, that's fine. It's just because we. I guess the atomic. What was it called? Like atomic array. Atomic reference array in Java. I guess you can dot get a particular index and it does both the indexing to find the right atomic pointer and the load in sort of one method.
04:36:11.550 - 04:36:32.005, Speaker A: Whereas for us we have to index to get the atomic pointer and then do the load. They're equivalent in terms of like what it actually computes to. But that's fine. But why does it do the. Why does it do two loads? That seems. That seems bad. Just like.
04:36:32.005 - 04:37:34.585, Speaker A: Like it. It's doing the load twice, which seems like the Java code does an atomic load here and then another atomic load here, which even if it's not wrong, seems kind of wasteful. Unwrap or 0 because less than or equal. The Java code uses minus 1 for which this will always be true. And for us, none will be turned into zero for which the condition is true because it's less than or equal. Great. So we now have this still pending and I guess we can move these down so that they're in the same places in the Java code.
04:37:34.585 - 04:39:03.795, Speaker A: All right, so now we have the help functions left. Let's do help finish ink. So help finish ank. Last is self tail dot load Next is am I going to have to do anything with lasts? This is last pointer. And again for all of these, the safety is we don't deallocate like this is why it I kind of consider doing hazard pointers as a second stream because ultimately there's going to be so much unsafe here that really just is we need memory reclamation and ultimately that that shouldn't even need to be unsafe. I don't think the hope is that the hazard pointer library that we end up writing takes care of the unsafety around whether things are safe to dereference or rather ensuring that they always are safe to dereference. So this is next pointer.
04:39:03.795 - 04:41:11.675, Speaker A: So there's like a if it's not equal to null then indent this whole block and we're just going to do if it is null then return because that seems nicer. So now we're going to do let's ID is I guess let next is unsafe next pointer next dot ENC ID and I think here we next is never the sentinel. So we know that this ank ID must be set, it must be some and then we get I guess curdesk is state dot get of id, which is really state ID dot load this is pointer. The reason I'm splitting pointer and not pointer here is because sometimes we'll want to refer specifically to the pointer even after we've dereferenced it. This matters things like compare and swap where you actually need to give the raw pointer argument rather than the reference that we end up producing. The other reason is because that way I can easily mark just the dereference as being unsafe rather than marking like the indexing as being unsafe as well if I put an unsafe around this whole thing. All right, so if last pointer is equal to why does it load the tail again here if the last pointer is still the same.
04:41:11.675 - 04:43:01.555, Speaker A: And see, this is also weird. Like why does it do another load of this here after reading the tail again? It's annoying because this one might actually be important. It might be important that you do this read after you do this read. But it's not entirely clear from the way that it's currently set up. Like this could be ker desk node but instead each of these is a separate load, which is a little worrying. I'm like, is it intentional that each of these is a separate separate load of the state? Maybe, but also maybe not here too. We can like invert it to avoid the indentation marker dot.
04:43:01.555 - 04:43:52.985, Speaker A: Well, what does. I want to try to understand what this code actually does. So it like okay, first of all, let's get proper indentation by not having the Java code in here. So what does this actually do? It looks at the tail pointer. It looks at what comes after the tail pointer. Oh, I see what's going on. So again, the name is pretty indicative, right? Help finish nq So I think the idea is that the actual enqueue is probably just.
04:43:52.985 - 04:44:34.245, Speaker A: Is probably just setting the next pointer of whatever the tail is. But then something has to update the tail pointer as well. And that's what this help finish ank is is updating the tail pointer. So if the tail pointer okay, so the tail point somewhere if the next of that node is null, then the tail pointer is already correct and there's nothing to help. So this is tail pointer is already correct. So nothing to do already. Let's go with up to date.
04:44:34.245 - 04:46:27.787, Speaker A: So then it needs then it reads what is the next pointer. Looks up the current operation of the thread that enqueued the thing that is after where the tail pointer is pointing, right? So the tail pointer point to something that isn't the tail. So we look at the next of that node and then we look at what is the thread that include that next doing. That's what cur desk is. And if the last pointer has already been updated, I guess this can be like just to make it very clear, tail pointer has already been updated. And then this is if this is if the owner of the next node is now working on a subsequent operation, the NQ must have finished. Oh, I'll fix that in a second.
04:46:27.787 - 04:47:04.515, Speaker A: Right. So the observation here is that if the tail has already been updated, we're done. This is we're looking up the basically the current operational state of the node that added the thing after the tail if that node is currently working on something that isn't the next node, then it's nq must have completed for it to do something else instead. Which means that its call to help finish nq must have completed which means that it's must it. The tail must be up to date. I think. I think that's what it's going for here.
04:47:04.515 - 04:48:09.655, Speaker A: Cur desk.node I think this cur desk node is actually just a raw pointer. Maybe like maybe this shouldn't be a node. I think this has to be a star mute node T because this is sort of like this is the pointer I'm trying to put into the data structure which is going to be a raw pointer because we've already turned it into a raw pointer. We don't really have a node. Nodes are ever only ever represented as raw pointers. So I think this is going to be like box into raw box new.
04:48:09.655 - 04:49:50.385, Speaker A: In fact I think node new should return a star mute to self. Honestly, this is an option. Why is it an option? It's an option because of the Sentinel. Because initially opDesk won't be modifying a node and so what happens if that's the case? The next pointer I see this really should be like curtis.no.unwrap or standard pointer null unwrap or else Right. Like if node is none I don't think we can get here if node is none but if node is none then that's equivalent to it to being a null pointer that we want to compare because that was. That's what it was in the Java code.
04:49:50.385 - 04:50:50.605, Speaker A: Great. So now we sort of want a new desk. Is box actually? Yeah, box into raw box New op desk phase is going to be. See this is another place where like presumably the phase isn't changing just because we made progress on the enqueue. Nothing changes the face. There's no increment to the face except by the owning thread. It's like this.
04:50:50.605 - 04:51:57.315, Speaker A: But yeah, I guess you see like now the operation is no longer pending because it completed and this is now some next pointer. But isn't that. We're already comparing whether node is equal to next so this is just the same as it was like this is. This is just cur desk.next otherwise wouldn't. We wouldn't get in here in the first place dot node because this comparison checks that they are the same. So really this is just setting pending to false which feels like it shouldn't be necessary to allocate a new.
04:51:57.315 - 04:53:17.905, Speaker A: This is really just setting pending equals false. Shouldn't need full RCU but and then it does state dot compare and set so this is self dot state of ID dot compare exchange cur desk pointer with I guess this is going to be new desk pointer. Like this sort of feels like it could maybe just be an atomic bool. I feel like this could be an atomic that we just updated but self tail.com compare exchange last pointer to next pointer ordering SEC ordering relaxed and it doesn't even check whether these succeed or failed. Just kind of interesting. It just like does this.
04:53:17.905 - 04:54:54.585, Speaker A: I really feel like that could just be an atomic pool. But let's not mess with the the algorithm too much. Okay so I think that only leaves so this help and this help inc. So I guess let's do help inc. Fn help inc. Id is us and phase is u64 why and it's on self while self dot is still pending at last pointer and next pointer. I guess this is really just the same as what we did here right? And if if last is equal to see.
04:54:54.585 - 04:55:31.139, Speaker A: I see. So it has to tail was concurrently updated. This just seems like a huge waste right? Like here you. You read the updated pointer so why not use that the second time around the loop like reading reading the tail pointer. Then we're reading its next pointer. Then we're reading the tail pointer again to see that it didn't change. Okay, that's reasonable.
04:55:31.139 - 04:56:14.045, Speaker A: So you want to make sure that you actually got the next pointer you got actually corresponds to that tail. So that's fine. But now the next time around the loop you could say save yourself a load but let's just leave it the way it is for now. That's fine. If next pointer is null then do something. Otherwise self dot help finish inc. So this is tail is not up to date.
04:56:14.045 - 04:57:08.375, Speaker A: Help make it help update it. And this is a continue. Okay and then here this is next pointer is null. So this is if self and then it calls it pending again phases already over. So in that case we can continue. But I'm guessing that if the phase is over like if. If the phase is over then it's going to remain over.
04:57:08.375 - 04:58:01.521, Speaker A: I would think so. I think this can just be a return. I mean it's just going to end up checking twice which is fine. But I guess like to do can this just return? So notice that instead of doing this indentation I'm just like inverting all the conditions and returning early. I find it leads to slightly easier to read code but each to their own. If last.next compare exchange next pointer node is self.state
04:58:01.521 - 04:59:13.307, Speaker A: ID load. There's like a. This is a very involved protocol and like I'm not explaining why it's safe because I don't necessarily know like wait, free algorithms are super complicated. So I'm sort of sort of taking this on blind faith that the implementation that's provided is like correct. So this is going to be like cur des pointer desk is unsafely dereference that compare exchange to this with curdesk node and I guess this will be an unwrap or else pointer null mute. This Java code needs to be in a comment because otherwise I can't see what I'm typing. I see.
04:59:13.307 - 04:59:51.643, Speaker A: So this is like if that is okay, then help finish in queue and return is what the Java code says. Right. So this is. So what is this doing? This is helping in queue a thing. So while the phase is still pending. So this means there's still some operations that some threads have to complete in this phase. Then try to make sure that we know what the tail is.
04:59:51.643 - 05:00:38.835, Speaker A: Like the tail. This is basically us checking that we have a tail and a next pointer for that tail that we know are like both up to date. That's what this doing up to here. So here we know we have a valid tail tail dot next pair consistent if you will pair and that it needs. It likely still needs to be updated. So let's try to actually execute the enqueue. Right, that's where we're at here.
05:00:38.835 - 05:01:53.349, Speaker A: And we do that by looking up the enqueue from the enqueueing threads descriptor the to be enqueued node from the enqueueing thread system descriptor. Right. So we look up the descriptor for the enqueue operation for that thread and then we try to update the tail the tail pointers or the last nodes next pointer to point to the node that is to be inserted. And in fact I think this can even just be an expectation pending and queues are always. It's funny, this doesn't actually check does this check whether this is pending. Like something has to check that we're actually trying to help something that isn't in queue. Oh, that's up here.
05:01:53.349 - 05:02:34.405, Speaker A: Yeah. Okay, great. So this is like. No, I'm actually go ahead and gonna go ahead and leave this as this because this would only be safe to unwrap if we know here that the which we really should. Okay, fine. Ah, so here. Oh, where did I jump to here? This should be an expect node should always be sum for Pending pending enqueue.
05:02:34.405 - 05:04:01.555, Speaker A: And so here we should really do like an assert. We can have it be a debug assert that's fine search that this is indeed a pending operation and that this is an enqueue operation. And in fact if it's. If this operation is not pending, then why are we still executing it? Is the other thing I want to find out. Like if, like if this is not pending to do, can we continue Feels like we can can we assert this isn't this is still pending. Like it feels like we probably can assert something about this. We should certainly debug assert that this is indeed an enqueue operation because you shouldn't be calling help enqueue if that thread doesn't have an enqueue operation anymore.
05:04:01.555 - 05:04:53.815, Speaker A: I feel like we can probably assert here that it is still pending, but I'm not. I'm not confident enough to make that an assertion. This needs to be id and then I guess help is all that's left. And what's nice is that we should be able to write a a unit or unit test specifically for the help queue. Like this is definitely something that I want us to write because this is like really gnarly code and even just for the single threaded variant, like it doesn't need to concurrency even. I just want to see that we wrote code that was correct. Of course it's going to leak all sorts of memory, but that's fine as long as we check that the logic is actually correct.
05:04:53.815 - 05:05:52.105, Speaker A: This is for I effective doesn't even need that. It's just for this is for descriptor in self state pointer. It is desk load. I mean this is not. This is like the desk atomic and then this is the. Oh, that's not what I meant to do. This is the desk pointer.
05:05:52.105 - 05:07:13.729, Speaker A: This is the desk the desk pointer. And then if desk pending and desk face is less than equal to face and then there's an additional if which could sort of be collapsed this but really what this means is this operation needs help and then if desk enqueue which is currently the only helpable operation is enqueue right. So this feels like it could probably be simplified a decent amount. My guess is what they did was they started with a wait free implementation that supported other operations, but then they sort of stripped it down to only the things they needed for the wait free simulation stuff, which is basically like you need peek, try remove, front and queue. And as long as you have those three, probably a bunch of the other operations just went away. And so therefore Here they probably used to be like if desk delete or something, right? Then help in some other way. But in this way, in this case there are only in queue operations to help and so you just.
05:07:13.729 - 05:07:44.925, Speaker A: That's just the only one that's left. Why does helping take I. Oh, I see. So this is self state, air dot enumerate, taking id and this. And this takes id. It's a little stupid here too because. Unwrap or.
05:07:44.925 - 05:08:40.775, Speaker A: Because the other thing that having this be an array does is that you actually need to iterate through all of the. You need to iterate all through all of the threads every time you want to help. Not just like this isn't a vector, it's a. It's an array and so you can't like skip to the first one that needs help you through all of them. So this is like linear in N. So if someone tries to go like, oh, let's just make this 1024 threads or something like support for 1024 handles, then this code is going to iterate through 1024 things, even if you only have like two handles. Which seems maybe unfortunate but you know, maybe we should document that somewhere.
05:08:40.775 - 05:10:09.049, Speaker A: Operations are linear in N, so be careful it doesn't compile because 74. That's not right. I think we're actually going to do here is be. I see what's going on. The way we're going to use this help queue is actually it's going to store pointers. So like when we return a reference to T here, it's going to be like a reference to a pointer. I think we're actually going to do.
05:10:09.049 - 05:11:05.845, Speaker A: We're going to have this be T. Have this be T and just say where T is copy and partial E and E. And then now this is no longer that. Now what? That's fine. That can just now be an expectation. Where's the other place I used as ref? Because this is copy. This can now just be that.
05:11:05.845 - 05:11:40.605, Speaker A: So that's nice. Ooh, only warnings. No, something's wrong. Oh, private type. Oh, right. Because we made. We made this be a typealias, which means that this type now actually is the one we're really using, which means it also needs to be pub crate, which is a little dumb, but it means that we get to.
05:11:40.605 - 05:12:24.389, Speaker A: It means we don't have to implement all like these forwarding functions here, which is a little unnecessary. It also means I can get rid of this. And here we specifically don't care about the result of this compare exchange, which like it's A little weird, but I feel like maybe we like, can. Should we assert on these? I think the answer is no. I think the answer is that if they fail, we're just going to continue helping anyway. So I think we're just going to not do that unused import. That's good.
05:12:24.389 - 05:13:00.915, Speaker A: That makes me happy. And the deck ID was probably there just for debugging, which means we can do this and config test and just ignore it entirely. And in fact, I'm just going to go ahead and remove that because just complicates the code unnecessarily. And now we don't need Atomic Eyesize. And that makes me happy. And it compiles. Oh, all right.
05:13:00.915 - 05:13:40.347, Speaker A: Why does the unsafe block on desk draft only lasts one line? Shouldn't it last for the whole function? The unsafe block on desk stun and help. What unsafe block? Not sure I follow. Oh, yeah, so. So all of these unsafes are like. Because we never deallocate. Is the answer why all of the unsafes are safe. Oh, I see.
05:13:40.347 - 05:14:18.501, Speaker A: So what were. What this. So the question is slightly different. The question is, why is it okay for this unsafe to end here? I think this is a question. Why is it okay for the unsafe to end here when the reference that we got out of it we keep using until the end of the block? Shouldn't that mean that the unsafe block also needs to extend to here? And the answer is no. The answer is because unsafe in rust is an encapsulating operation. If you, or annotation, rather, when you annotate something is unsafe, you are promising that what you are doing in that instance is safe.
05:14:18.501 - 05:15:27.915, Speaker A: So in our case, this unsafe is promising that taking this raw pointer and constructing a reference to it with the returned lifetime is safe. So what we're saying here is, first of all, this is a valid pointer. It's aligned and it's not null and whatnot. And we're also claiming by writing unsafe here that the lifetime of that pointer is going, or that rather that pointer is going to continue to be valid for as long as this desk lives. Basically think of this as like this, this unsafe is really. Is really like turning a star cons to T or whatever, right into a tick at. And so the unsafe promise that we're making is that it, this pointer really is valid for tick A and that take a here is until the end of the scope.
05:15:27.915 - 05:16:25.825, Speaker A: Okay, we're at the five and a bit marks. I think we're probably going to end here. At least now we have what in theory is A complete implementation. We have the wait free queue, we have all of the simulation stuff. And so what's left now is testing obviously in particular I want to test the help queue because that's some super gnarly low level code. But we need to write a data structure that uses this as well and then test the overloads, overall simulation. And then separately we now have all these issues around memory reclamation, right? Like all the stuff that's currently unsafe is only safe because we never deallocate and that's not good enough.
05:16:25.825 - 05:17:11.785, Speaker A: We need to deallocate memory and in order to do that we need to implement something like hazard pointers or. Now that we have. Now that we're sort of forced to have this. Where did it go? Where did it go? Now that we were forced to have this like handle approach where it's not just like you can share a single weight free simulator, you need to like fork it for each thread that needs it. Now that we have that, it might be that we can just go with like crossbeam Epic or something that. That already has a guard primitive that we might be able to use for the memory reclamation here. I'm not quite decided either way.
05:17:11.785 - 05:18:02.515, Speaker A: I kind of want to port the hazard pointer stuff anyway because hazard pointers are cool and neat in their own way, but that's all. There's like an obvious shortcoming of this implementation now too. So I think from here there's sort of a branching point where we could either go the let's just test it and not worry about memory reclamation now or we could go the let's fix memory reclamation and then test it. I'll see which one I feel more like. My guess is the next stream will be in like three weeks roughly. Been doing like a three week schedule lately and I'll try to sort of stick to that. So yeah, I think, I think now that it all compiles and we have code, I don't think we have any like big to DOS now.
05:18:02.515 - 05:18:50.435, Speaker A: That's fine, that's fine, that's fine, that's fine. I don't think we have any actual like to do macros. So I think we wrote all the code or we wrote a first try at all the code at least. So I think I'm happy to sort of end it off there. Before I do though. Are there any questions about what we did today? I know that's a. The large question, but like things where you're still like, I'm a little fuzzy on how this piece fit into what we did, what we did with like the handles, like anything I can clear up so that you don't have lingering questions until like three weeks from now or six points from now or something sort at the tail end.
05:18:50.435 - 05:19:44.845, Speaker A: What would point in favor testing last would be good to see whether it actually works before memory reclamation gets implemented. So my concern with testing first is that the code may end up having to be re architected a bit to do memory reclamation. So the testing that we do may not buy us that much confidence. That said, if we run the tests and they all pass, that's great. That's obviously a big sort of point of value. But my concern is we add memory reclamation, a bunch of the code ends up shifting around. So even if the tests pass, that doesn't give us any indication of whether the new implementation is correct.
05:19:44.845 - 05:20:46.919, Speaker A: So we may end up having to redo most of the test. I don't think that'll be true, but I think these are really fairly orthogonal. Implementing memory reclamation is just the first stream on memory reclamation will be a new library that just exposes memory reclamation and then there'll be a third stream that brings that into what we've done here. So what we might do right, is implement hazard pointers just for a. For a like bresh of freth air actually breath of fresh air and then do like a. Then do a stream of writing tests for the non memory reclaimed one and then do one that we bring it together. Does the wait free queue require a wait free queue to be considered wait free free? How does contention get resolved for this queue? So this queue that we just wrote is itself weight free.
05:20:46.919 - 05:21:12.731, Speaker A: This queue, it is itself. It's weight free. There's no inner Q in this one as well. The simulation we wrote depends on a wait free queue. But this weight free queue is sort of weight free in and of itself. And if there's contention, basically what happens here is that every enqueue operation, so every. Every peak, right is obviously wait free.
05:21:12.731 - 05:21:40.365, Speaker A: There's no nothing that waits in here. Every try remove front never waits. It does call help finish, but help finish is guaranteed to make progress. So and in a finite number of steps. So it's wait free. The. The complicated one is NQ and what makes enqueue wait free is that it always just immediately stores that it wants help and then it tries to help everyone in its phase.
05:21:40.365 - 05:22:29.195, Speaker A: So if I enqueue, I must help others that are stuck before me in order for my thing to make progress. And therefore everything makes progress in these phases. All right, yeah, I'll push this code to GitHub as well and include that in the, in the video description where I ultimately upload it so that you have a place to actually see it. Yeah, there's, there's still some way to go. My guess is another two or three streams. Right? I'm imagining that porting hazard pointers is a stream in and of itself to a library. Maybe two, probably one.
05:22:29.195 - 05:23:18.271, Speaker A: A little bit hard to say. It's hard to say, but let's say it's one. Testing this is like half a stream. Depends how many problems we find. Half a stream moving all of the code to use hazard pointers or whatever the memory reclamation scheme is about a stream. So it's like two and a half streams left is my rough guess here. If getting a fork is not weight free, why does it not affect the weight freeness of the queue? It's a good question.
05:23:18.271 - 05:23:45.053, Speaker A: So the idea here is that every thread is going to have its own handle. So once all the threads have handles, all operations are weight free. Getting a handle is not weight free and the two are. It's important to keep them distinct because in general, getting a handle is something you'll do very early on. Like you'll just. It's sort of set up almost. It is initialization.
05:23:45.053 - 05:24:21.315, Speaker A: But in the main running of your code, you won't be forking like you fork is like you won't be forking the process, but you won't be forking your handles either. You won't be cloning the handles all the. All over the place. At least you shouldn't be. Oh yeah, so sorry. When I say testing is half a stream, I meant testing of the weight free queue, testing the whole thing. I don't think we should, we should probably not do until the memory reclamation is in place too for the reasons I outlined earlier.
05:24:21.315 - 05:24:48.235, Speaker A: So I think realistically it's like three streams until we're done. All right, thank you all for coming out. I hope this was interesting and I'll see you again in like 3ish weeks. And I'll let you know a little bit in advance whether we do hazard pointers or more of this. So long, farewell, auf Wiedersehen, goodbye. Ah, no, that's not what I want to do. I want to do.
