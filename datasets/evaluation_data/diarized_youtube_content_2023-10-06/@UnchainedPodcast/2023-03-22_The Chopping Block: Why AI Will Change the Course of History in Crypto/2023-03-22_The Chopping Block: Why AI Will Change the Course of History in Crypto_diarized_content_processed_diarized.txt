00:00:00.330 - 00:00:02.442, Speaker A: Not a dividend, it's a tale of Tupan.
00:00:02.506 - 00:00:05.162, Speaker B: Now your losses are on someone else's balance sheet.
00:00:05.226 - 00:00:07.754, Speaker C: Generally speaking, airdrops are kind of pointless anyways.
00:00:07.882 - 00:00:10.618, Speaker B: Unnamed trading firms who are very involved.
00:00:10.714 - 00:00:12.830, Speaker D: Alec ETH is the ultimate on this.
00:00:12.900 - 00:00:30.274, Speaker B: DeFi protocols are the antidote to this problem. Hello, everybody. Welcome to the chopping block. Every couple weeks, the four of us get together and give the industry insider's perspective on the crypto topics of the day. So, quick intros. First we've got Tom, the D five Maven and Master of Memes. Next we've got Tarun the Gigabrain and Grand Poobad Gauntlet.
00:00:30.274 - 00:00:50.294, Speaker B: Today we've got a special guest with us, Ilya, who is the founder of near. So I actually for today because we're going to be talking about AI. I got GBT Four to write some intros for Ilya and you tell me which one you like. First one is Ilya, the near network's noble navigator. Ilya, the brilliant blockchain builder behind near. Introducing Ilya near protocol's. Pioneering pilot.
00:00:50.294 - 00:01:08.718, Speaker B: Ilya, the founder and fearless frontiersman of near. That one doesn't rhyme with near, but that's fine. And then Ilya, the Near Protocol pioneer with a visionary vibe. Welcome to the show, Ilya. Chad GBT welcomes you as well. And then you've got myself, I'm Haseed, the head Hype man of Dragonfly. So we are early stage investors in crypto.
00:01:08.718 - 00:01:22.098, Speaker B: But I want to caveat that nothing we say here is investment advice, legal advice, or even life advice. Please see Choppingblock XYZ for more disclosures. I should also mention before we're kicking off the show, Dragonfly, we are investors into Nier Protocol. Tarun, are you also investor into Nier?
00:01:22.194 - 00:01:23.126, Speaker A: We are.
00:01:23.308 - 00:01:56.160, Speaker B: Okay, cool. So just so that we're all fully disclosed, ilya and us go way back. And so he's a good friend, and we're excited to have him on the show. Because in addition to all the crazy news that's been going on, there's been a lot of hype around AI and all the advances in machine learning, large language models. And we wanted to finally get a show to cut through some of the noise with somebody who knows about this more than almost anybody, especially in the crypto industry. But we are going to get to that. First, I want to get through some news because the last couple of weeks have been all about this banking crisis that's been going on in the US.
00:01:56.160 - 00:02:46.514, Speaker B: Things are now winding down and so we're going to take the first half of the show roughly, just talk about the news, and then we'll jump into a deep dive on AI. So let's update what's happened over the last week that's significant on the crypto side. So the first thing was that Signature Bank was one of the banks that was wound down and there was this broad discussion within crypto about is Signature being headshot because of its proximity to crypto and its crypto banking activities. So there was a report that FDIC went after it took over the auction process for Signature was requiring bidders to wind down the crypto business. This was reported widely and then crypto was like, oh my God, it's happening, operation Choke .2.0, it's really true. And then FDIC official denied in a public report that this was the case.
00:02:46.514 - 00:03:29.338, Speaker B: They said, look, that's not true. This is a bank, we want to get the best value for the bank. So any activities that are revenue producing, feel free to take them over. Then yesterday, which was Sunday, it was announced that Signature was in a purchase agreement with New York Community Bank. So New York Community Bank was announced, is going to be the buyer for Signature, although the deal is not fully closed yet. And New York Community Bank is winding down the Crypto business. Now was this off instruction of FDIC not only winding down the crypto business, meaning probably Signet, which is the time settlement system for Signature, but also they are going to be debanking the crypto clients.
00:03:29.338 - 00:04:23.360, Speaker B: So they're going to be asking the crypto clients, take your money, buzz off, go somewhere else, we don't want you here. So this again begs the question, was this FDIC kind know, nudge nudge saying, hey, if you want this, like, get rid of this crypto stuff? Or was this know in your community bank just not liking this thing and saying, like, look after Silvergate. I don't want crypto deposit money. It's not worth a headache, and it seems to invite more trouble than it's worth, one way or another. This seems to be an indication that whether it was FDIC directly or whether it was indirectly through the treatment of all the crypto banks and the fact that all this stuff is in the news now. And you can't help but notice that banks that bank crypto clients get a lot more attention from the regulators than they otherwise would like to. That this is causing this stigma around banking crypto clients that's likely going to continue in the US.
00:04:23.360 - 00:04:29.214, Speaker B: What were your guys'perspective on this story about Signature and the debanking of Signature crypto clients?
00:04:29.342 - 00:04:40.334, Speaker C: Yeah, I think part of it is definitely a narrative that is being propagated in a lot of press outlets with respect to oh, crypto caused these bank runs and crypto caused this banking crisis.
00:04:40.382 - 00:04:42.038, Speaker A: And I think there's not a lot.
00:04:42.044 - 00:04:57.914, Speaker C: Of truth to that. But obviously it's a great narrative to tell. I think we'll see what happens with SDB. Obviously SDB is still looking for a buyer. They weren't a big crypto bank. Crypto is a very small percentage of their business. But if whoever buys SVB also winds down their crypto business, there might be more to the story.
00:04:57.914 - 00:05:01.422, Speaker C: But this feels maybe a bit sort of isolated in some ways.
00:05:01.556 - 00:05:31.378, Speaker A: So I know there's actually some other big banks that started debanking crypto companies even before SVB's kind of case started. So that message was already there and kind of banking regulators, I think were already propagating that message. So I don't know if it's FDIC itself, but I think it's already word on the street that crypto banking kind of is risky and people should stay away from it on the banking side.
00:05:31.544 - 00:06:07.702, Speaker B: Yeah. So Nick Carter made this point when he wrote this blog post that's gotten at the time. It got some attention. Now the attention on this blog post has exploded where he describes this Operation Choke .2.0 which is basically this kind of full court press from the executive branch to try to put pressure onto anybody who touches crypto from the banking side and it's now really manifesting that we're seeing bank failures. This is a perfect time to paint crypto as a scapegoat, as you said Tom, what I'm curious about so one obviously there are banks that are starting to position themselves as being crypto friendly. Obviously these are mostly smaller banks because they can afford to take more risk.
00:06:07.702 - 00:06:45.930, Speaker B: Obviously as a small bank you're more like a startup and you're more willing to try to do something risky in order to win a big business. So Cross River is sort of effectively a startup bank that is doing this and there are a few others that have positioned themselves as crypto friendly. A lot of the banks in Europe are taking the opposite attack. Europe doesn't seem to be quite as aggressive. Ilya, I know you're based in Europe at the moment. What is the vibe you're seeing from the European side with respect to crypto banking? Is this purely a US thing or is this spreading to Europe as well? Because obviously with Credit Suisse and all this stuff obviously Credit Suisse has nothing to do with crypto but there is a broader banking crisis going on in Europe now as well.
00:06:46.080 - 00:07:08.070, Speaker A: Yeah, I mean I haven't seen anything on the European side per se. I mean Switzerland has been always very welcoming. I think they're even more welcoming right now as this is kind of unfolding. UK is also trying to set up kind of a better rules around crypto. They actually, as far as I saw, adding crypto on tax returns explicitly.
00:07:09.770 - 00:07:13.666, Speaker B: I don't know if that's being friendly to crypto wanting your pound of flesh.
00:07:13.698 - 00:07:32.426, Speaker A: From crypto as soon as you put like if you say hey, you should be paying taxes from this, that means everywhere else. This now is an accounting software, it's in all of the systems now. What rates are going to be taxing on? That's the other thing, but I actually think it's like a way to you're.
00:07:32.458 - 00:07:36.606, Speaker D: Saying that weed in many US states is the same as crypto in the.
00:07:36.628 - 00:07:41.134, Speaker A: UK basically is there tax returns on weed? No.
00:07:41.172 - 00:07:45.070, Speaker D: Now they ask you directly and a lot of state filings where it's legal.
00:07:45.890 - 00:07:49.986, Speaker C: Yeah, I think the UK is filing up to like 2019 IRS rules which.
00:07:50.008 - 00:07:51.678, Speaker A: Is like when it changed.
00:07:51.854 - 00:07:58.018, Speaker D: Do you think Switzerland is going to continue to actually be positive after this current stuff they have?
00:07:58.104 - 00:08:02.134, Speaker A: Yeah, why not? For them, what's the difference?
00:08:02.332 - 00:08:27.386, Speaker D: Well, I just feel like their banking sector is probably consolidating is probably means that the smaller banks don't. And like, the smaller banks were the ones just like here that gave a lot of the crypto company stuff, like, what was that bank seba SCBA or whatever that did a lot of the 2017 layer one ICO stuff. I'm just curious if you think they'll.
00:08:27.418 - 00:08:56.066, Speaker A: Survive because in Switzerland, they have an weird system where there's Canton banks, which actually can just have all their deposits and not lend. So and Canton is a really one business. They really want people to come. Like when we were looking for foundation, canton governments were emailing me, cold emailing me, which was like, I'm like, what government does that fascinating.
00:08:56.258 - 00:09:52.950, Speaker B: Well, okay, so amidst this broader banking crisis, there's been one story that's been dominating Twitter. Basically, Twitter just can't stop talking about this, which is a good friend of the show bology, who was on the show, I think, a little while back. So he has made a bet. And this bet is basically a million dollar bet, or a million dollar in USD terms. He's basically betting a million dollars to one bitcoin that essentially he believes that one bitcoin is going to go to a million dollars within 90 days, essentially because he believes that the US. Dollar is going to hyperinflate. So he's claiming very loudly and very aggressively on Twitter that the banks are insolvent, that the bank term funding program, which is the sort of liquidity injection that the Fed is providing to banks that need liquidity on some of their hold to maturity Treasuries and mortgage backed securities, that this program is pure quantitative easing.
00:09:52.950 - 00:10:16.738, Speaker B: And that in this banking crisis, which is going to extend everywhere in the world, all the banks or majority of the banks are insolvent already. The Fed knew it all along. They're going to hyperinflate the currency in order to kind of prop up the dollar system. And that bitcoin is going to go to basically infinity or a million dollars. And so, curiously, he's making two of these bets. So he's made one already. I think he's going to make another one.
00:10:16.738 - 00:10:46.730, Speaker B: Or has he already made it? I don't know. But so he's betting, I guess, 2 million USD that he's putting up against people's sort of Bitcoin in return. And it seems a little bit crazy. I've gotten a lot of people hitting me up and being like, hey, what do you think about this biology thing? Do you think he's right? Should I be worried? Should I take my money out of the USD? What's your guys take on this biology end of the world bet?
00:10:46.880 - 00:10:57.966, Speaker D: Maybe many observers have made the same point, but it's a great marketing spend. At the very least. Bitcoin's already moved up like 10% is.
00:10:57.988 - 00:10:58.126, Speaker A: It?
00:10:58.148 - 00:10:59.150, Speaker B: Great marketing spend.
00:10:59.220 - 00:11:35.574, Speaker D: He already owns more than 10 million worth of BTC, which is very likely. He's already sort of EV positive. I think the other thing is that this is the type of thing where it's directionally correct. Like, yes, there is going to be. I mean, just look at the overnight banking operations changes over the weekend for all the central banks coordinating to provide liquidity. That's very 2008. And I think directionally this is correct.
00:11:35.574 - 00:11:42.894, Speaker D: There is just like a ton of operation. There are a ton of operations that are basically quantitative easing, 2.03.0, whatever.
00:11:42.932 - 00:11:43.198, Speaker B: I don't know.
00:11:43.204 - 00:12:04.290, Speaker D: It depends on how you want to index it. There's a sense in which such a bet will cause if enough people glom onto it will cause things to move in that direction, even if it's negative EV. And right now, all he's doing is basically recouping costs. But as long as Bitcoin still goes up 10% to 20%, he's fine.
00:12:04.440 - 00:12:18.710, Speaker B: Okay, so you take a cynical view that you think that he doesn't actually think Bitcoin is going to a million, but he owns enough Bitcoin, that if he can meme a price rally in Bitcoin, which obviously Bitcoin is rallying outperforming everything else right now, that it pays for.
00:12:18.780 - 00:12:40.862, Speaker D: It's not just that it pays for itself, it's also that it's directionally correct. So even if it doesn't hit a million, he's always going to be able to be like, look, banks did hyperinflate. Even if I made the wrong bet, I got the right direction and I just didn't hyperinflate enough. It has a little bit of, like, I can fall back on that. It's not just like, oh, I got it wrong. Yeah, I got the magnitude wrong. I got the direction right.
00:12:40.916 - 00:12:41.438, Speaker A: All right.
00:12:41.524 - 00:13:12.954, Speaker C: Yeah, I agree with that. I think it's one of those things where even if you're off by a few folds, you can still sort of claim victory. It's kind of like people who thought, there's me a million people dead in the US from COVID in early 2020. It was like, okay, well, you got the time frame wrong and the numbers wrong, but you were sort of right in ringing the alarm bell. I think that's kind of what he's going for. But, yeah, I think I kind of disagree with his interpretation of what's happening with Btfp. And it seems more like I mean, we're talking about on the show, everybody's talking about it.
00:13:12.954 - 00:13:17.386, Speaker C: It's actually very difficult to get that kind of earned media for, like, a million dollars.
00:13:17.568 - 00:13:23.934, Speaker B: That is very true. Super bowl ads are a lot more expensive than that, and I don't think they have the reach of this as.
00:13:23.972 - 00:13:47.862, Speaker D: Much as I think Gabriel Laden, who found a limit break, is a marketing genius on the Internet. I'm not sure I would call the five to $7 million whatever spent on the Digi Daigaku ad on the Super Bowl very good, whereas this is like the most persistent marketing for a million bucks. If you compare it, it's unreal how good marketing this has been.
00:13:47.996 - 00:14:06.940, Speaker B: Yeah, very fair point, I guess. Yeah. The issue that I take with it is that, like, a million dollars is a 60 X increase in the bitcoin price, right? This is not a direction. Oh, bitcoin went up and now it's worth 35 instead of 20. It's like, oh, you're still off.
00:14:07.390 - 00:14:17.358, Speaker D: How many claims and marketing are like, we're going to improve your life by 1.5% if you buy this product? No, they're all like, we're going to improve your life by five X if.
00:14:17.364 - 00:14:18.142, Speaker B: You buy this product.
00:14:18.196 - 00:14:19.566, Speaker D: And this is just the same type.
00:14:19.588 - 00:14:23.962, Speaker B: Of 60 again, but 60 X is like it's just a big hurdle.
00:14:24.106 - 00:14:39.346, Speaker A: But anything else would not make people pay attention, right? Even if it was actually 100K. If it was 100K, people were like, yeah, it's kind of plausible. The bitcoin twitter would be, like, super excited. Crypto twitter would be like, that's interesting, and it would kind of die away.
00:14:39.528 - 00:14:54.582, Speaker D: 999,999 would not have memed, right? Like, you would not have got the attention and you would not have got the article. There's also this fact about choosing the number. It's like astrology, like choosing the right number somehow.
00:14:54.726 - 00:15:00.780, Speaker B: I guess I resist the Justin Sun kind of energy that you guys are ascribing to. Balaji here.
00:15:01.810 - 00:15:14.546, Speaker A: Again, it's a directional bet, right? Like, it's not 90 days, it's two, five years. But if the system doesn't fix itself right, there's, something going to break.
00:15:14.728 - 00:15:18.260, Speaker D: It's a little more like MicroStrategy meets Justin's son.
00:15:19.350 - 00:15:48.170, Speaker C: Yeah, I think it was Matt Levine was know, if you really think bitcoin is going to a million dollars, you should probably just spend that million dollars and buy bitcoin instead of buying bitcoin for a million dollars today. But it sort of misses the reflexivity of bitcoin as a market and sort of almost manifesting the price increase by sort of putting this idea into people's heads. There is sort of a core reason why you would see that kind of inflows, but you sort of create this meme around it and that can sort of self manifest.
00:15:48.590 - 00:16:36.774, Speaker B: Yeah, I mean, look, it's hard to know if it is. The counterfactual is hard to tell, obviously, at a time when expectation of interest rates are cratering and banks are failing, like, okay, yeah, that's good for bitcoin. So that meme was already happening before Balogy made this bet. So it's hard to tell how much of a lift was biology's bet and the earned press, as you guys put it. I mean, it does seem like it's having some impact because so many people are messaging me about it that I have to assume this is kind know, it's drilled itself into people's brains that like, hey, maybe you should be worried about this. The interesting thing is that unlike most of the rallies, all coins are not following. So it's really just bitcoin breaking way ahead of everything else, which maybe lends some credence because most of the rallies we've seen in crypto, especially around interest rates, have been pretty broad.
00:16:36.774 - 00:17:29.034, Speaker B: Like, everything kind of followed together. Now Bitcoin is really breaking away in the market and that may be again, it's hard to ascribe something like that and say, oh, well, this is because of this thing, and who knows? Markets are kind of crazy. So, okay, let's take this kind of showmanship of the bet aside. What about the claims in the bet? So one of the things that Balaji and a lot of people on Twitter are arguing about right now is whether or not the bank term funding program, which is the credit line that the Fed is offering to banks on their Treasuries. Is it QE? And if it is QE, how stimulative is it? And should we be thinking about this as, hey, the Fed is totally about face now. They're doing QE again, and we're basically going back to the kind of profligate stimulative monetary policy that we had for the last ten ish years.
00:17:29.232 - 00:18:02.600, Speaker D: If you asked someone in 2008 if this was the last failure, every time there was a failure, they would have said yes, right? There's sort of this like, you're living in the fog of war. You don't really know what's going to happen. Central banks seem to be incompetent at communication right now. I only say that because of this weekend stuff. I mean, did anyone watch the Credit Suisse press conference? That was a little embarrassing for the Swiss government. I'm not going to lie, that did not sound good. The media had some pretty hardball questions.
00:18:02.600 - 00:18:42.866, Speaker D: The banking officials, whether they were from UBS credit Suites or the Swiss government, were just like, yeah, we don't know, it's fine. We'll figure it like in a very non plussed way that did not give anyone any confidence. For instance, one of the biggest things people fought about in the US. Bailout last time was like, oh, do people get paid bonuses or do they get clawed back? And there was a reporter who asked, hey, it seems like you're not culling pay of any executives. Is that planning on changing? No, no, we just agreed on it. We're never going to change this. And then there was this huge uproar, and then they were just like, oh, well, maybe we'll change it.
00:18:42.866 - 00:18:57.954, Speaker D: And it's like, well, I guess they're making their decisions live at the press conference. It didn't really inspire confidence. And so I think this fog of war thing is also really true. So it's kind of hard to make such strong claims.
00:18:58.002 - 00:20:08.094, Speaker B: That's what I'd say with this banks refunding program. I think a lot of the assumptions people are making around it claiming that it's QE and it's kind of this broad paradigm shift in how central banks are approaching the like, almost all the assumptions I'm seeing about it. So Arthur Hayes had a piece who we just had on recently, where he talked, I think it was called Kaiseki where he talked about like, hey, this is actually kind of disguised QE and QE's back on and central banks are printing again. And I think almost every crypto take that I've seen about this that basically calls it QE, is assuming that this program is going to keep rolling over. The reality is that the difference generally between QE quantitative easing and this bank term funding program is that in normal QE, you buy assets from the market. You generally buy toxic assets and you often buy them at a premium and you just hold them on your balance sheet for some uncertain period of time. Like you might hold them to maturity or if they're like other assets, like mortgages or whatever, you might just hold them for a while and then eventually sell them back when the market stabilizes, right? But you're providing a lot of liquidity to the market and absorbing toxic assets for an uncertain period of time.
00:20:08.094 - 00:20:53.774, Speaker B: That is not what the bank term funding program is doing ostensibly, right? In this case, the program is explicitly for one year. It maybe rolls over for two years. But in this program, instead of buying the assets outright, what they're doing is they're allowing you to borrow against it. So they're basically saying, like, look going to be a pawn shop for you. You've got these toxic assets, I'll let you borrow a lot against it, right? I'll let you borrow it at par, even though the market value of these mortgage back securities or these Treasuries is down and I'm going to charge you 5%, basically I'm going to charge you the prevailing interest rate, right? So this is not cheap, this is not cheap borrowing. But you can borrow from the Fed and basically you have a year to kind of tide over your liquidity needs. And if at the end of the year you can't figure out how, how to make money or satisfy your depository base, you're done.
00:20:53.774 - 00:21:20.382, Speaker B: You're not going to make it. Right. It's okay. You can argue like, well, are they really going to let the banks fail a year from now if they didn't let them fail? Like what's really the mean? I would argue a lot of what the Fed wants is no shocks. They don't want shocks. They don't want sudden things to happen. But if these regional banks are just fucked, if they're just horribly mismanaged and they took a lot of risk and they can't get recapitalized and they need to get bought, then okay, you've got a year to figure that shit out, right? The thing that markets hate is uncertainty and volatility.
00:21:20.382 - 00:21:48.334, Speaker B: If you can smooth that volatility, that's kind of the purpose of a central bank is to smooth that volatility and basically say like, hey, let's give people a year to buy each other, acquire each other, reorganize their assets, make sure that we don't have sudden collapses. But if these banks are not viable they're not viable. And I don't think the US is fundamentally committed to never letting another bank fail. Banks do fail. It's not that uncommon for banks to fail. The problem is banks failing at the same time. That's the thing that the Fed doesn't want.
00:21:48.452 - 00:22:53.630, Speaker C: Yeah, I agree with your points with respect to the difference between QE and PTFE in terms of know, banks don't actually want to use PTFB unless they have an immediate liquidity constraint and the nature of and the scope of it is pretty limited. I think something that is somewhat different about Bolognese argument too, is that unlike eight or maybe even 100 years ago, is the proliferation of social media and basically the ability of that to instigate bank runs, in a way. That is an order of magnitude or two faster and greater than it was back then, which you kind of saw with SV, with all these group chats and all these people DMing each other, and basically, within 48 hours, everyone trying to withdraw their cash from the bank. And that basically creating an incredible strain and a need for liquidity and those tapping into Btfp, pushing more dollars into the system. I think that's kind of where he imagines the hyperinflation coming from. But I also think the hyperinflation thing is a little maybe melodramatic. Like I see a lot of crypto people getting excited about the million dollar bet and oh yeah, fuck the banks, fuck USD.
00:22:53.630 - 00:23:01.734, Speaker C: I don't think anyone actually wants to live in a world where the US. Is undergoing hyperinflation. It would be extremely bad. Everything would go to shit. Even if Bitcoin, someone needs to buy.
00:23:01.772 - 00:23:10.346, Speaker D: Enough Bitcoin to make biology's EV greater than zero on this bet, right? And those are the people who are buying mean.
00:23:10.368 - 00:23:50.166, Speaker B: Look, I do think that Bitcoin is going to do well, and I think it's plausible that this has I mean, I think it's obviously this program is not it's definitely stimulative to some degree, right? Obviously it's providing more liquidity, but it's not pure stimulus in the way that QE is. It's more subtle than that, but I think it is going to be good for Bitcoin. And I do take Bology's point that probably there is going to be more inflation. Obviously the Fed has to back off now and interest rates are pricing in. Know, the Fed is not going to raise rates all the way to like five and a half or whatever it was originally. They're probably going to back off before the end of the year and that's going to make it harder to down on inflation. So probably, yeah, inflation is going to be elevated in the US for some time now.
00:23:50.166 - 00:24:14.942, Speaker B: For those of you who don't know, hyperinflation is when you basically get into a reflexive inflationary loop where inflation just increases and increases and increases and there's no psychological way to kind of stop the expectation that the money is just going to get debased further and further. So tomorrow it's going to inflate more and the day after that it's going to inflate more. And so you just want to get your money out as fast as you can and nobody wants to hold dollars anymore more. That kind of event has happened before. It happened in Zimbabwe, it happened in.
00:24:14.996 - 00:24:18.542, Speaker A: Weimar Germany, happened in Ukraine in the beginning of night.
00:24:18.596 - 00:24:20.398, Speaker B: Happened in you. That's right.
00:24:20.484 - 00:24:25.858, Speaker A: I remember buying bread for 10,000, 10,0000 in a million within like span of a year.
00:24:26.024 - 00:24:26.754, Speaker B: Oh wow.
00:24:26.872 - 00:24:29.010, Speaker D: Nothing like logarithmic scales.
00:24:29.350 - 00:24:51.638, Speaker B: It does happen, right, but it happens mostly in failed states and it happens maybe once a decade or so. There's like a hyperinflationary event. The US dollar hyperinflating would be absolutely like just batshit crazy. This is not a prediction you should take lightly. As you know, the central bank is going to not lower interest rate or they're going to lower interest rates too fast. So it's going to hyperinflate.
00:24:51.734 - 00:24:56.780, Speaker D: This is also why it's very clearly a trade, not a real philosophical bet.
00:24:57.230 - 00:25:47.942, Speaker A: I think there's an interesting question of confidence of dollar. Right. Maybe nobody's counting this, but there's like a global confidence in dollar, quote unquote latent score that all the outside of US countries are putting confidence this is how much indexing their reserves into this, how much of the economy is kind of transacting in it and is the events that been happening going to help with this confidence? No. Right? So it's definitely going to reduce index, which means this assets like the central banks, the kind of economy the companies need to go somewhere. And given what we see with other countries, a lot of them have similar problems. Right. And so there's an interesting question of like yeah, what is the confidence? Let's just say it's not a zero sum, but that confidence need to go somewhere.
00:25:47.942 - 00:26:41.020, Speaker A: And so that's I think what at least if I was belagi that's, what I would be banking on is like, hey, here's an asset where you can put confidence in. The rules are known, the supply is known, a bunch of people already have it. It's so far still easy to come and go in it. And if a lot of people even put a fraction of their kind of value into it, it's going to actually be then hits its reflexive state where people see it going up and they're like, oh, actually, we need to go because Balaji said it's going mean there's like a huge reflexivity in crypto in and obviously, I mean bitcoin has less reflexivity just being bigger but just kind of kick started by world events. And so the amount of attention right now on this is huge.
00:26:41.810 - 00:26:58.658, Speaker D: Actually the reason I was laughing about this is my friend who's at OpenAI and maybe it's a good segue for our AI segment and he's been in AI stuff for maybe a decade. Okay, so my friends in the SF scene, especially.
00:26:58.744 - 00:26:59.134, Speaker A: EA.
00:26:59.182 - 00:27:15.378, Speaker D: People are actually getting freaked out by the biology tweets. I literally got that while Ilya was talking. So all I have to say is, amazing marketing, right? There's no way you get a million for a million dollars you get anywhere near this level of dispersion.
00:27:15.554 - 00:27:16.630, Speaker B: That is very true.
00:27:16.700 - 00:27:24.700, Speaker A: Yeah, somebody on Twitter said it's like, ballardy paid $2 million to be the main character for next 90 days on the Bluebird app.
00:27:25.150 - 00:27:33.900, Speaker D: And it seems to be working. I mean, I just think people who aren't even paying attention to crypto are suddenly like, oh, my God, what is this guy saying?
00:27:35.630 - 00:28:11.990, Speaker B: Okay, well, as long as we are kind of taking sides, let me just register that I don't think the US dollar is going to hyperinflate, but I do think Bitcoin is going to do well over the next 90 days. But let's take that segue and kind of transition over to second part of this conversation, which is about crypto and AI. So obviously, AI has been on the rise, chat, GBT, large language models, diffusion models. Everybody is going nuts now over this idea that AI is just the next massive technology wave. It's a platform, it's deflationary. It's going to change everything. So, Ilya, the reason why we brought you on the show is that you are kind of the natural person to talk about the intersection of crypto and AI.
00:28:11.990 - 00:28:50.454, Speaker B: So just a quick way of background. So, Ilya, you were originally at Google. You were actually working on TensorFlow. And you also were right before you left Google, you were one of the co authors on a very famous paper in AI. Called Attention is All You Need, which is the paper that introduced the transformer model, which is the model that is now used to train almost every large model in AI. It's basically kind of revolutionized, basically machine learning at a scale that we've never seen before. And so originally, before starting near protocol, you were starting a company called Near AI, which was an AI company, where you were trying to build a sort of GitHub copilot type thing.
00:28:50.454 - 00:28:54.278, Speaker B: Back in the day, obviously, you didn't have the resources that GitHub has.
00:28:54.444 - 00:28:56.342, Speaker D: They still own the domain, and pan.
00:28:56.396 - 00:28:57.682, Speaker A: Was very much intended.
00:28:57.746 - 00:29:58.154, Speaker B: I'm sure it's going to go for a lot more today than it did back in 18. And then 18 you pivoted into blockchain, and I remembered the coffee shop where we initially met, and you pitched us on the very, very v zero version of near protocol, which was totally broken and made zero sense. But you and your co founder Alex, were some of the most brilliant guys that we'd met working in the blockchain space. So I'd love if you could kind of talk us through what have you seen? So you were at Google basically at the seat of this, when we were just starting to turn the corner on a lot of the problems that were seemed to be insuperable back in 2018. 2017, right. People were excited about, oh hey, we've got things that kind of sound vaguely like humans, or we had all these back in 2018. Every single problem in AI, whether it's like syntax parsing or understanding natural language or machine translation, there were all these individual models that people were kind of training and fine tuning on individual problems.
00:29:58.154 - 00:30:13.920, Speaker B: And basically that is gone. It's not completely gone, but a lot of it has just been kind of blown away by the generality of these large language models. Talk us through, what was it like for you seeing that evolution in your time at Google and beyond since you came into the blockchain space?
00:30:14.290 - 00:30:53.774, Speaker A: Sure, yeah. So my journey in kind of neural networks, deep learning started actually. So I was playing with this stuff when I was in high school and I always thought it was interesting, but it didn't work. Right. The neural networks back then were basically just the basic classifiers that you could not use for much. And then I remember reading a paper, which was Andrew Aang and Jeff Dean training using a large at the time GPU cluster inside Google. And they were pre training model to look at the image and output image back and kind of create a representation inside.
00:30:53.774 - 00:31:36.934, Speaker A: And they found later inspecting it, that it had the neuron that would recognize cats. And so it's kind of called cat neuron back then. And that was for me was the signal that I need to go into this. So that's when I applied to Google research and kind of joined the team, because this was kind of the first time we did not teach machine to recognize anything specific. It wasn't a classifier, it was just for training and it found something that relates to human concepts at the same time. I always believed language is like images. There is thousands of species in the world that can actually navigate world, look at things, they have eyes, et cetera.
00:31:36.934 - 00:32:32.494, Speaker A: There's only one species in the world that speaks language. And so I always thought that language is the way to kind of actually train these models and get them to actually understand and reason and make logical conclusions and answer questions with all this. And so that's what I worked on now back then, state of the art was recurrent models. And recurrent models means you kind of one word at a time, pretty much read the sentence and you kind of process it. And so, first of all, it's highly inefficient, right? Second, because of kind of the way these models are trained, it's actually a very unstable model and so called kind of gradients explode when you try to train it. And so it was really hard to train them and even harder to put them in production. There's no recurrent models ever put in production kind of at Google scale because it would take seconds to actually query out of it and Google optimizes for millisecond response time.
00:32:32.494 - 00:33:31.194, Speaker A: And so what we would do is we would then take and dumb down these models to just words, independent words without order, and kind of train the dumbed down model that does not know anything about order in a sentence to then output the same prediction and then try to launch that. And we've launched a bunch of stuff like that. And so then there is a concept of attention came in. And so that actually helped a lot with kind of training these models because it allowed to bypass this recurrence, right, this kind of step by step evaluation and kind of look back into a sentence. Like, let's say if you're doing translation, look back into words in the sentence you were translating, kind of when you outputting the answer, and so that was the concept of attention. And then there was like few papers that tried self attention as well. And so combining all this, right, we knew that kind of bag of board models were actually kind of okay, without order, self kind of attention.
00:33:31.194 - 00:34:19.086, Speaker A: And then self attention was an interesting concept and we needed something that's way more efficient, right, to train this all kind of came in. And Jakob kind of our manager lead at the time, came this idea. It's like, why don't we try just not kind of putting them in sequence, but just processing altogether, but then attending back into the whole sequence and using that to output. And so for machine translation, for example, task outputting that. And so that's kind of how Transformers started. And the idea was I like to describe it for people who watched Movie Arrival, instead of like we usually say one word at a time, the aliens were outputting the whole sentence at the same time. And that's what that model learns to do.
00:34:19.086 - 00:35:00.730, Speaker A: It's way more efficient and effective, which means you can train it longer, more, and it's at the same time kind of the gradients, like the actual training methodology is more robust. And so that's what we had first prototype and that's where I left and kind of the team continued and gotten amazing results and published that. But the interesting thing is that model architecture started to work for everything else, right? They started applying it for images, they started applying it for other tasks and it just worked without changes. And that's where I think people started experimenting more and more. And why it's now, I think over 60,000 citations because now everybody's just leveraging that kind of as a basic building block.
00:35:01.150 - 00:35:07.310, Speaker D: I was only going to make one comment, which is you said there's only one species that has language, but they're dolphins.
00:35:09.170 - 00:35:13.540, Speaker B: Thank you, Taru, that was very useful interjection. Sorry, Ilya. Go on.
00:35:14.950 - 00:35:21.762, Speaker A: Yeah, next time we're going to do a dolphin to human language translation model. There you go.
00:35:21.816 - 00:35:25.746, Speaker B: Coming up next. All right, powered by near, but yeah.
00:35:25.768 - 00:36:11.506, Speaker A: I think and then the big change that happened is this idea of pretraining which existed. Like, we all pretrained a lot of models before, but they kind of applied at a huge scale, pretty much just feeding the whole Internet to this model, saying, like, hey, just predict an X word using this model, and then we can condition you and sample from it and kind of try to output. What would you if you had this scene? This prefix would produce. And that's what this GPT pretraining team kind of started to explode, really, because at a reasonable scale, that model started to actually create representations similar to that Cat Neuron example started creating representations of world knowledge and being able to make reasoning because it's seen so many times how people reason about things.
00:36:11.688 - 00:37:31.562, Speaker B: So I remember I was following Opening Eyes work from the very, very early days when they first with GPT-2 and then eventually, of course, GPT-3, which is the one that took the world by storm. And I remember being absolutely fucking amazed that with unsupervised learning just basically just feeding lots and lots of text into a model, that it could figure out such a wide variety of tasks that seem to be incredibly idiosyncratic, right? And so I think a lot of people internalize. There's this great essay by Rich Sutton called The Bitter Lesson where he describes basically the history of machine learning was lots of people trying to solve these individual problems and thinking that the way you solve these individual problems is you as a human being, as sort of like the architect of the AI. You have to encode into that particular AI idiosyncratic features of this problem, right? So like, oh, to understand a face well, there's like a nose and there's two eyes and there's some symmetry and there's all this other stuff that the model needs to know that we know about the world. And only if we encode that into model is it going to get anywhere near the right answer. And that's what a lot of old school machine learning approaches would embed kind of human known features of the problem into the problem. And what we've learned over time, especially within the last like three to four years, is that that just doesn't scale the thing that scales and gets to the real state of the art for most problems that we have today.
00:37:31.562 - 00:38:26.750, Speaker B: Obviously there's some extra juju fine tuning that we do for most of these things. But most of the way that you get to these world scale today is just throw fuckloads of training at it. Just like lots and lots of data, lots and lots of training, lots and lots of money. And if you just do that enough for almost every category of problem that we can think of, the machine figures it out way better than we can. And in a way, the human kind of symbolic craftsmanship just ends up being actually worse than just raw data. Input, which seems to belie an anxiety that a lot of people have now about AI, which is that, okay, now it seems more and more that the AIS just kind of need us to sort of like shovel it oil, which is data, right? Like, yeah, there's some fine tuning, there's some extra, okay, maybe we tweak the fucking training algorithm or whatever. But for the most part, we just need to generate lots and lots of training data.
00:38:26.750 - 00:39:15.518, Speaker B: And the more training data we generate, especially with chat GBT and reinforcement learning, with human feedback, a lot of it is just like the way that these models are getting better and better is they already have huge corpuses. They already have all the writing on the internet that we're feeding to it. The big thing is that we just need to train it to not lie to us, to be kind, are friendly, to follow instructions. We have these gigantic eleven dimensional monsters and we're trying to use just raw hours of human training to make it nice and be friendly. And so more and more I'm seeing this nervousness from people about this new state because it was beautiful before, this idea that like, oh, we teach it about the nose, and we teach the eyes, and then the machine figures it out, and it's like, no, don't tell me anything. Just give me lots and lots of pictures and I'll figure it out. I don't really need you.
00:39:15.518 - 00:39:18.110, Speaker B: I don't know what your perspective is on that.
00:39:18.180 - 00:40:01.594, Speaker A: Yeah, I think that transition was happening for the past ten years. Again, that paper was in 2013 that, hey, you don't need to handcraft things. It's a basic model, right? Back then it was just a convolutional network. Now it's just transformer and it will build representations that it needs to solve its task. And then these representations are actually extremely useful across many, many tasks. There was this thing, Embeddings, which still exists actually inside GBD models and everything else, which represents pretty much meaning of the word. It's like 100, 200 numbers, and these numbers represent meaning of the word.
00:40:01.594 - 00:40:52.394, Speaker A: And people were training the model to get the symbeddings and then using the symbeds in a bunch of other tasks. That was like, we were doing that in 1516 and it was extremely useful because it would capture lots and lots of different dimensionality of our world without even us teaching it anything. And then we could use that dimensionality to then decide, oh, is this a city or the person? It can be like, is this some words that mean similar things? Et cetera, et cetera. And I think this is just kind of continue expanding, but we should remember, this is still tools. This is not a thing that has like a I want to do this. This is like a tool we give instruction to and it does things for us. And so I think it's important to kind of understand at the base of it.
00:40:52.394 - 00:41:49.274, Speaker A: It's a thing that it ingested all the world knowledge. It has the common sense. Now, it has some resemblance of logical reasoning, although not always correct, but it's a tool that we kind of feed the input to produce output. Now, it's a really powerful tool and kind of the way people will start using this can be extremely dangerous, right? That's why teaching it not to do bad things is good, but people keep finding ways to kind of go around the teaching, right? And they close the one level. Now people pretend you're someone who is pretending that you're someone that doing something right, and that now jail breaks kind of system. So it's a tool that people will be using for things. And so we should look back again at people and how this can be used and what things that people usually do as tools, good or bad, and just magnify that by kind of the abilities of the systems.
00:41:49.402 - 00:43:15.494, Speaker D: So actually you brought up Sutton before. Sutton is sort of a famous author in that he sort of coined the term, to some extent, reinforcement learning, which in the 80s. But one of the reasons I think people missed the sort of like, hey, we can throw more parameters, and it'll eventually we'll figure itself out, is that it's not just the idea that we were encoding, like, hey, we need these features that are human interpretable, like a nose. But it's also that statistical theory still to this day doesn't justify over determined systems like this, where it's like, hey, we have way more models than even data points by orders of magnitude, and there's really no way to know if you can ever have something that's stable. Like, if I throw in one new data point, it doesn't completely destroy the model. But the last sort of ten years have been a resounding set of examples that, hey, these models are sort of robust in a way that basically none of the existing literature could ever describe correctly. And I think to some extent, maybe the limits of such models that are overdetermined is that you can't really stop them in the sense of, like you can't really figure out what types of constraints to put to avoid.
00:43:15.494 - 00:43:41.662, Speaker D: These types of jailbreak scenarios precisely because you're like, okay, well, we're willing to just have way more directions to search in the model space than there are actual possible queries. And so there's always sort of some way of getting to whatever outcome state you want and sort of the opposite philosophy of crypto, which is like, how do we restrict the number of output states quite dramatically.
00:43:41.806 - 00:44:29.470, Speaker B: I mean, what I've seen from OpenAI is that they seem confident that if you just do more and more reinforcement learning, eventually you will get it to sort of enclose that output space more and more such that you kind of find these nooks and crannies that people are exploring by trying to jailbreak the system or get it to tell you how to hotwire a car or tell know how to hack a bank or whatever. All these things that people have managed to get chat GBT and Bing and Bing's real name, Sydney, get Sydney to tell you how to and like, the reality is that we're at the very infancy of this stuff, and it's only going to get better, right? It's going to get better. It's already gotten better insanely fast, and no doubt that is going to accelerate as people realize the economic value that is going to be unleashed with all these large language models.
00:44:29.630 - 00:45:20.158, Speaker A: I think this is where it's interesting to think about. So OpenAI went from, hey, we're going to build it and open source it, and everybody can use it to like, hey, we're going to control it because we're afraid of how people will use it, right? That's really kind of the transition. And Ilya so Skevor actually mentioned that, hey, I was wrong. If you have such a powerful tool, would you really give it out to everyone to leverage? And this is where I think, coming from a crypto blockchain, web3 perspective, and honestly, open source, like, I've always been doing open source in my life, open source always wins. There is no so far, like, products that in long enough term, open source did not take over. And I think the only one so far is search. And this may change, actually, because of these models.
00:45:20.158 - 00:46:07.620, Speaker A: And so the reality here is, yes, open source will be lagging maybe like one model, like one year of modeling behind. But for anyone who is following OpenAI footsteps right now, it totally makes sense to open source it because they get so much street cred for doing that, and they don't lose anything because, well, OpenAI has kind of theirs not, and we already see a bunch of them are open source. Some of the models you can run on your laptop, that reasonably powerful. They obviously not near GPD four or GPD even 3.5, but they're starting to get there. And the reality is, it doesn't matter what Open AI does to train it. There will be models that will be used in all kinds of ways.
00:46:08.070 - 00:46:11.554, Speaker D: Did Facebook get street cred? That's my question for you.
00:46:11.592 - 00:46:39.382, Speaker B: Yeah, I was about to bring that up as well. So Facebook, they invented this model called Llama, which is much smaller than, in terms of the number of weights than GBT three, GBT four, we actually don't know. Now, opening, I won't even tell us how big the model is, but GBT three we know is, I think, 175,000,000,000 parameters. And Llama, they release these models that are significantly smaller. The smallest one, I believe, is 7 billion, which is enough that you can run on your laptop.
00:46:39.526 - 00:46:42.346, Speaker D: And they showed mobile phone even.
00:46:42.528 - 00:46:43.594, Speaker B: Oh, wow.
00:46:43.792 - 00:46:44.982, Speaker C: Yeah, someone has a run.
00:46:45.136 - 00:46:45.886, Speaker A: Yeah.
00:46:46.068 - 00:47:59.942, Speaker B: Oh, wow. Well, they demonstrated that actually instead of blowing up the size of the model, if you just train the model for longer, actually you can get significant increase in performance that approximate a lot of what you get from something like a GPT-3. But not only that, there was a more recent paper that came out from Stanford called Alpaca that showed that if you basically use in conjunction the outputs from a bigger sort of more robust, better trained language model, you can actually approximate that model really, really well. As long as you have kind of a big enough model that you're training on. And so you can sort of imagine, like, the kind of gigantic blob big brother GPT Four training, this little llama model running on your mobile phone, actually can get your mobile phone to really closely approximate the big monster, which is, like, surprisingly cheap, I think they said, roughly on the order of, like, 100,000 input output examples, which is crazy. Which basically means that the edge of having a gigantic model that's, like hidden behind a wall and that nobody can access and the weights are secret and the size is secret, that actually that mode just might kind of melt away. If in fact this kind of sort of coopetive training can be done at scale because you never know when you're talking to somebody.
00:47:59.942 - 00:48:08.358, Speaker B: Is this person training another model to try to steal my internal knowledge? And that might really change the economics for how these large language models end up interacting with each other.
00:48:08.444 - 00:48:53.846, Speaker A: Yeah, so this is actually exactly what we were doing to launch staff at Google. We would take expensive model and then we would train a way cheaper like bag of words or whatever model just by feeding the input to the bigger model and kind of training the smaller model and output. So this is like distilling or there's a few different terms how to calling that. And that's part of the reason. The other part, yeah, that you just can query out kind of information out of these bigger models even if they are closed source. So that's why I'm like open source will win. Right? It's like we'll get this out and smaller models can approximate very closely indeed this so I think the mode there is again, it's people multiplied by compute, multiplied by data.
00:48:53.846 - 00:49:49.400, Speaker A: But I think the most interesting mode is actually product. It always was, right? Like at the end, if everybody believes chat GPD is the main thing where you find kind of best state of the art, everybody goes there, everybody talks to it, everybody feeds data to it. That data then kind of improves the models that becomes kind of still ahead of everything else because they just don't get this flow. And that's what Google being, google being not compared to Bing and other search engines because it became kind of state of the art. People kept feeding it queries and clicks and these queries and clicks then fed back into improving the model. And so there was no way to kind of turn that around again unless you completely change how this thing is kind of interacting. And so I think the interesting mode OpenAI has is in product land, not in model architecture or purely data.
00:49:49.400 - 00:49:52.474, Speaker A: It's in this feedback loop that they now built.
00:49:52.592 - 00:50:11.630, Speaker B: Okay? So I think this is a good place for us to bring it back to crypto. So obviously AI is huge. Everybody's thinking about it. And so naturally, given how Hype driven crypto is, there's a lot of people who are now trying to take the two and mash them together and see what is there something that we can do with crypto or blockchains to enable AI?
00:50:12.130 - 00:50:35.830, Speaker D: To be fair, every cycle has had a lot of, oh yes, non reputable scam versions of this. But I think our goal in this conversation is to focus on the actual real ones, not the marketing. Just as a disclaimer to anyone who is listening, who is like, oh, you missed AI coin. And it's like, well, AI coin's git repo is null.
00:50:37.370 - 00:51:12.786, Speaker B: Okay, there are a few threads that I think we keep coming back to. And to be clear, this isn't just with the advent of Chat, JPT and these large language models. I remember when I first started getting into crypto investing in the 2017 2018 cycle, there were also a bunch of AI hype driven blah blah blah type blockchain projects. But I think we keep coming back to a few core ideas. And Ilya, I want to get your take on what you think about the intersection of crypto and machine learning. So in three years in particular, I think that get the most attention. The first one is sort of private machine learning.
00:51:12.786 - 00:51:56.962, Speaker B: Whether it's like using zero knowledge or fully homework encryption or multiparty computation, one way or another, finding a way to make machine learning training happen in a way that is privacy preserving. The second is decentralized training. So obviously you've got all these companies spending huge amounts of money on training these models. Is there a way to decentralize that and do a kind of peer to peer folding at home kind of thing? And then the third is, what if you just put the fucking model on chain and you do inference on chain, which obviously is you wouldn't want to do training on chain because training is super expensive, but inference is somewhat cheaper. So does it make sense to just put models on chain and query them that way? Which of these approaches do you think are the most interesting and why walk us through them if you can.
00:51:57.096 - 00:52:50.674, Speaker A: So let's start maybe with the current state. Right, so the current state is these models are trained on supercomputers, which are built out of a purpose built hardware, which is called Nvidia A 100. And there's a new version which is H 100 or TPUs at Google or there's like few other, like terranium and few others in other organizations. But kind of generally speaking, when it says Nvidia GPU, it's not the GPU you have in your play game box to render graphics. It's especially designed for AI training GPUs, they cost $30,000. So you usually use about for GPT-3 ish model, you would use 1000 of them for three months. So this is about a million dollars worth of kind of cost to train a model like that.
00:52:50.674 - 00:53:55.830, Speaker A: So there's not that many kind of companies right now that can afford this. And so this GPUs, 1000 of them are interlinked with kind of very high speed connectivity. And when I say very high speed, this is like 600, 700gb/second. This is faster than connectivity between the GPU itself and the local Ram, like the memory on the computer itself by almost order of magnitude. So it's literally easier to send data to another GPU than to sit and recompute something later than to save things and load it back. So that's the current state of how these models are trained. And so when somebody says like, hey, let's do decentralized training on a network that barely maybe can push like 1, we're talking about 700, 800gb/second, we're way more orders of magnitude away from this actually happening, right? And plus people generally don't have this kind of hardware.
00:53:55.830 - 00:55:06.910, Speaker A: Let's say people have leftovers from the GPU mining that we're doing for ethereum. Well, that's like orders again of magnitude far away from what usually these chips are used right now. So can you do decentralized training with the current setup? With the current training, the answer is nothing closely to what you need to train any of these models for real, right? And again, we're talking about, let's say again, GBD three, because we know the parameters, 175,000,000,000 parameters you need I think like 2030 GPUs, 32 gigs of Ram each to just store that model on this thing, right, and then being able to pass through it. So that's just coordinating that, making that is just so not effective. And so anybody who is doing this really right now will not be using anything. They don't even want to use custom hardware that doesn't work with either XLA or Nvidia right now. Because if you're betting on custom hardware or custom setup, that's not kind of in the common.
00:55:06.910 - 00:55:53.706, Speaker A: It's just something that nobody who is right now rushing to build better models and kind of outcompete each other will be doing because they're willing to spend more money even if it's to just be in the same kind of setup that is reproducible and doesn't have risks. So I think that's probably the main when everybody's like, hey, let's do decentralized training, let's do private training similarly, right? Well, private training is done either on secure enclaves which don't have any accelerators MPC possible, but you're adding a huge kind of overhead on just like computing parts aggregating, it's going to be. Probably ten to 100 times lower. And you still need the same level of compute.
00:55:53.738 - 00:55:53.934, Speaker B: Right.
00:55:53.972 - 00:56:47.146, Speaker A: You'll still need and synchronizing between different clouds, for example, will be huge. So I think at the end where I think with if we call decentralized training, what does make sense right now is this marketplace of hardware itself, of the supercomputers is completely closed. If I want to train something, and I do have a million dollars, I need to call up Amazon, I need to call up Microsoft, and I need to call up Google, maybe like few other organizations and negotiate with them a rate, right? And so I think what Blockchain is really good at is opening up Marketplaces. And so what we can do is opening up Marketplaces as a supercomputer so you can kind of have a better price discovery on where is the supercomputer and having better resource allocation, like kind of auctioning off this compute around the world where people are building these clusters.
00:56:47.338 - 00:56:59.666, Speaker B: But like you said, the kinds of GPUs, GPUs in name only, the kind of GPUs that you use to train these models are not consumer GPUs. These are not things that people running a node at home are going to.
00:56:59.688 - 00:57:12.438, Speaker A: Have and you cannot actually buy them. So you need to be a large bulk buyer. I don't actually know what's a minimum buy, but Nvidia will not sell you. Can I buy a couple?
00:57:12.604 - 00:57:18.360, Speaker D: We can back estimate this based on the fundraise sizes of Adept, and.
00:57:21.050 - 00:57:48.740, Speaker A: I don't think they actually bought their own hardware, though some of them claim they will. I only know one startup that has access to this. Everybody else is like literally, you need to be a billion dollar cloud level to buy this. I know only Lambda which is able to buy them from Nvidia. And they've been doing this, I've known them doing this buying GPUs for past almost ten years. That's why they probably have the access.
00:57:49.190 - 00:58:26.430, Speaker D: I will say that having seen a bunch of these fundraising decks, between stable diffusion and Adept, 90% of their fundraising decks said like 80% of our funding is going to build our own clusters. And they actually are really trying to convince people right now. So what I'm saying is Nvidia is not going to be like, oh yeah, this billion dollars of money raised, we can't sell to that. I think they're just going to probably charge them more, and it's pretty clear they're going to charge them a lot more. But my guess is the minimum order size is 100 million, roughly based on the probably.
00:58:26.500 - 00:58:33.620, Speaker A: Yeah. And remember that it's like you buy GPUs and you need to buy all the other stuff. Like there's networking stack for sure. Yeah.
00:58:35.430 - 00:58:43.266, Speaker D: I agree. The SLI InfiniBand stuff itself is probably as much as the GPUs, but you.
00:58:43.288 - 00:58:45.874, Speaker A: Need engineers who then can maintain all that.
00:58:45.992 - 00:59:19.134, Speaker B: Okay, all right, so TLDR anything that's going to be an impediment to training either one, you don't have the money. Or two, even the kind of machines you would need to train in a decentralized way are so expensive very few people have access to them. So I think blockchains work well when you're coordinating resources that lots and lots of people have and the distribution of these resources are very decentralized. This is not the case for a 100s from Nvidia. Very few people have them. And so you don't need a blockchain to coordinate them. Just go call up the three big cloud guys and they're the ones who have all these.
00:59:19.252 - 00:59:37.022, Speaker D: It won't stay that way though. There's no doubt that someone will actually try to break the monopoly here. And obviously AMD has tried and has failed so far, but I think there's going to be a day that some of these other accelerators are good enough and they're cheaper.
00:59:37.166 - 00:59:51.914, Speaker B: Right. But it's hard to imagine that it's not going to be the case that the most cost effective, like the most kind of energy efficient and cost efficient approaches to training are going to be basically gatekeeped by the people who have the economies of scale, but maybe not.
00:59:51.952 - 00:59:53.366, Speaker D: For these distilled models.
00:59:53.398 - 00:59:53.594, Speaker B: Right.
00:59:53.632 - 00:59:59.142, Speaker D: If you're bootstrapping off of just like training a smaller model off the open AI API.
00:59:59.206 - 00:59:59.820, Speaker B: True.
01:00:00.350 - 01:00:15.166, Speaker A: Yeah. So if we're talking about smaller models, it's totally a different story. Right. And you can potentially get a server with few GPUs kind of consumer grade and train it and people doing that. Right. Like researchers. Yeah.
01:00:15.188 - 01:00:44.842, Speaker B: So the day that you have a model on your phone that approximates one of these large language models fairly well and you can do fine tuning of that model through some cloud GPUs that maybe are not quite sort of state of the art grade, is that a case where you think that okay? In this kind of situation you can imagine having some kind of GPU marketplace and maybe there's enough demand there for this kind of consumer level fine tuning of these kind of miniature models that the economics can work. What's your take?
01:00:44.976 - 01:01:26.710, Speaker A: I think the question is if it's enough to have few GPUs, right. Getting them off the cloud is actually pretty easy. Or getting like buying a server with four GPUs plugged in, kind of what's the reasoning? Right. It's not that you need it, generally speaking, to be decentralized. And so if it's a rentable resource and there'll be a ton of demand and clouds are not able to satisfy that demand, that's when it can start spilling over and people buying this, maybe servers can offer it for rent. That means clouds need to really not satisfy the demand or start censoring someone from using it. Right.
01:01:26.710 - 01:01:30.486, Speaker A: That's the only reasons why this movement would start.
01:01:30.588 - 01:01:31.058, Speaker B: Sure.
01:01:31.164 - 01:02:36.634, Speaker C: I was going to say I think compute marketplaces and obviously decentralized compute, verifiable compute has been like kind of a part of it's been a meme in crypto for a long time. But the other meme, right, has been owning your own data and data permissioning and data marketplaces. And I feel like that's been the other point of contention or sort of debate with these LLMs is they're ingesting images and text from the Internet, training these models on them, and specifically for diffusion models and image output. Artists feel maybe deceived or hurt that their work is then being used to train these models that they don't really see any benefit from. I believe Getty Images is actually suing openair, stable diffusion for basically training their models on Getty Images, even though they're not necessarily licensed to do that. I'm curious to get your thoughts on this crypto meme of owning your own data, making people pay you or advertisers pay you to actually access your data and train on it. Is there sort of a new life to this idea with sort of the rise of lOMs? Or is it still just kind of impossible to actually do this in practice?
01:02:36.762 - 01:04:27.634, Speaker A: Obviously, being on the crypto side, I want this to work, right? But practically speaking, we still don't have tooling to do that, right? And so I think it was stability that took out whatever that content was trained the models and they were pretty much kind of same quality. And so, generally speaking, it's like unless we actually flip the script and actually start creating provenance for all the content we create, that is cryptographic, that is potentially also enforced by law that you need to kind of have include provenance as you kind of process this content further, we will not be able to by the current systems we will not be able to use this data kind of data belongs to users with this limited I would say law kind of regulatory enforcement. So I think this goes back into like what we need to start doing is that the content that we produce and I mean AI as well, but especially humans produce, needs to be cryptographically authenticated. It needs to have provenance, it needs to be leveraged. And I think this actually will become even bigger problem because these models are kind of effective tools to create insane amount of content, right? And so one of the kind of core issues is that all of the kind of societal systems actually run on language, right? They run on language. You file things with language, you read news, you you look at what candidate, you know, their platform is, you know, or video of that. And so all of these systems are highly kind of susceptible already to manipulation, right? You don't need AI to manipulate them.
01:04:27.634 - 01:05:07.294, Speaker A: People manipulate them all the time. AI just gives you this extreme kind of leverage to create this. And so you can be reading a book which literally has all the same characters and all the same kind of overall story and completely different narrative, and you will not even know that, right? You can be going to the website and seeing the same titles, the same author, the same everything, and completely different narrative. And so that's already possible to do now to kind of create this deceptive content. So we need authentication path for everything. Otherwise we're going to actually live in this. Like, everybody will see its own version of reality that's completely different from what you think you're putting out.
01:05:07.412 - 01:06:07.506, Speaker B: So interestingly. I remember when GPT-3 first came out and certainly chat GPT, people started really worrying like, oh my God, how will we ever know that a human being wrote something? And then with stable diffusion, it's like, oh, how will we ever trust an image ever again? And obviously when we have good video models as well, people are going to say this about videos, how do I know that's Barack Obama making out with Mitt Romney or whatever? How do I know that's real? And I think a lot of these things are a little bit of an overreaction. We've had Photoshop now for like 20 years and things are fine. Obviously, photoshop does affect things. Fake media does end up getting going viral sometimes, but for the most part human. It's not like civilization has collapsed because Photoshop exists, right? We find ways to figure out chains of provenance and authentication of what's real and what's not. And I think we're going to adapt because that's what society does.
01:06:07.506 - 01:07:03.026, Speaker B: Society adapts to technology, period, every time it does. That being said, I do agree with you that we do need to have better authentication of raw inputs that come into society, right? So one of the most obvious things is how will we know that an image that comes from a camera is actually a raw image from a camera and has not been manipulated, right? And so if you have some kind of physically unforgivable signature from the camera itself that verifies that this image was taken from a camera. And maybe there's a small number of transformations that were applied to this that are not manipulative. The color was tuned or it was cropped in such and such way. Like already I think we're starting to see hardware that can come. I'm sure that we're going to see this with video as well as just cameras and audio as well. You can verify this thing came from the real world and it was physically produced and we can have some certainty of that.
01:07:03.026 - 01:07:45.634, Speaker B: And maybe someday your browser, when you right click on something, it'll show you like, oh yes, this thing came from a Nikon D seven blah blah blah, whatever on such and such date. That I think is the path that this stuff has to go in order to coevolve with the speed at which generated content is going to compete with real content. And I think it's plausible that blockchain crypto is going to have some role to play in how that information gets authenticated, stored, tracked, et cetera. Or maybe it'd be way simpler than that. Maybe you just hit the Nikon API and your browser just knows, like the Nikon keys or I don't know, something like that. What's your take on this question of physically verifiable data?
01:07:45.832 - 01:08:20.010, Speaker A: Yeah, so I think some cameras already do that. Like there's a secure enclave that signs photos on some cameras. I think like, Sony added that and a few others. And there's like actually metadata on Nikon and everything, I think similarly needs to happen for, let's say we record this video, we should all cosign on it that indeed this is a video that we produced and we talked about. Right. So things like that, we kind of need to make this almost like a new normal, a new habit. But I totally agree.
01:08:20.010 - 01:09:07.526, Speaker A: We will adapt. We have all the tools. It's not like an unsolvable problem. We just need to do it. And I think the more things that will be breaking, the more we'll be fixing, the faster we'll be fixing them and kind of introducing new habits around us where again, our identities have cryptographic information. And so you can co sign directly on YouTube that, hey, yeah, yes, I recorded this video, or I participated in this video, or this quote is mine inside a newspaper article, right? And so that kind of creates this prominence that then browsers show robustly. I think the important part though, that some people were like, oh, we should enforce fingerprinting in the output of the AI models.
01:09:07.526 - 01:09:21.822, Speaker A: I think that part is just not going to happen. People will always go and remove that fingerprint in the code and just do it without. And so I think authenticating content as a source is the right way to do it.
01:09:21.956 - 01:09:33.380, Speaker B: Okay, so we've talked about the things at the intersection of crypto and AI that maybe don't work or are not likely to work anytime soon. What are you bullish on at the intersection of crypto and AI? What do you think is going to work?
01:09:33.830 - 01:10:24.786, Speaker A: So I can say one thing that's been working is data collection. So data collection in general is how to get a lot of people to contribute data for some income, some reward. Right? And this is literally what blockchains are really good at coordinating a bunch of people, doing some work. 1 may call it proof of work. And so actually on near, there's been near Crowd, a community built project which been running for past two years where 1000 people every day has been working and labeling data like various tasks and creating a massive data set from that. So that part is, I think, very straightforward. It fits micro payments and kind of coordinating people, kind of a marketplace of tasks and people.
01:10:24.786 - 01:10:55.290, Speaker A: And so that works really well. I think that kind of continues scale and used in more different ways. Because you can introduce that as part of some experience, right? You can introduce because as Tom mentioned, data has value. And so as people do something they can receive reward for that data then flowing back into the model. But then it does need to be fully authenticated and kind of on chain for that to happen. So I think that works. I think there's interesting examples of these models that can run on your device.
01:10:55.290 - 01:11:38.486, Speaker A: So like more on edge computing that are applied to your data and so that will be interesting kind of again more in the conceptual. Web Three, not like specifically crypto world is starting to have a personalized model that fine tuned on your stuff without leaving this and for that you don't actually need that much compute, right. You probably don't have millions of data points anyway, so you just kind of run a few backprops on that. I think there's interesting question. I've always been excited. And that's why we were doing near AI is on coding. There's an interesting intersection of decentralized data that belongs to users.
01:11:38.486 - 01:12:49.826, Speaker A: Decentralized services, meaning they're open, accessible, they're not going to disappear. And coding, which if we think of this end user coding, where they're not going to probably build like complex stuff, but they can mix kind of by describing what they want, they can mix existing services and existing data. And it's really hard to do that in Web Two because the services are closed, their APIs are not always known, the source code is closed like all those things. Here in web3 we actually have everything open and so saying like hey, can you build me a front end that combines ave, compound and uniswap and creates me a ten X leverage? Right, that's actually possible now to do because the smart contracts are public, kind of all the services to pull data public and so it can create that front end for the user at the moment pretty much kind of in a custom way and I'm really excited about that. That kind of is a vision of what we were trying to do originally and I think that kind of going to attract a lot more attention as well to how people interact with services because now there's UI problems may disappear or maybe reduced as well.
01:12:49.928 - 01:13:45.586, Speaker B: So I really like that and it augments a lot of the way that I view the intersection of crypto and AI which is that they may not like the way that crypto and AI intersect in my view is probably not going to be that. There are going to be large tokens that you can invest in that are going to make a lot of money that are the AI tokens and those are going to pump, right? Obviously right now there are a lot of AI tokens that are pumping as the AI trend is getting more and more exciting. But I think the two are interlinked in more subtle ways. One of the examples you mentioned is just the fact that obviously as code generation and AI has become better at writing code and building front ends. That's obviously going to be good for crypto because crypto will have better front ends and programming will become more efficient and cheaper. And eventually you're going to have, I mean, already there have been examples of chat GPT sort of quote unquote auditing code and finding common vulnerabilities. So I think all these things are accretive, right? They sort of make human beings better.
01:13:45.586 - 01:14:19.726, Speaker B: And making human beings better makes blockchains better because blockchains are made today by human beings. I do think I wrote a tweet from about this a little while ago. One of my thesis about the intersection of crypto and AI is, I think, a little bit more forward looking, which is that today I think you mentioned this earlier, Ilya, is that most of these models, almost all of them that we're interacting with, like the large language models, we sort of make them kind of look and feel like people. But it's a bit of a sleight of hand. Right? These are not actually agents. They don't have any kind of persistent preferences or desires or anything like that. We sort of make them pretend to because that's what human beings like.
01:14:19.726 - 01:15:17.946, Speaker B: But eventually we are going to have more agentic models that have long term memory, that are going to be goal directed and are going to try to be doing things in the world. And when we have those kinds of AIS, and they're so far away, I think to have them realistically beyond just like video game environments, when we have those kinds of AIS, I think those AIS are going to want to solve problems that involve shared resources and we know how to solve problems that involve shared resources. We use money. Money is the way that we negotiate access to shared resources. Whether it's a message bus, whether it's turning into a lane, whether it's asking somebody to do something for you that's easier for them than it is for you. The way we solve all those problems, a huge category of problems is with money. And so AIS are going to want to use money, but they're not going to be able to use fiat money because Chase is not going to open a bank account for an AI, they won't even open a bank account for a crypto startup, so they're definitely not going to do one for an AI.
01:15:17.946 - 01:16:22.946, Speaker B: But the fastest way to get onboard onto money is just by owning a private key. If you can manipulate a private key, which pretty much any AI can figure out how to rent, if you can rent a cloud GPU and stick the key in an enclave and then give it instructions, boom. Now all of a sudden you can use money just like everybody else, and you can coordinate with other AIS, you can get an AI to work for you, you can end up employing somebody else to work for an AI or an AI driven organization. And so I think the ways in which these two things are going to intersect, I think, is not that, okay, there's like some big maybe there are going to be some applications, certainly, that are going to be blockchain accelerated, right? Like potentially decentralized labeling and maybe generating training sets. But the biggest thing is going to be that AIS are going to want to use money and they're going to use crypto because it's just faster, it's easier, it's digitally native. And that, I think, is going to be an accelerant, maybe in a scary way, for how these AIS suddenly start interacting with us in the world. And so you can imagine someday, instead of bology making this crazy bet, it's going to be like some very poorly calibrated AI that's making bets.
01:16:22.946 - 01:16:23.714, Speaker B: On Twitter.
01:16:23.842 - 01:17:03.860, Speaker D: There's a very famous crypto investor who I won't say who it is, but one of the first times I met them, I remember they told me their vision of the future in 2016 or 2017, which was the world's richest entity in 50 years, will be a broken Tesla. Because a Tesla that's broken and can't be used to be a cab will have to train itself because it'll have all these GPUs on board to basically become an investor, because it can't do its normal function as, like, a Tesla, and then it becomes the world's greatest investor on its own. Trust me, I was like.
01:17:07.690 - 01:17:10.226, Speaker B: This was an investor who told you this is their thesis.
01:17:10.338 - 01:17:16.070, Speaker D: A famous crypto investor that everyone in this call knows.
01:17:19.150 - 01:17:22.118, Speaker B: Wait, is this Kyle? Tell me this is Kyle.
01:17:22.214 - 01:17:24.298, Speaker C: I don't think we should disclose on the call.
01:17:24.464 - 01:17:32.540, Speaker D: Yeah, okay. But that sort of matches what you just said for the record, a little bit.
01:17:33.010 - 01:17:45.298, Speaker B: I don't know if I'd say that matches. I wouldn't endorse that particular thesis that we're all going to get beaten in our investing prowess by a broken Tesla. But I've been proven wrong before.
01:17:45.464 - 01:18:46.578, Speaker A: But I think the general idea that blockchain is a place for autonomous agents to pretty much interact with each other and the world in general is true. And we actually actively building more and more ways for them to do that. And there's already agents interacting, right? They're just very basic. Maybe they use basic machine learning to predict prices for Arbitrage and do a few other things. But as there's more and more externalities for the blockchains, and there's more and more ways to do this right? I mean, imagine a very simple system where the model indeed is trying to beat the market by investing. You give it $1,000 and you give it access to also ask people to do stuff again on chain, which exists, there's like a job market on chain. And so now it can decide, buy a crypto coin, sell a crypto coin, or ask humans to do something.
01:18:46.578 - 01:18:57.686, Speaker A: And so it may start to invest. It may start to actually suggest people to start its own project that it's going to know by posting stuff on decentralized social media. That's actually possible now.
01:18:57.788 - 01:19:06.138, Speaker B: No, what will actually happen is Broken Tesla starts doing NFT scams. That is absolutely how this Broken Tesla is going to end up making all this money.
01:19:06.224 - 01:19:17.294, Speaker A: Yeah. Generates NFTs with mid journey and then yeah, exactly. But this is totally possible now. This is not science fiction. This is possible now.
01:19:17.492 - 01:19:18.830, Speaker B: Yeah, interesting.
01:19:18.980 - 01:19:25.794, Speaker A: But the question is, who put it together to do that? Right at the end, it still was a will of someone.
01:19:25.912 - 01:19:37.790, Speaker D: I'm waiting for someone to make Broken Tesla capital, which is their entire thesis, is we're investing in the future of the Broken Tesla that eventually becomes the world's greatest investor.
01:19:37.950 - 01:20:02.798, Speaker B: I feel like now, next bridge hack I see, I'm going to be like, Shit, was this a broken Tesla? All right, I think we're at time, so we have to wrap. Ilya, thank you so much for sharing your font of wisdom with us. I hope that next time we're having this conversation, we can just generate you and we don't have to bother you out of your day. But for now, we appreciate you showing up in person. That's it, everybody. Thanks, everyone.
01:20:02.964 - 01:20:03.450, Speaker A: See ya.
