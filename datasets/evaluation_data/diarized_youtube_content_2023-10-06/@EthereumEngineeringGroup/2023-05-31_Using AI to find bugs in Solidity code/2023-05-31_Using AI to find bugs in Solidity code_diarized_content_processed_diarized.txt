00:00:02.970 - 00:00:59.134, Speaker A: Hello and welcome everyone. My name is Peter Robinson and this is the Ethereum engineering group meetup. Today I'm going to give a bit of a talk on AI and what I've learned about trying to find bugs in code using AI and let me share my slides. Okay, and let's drag you all over here. Can people see my slides? Brilliant. Okay. Yes, it has been a bit of an adventure and so thank you to quite a range of people who provided some thoughts at various points whilst I was developing the talk, some people who contributed a script and a few other people who answered a question or two.
00:00:59.134 - 00:01:59.118, Speaker A: So it's certainly helped make this a more fuller presentation. And so before I dive into these text based analytic tools and how they can fix bugs or find bugs in code, I did a bit of an adventure through AI image generators. And I think they've got a lot to tell you about AI generally, so it's certainly worthwhile going through that. And then I'm going to talk about the methodology that I've used for doing this analysis, this study. And then we dive into one particular buggy bit of code and we look at the effects of naming and then adding comments and just subtle changes in that code and see how those subtle changes change the outcome of what AI can do for us. And then we look at a whole lot of different buggy code. And then I had an attempt at reviewing the GPAC cross chain control contract.
00:01:59.118 - 00:03:15.062, Speaker A: So for those who remember, that's a quite complicated contract, quite large, multiple sets of inheritance happening and lots of things happening. I then look at the legal implications and look at the essentially terms of service that you're going to hit when you're using these tools and some limitations of the study and then finally go through the results. So AI image generators. So the first one I used was mid journey. And so after you've signed up for mid journey, what you do is you join a discord, and then on a certain discord channel you can put a request in and then based on whatever you type, it will then in about, say a minute or two's time come up with four alternatives. And so for instance, you can see that someone has put in a request for oblique view, 45 degrees game equipment, and so they've gone really very precise over what they want. And then you can click on the u there, say u one U two U three u four to upsize the top left top right, bottom left bottom right image to make it bigger.
00:03:15.062 - 00:04:04.940, Speaker A: Or you can say, hey, I really like say that colorful one bottom left. And so that's of three. And so I can click on v three and it'll give me variations based on that image. And so it'll just generate something based on the text. And so I put Ethereum engineering group into v prompt and to see what does mid journey think Ethereum engineering is. And so you get all these fantastical Sci-Fi like things, and one thought I've had is that Sci-FI like stuff is easy for it to generate because you don't have a preconceived idea of what it should look like. And so I think that that's really interesting all by itself.
00:04:04.940 - 00:04:44.230, Speaker A: But, yeah, it comes up with four quite different images, and it has that general Ethereum, the pointed motif thing happening. And so that was pretty good. And I thought, well, let's zoom in. So we upsized one, and then I created some variations and interesting variations, and then upsized one of the variations. So quite interesting. I then put it in again. And the thing to know about these models is that quite typically, you have some randomness put in.
00:04:44.230 - 00:05:29.430, Speaker A: And so for mid journey, in fact, you can specify the random seed. So it's, I think, zero to 4 billion, so a 32 bit seed. And so you will end up with completely different things, but it means that you can ask the same question more than once and you're going to get a different answer, potentially. And I put it in again, and it's just really interesting to see they're quite varied styles, and it's almost like some of these are specific styles, like that one to the top, right. It really feels like, I don't know, hardcore communist China in the 50s, or Stalin's Russia. I mean, when I look at that, that's what I'm thinking. Whereas the other ones like a quite different style.
00:05:29.430 - 00:06:45.198, Speaker A: I use stable diffusion, so stable diffusion. I used the online version, but the idea is that the model can be run on your own computer. So it's quite a different sort of model and probably much smaller in size and scope. And if you look at the people that you've got these horrific, distorted faces, and I think that's just part of the stable diffusion model, and I'm not exactly sure what that's about, but you've also had some really bizarre stuff, like you've got that person growing out of a table, which is quite worrying, really. So it shows errors in the actual generation process and things that we would just automatically know as wrong. It's created. I used image creator to have a go and I said Ethereum engineering group, and I got this and I thought, right? And then I said, all right, well, what about Ethereum engineering group creating the future? And then I got this and I'm thinking, well, I guess she's got some gold or something just falling into her hand, but really.
00:06:45.198 - 00:07:34.714, Speaker A: And then I got this and then I had a look and I thought, just a sec, you can actually have categories of the type of output you actually want. And so the default one is anime. So I was having this anime sort of based ethereum engineering group, which is interesting in itself. And what it really shows is if you don't know how to use the tool or haven't spent some time using the tool, you are going to get some weird and wacky results. And just that all by itself, I think, is a great learning. And then I realized that you could have image input as well. So I took the image from mid journey just to see what it would produce, and it produced interesting variations on an existing image.
00:07:34.714 - 00:08:17.690, Speaker A: And I do wonder whether this will allow people to say, create derived works. So grab something that's copyright and then create a derived work. And where do you legally sit with that, I wonder? I have no idea. And then I tried dream AI and it produced something quite possible. It's all Ethereum like shapes as well, so it's interesting. So after all of that, I created a t shirt for those who are interested, and you can buy it, have one of your own. I think the thing to think about is that these different tools are targeting slightly different applications.
00:08:17.690 - 00:09:11.354, Speaker A: Mid journey is one area, but then that stable diffusion was trying to have it, that you could run it all on your own computer and they're designed differently. And you've got to understand the tool you're using to be able to use it well, because otherwise you're going to have results that you don't expect. And the other thing that I noticed was that I'd go back to user tool and it's been updated and suddenly its capabilities have changed. And so I found that to be quite interesting all by itself. So now let's get into the actual, let's try and review some code. And so I tried out OpenAI's Chat GPT V 3.5, and I tried v four as well.
00:09:11.354 - 00:09:57.622, Speaker A: So v four at the moment you've got to pay us dollars, 20 per month. Perplexity and barred. And I had a little look at copilot, but copilot is only for code generation at the moment, so you can generate some boilerplate code to do something quickly, but it doesn't have the ability to actually analyze some existing code saying that copilot chat is coming. So that may offer those sort of services. And there are dozens of other services as well available. And I just had to put a bit of a limit on what I was going to look at. So what I've done is I've analyzed the output to be bugs.
00:09:57.622 - 00:10:54.080, Speaker A: So they've identified a real bug in the code, which is great improvement. So they've identified an improvement that could be made to the code wrong. They've given me a recommendation which is just completely wrong. And in fact, if you went down that route, you are doing the wrong thing. Strange something which is not necessarily wrong, and it's definitely not locating a bugger and improvement, but it's just something where you read the paragraph, you reread the paragraph, and then you reread the paragraph and you're still going, what? And generally those are just like that bloke or that person who was coming out of the table. It's just a strange output which doesn't match anything real. There were also a lots of filler stuff, which was great information if you're a newbie, and often all correct information.
00:10:54.080 - 00:12:08.330, Speaker A: But if you're trying to find bugs in code, it's not really going to help you. Okay, so let's start looking at some code. Can anyone tell me any bugs that are in this code? Who wants to be the brave soul to put in a guess? And no guesses bad. I can see David there looking closely. All right, how's about we look at. Let's have a look at this code. This is the same code, but I've made the variable names a little bit better, the identifiers better.
00:12:08.330 - 00:12:46.372, Speaker A: What's wrong with this code? You haven't used the modifier when not forced. Is that relevant? Or just because you're going to use it in something that inherits from this? Yeah, well, that is true. You are going to use it in something that inherits from this. So that's not an issue. All right, well, and then we had some comments in chat, I should open up the chat pause and not pause the equals true as well.
00:12:46.426 - 00:12:47.030, Speaker B: Sorry.
00:12:48.520 - 00:13:39.460, Speaker A: Yeah. Is it the wrong way around? Yeah, that's right. Someone's identified the logic being wrong and then. Yeah, so, yeah, so people have identified that they're not paused. Equals true is we've got a logic error. The other one is that this is allowing anyone to pause the contract, which is not ideal, is it? Normally the idea is that these would be internal and then you'd have somewhat have some other function at the top layer that would do however you're going to do pausing or you might have it external but have some sort of access control like only owner or only some user with the role based access control. For pause.
00:13:39.460 - 00:15:13.700, Speaker A: What about improvements? Can people see improvements in their code? Two small ones? Well, one is I put that abstract to really make it sure that everyone knows that they are supposed to be inheriting from this. And as well you could have a specialized error message for this and then do the revert with the error message and doing that will save on gas during when you're executing and also when you're deploying the contract. So we've got an improvement from having abstract not as efficient as it could be. We've got either have them internal or have access control, and we've got a logic bug in that small bits of code. So I gave the original code with the zero information, really bad variable names and identifier names and said to Chat GPT, are there any bugs in this code? And so it said code that you provided appears to be written in solidity. Tick, that's a good starting point. I've reviewed the code and no bugs, but there are some things that you could consider doing.
00:15:13.700 - 00:16:27.210, Speaker A: And then it tells me that it's recommended that I have start function names with a lowercase and the contract with an uppercase, and you go, well, that's what I'm doing. So why are you telling me this? That wasn't very useful. And then it tells me stuff about explicit visibility modifiers, which I've already got and as of I don't know what version of solidity, but you had to have it wasn't quite an optional you had to have them anyway. So that's out of date information and also doesn't match what the code is. And then it said the error message provided in the require statement is used for debugging. However, generally better to have a more specific error message to clearly indicate the cause of the error, you think, well, yeah. And so they gave me updated code and really the big change is that they said contract is paused rather than paused.
00:16:27.210 - 00:17:11.370, Speaker A: I don't know about you, but that doesn't feel like that's overly much more helpful. And they said it's good practice to thoroughly review your code and have a security audit. Yes, great. So Chat GPT didn't find anything useful and some of it seemed to be a bit out of date. Chat GPT four again, like I said, that it looks like it's written in solidity. But they don't appear to be any bugs or errors. And then it gave me a rundown of its analysis of how the code worked in writing, which is all true, but not overly helpful.
00:17:11.370 - 00:17:46.930, Speaker A: And then it did detect that stuff one could be modified by anyone on the planet and that that could well be a problem. And so maybe you want to put some permissioning in. So it did detect that. So that's good. It found an access control issue, so perplexity, it didn't really find any bugs at all. It gave me acres and acres of text, which was great information if you're a newbie. And in fact, it even gives references to that information, which I found to be really quite helpful.
00:17:46.930 - 00:19:03.102, Speaker A: But it didn't actually find any issues. So there's Google barred as well. And so it said there's a bug in the code and that stuff three, stuff three requires that stuff one be true, and that means that it's possible to call all stuff four when stuff one is false and you're thinking that doesn't make any sense at all. And so its recommendation was to change the logic so that stuff one would return, not that was just outright wrong. And so Bard didn't fix it and find any issues, but importantly, it also recommended something that was just wrong. So analyzing coming up. In summary, for that, GPT was able to find one access control issue, but didn't find the logic issue, which is fair enough, because it didn't have enough information, and Bard and GPT 3.5
00:19:03.102 - 00:20:16.770, Speaker A: gave wrong suggestions. So now we've got that pause with the variable names, and GPT-3 started talking about state persistence and was trying to say that the state variable, so if we go back, that not paused variable wasn't going to be stored between invocations, so between transactions, which is just wrong, it did find the access control issue, which was great. And then it started talking about gas costs and saying that essentially if pause or unpause a call, they're going to cost gas. And you're thinking, yeah, so that wasn't overly helpful either. And then for GPT four, so that was 3.5. Then for GPT four, it did find the bug, which was good and which is all about the access control. And then it gave some sample code, which shows the logic being flipped.
00:20:16.770 - 00:21:14.598, Speaker A: So it doesn't fix the access control, but it's mentioned the access control already, actually. Now the previous one was the logic switch, and this is the access control and saying so you could set it up as ownable. Perplexity, again, had lots of lovely information, but didn't actually find any bugs. So again, great if you're a newbie to give you some pointers on what to do. And Bard found the issue but didn't actually fix it. So it said what you could do is, or rather it said for the pause and unpause, maybe make them internal and then you're all good. There, it's done it, but it's still got the logic incorrect, so it hasn't fixed that.
00:21:14.598 - 00:21:59.510, Speaker A: So for the pause version where you had some variables, Chat GPT was able to find two bugs, and Barden GPT 3.5 found one each. And there was a few strange things, wrong answers and such like, all right, now let's add some comments in right at the top. So this time GPT 3.5 came out with a confusing thing where it was saying you should change your code to be this. And then you look at your code and say, well, that's what I've already got. So that doesn't seem to be very helpful.
00:21:59.510 - 00:22:51.800, Speaker A: And then GPT four though, so it found the problem with the logic being flipped. And also it talked about having pause and unpause, either only owner or something like that. And so perplexity didn't find anything and Bard had a wrong recommendation, but did find one of the bugs. So in summary, with some comments, curiously, GPT 3.5 went backwards. And I don't know, maybe if I'd regenerated the output it would have come up with something better. But I always, each time just tried the first output that was given.
00:22:51.800 - 00:23:55.034, Speaker A: So I've added the word not to paused state at the top there. So initially starts contract in the not paused state. So in other words, now we've got an additional bug that not paused is not going to. Rather than starting off false, which worked before, now it needs to start off as true. So we're essentially got the information in the comments isn't matching what's in the actual code. And so based on this, we had the GPT, found the access control. But interestingly, GPT four and perplexity found the constructor issue and found that you needed to set up that not pause to be true.
00:23:55.034 - 00:25:18.066, Speaker A: And so that means that those two tools were able to read the comments and based on what was in the comments, do something and tell you about a problem. And so what I think we're hearing here is that writing good comments that are helpful for humans will also be helpful for AI. Writing good identifier names that are meaningful will also be helpful because your AI tools will be able to help you. I also changed the prompt as well, so I'd previously been just using are there any bugs in this code? And then I said, well, are there any bugs in the code? Can you suggest improvements? And suddenly Chat GPT and perplexity got three were just as good as Chat GPT, four, and I'm thinking, whoa. So there you go. What I think this is really about is how does the model understand the word bug and what does the word bug mean? And maybe the word bug means different things to different people. And so it was quite interesting, but just doing that subtle change in language about what we're actually looking for, they were able to find all of the bugs.
00:25:18.066 - 00:26:06.674, Speaker A: So that was quite interesting. Okay, so some of my coworkers at immutable actually came up with this much longer prompt. And I added in a little bit saying give me the line numbers as well. And so when you feed that in, then you get a tabulated. So rather than paragraph by paragraph you get a tabulated piece of information. You get the thought on what the severity is, who could actually execute the issue, and far more detailed information. And so I guess what that shows is that you can do a lot more with the prompts than I did.
00:26:06.674 - 00:27:11.814, Speaker A: I just did the simple are there any bugs? Are there any bugs? Do you have suggestions? But you could do a lot more with the prompting to get more detailed information. So I think that's really quite useful. So are there any questions on all of the stuff I've gone over so far or any thoughts? Can I show the prompt that got that table? All right, so provide an exhaustive list of issues and vulnerabilities inside the contract. Bit of misspelling there, but anyway, be the issues, have issue descriptions. Describe the actors involved. Include one exploit scenario in each vulnerability output as a valid markdown table with a list of objects that have each description, action, et cetera, et cetera, and line columns. Type can be usability, vulnerability optimization actors, to be list of involved actors.
00:27:11.814 - 00:27:39.620, Speaker A: Severity can be low ice block emoji, medium or high fire emoji. Line is the line number of the issue. So you're providing all of the code one line after another after that. Yeah. Okay, so let's have a look at some more. Are there any other questions before I go on?
00:27:43.330 - 00:28:04.070, Speaker B: Yes, maybe I have one. There's the example of the security column, for instance, because with the prompt you're basically reducing the search space. Somehow your problem is more constrained for the severity, for instance, you've got a few, let's say values for severity.
00:28:05.530 - 00:28:08.166, Speaker A: So I would try to compare it.
00:28:08.188 - 00:28:21.100, Speaker B: With assignment of random values to severity. For instance, is there any benefit in using Chat GPT or an analysis compared to assigning to having random values? I don't really know. So do you have any idea?
00:28:22.750 - 00:30:02.044, Speaker A: I have not gone through that enough and it would be really interesting to analyze. Someone could launch and go off and do a whole PhD on this, but that'd be a scary thing because I think these tools are going to change over the next two or three years, so it's not a good topic for a PhD. But I think you could do a lot of analysis into severity to try and see are the problems identified here, do you think of them with that same severity or not? So I don't know at the moment. I think that's just an interesting output, but you'd want to verify. Sorry, if I may ask? Yeah, sure. So have you tried regenerating the response and to see if the response provides the same result, or would that be different? Yeah, so I haven't done that and I could have because you're getting a randomized output, and so you could imagine it may be interesting to get it to regenerate a few times and then try and take the amalgamation of the output maybe. So I'm not sure whether if you say how to run Chat GPT-3 times, whether it would have given you all of the bugs, even though if you just run it once, maybe it doesn't give you something useful.
00:30:02.044 - 00:30:54.520, Speaker A: So yeah, I'm not sure. Okay. In some ways, though, the logic and the access control issues are not that hard to find. And so I put together some other code, which I think people who are familiar with some previous talks will recognize and maybe even know the bugs in the code themselves. Does anyone know the bug in this code? Yeah. All right. The bug in that code is related to reentrancy.
00:30:54.520 - 00:32:04.038, Speaker A: So at the moment you could hit flash loan, and then in your flash loan receiver you could call flash loan again and do some interesting stuff there. Or potentially if you had a deposit function within the flash loan contract, you could call that. So you need some sort of contract wide reentrancy guard so that you can't use a flash loan to, say, fund a deposit to then do a withdrawal and attack the flash loan system or try and do some other attack. So Chat GPT four found the reentrancy bug, and none of the others did 3.5 and Bard gave a specifically wrong suggestion. All right, what about this code here? So it's a bit of proxy code. Does anyone want to postulate what could be wrong here? Yes.
00:32:04.038 - 00:32:56.578, Speaker A: Got on you, Raz. So there are actually two bugs, but Raz said storage slot, which is true, and the other one is function selector as well. So implementation is being stored here in storage slot zero, but this is a transparent proxy, so it's likely to clash with whatever code. Rather, storage is being stored in storage zero in the implementation contract, or equally in the implementation contract. You could update the value of implementation mistakenly. The second part of it though is the function selector. Because implementation is a public, it means that you're automatically having a function which is called implementation, which is essentially a getter for the value of implementation.
00:32:56.578 - 00:34:16.308, Speaker A: And so if in the implementation contract there was a function called implementation proxy would shadow it and intercept it first, so will any of the tools find it? No. So GPT came up with some improvements in the code, and I can't remember what the improvements were, but they were minor things, but none of them actually found the actual collision. And then there's this one here. So this is really quite a subtle issue. So it's our pause contract again, and we've got abstract written there. I didn't bother changing that, but the change that I did do is I put not paused equals true in an initialize function. So based on that initialize function, what are the bugs that are in this code? Okay, go ahead, David.
00:34:16.308 - 00:35:23.510, Speaker A: Well, there's no constructor. The initializer is used for upgradability, but there's no constructor, so the constructor will set it to false to start with. Yeah, so if you're using an initialize function, it's implying that this code is being used as an upgradable contract, which means it's being used via a proxy. And so if you're having delegate call, then in the constructor you'll be setting things up in the context of the implementation contract, when really you need the initialize function to set things up in the context of the proxy. So the actual issues are twofold. So you really should have a gap here after not paused, so that you won't be using up storage slots after not paused. So that then if you do upgrade this contract, you won't cause a ripple effect of all of the code that's included after it needs storage suddenly moving.
00:35:23.510 - 00:36:37.292, Speaker A: And the other thing is that it's using access control, which is an open zeppelin contract. But really, because this is going to be an upgradable contract, which is implied by the fact that it's got initialized there, then it should be access control upgradable, which will again have things like these gap storage values. And probably not surprisingly, for a reasonably complex problem like that, it didn't find the bug. So someone said, what does correct but filler mean? So correct but filler meant that it's informative text, it's not wrong. It'll be helpful for some newbie users, but if you're reviewing lots and lots of code, it's not really going to be helpful. It's going to waste your time because you're going to be reading through stuff that's not going to be helpful. Okay, so we've had a look at code that fits within a slide, so you could view that as being not real code, I guess, interesting, but most code is a lot bigger than that.
00:36:37.292 - 00:37:26.140, Speaker A: And I guess I went completely nuts and decided let's try the GPAC cross chain control contract. And so that's a very large complicated contract with large complex hierarchy. And so the first thing I did was I flattened it. So flattening is a process where you have code across many files and you use a tool to flatten it, say truffle or one of the many others, and then you have all of the code all in the one file. So the separate contracts, but they're all in one file. And so then you can use that to just dump it into your chat interface. And the thing I ran into though was that it said, hey, that's too big.
00:37:26.140 - 00:38:22.860, Speaker A: And so maybe for contracts that are obviously smaller than the GPAC crosschain controller would have worked okay. But for something that size, it just blew up. And it said I had 21,000 tokens, which was more than it could handle. And so I had a go at using the API, actually the API interface, and actually it blew up as well. But I can see that there are 32k models, so it's just that I didn't access the 32K model. And so to use the API interface, I used some code, again developed by my code workers, and any bugs in there were inserted by me in the last day or so. But I modified their code and it's available, we've got it available open source here in the examples, repo.
00:38:22.860 - 00:39:31.216, Speaker A: And so you have an environment variable, which is your API key, and you have the API key, that's the flattened file, which I loaded up. And so you could have multiple files, but I'm just using one file. And then you read in the file and then into a variable called code, and then this is that big long prompt, and then you call the open API, and you've got a set of parameters here, and the first one is the model. So we've been talking about GPT four and 3.5. And so another one is da Vinci three. And so prompt is that prompt temperature. So if at zero you are after a deterministic response, and as that number goes up you get a more randomized response.
00:39:31.216 - 00:40:30.852, Speaker A: So Max tokens, so you remember how I was talking about the error message was talking about numbers of tokens. So Max tokens is the amount of tokens that can come as the response. And so tokens are essentially words or non space things. So if you can think of a tokenizer, tokenizing text, it's each of those tokens in the text, and there's a whole lot of other parameters. So this top p frequency penalty and presence penalty, I think they're to do with model tuning. So I didn't get around to analyzing what they mean, but there's a link down the bottom there which will give you the meaning of all of those variables. And so to me it seems like the Da Vinci one must have been an earlier model, and then the GPT 3.5
00:40:30.852 - 00:42:00.000, Speaker A: turbo. And so when you're using the chat interface, that's the one you're using, it's supposed to take one 10th of the cost of it, so I imagine one 10th of the time cost as well. And then you've got four. And so the number of tokens, if you could get the 432k which I've asked, I've applied for, and we'll get at some point, I guess then you should be able to process even large contracts like the GPAC contract. Any questions? Okay, so I am definitely not a lawyer, not a solicitor, barrister or any other legal representative, so do not take this as legal advice or anything else. So open API, lots and lots of text in there terms of use, but you own all of your input, so you give it input and you still own it, which is good to know, and it assigns you the rights and title and interest in the output. So any output you get is yours and you own it.
00:42:00.000 - 00:43:18.230, Speaker A: But of course the output might not be unique across all users. So the example they give is you say what color is the sky? And it might say the sky is blue or something like that. Or the standard question I always use to test out these tools is where does the sun rise? And the answer you're expecting is going to be the sun rises in the east, but in particular rises in the east on two days of the year, the equinoxes during summer it will rise a little bit to the north and then a little bit to the south, stuff like that. So obviously if everyone asks the same question and it's got a straightforward answer, then everyone's going to get the same answer. Yeah, we don't use the content that you provide and that they receive by the API to develop or improve their services. However, if you aren't using the API, for instance, chat, then they can. So whatever you give them, they can create derived works based on your input.
00:43:18.230 - 00:44:21.126, Speaker A: But you can fill out a form to say I don't want you to use my chat input to do stuff. Perplexity. So this is a really interesting one. This is right at the start you agree not to republish any content generated by the service without clearly citing the service as well as the context associated with the content. So I think they're hoping that you're going to say, I used this prompt and I generated it using perplexity. And if you misrepresent the source, then they're upset with you. And they again say you own the retain the ip that you hold in the content that you have put in, but they've got a worldwide license to store, reproduce, modify, create derivative works from your stuff, but they're talking about translations.
00:44:21.126 - 00:45:07.050, Speaker A: But then I read their license and I was not sure whether perplexity was going to use your content and create derived works. So that's hard to know. But interestingly, they had an IP bit right at the end which said, and by the way, you can't train your AI model using our AI model, or you can't create some AI service based on our AI service. So Google barred is Google. And so Google's terms of service. So it's got the general terms of service and then it's got the AI terms of service. And the main terms of service are enormous.
00:45:07.050 - 00:46:09.660, Speaker A: But the bit that is probably relevant is that your content remains yours, which means you have ip to the content. Fair enough. And they reserve the right to host, reproduce and do pretty much anything with your content and save it so that it's available to you. And if you publish it, then they'll make it visible to everyone and they can modify and create derivative works for reformatting and translating. And importantly, they also didn't want people using machine learning models and related technologies based on bard. So if you want to create your AI tool and use bard as the back end, I think you're going to need to talk to Google. Okay, limitations.
00:46:09.660 - 00:47:10.128, Speaker A: So the Chat GPT V four currently cost us 20 a month. And I didn't use any other paid services, and I don't think you can do a paid for barred subscription. And I don't think perplexity has paid for services either. And I didn't look at the absolute multitude of services to try and understand what they all were. So what can I say in summary? So AI tools can find bugs in solidity, there's no doubt about it, and they can find bugs that I don't think like static analysis tools are going to find. And the reason why I say that is that it's able to look at the comments in the code and look at the variable names, look at the english language and maybe multilanguages as well. I'm not sure how well it goes in other languages, but look at the language that's being used and not just the code itself.
00:47:10.128 - 00:48:07.908, Speaker A: It's not going to find all the bugs, and there's lots and lots of text that gets generated. And so I think that you're going to need to have someone who's moderately qualified to go through that text, and so I don't think you can have it as a CI loop. I think it's got to be a one off tool that you use as part of your arsenal of things that you do before you launch a contract where you get the AI to have a look at it. So GPT four certainly did come out best as far as actually finding bugs. I do like perplexity though, and the way that it can give you sources of information, it says something quite insightful. And then it says where it found that insightful piece of information barred. Right at the moment, there's just too many errors and it really is very emphatically wrong.
00:48:07.908 - 00:49:17.576, Speaker A: So it's very sure about its answers, and it tells you the wrong thing with great certainty, which I find a bit worrying. Most of the others chat, GPT and perplexcity tended not to do that as much. Yeah, to help the AI tools do stuff with your code and human reviewers as well, you need to name your identifiers to match what they're actually being used for. So I think the stuff example that I did right at the start, compared to having the names as if it was a pause contract, it made it so much easier for us humans to understand and also for the AI. And putting comments in your code helps the AI tool and your human reviewers understand what's going on. And if you are worried that your comments are going to lag the code, well, the AI tool is going to tell you that it's inconsistent and you'll get around to fixing your comments. So using a prompt, I think trying to play around with prompts is something that is probably worth doing.
00:49:17.576 - 00:50:09.500, Speaker A: And I think that the prompt that the guys my coworkers generated is good, but I think that could just be a starting point. And spending half a day working on a prompt, I'm sure you'll get better results again. So it's one of those things that not a lot of work and you'll get some great results from the prompt. I'd use more than one tool. I think that GPT four is great, but I'm sure there are other tools out there that might end up being better. And as per what one of the people said earlier, regenerating the output and essentially rerunning using the same tool could be an interesting exercise because you're going to end up with slightly different output. So a common phrase you hear is trust but verify.
00:50:09.500 - 00:50:55.480, Speaker A: But really, I think when it comes to AI, I think it's possibly useful information or indication, but you've always got to verify. You can't just use AI as the single source of truth. Lots of links to lots of tools and things. And for those interested, the merch store is open, including the AI generated Ethereum engineering t shirt. There are a lot of interesting talks coming up. We've got David Highland Wood is going to talk about bridge agnostic messaging in two weeks time. And then in a month's time we've got Frank who's going to talk about roll up related technologies.
00:50:55.480 - 00:51:56.670, Speaker A: And then possibly on July Twelveth that's possibly going to move. We've got talk on bridge aggregation and then we've been able to get Jaden in to talk about ERC 65 51. So if you're watching this today, it's going to be on YouTube. There's a slack workspace where you can meet and discuss the talks. If you're watching this on YouTube and you want to turn up to the meetups live and ask tricky questions, there's the link and that example code. Is there a formal methods reading group happens as well for people interested in that, join the slack workspace and the formal methods channel. And I have talked for quite a while now and interested in what thoughts or questions do you all have?
00:52:05.570 - 00:52:33.770, Speaker B: Yeah, maybe I have one question. I would say if you ask a question, is the program correct? There's no answer to that, right? It should be related to a specification. So did you get this kind of answer when you ask a Chat GPT, for instance, is there a bug in the code? Did you get the answer? I can't answer this question. You should be more precise.
00:52:35.150 - 00:52:52.800, Speaker A: I don't know that it does that, but I wonder if you did provide a specification. I think there's a real, if you could provide a specification and link the code, it might be able to do the formal verification for you. Frank, how'd that be?
00:52:53.810 - 00:53:20.520, Speaker B: I don't think it can do that. But if you ask a question, is there a possibility of a division by zero in the code? That's a bit more precise than if you can answer it. But most of these questions, reachability questions in the code, they are undecidable problems. So you can use sort of an engine, like an AI engine, that would do some pattern matching and give you an answer, but it's not 100% correct.
00:53:20.970 - 00:53:50.954, Speaker A: It's not going to be 100% correct. But I think the fact that the AI could find bugs in code, and it's just me putting the code in, hitting enter, and then the AI coming back within seconds, which is faster than people, I think at least on this talk, could find the bugs. I think some people were able to find things pretty quickly, posted them to chat. All right, Ron, you've got your hand up.
00:53:51.092 - 00:54:11.890, Speaker C: Yeah, I'm just wondering what information we have about what sorts of sources they've trained this model on that it's able to do anything useful with respect to bug. I mean, really, these large language models are just sort of telling you about probabilities of sequences of words, and that's what they use to generate their responses.
00:54:11.970 - 00:54:12.600, Speaker A: Right.
00:54:13.370 - 00:54:29.200, Speaker C: So I just wonder, have they trained this model, for example, on audit reports? So what it's feeding you is sort of probabilities that have been generated by learning from audit reports in terms of the sorts of word sequences that occur in those.
00:54:30.530 - 00:55:09.078, Speaker A: Yeah, I don't know. And so I started reading up on how AI worked in detail, because I learned AI back 30 years ago, which isn't quite so relevant now. I'm sure the basics are still there. And I started watching videos on how AI worked and what it's trained on and everything. And then I realized actually for this talk, I shouldn't care about how AI works in detail. I don't know the answer to the question. And I'm sure there are other people who have better ideas about what it's.
00:55:09.174 - 00:55:28.218, Speaker C: I think definitely what it's not doing is any type of logical reasoning. It isn't sort of doing any sort of static analysis type of reasoning, even certainly not the sort of reachability analysis, it's just. No, it's just doing word probabilities from sequences of words and code that it's learnt.
00:55:28.394 - 00:56:05.660, Speaker A: Yeah. It's doing analogous stuff. I've seen this before. This is similar. And so it'll be why having a model that's reasonably up to date would be useful, but it means that if there's a new zero day vulnerability, if you will, then it's not going to find it. Yeah. David Highland Wood, do you have a question? Can't hear you, David.
00:56:05.660 - 00:56:20.010, Speaker A: Frank, let's go to you whilst David's working out his technical stuff.
00:56:21.820 - 00:56:49.190, Speaker B: Ok, thank you. So, following on what Ron was saying, right, because it sort of puts together a sequence of words. If the majority, there's a majority for this is not a bug in different programs, or even some people would say, in auditing reviews. And so, no, that's not a bug. That's perfectly correct and that's wrong. Right. If you would apply the semantics of a language, there would be an issue.
00:56:49.190 - 00:56:53.476, Speaker B: An AI based tool will tell you.
00:56:53.498 - 00:56:56.676, Speaker A: Yeah, that's perfectly fine, because what I.
00:56:56.698 - 00:57:01.176, Speaker B: Hear is that it's fine, but there's no semantic analysis. Right. Of the code or anything like that.
00:57:01.198 - 00:57:28.530, Speaker A: So that's still dangerous to use. No, look, I think the thing to think is that, as I said, you can't rely on this as your only tool. It's got to be something that you use in conjunction. If it can find stuff, even if it is just analogies, I think it's still useful. Daria, do you have a question?
00:57:30.580 - 00:57:48.180, Speaker D: Hi, yeah, I was wondering if you have compared performance of the AI tools with regular static code analyzers like slither or others solidity to see what bugs would be found by the static analyzers compared to the AI?
00:57:49.000 - 00:58:12.220, Speaker A: No, I haven't, but that's a great area of research that could be done and it was one of the many paths or options that I thought about and I thought it'd be really interesting to see what they could find. But no, I didn't try that. So I don't know the answer to that question, but I think you'd use both of them in parallel.
00:58:12.640 - 00:58:22.130, Speaker D: You got me curious now. So if you want to collaborate on a follow up on this, I feel like I have an itch that I'm going to scratch anyway.
00:58:23.140 - 00:58:55.180, Speaker A: Yeah, I don't know whether I'll get around to doing the talk with you, but if you do the research and come up with the answer to that question, that would be interesting for me. My gut feel is I'd want to do both slither and use the AI tools, and surely everyone wants to use any tool that is available to them. But yeah, it's a different interesting area of endeavor.
00:58:55.920 - 00:59:07.916, Speaker D: If you would be able to share the code samples that you used in follow up to this talk, then I can run those static analyzers and see how it compares. We'll send you the results.
00:59:08.028 - 00:59:23.590, Speaker A: Yeah, for sure. Right, sounds great. I'll do that. And I think I'll be able to google you and get your email. All right, no problem. All right, David, over to you.
00:59:24.760 - 01:00:28.360, Speaker E: Hopefully you can hear me now, Peter. So I think my response to Ron would be, yes, it probably does pick up the public audit reports, but it probably also has a huge corpus of general computer science material. And so even though the thoughts about how this stuff works are correct, the corpuses are just absolutely huge. And I think that's why it does as well as it does. But I agree, it's certainly not doing any structured or formal analysis or any kind of structured thinking. My comment though was more about iterative prompts, which I brought up briefly in chat. I think what you'll find is that if you tell some of these models that are built as chat bots, if you interact with them and you say, yeah, you didn't get that bit right, here's what to look for.
01:00:28.360 - 01:00:42.220, Speaker E: Go and check it again with this new information, and you feed it the information that it needs to be more effective, you'll actually increase the hit rate substantially. At least I've found that in using the tools.
01:00:48.030 - 01:01:11.776, Speaker A: Thank you, David. Are there any other questions or thoughts? All right, well, look, thank you everyone for coming along today. I'm going post the video on YouTube in a couple of hours and have a great rest of your day. Talk to you later, everyone. Bye.
