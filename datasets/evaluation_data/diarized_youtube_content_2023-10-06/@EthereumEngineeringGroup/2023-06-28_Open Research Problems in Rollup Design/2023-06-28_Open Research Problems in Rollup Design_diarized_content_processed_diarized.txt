00:00:02.730 - 00:00:22.960, Speaker A: Hello and welcome everyone. My name is Peter Robinson, and this is the Ethereum engineering group meetup. Today I've got Dr. Frank Casse, who is going to talk about his perspectives on the open problems with roll up design. So, Frank, before you dive into the talk, can you please introduce yourself?
00:00:24.290 - 00:01:22.520, Speaker B: Yes. Thank you, Peter. So I'm Frank. I'm a computer scientist with an interest in concurrent systems and formal verification. I've worked as an academic for a number of years in France and in Australia, and four years ago I joined consensus and started to work on blockchain systems. Things I've worked on mainly were formal verification problems, like verification of a beacon chain, the deposit contract, smart contracts in general, and more recently, semantics of Ethereum virtual machine with David and Joanne and Horace that I see on this talk. And a couple of months ago I joined Mantle, which is actually formerly Bitdao, which is an institution developing a layer two, a roll up.
00:01:22.520 - 00:01:39.180, Speaker B: And as the head of research, of course, I'm interested in identifying research issues and problems that we can try to solve to make our technology better. Hence this talk on trying to see our roll ups work and what can we do to make them better.
00:01:41.550 - 00:01:52.062, Speaker A: All right, well, it sounds like you got the right background to give us an awesome talk, so let's hear it. Do you want us to ask questions at any time, or what do you want?
00:01:52.196 - 00:02:07.640, Speaker B: So feel free to interrupt me anytime. And I'm not, as I said before, a roll up guru. So if I make a mistake and you think it's inaccurate or completely wrong, feel free to stop me and I'll fix that for next time.
00:02:08.010 - 00:02:10.840, Speaker A: Okay, cool. All right, please, far away.
00:02:13.370 - 00:02:46.820, Speaker B: Right? So I won't see you. I'm going to minimize the window. So, Peter, I rely on you to interrupt me, but something in the. Yeah. So today I'm going to talk about what I think are some open problems in rollups. I'm not claiming that these are all the open problems that you have in. So the first thing, what I'm going to cover is probably to go back to the origin.
00:02:46.820 - 00:03:35.410, Speaker B: What's the issue that rollups are trying to solve? And this means the scalability problem in Ethereum, where does it stem from, and what are rollups doing to try and improve on it? And I'll talk about the design of roll ups. And again, open problems. And at the end I've got a short list of resources, not exhaustive again. So if you're like me, and a couple of years ago, I started to try and understand what wallops were, if you google it, you'll find lots of different things. And it seems to be magical because it solves the scalability issue. And again, if you look it up, you'll see all of these words. If you understand all of them and you can explain all of them, this talk probably won't give you anything new.
00:03:35.410 - 00:04:23.162, Speaker B: But on my side, for instance, I was trying to understand how roll up can provide scalability. What's the magic in there? What are the differences between prop and validity? Prop. So that's not too hard to understand some notions of finality, things that you see in standard blockchain, let's say technologies. So I'm going to try and talk about some of these things, not all of them. If you want me to talk a bit about some of the topics on this slide that I'm not going to cover later on, feel free to ask questions. I may have some answers. Right, so what's the problem with Ethereum? So just a reminder that the Ethereum network is basically implemented a replicated state machine.
00:04:23.162 - 00:05:27.082, Speaker B: So in this state machine you've got users submitting transactions to local nodes, and then these nodes are going to communicate the transactions, run a consensus algorithm to order them, and then execute, let's say a state transition, and replicate the state in an asynchronous manner. So the idea is that everybody's going to compute the same sequence of states, of course, at different speed. And that's how the Ethereum network works. You archive everything, the full nodes are archiving everything, the transactions, the states and so on, and it's public, so you can replay the transactions and check that the computation of the next state. So whether your transaction has been processed correctly, whether it has been processed correctly or not, and so on. And at the core of the Ethereum network, there's what's called the Ethereum virtual machine that's in charge of computing the next state. So you take a given state with, let's say, the current balances of the accounts.
00:05:27.082 - 00:06:28.294, Speaker B: There's a transaction transferring some assets from Alice to Bob, and this Ethereum virtual machine is going to execute and produce a new state. So in this design there's lots of communications, there's some security provided by some mechanisms staking and slashing nodes that are trying to cheat. So there's a lot of things that are going on. So more details, sorry, what does it mean in terms of timing and scalability? So I'll try to show you this slide. It looks complicated, but it's not too complicated. So you start in the Ethereum network by submitting a transition. So your transition, you want to transfer some assets and it goes into the memory pool of some local node, and from there on this node is going to send to his peers the transaction, sorry, that you want to execute, and at some .1
00:06:28.294 - 00:07:30.598, Speaker B: of the node is going to get this transition, this transaction, sorry, from the Mem pool and propose it as the next transaction to execute. So for simplicity, I would say in restock I'm going to assume that in a block we put only one transaction. This is without trust of generality, but it simplifies the problem. So this transaction is going to be picked by a given node, and this node is going to propose it as a transaction, producing a state transition from s to s prime, and it's going to broadcast this proposal to other node. So at some point this transaction will be in a proposed block, and then these other nodes in the network are going to reexecute the proposed transaction to check that the computation of the next state has been done correctly and produce a next state. And there's also a next stage in this, let's say process. There may be, let's say different options for successive states, because everything happens in parallel.
00:07:30.598 - 00:08:23.546, Speaker B: So every node doesn't have actually a sequence of states, but a tree of states, and they have to decide which tree they want to keep. And this is happening in a consensus mechanism where nodes are voting for the head of the chain. And after a certain amount of time, when some blocks have been, or transaction in blocks have been somehow voted for by a supermajority, a transaction in the block will be confirmed. So the node that you submitted your transaction to will get a sort of a notification that the transaction has been included in the block. And this block has achieved the special status which is called finalized, which means that it's very unlikely to be reverted to. Reverted and cancel this block. You would need a very high stake in terms of ether and in terms of money to revert it.
00:08:23.546 - 00:09:04.118, Speaker B: So it's very likely that this block is immutable and won't be changed. And that's called the finalization of a transaction. So overall, the trip of your transaction takes around 16 minutes. And you can see here that actually if you want to have blockchain systems adopted as mainstream payment systems, for instance, it's far from optimal. And we can't really wait when you're in the shop 16 minutes for your transaction to be validated. So what can we do? I'll give you first maybe more insight into what's happening. So there's a real scalability problem in Ethereum.
00:09:04.118 - 00:09:48.266, Speaker B: And on the left hand side, I've got this table that summarizes actually some of the timings, how things are happening. So in the Ethereum blockchain, time is divided in two slots, and slots are approximately 12 seconds. And 32 slots make an epoch, which is approximately 6.4 minutes. So in the Ethereum network, a block is produced on average most of the time, right at every slot. So every 12 seconds, sometimes there's a slot without a block, but it's supposed to be a rare event. And at boundaries of epochs, there are things happening, and these things are finalizing blocks and so on.
00:09:48.266 - 00:10:23.806, Speaker B: So the blocks are not actually finalized at every slot, but they are finalized every 6.4 at every boundary of an epoch. And to finalize the blocks, of course, you need to exchange messages and so on. It takes some time. So the average time here in this table is that a new block is produced every 12 seconds. An intermediate status between a new block and a finalized block is a justified block. So this is unlikely to be reverted, but still more likely than the finalized block.
00:10:23.806 - 00:11:10.654, Speaker B: So there's a sort of grade of, let's say finality for blocks. So a justified block is supposed to be happening every 6.4 minutes, a finalized block every 13 minutes. And for your transaction, because your transaction can be submitted just in the middle of an epoch, on average it takes 2.5 epoch. So this is around 16 minutes. So if you want to know how it translates into, let's say, transactions in terms of the throughput, so we can take something which is the maximum block capacity in ethereum, which is given by the maximum, let's say, gas or resource that can be consumed in a block.
00:11:10.654 - 00:12:04.562, Speaker B: And this maximum amount of gas is 30 million gas. So this is when the block is full. So this is not the target, I would say in ethereum, the target is to achieve on average 15 million, half of it you take the best case for your transaction. So transaction that costs the minimum, which is just a transfer of assets of ether, which is 21,000 gas, and we divide by 12 seconds. So that means that what you can expect for very simple transaction is a throughput of 120 transaction per second per second, which is TPS, and what's observed on ether scan and so on statistics is actually 15 transaction per second. Because of course you may, let's say, send transactions that are not the simple transactions, but more complicated one, running some smart contracts and so on, they would consume more gas. So that's what's happening.
00:12:04.562 - 00:12:47.950, Speaker B: And the blocks of course, are not at full capacity every time. So what is observed is 15 transactions per second. So clearly this is far from being enough to be adopted as a mainstream payment system. Any questions so far? So if none, I'll continue. So, is there any magic so that we can improve things in the blockchain ecosystem? There's what's called the blockchain trilemma. So for me, this is not a theorem or whatever. This is something that people believe is true, which is the following.
00:12:47.950 - 00:13:40.562, Speaker B: What you would like to get is decentralization, security and scalability. So decentralization means that you need only low trust in the components, low costs, so they can be some malicious actors in the system, but if a fraction of them is malicious, you're still going to achieve correctness, low cost, everyone can participate with some custom, well, standard hardware. You don't need to buy very expensive computers. And decentralization is usually achieved with a consensus mechanism. For instance, in Ethereum there's a fork choice, and LMD goes through security. What you want to do is to have your system to be resilient to attacks, to have the data publicly available so that people can verify that transactions have been correctly executed. And scalability, that's what I was mentioning before.
00:13:40.562 - 00:14:31.314, Speaker B: You'd like, of course, your system to be efficient with high throughput and so on. So there are solutions that can be implemented on the Ethereum network, which is called layer one, and these solutions will be implemented in the next couple of years. The main one is called sharding, which is dank sharding, or proto dank sharding, which is supposed to get 100,000 transactions per second. So we'll see if they can achieve it. But in the meantime, there's no really good solution to get more transactions per second. So what wallops aim to achieve is to provide more scalability, so higher throughput. And of course, if you believe in the blockchain Philemma, this means that you're going to compromise on either decentralization or security.
00:14:31.314 - 00:15:17.330, Speaker B: So I'll try to cover these topics in the next couple of slides. Right, so how do you scale with roll ups? So the main idea is that you're going to introduce some sort of parallelism. So you're going to remove the execution from what's called the Ethereum chain, which is usually called on chain. On chain means on Ethereum. On the Ethereum network, you're going to execute the stake transitions off chain. So of course this is nice because you provide, you get more cpu, you can have activities running in parallel but at the same time it means that you have an execution engine. So basically the Ethereum virtual machine that's running on another chain.
00:15:17.330 - 00:16:29.258, Speaker B: So how does it work? What are the problems that can arise with that? The other thing is that you really need to be able to bridge or transfer your assets from on chain to off chain to the off chain, the roll up, because otherwise there would be some risk conditions if you were in two parallel thread modifying, let's say the balance of two different accounts, they won't be synchronized. Or you could be double spending or have some, again, risk conditions on some state of the system. So you will need to transfer your assets from the main chain or part of your assets from the main chain, ethereum on chain to the off chain to be able to work with them. The other id in wallops is to use what's called sequencing, and the id is to batch states and transitions. So I'll describe it later on. But what it results in is that there's no consensus usually in rollup, so you don't run a consensus, these states types, but of course at the detriment of decentralization. So that's basically where the blockchain trilemma is showing up.
00:16:29.258 - 00:17:41.166, Speaker B: You increased the throughput, but you decreased the decentralization. And finally, how do you preserve security? You're going to commit the transactions that you're processing and the results of the computation, you're going to commit them on the layer one on the main net. So you're going to make some data available so that the computations state transitions and so on can be audited or verified by a third party, which means, again, that you need somehow some cross chain communication. There will be still some throughput limitations because you have to send some transactions to this layer one. So if layer one is limited in terms of the number of transactions per second, this commitment of data or states to l one will at some point limit your ability to increase your throughput. Right. So how does it work in a roll up? So I don't make a difference right now between roll ups, let's say optimistic roll ups and ZK roll ups, if you know what that is.
00:17:41.166 - 00:18:34.594, Speaker B: But this is the main mechanism that's adopted for them. You've got a user, and users, they submit transaction to a single main pool and the transactions are ordered. So there's usually a single actor on the roll up which is in charge of ordering the transition, deciding the order of the transition, and then the order transitions are published via smart contract to layer one. So you publish the data and what you're doing here is that you only publish the transaction data. You don't require the layer one to execute them. So what you can do is to use a mechanism called using the call data. When you call a smart contract on layer one, and it's fairly cheap to publish data corresponding to transactions compared to executing some code.
00:18:34.594 - 00:19:07.782, Speaker B: So you're publishing your ordering on layer one. And at some point, of course, when this transaction of publishing will be finalized, you will have committed to this sequence of transactions. And in all of the roll ups, this is immutable. When it's decided, you're not going to change it. And at the same time, on the roll up, you're going to compute the state transitions for this sequence of transactions and create a new block. So that's your sort of state transition on the layer two. So you create a new block with the sequence of transactions.
00:19:07.782 - 00:20:02.910, Speaker B: And where you're going to speed up and gain in terms of time and space is that instead of committing all the single states on layer one, you're going to commit an encryption, or let's say an encryption of the sequence of states that you've been computing. In practice, this is done via smart contracts also. And you commit a hash of this sequence of transactions. And if you want to know how to do it, you can actually use a Merkel tree, and you commit the hash root of the Merkel tree of this sequence of states that you've been computing at layer two. So the layer two is going to store, of course, the different states, but you're going to commit in one, let's say in sort of one hash. You're going to use one hash to commit the sequence of steps that you've computed. And that's where you gain in terms of cost as well.
00:20:02.910 - 00:20:38.154, Speaker B: You're reducing the number of data you're posting to layer one. So that's the main idea of our role. So I haven't detailed in this slide the other mechanism, right. Because you've got no decentralization in this system. There's a unique actor determining which sequence of transactions is going to be executed. You don't really know whether this actor is malicious or not. So they could be computing, of course, bad states, right? They could be tampering with the states that are obtained after each computation and so on.
00:20:38.154 - 00:21:38.122, Speaker B: So we need mechanisms also to add to this first phase. We need mechanism to be able to verify that things have been going well the way it's usually done in roll up. So there's two things that you can do. You're publishing the sequence of transactions so the order is published and is immutable. So what you can do is that external parties can actually read this sequence of transactions, and if they know the initial state, the genesis state of the roll up, they can, in theory, right, recompute everything, the state that would be obtained after executing this sequence of transactions. And they can detect whether this state is different to the committed states that have been published by the main actor running the roll up. So in optimistic roll ups you will have a sort of a time window to contest the states that have been computed in the layer two.
00:21:38.122 - 00:22:12.406, Speaker B: So you have, let's say seven days to check the sequence of transactions, the corresponding states that have been committed, and to contest it. In ZK roll ups, when you push the states, you have to push as well a proof that you've computed properly the sequence of states. And this proof is going to be checked before you can actually commit this state. So there's different, let's say, strategies that can arise from this design. So what are the main things that you have to take care of? Any question? Yeah, so in the chat, you might.
00:22:12.428 - 00:22:13.538, Speaker A: Want to have a look at the chat.
00:22:13.634 - 00:22:15.110, Speaker B: It's been quite active.
00:22:17.370 - 00:22:22.650, Speaker A: We might want to have a run through. Can you open the chat or do you want me to read the.
00:22:22.720 - 00:22:47.090, Speaker B: I'm doing that now. Yeah. All right. So are these changes specific to all roll ups or primarily zk or optimistic roll ups? I think they are not specific to any ZK or optimistic roll ups. They apply to all of them? Yeah. And then the consensus will need to exist for decentralized sequencing. Yes, I'll talk about that later.
00:22:47.090 - 00:23:25.230, Speaker B: This is one of the challenges. Does a mempool structure exist in roll up nodes? Yeah, they have to buffer the transactions. So it's usually called an inbox or a mailbox. So you push your transactions in and the sequencer is going to pull transactions from the inbox. So roll up nodes, are there multiple different. Yes. Are there decentralized layer two networks right now? I don't think so.
00:23:25.230 - 00:23:55.560, Speaker B: So I'll talk about decentralization later on and I can come back to this question specifically. I think it's in the next few slides. So if I don't talk about it, remind me about it for sure. Thank you. It looks the sequencer operator, the issue about the sequencer operator and front running and so on is actually related to decentralization. So I'll talk about it later.
00:24:00.310 - 00:24:08.798, Speaker A: Sounds good. Sorry, I said thank you, and sorry to throw you off your slides slightly.
00:24:08.974 - 00:25:02.354, Speaker B: That's okay, so these are the things that I'm going to discuss in the next couple of slides. So I'm usually used to provide technical talks and solving, let's say, well, formalized problems. So this talk is not about that, it's identifying issues. And I'll give you, let's say, some description of the problems that can arise. So that if you're a researcher or an engineer, whatever, you can probably say, oh yeah, I can solve it, or maybe I should try to solve it. So what do we have to do? Again, we have to bridge assets from one layer to another one and bridge them back. So the way roll ups work is you're going to take your ether, for instance, and say, yeah, I would like to use the roll up to use my ether, but you need to have sort of a bridge to enable you to work with a copy of your ether and so on.
00:25:02.354 - 00:25:49.934, Speaker B: And then at some point you'd like to, let's say, off ramp and get some money or transfer them ether to another currency. You need to transfer them back to ethereum. So you need a bridge in both directions. You also need, if you want your roll up to be successful, is to be able to run the code that you would be running on layer one, to run it on the, layer two on the roll up. And it should probably give the same result, right? So that could be an issue. You want to make sure that the data you are providing, so when I was talking about posting the sequence of transactions that you have committed to, you want to make it available for third party to be able to verify that you have computed the state correctly. So how do you do that? On layer one, you've got the blockchain, you've got the Genesis block.
00:25:49.934 - 00:26:36.594, Speaker B: All the blocks are available so you can verify how it works. How do you do it on a layer two, there's some finalization issue as well. So when you submit a transaction, when is it irreversible, when is it finalized and immutable? So on layer one, again, there are protocols, the LMD, ghost, supermajority, justification, finalization, and so on. And as I was mentioning before, it takes an average 16 minutes. There's another issue as well. You're going to post, let's say, trans sequences of transactions and the computation of the sequence of states that should arise from executing this sequence of transactions. So you would like to verify that the states computed are correct or not.
00:26:36.594 - 00:27:43.634, Speaker B: So if you're on virtual machine, your computation is transferred, posted, sent to lots of other nodes, and they can recompute the next state. And if they disagree, of course they wouldn't vote for your next state computation, they would vote for another one, and your wrong computation would not be accepted. And there's a mechanism with some stakes and so on, and you can show that if you want to force, let's say, a wrong computation, you would need more than one third of all the stakes of the validators in the system. So with client diversity and the fact that the computation is performed by lots of different nodes, you can ensure that the computation of the next state is somehow correct. According to what? I'll discuss it later as well. The decentralization is also an issue as well in the chat, right? Because you've got a single sequencer deciding which transitions are going to be in the next batch. So the thing is, of course, you can censor some transitions.
00:27:43.634 - 00:28:30.006, Speaker B: You can decide to order transition as you will, and maybe front run or whatever. Yeah, this can exist as well. So in Ethereum, this is supposed to be handled by choosing the validators in a random order, having the proposal build the separation scheme and stuff like that. But we need something as well. On layer two. And now on the Ethereum virtual machine, when you execute your transaction, of course, you pay for the execution of your transaction as a user and as an actor, let's say validator and so on, you can get rewards or be slashed when you verify some transactions or where you do something wrong. So the same mechanism may be needed on the layer two.
00:28:30.006 - 00:29:32.054, Speaker B: So how does it work exactly, and what are the problems that you can have? So I'll try to cover some of these issues in the next few slides. Right, so the first one is bridging. And so in this context, what you have to do is to make sure that you can take some of your assets, let's say ether. Have a sort of a copy of your ether on the roll up on the layer two, and use your ether on layer two. So this is usually the simplest way to do it, is to have some smart contracts, to sort of twin contracts that would bridge from layer one to layer two and layer two to layer one. And the standard solution is when you want to send your assets, let's say ether, to layer two, you would lock some of your ether in a bridging contract. Then there will be some mechanism to trigger some minting of the equivalent number of tokens, but copy of them, they are usually called wrapped whatever tokens or wrapped ether.
00:29:32.054 - 00:30:19.530, Speaker B: So a copy of your token that you've locked in the bridging contract on layer one will be made available to you and minted on layer two. You use them on layer two, and at some point when you want to transfer the remaining tokens that you have on your account, you will try to transfer them back to layer one. And the mechanism that's usually used is that you're going to burn them so they will disappear on layer two. And this contract is going to synchronize with the bridging contract on layer one. That will deliver, somehow unlock some of the ether and send the equivalent of the tokens you have on layer two back to your account on layer one. So you can see here that there's communication, l one to l two, messaging. So you need to have some good communication.
00:30:19.530 - 00:31:24.602, Speaker B: And if you want to transfer the assets from one layer to another, another thing that you have to do is to be able to execute bytecode. So EvM bytecode that you would have executed on layer one, you'd like to execute the same on layer two. So why? It's because if you have your smart contracts, for instance, compiled to EVM bytecode and you need to change them or have a specific compiler to run them on layer two, you may not be very happy with it and you can't really ensure that it does the same thing as it would be doing on layer one. So these are the two problems that you have to solve. And what are, let's say the main challenges in there is that there's a cross chain interoperability. So do you communicate, let's say generically from layer one or a layer two to another one? So the way it works, if I understand well, correct me if I'm wrong at the moment, is bridging from l one to l two. You've got the bridging contract on l one and it will produce some log or event messages.
00:31:24.602 - 00:32:09.402, Speaker B: So there should be some mechanism on layer two that's monitoring the log messages on l one and retrieve them and trigger some minting when they see some messages coming from their own bridging contract and so on. So that's a complicated mechanism. There were lots of attacks in the last few years on bridges. And if you want to get a reminder of this, there were recent talks at this meeting, actually by ernias a couple of months ago and David a couple of weeks ago. So I encourage you to watch them again. So basically the problems here are to design some cross chain interval operability mechanism that can be used out of the box. So this is not fully solved as I understand it.
00:32:09.402 - 00:33:25.554, Speaker B: And let's say mechanisms or templates that you could try to get to show that your bridges or the smart contracts that you implement to perform the operation of the bridges are secure so you won't lose assets, they won't be attacked and so on on the, let's say, code side. So you want to execute the same bytecode. So do you have to modify it or not? So the issue here is that some of the opcodes in the EVM, they have some meanings that are actually strongly tied to what's in the EVM on layer one. So some of them, for instance like the base fee EIP 1559 gives you the current base fee, how much it costs, how much the gas is in the current block. Block number is also an opcode in the ethereum virtual machine that's of course related to the blocks that you have built. So when you port your code and you execute it on layer two, you have to deal with these opcodes and decide what's going to happen. What's the base fee? What meaning has the base fee on layer one? On layer two, sometimes you can even pay in another currency, the gas on your layer two you don't need to pay it in if you could pay it in arbitrum or optimism token and so on.
00:33:25.554 - 00:34:10.430, Speaker B: So what does it mean, the base fee? The current solution is that some of the opcodes are somehow disabled so you can't really use them. So that's an issue. And this issue is usually bundled under the term EVM equivalent. So what exactly can you execute on layer two? Can you execute unmodified code? And does it have exactly the same meaning? So there are currently solutions that are trying to implement all the opcodes. So there are also different levels of what's called EVM equivalence. So again, equivalence here, I'm a bit sorry about it, but it's not formally defined. Usually it's a wish.
00:34:10.430 - 00:35:06.750, Speaker B: So there's an article by Vitalik describing different types of EVM equivalents in increasing order of strength. So now another issue that can of course arise is what's called the data availability issue. So in this slide I've shown that actually you publish the transaction data, so you publish them on layer one, at some point they become finalized. The thing is that if you publish them on layer one you still have some interaction with layer one, and again it's going to limit your throughput. You can't publish more than 15 are dependent on the somehow performance of layer one. To determine your overall performance, you may have to wait to push more sequence of transactions to be archived on layer one. And when you push them.
00:35:06.750 - 00:36:38.078, Speaker B: So the solution, as I'm going to show, is maybe to change layer one to have a dedicated layer to handle data availability. But then you'll have the issue of deciding and showing that you actually can retrieve the data and the data are available. So how does it work? So you could, for instance, a couple of weeks ago as well, there was a talk by Sriram Kenan on Eigen layer and eigenda, which is sort of a layer that can for you, handle data availability. So if you do so, then you may increase, if the availability layer has better performance than layer one, of course you increase the throughput potentially, but at the same time you'd like to know whether you can retrace the data. So when you push the data to this data availability layer, what kind of proof do you have that the data has been stored and not corrupted? How long is it going to be stored for? And so on, how much is it going to cost to retrieve it? So just a minor point on that note is that if you use different layer one data availability layer, the roll up that you're building is usually called, I think, a validium rather than a roll up because the security is not inherited directly from day one ethereum. So you have to solve these issues. So another problem that rises, as I mentioned before, is what's called the finality problem.
00:36:38.078 - 00:37:53.270, Speaker B: So on neural layer two at the top right, you're taking transactions, execute them, pushing the sequence of transactions to the layer one. And on layer two, what's happening is, well, once you've determined the sequence of transactions that you're going to execute on layer two, this is final. This is final because you're not going to change this order and this transaction, pushing the order of transaction on layer one, at some point you're not going to change your mind, right? You've decided which order it was and you're not going to change it. So the order of transaction is final and you can recompute the state by processing the sequence of transactions to retrieve the current state. So this is not going to change. But now, assuming you would like to transfer some assets from layer two to layer one and you've performed some computation, you still need to wait for the confirmation that your sequence of transactions and of course the sequence of states that you've computed has been accepted and has been validated somehow so that the computation is not corrupted. So you'll have finality at some point on layer two.
00:37:53.270 - 00:38:27.182, Speaker B: So here I wrote l one finality. That's a bit misleading. It depends on l one finality, but it's l two finality. So at some point you will have finality of the transaction. L two. It depends on the finality of the transaction on l one or the data availability layer if you're in an optimistic roll up, because the mechanism to verify that the computation has been done properly is sort of a time window that you have to wait for. You may have to wait for up to seven days before your transaction is final on layer two.
00:38:27.182 - 00:39:12.314, Speaker B: And then that means that you can't actually withdraw your funds from layer two to layer one, bridging them before this period of seven days. If you're in a ZK roll up. So you have finality when you publish your sequence of states. But still you may be dependent on the 2.5 epochs finality for layer one, because your verification in a ZK roll up is a transaction and a call to a smart contract. So you need to wait until this process has been fully done. So the question here is, how do we determine finalization? How can we shorten finalization? There are lots of, I think, solutions that have been proposed.
00:39:12.314 - 00:39:52.300, Speaker B: If I understand well, arbitram, they rely, which is an optimistic wall up. They wait until transactions have been finalized. Well, the publication has been finalized on layer one, plus, of course, the seven days optimism. There are some mechanisms to try and fast, somehow speed up the finality, but you need a mechanism to sort of handle potential reorganization on layer one that would impact the result on layer two. So that's not very easy to understand. I don't really know how it works. If you know, please let me know.
00:39:52.300 - 00:40:40.454, Speaker B: But this is an issue to try and shorten the time between finality on layer one and the other layer and be able to transfer funds rather quickly between the two layers. So it's not a matter of transferring the funds. But when can you do it and can you do it fast enough? If you have to wait for seven days to withdraw your funds from layer two to layer one, that's a big issue. So yeah, I've talked about that already, so I'm going to skip that. So now the verification issue. So in a roll up, what's called the sequencer is going to pick up some transactions from the mempool, order them, and compute the sequence of states. And to do so, of course, this sequencer is going to use an implementation of the EVM.
00:40:40.454 - 00:42:01.498, Speaker B: So an EVM interpreter, it's going to compute the transition function. And it doesn't matter whether it's an optimistic roll up or a validity proof roll up you've got some mechanism to be able to verify that this actor, the sequencer, is not malicious. So in optimistic roll ups, you've got seven days to check whether this computation has been properly formed or not. And in ZK roll ups, you're going to get a proof posted with the resulting state, and you can check the proof. The commonality between these two things is that you've got the sequencer computing the sequence of states, or the new state with an EVM interpreter, and the verifier, be it a ZK verifier or a verifier, let's say an optimistic rollup that's going to recompute the state, has got their own implementation of the EVM virtual machine. So what's going to happen is, let's say in an optimistic roll up, you may have a verifier that picks up a sequence of transactions within the seven day window, replace it and say there was a mistake. Well, there was a difference in the state, and I found a state after one transaction that's not the one that has been computed.
00:42:01.498 - 00:43:02.138, Speaker B: So there's a dispute in that case, and you're going to have to resolve the dispute. The same thing could happen with the ZK roll up. You're posting a proof, and the verifier of the proof could say, no, I don't accept the proof. The proof is wrong. I reject it. So at the end of the day, what's happening is that you've got a mechanism with a resolver, again, an external third party node in optimistic rollup that's going to redo the computation and maybe narrow it down to one instruction in the EVM and so on, or a verifier on the ZK rollup, but they have their own version of the semantics of the EVM, and you're going to have to trust the semantics of this verifier. So the question is, how do you know what the reference semantics is that's going to be used? Is it equivalent to, in an optimistic wallet, the fraud proof resolver semantics or the ZK EDM semantics? So there are several issues in there.
00:43:02.138 - 00:44:18.430, Speaker B: So, for instance, in arbitrum, which is to my knowledge the only optimistic wall up with a fraud proof mechanism enabled at the moment. The fraudproof is actually a third party that can run GEF, and Geth has been computed with the target, which is compute. Let's say you've got an execution of Geth with the target code, which is Geth compiled to WaSM, and you execute Geth written in WASM, whereas the sequencer in arbitram is executing Geth compiled to go, but they are the same source version of Geth. If you want to trust this mechanism, you need to trust the compiler from go to the native bytecode of your sequencer where you execute the code, I don't know, arm 64 or m one or whatever. And you also need to trust that the compiler to WaSm and the semantic of WaSm and the runtime of WaSm, all of this is equivalent. So that's a huge trusted computer base for ZK rollups. You've got this sort of semantics of the EVM, which is given not actually by a function, but usually by a relation.
00:44:18.430 - 00:45:11.630, Speaker B: So you need to trust that the relation and the semantics of the EVm that's been written in circuits and constraints is somehow a trusted one and equivalent to some reference semantics that you would have, let's say the yellow paper, which is not a good reference semantics because it's ambiguous. So yeah, who do you trust? If you have a dispute, why would you trust a component that replaced the computation better than another one? So issues like that can arise actually. Right? If you have different versions of the EVM, you update one, but you don't update another one and so on. So when you resolve some disputes, how do you make sure that you can trust the resolver? Another one that I'm going to summarize here, I'm taking the time. Yeah. I'm going to conclude in a few slides. It's the fees and rewards.
00:45:11.630 - 00:46:05.458, Speaker B: So I'm going to give you a perspective in this slide of an operator of a roll up. So when you operate a roll up, what you have to do is you've got batches of transactions arriving on your layer two. You're going to execute them very fast, and then you're going to post the data of the transactions to layer one. But you don't really know at that time how much it's going to cost. So you know how much gas it's going to consume. You can compute it from the size of the transaction data, but you don't know the gas price at the time where the transaction, the data availability transaction is going to be executed on layer one. So when you're going to charge your users, you're going to charge them with the predicted version of what it's going to cost when you're going to post them later on.
00:46:05.458 - 00:47:20.506, Speaker B: So ideally you would like to get the right price or you can overestimate it or whatever. If you overestimate it, you're going to charge your users. Even if you spread the data availability fees, you're going to check your users more than they should have been charged. If you undercharge them, you're going to be out of your own pocket. You're going to pay fees for posting the transactions, but you haven't charged them properly. So what mechanisms can you try to implement to get a better, let's say, strategy? So in terms of problems, this is how do you ensure that you've got lower tax fees? And also, can you make some profit or at least not lose money when you operate your layer two? Again, if you post transaction data and every time you post them you lose some money, it's not good. So what are the strategies? How can you adapt dynamically to the current price of ether on layer one, so that you're getting your money back and you're not charging your users too much? So at the moment, most of layer twos, they work using gas oracles or simple prediction mechanisms to predict the price.
00:47:20.506 - 00:48:11.914, Speaker B: That's going to be the price of publishing the transaction data, and they try to adjust it dynamically. And finally, decentralization. So as it was mentioned in the chat earlier, you've got a unique sequencer in all of a roll up deciding the order of transactions. So that creates, of course, potential censorship or delays in processing your transactions or font running. So ideally you'd like to decentralize the sequencer, so have no central authority to decide the order of transactions. You can try to do so, but then you probably need a consensus mechanism which is going to introduce, to impact the performance of your layer two. They're going to communicate the different nodes involved in the decentralized sequences and so on.
00:48:11.914 - 00:48:57.062, Speaker B: And you may also have to deal, as on Ethereum with front running and met attacks. So it's not an easy problem to solve. It's the same actually, I think, as on Ethereum. So you have to be careful with it. So that's probably the best I can cover. So I can say that some mechanism here with the decentralized sequencer that has been proposed by, for instance, optimism in the upcoming bedrock or the next iteration of it, they are mentioning that they would transition to decentralized sequencing by rotating the sequencer. I think rotating the sequencer is not at all decentralizing the sequencer, it's rotating the centralization, which is a very different concept.
00:48:57.062 - 00:49:37.936, Speaker B: So a good mechanism to provide some, let's say some low trust, low cost decentralization on layer two is still to be designed and properly analyzed. I'm going to check the chat. Right. So I'll go to the chat after I'm going back. So that's the end of the problems that I have identified. Again, that's not an exhaustive list of all of the problems you can get, but that's some of them. And I'm going to look at the chat.
00:49:37.936 - 00:49:47.860, Speaker B: If there's no question, I'm scrolling back because I don't know which ones have been answered by Peter.
00:49:49.480 - 00:49:59.900, Speaker A: It attempted to be answered by Peter, but you should put in your own thoughts. And I know I've got a question or two myself, but let's just. Why don't we fire through a few of them in the chat?
00:50:00.720 - 00:50:12.240, Speaker B: Yeah, I'm still scrolling up. So this one I remember I answered.
00:50:13.780 - 00:50:16.640, Speaker A: It's good to have a popular topic, isn't it, Frank?
00:50:17.540 - 00:50:54.270, Speaker B: Yeah, it's good to have questions. So let's see the sequence. I stopped there. So is the future of roll ups dependent on the robust infrastructure of bridges or is it tilted towards a bridge less design? So there's some answers by Peter Bridges. L one roll up are straightforward. Bridging roll up to another l one. Yes.
00:50:54.270 - 00:51:37.288, Speaker B: So I don't know. I can't answer this question. I think it's moving very fast. I wouldn't claim that I'm aware of all the solutions. I think it would be nice, for instance, let's say for roll up designers, right? Your customers are going to ask you, how can I bridge my assets, my token ether, blah blah blah, to the roll up. So at the moment, if I'm correct, most of the bridges are on an ad hoc basis for each token. So you design a twin contract for ether.
00:51:37.288 - 00:52:07.976, Speaker B: Twin contracts to bridge ether from layer two, twin contracts to bridge near to layer two and layer one and so on. So it's another process. Even if it's an ERC 20 contract, there's no real good generic solution to bridge ERC 20 assets. They usually customize the contracts. That's what arbitram and optimism say that they are doing. So having a sort of a more streamlined solution will probably help. And the communication as well.
00:52:07.976 - 00:52:36.124, Speaker B: The n one and two messaging and l two n one messaging via the logs or the event. It's something that's probably not optimal. You have to extract from lots of text and also encrypted messages. You can't communicate in plain text. Right. You can't say, oh, I'm bridging ten ether from this account to another one because I could publish the same message and fake it. So you have to really make sure that it's properly encrypted.
00:52:36.124 - 00:53:02.632, Speaker B: So the bridging, I think, is an issue. And lots of attacks on bridges happened last year. I think it was more than $2 billion stolen with bridges within bridges. So the verifier. So Peter has answered some question. The verifier is a contract on l one. Proverb works with sequencer to generate proof to submit to verifier on l one.
00:53:02.632 - 00:53:30.640, Speaker B: Okay, how about the challenger of optimism? Where does it sit? So if I'm correct, optimism, even in the newest bedrock iteration, they don't have fraud proof enabled. I don't know what the challenger is in this setting. If the challenger is a verifier that's not enabled yet.
00:53:30.790 - 00:53:43.540, Speaker A: Yeah, I think I recall there were some contracts that sort of looked like they were going to allow you to challenge, but then there was another point that someone had that maybe that wasn't.
00:53:46.060 - 00:54:50.812, Speaker B: I think there's an interesting mechanism in arbitram. Of course, in arbitram, the mechanism is that you can have validators proposing new states, and there's a mechanism such that if at some point you have one state with more than two children, there will be a dispute and the dispute will be resolved. And the states that have proposed the different states, they have to stake some assets to stake to it. So the dispute will be resolved and the stakes may be flashed and so on. So this is a mechanism that's somehow incorporating some third party verification or validation. So it's in between voting, like on ethereum, and it triggers the fraud proof. So the fraud proof is triggered when, after a certain amount of time, you've got two children branching out of a given state.
00:54:50.812 - 00:55:34.900, Speaker B: They will be the dispute resolution in case the malicious. Where is it? This one? The challenger. Why? Optimism verification takes seven days. What's the seven day? Oh, yes. So the seven day time window, it's an empirical number. Some people have decided that seven days should be enough for you to contest, but it could be 1 hour or 2 hours. Honestly, I don't know if there is anything rational that can justify the.
00:55:35.430 - 00:55:56.780, Speaker A: So I think the idea is that you don't want to have something bad happen. Just as there's a lot of network congestion. Like you can imagine, someone does something that caused the whole network to be completely blocked, and then someone says, hey, let's do something malicious now, because no one will be able to put a fraud proof in.
00:55:57.710 - 00:56:03.834, Speaker B: Yeah, but the seven days, it could be two days or three, I don't know, actually. Yeah.
00:56:03.952 - 00:57:18.370, Speaker A: Why seven days, not one. It would seem incredible that the whole of the Ethereum network could be completely jam packed for a whole day. But I don't know, maybe, I mean, a week feels safer for a proof. So here's a question for you for asking one live in Polygon edge, you've got like a decentralized sequence, you've essentially got an IBFT two chain blockchain, so you've got instant finality, and so all the blocks are producing state and you've got these ordered list of transactions. So that's your decentralized sequencer and then you've got zero, which is taking the trace of the transactions and producing a proof. And you post that proof onto l one. And so we've been trying to think what happens if there's a bug in the proverb.
00:57:19.350 - 00:57:41.500, Speaker B: That's part of the verification issue that I was mentioning. You're right, you can have a bug in the EVM engine that you're going to use to replay a transaction in the verifier, let's say the ZK prover for instance. You can have a bug in the prover, you can have a bug in the circuit, you can have a bug in the prover and so on. So yes, I agree.
00:57:42.030 - 00:58:31.130, Speaker A: Yeah. And I mean the way we came down to for recovering from that is you'd have a hard fork of essentially the proving engine, like how you can have a hard fork of a blockchain having a hard fork of a proving engine. But I'm just trying to think of would you actually detect, because from what I can work out, you're going to create a valid proof. So it's still going to verify on the contracts on l one. So maybe you would have incorrect state, but no one notices or cares. I don't know, I've been grappling with that one for a few months to try and think about what does it really mean and does it really matter if you've essentially got two versions of the EVM running producing.
00:58:32.350 - 00:59:32.720, Speaker B: So the thing is basically nobody can tell, right? We can't formally prove that. Let's say a ZK proof, which in a ZK proof you have embedded the semantics of one, semantics of VM in your proof relating to different states, to sequence of states and so on. You have, let's say on the other side, get that's an implementation of the semantics. There are two implementations somehow the question is how do we decide which one is more trusted or trusted, right? Which one is the reference one? And I think it's something that you can't really resolve because you would have a formal specification of the semantics. But what you need to know is probably your computation is going to be checked against this semantics that's implemented in geth or whatever. So you should know the target or the reference of the semantics. Otherwise it's very hard to know.
00:59:32.720 - 01:00:19.882, Speaker B: I mean, you can honestly compute the correct next state, but the verifier will have the wrong code and you'll be slashed. So this is not right. And what you can observe is there's a difference into the next states that have been computed. What do you do in that specific case? Do you flash straightforwardly or do you say, all right, maybe we should check a bit more, who's right, who's wrong? Yeah, there's no reference semantics. So I think the problem to me is really between, let's say, geth or Besu and so on. You can test them, right? You test them against a huge number of tests, test suites. So you've got some confidence that they produce the same thing on a lot of different EVM bytecode.
01:00:19.882 - 01:00:49.990, Speaker B: But for ZK evms, you're encoding the semantics of the EVM into constraints and then circuits and then polynomials and blah blah blah. You don't have an executable semantic, so you can't really test it against an existing one. So that's a big issue. And the encoding into all of these different layers, right, polynomials, constraints, and so on, is not trivial. So there's a lot of room for errors in that encoding.
01:00:52.590 - 01:01:48.860, Speaker A: Yeah. So there's a discussion in the chat about whether patching the prover is not a big as big issue because it's non chain, the code is not on chain, but I think the proof would have been published on chain. The thing I was thinking is if in your constraints engine inside of your circuits, you had missed a constraint and then had a bug in the circuit that wasn't being detected because you were missing a constraint, then I think you're going to end up with an invalid in the wrong state, and then you'd have to do a hard fork to get back to the correct state.
01:01:50.910 - 01:02:42.918, Speaker B: Yeah, I agree with that. Yeah. But, so I see the chat, invalid proofs will be rejected. Yes, I understand that, but the Rejection will be the prover has rejected the proof, but we don't know whether the prover itself is not buggy. So the proverb should be, I would say, yeah, that should be okay to design a prover that's not buggy. The proof itself, the mechanism to generate the proof is very tricky. So you can have a mistake in the semantics that you've encoded in the ek proof and then you can't really trust the fact that the prover accepted the proof is a valid argument.
01:02:43.014 - 01:02:47.982, Speaker A: Well, and that's the thing. If you're missing a constraint, then you could accept the proof, but it still could be wrong.
01:02:48.036 - 01:03:13.080, Speaker B: Yes, we accept that there's some trust and some trusted components. And I think to me that's what we should be aware of, what is assumed to be trusted and we can't do anything about it. So it's not about having the correct semantics, but it's having the reference of what is the correct semantics. What is going to be used to resolve disputes that you need to know.
01:03:13.530 - 01:03:21.910, Speaker A: Yeah, and Ron has said something about Alio is working on a certified compilation.
01:03:24.250 - 01:03:36.542, Speaker B: Exactly. Again, these problems, these are not problems that I have identified on my own. These are problems I think are important. And there are some people working on them already. That's good to know.
01:03:36.676 - 01:03:40.554, Speaker A: Sounds good. Could you share your slides again and can we do those last slides?
01:03:40.602 - 01:03:43.422, Speaker B: Oh, yes. Sorry. That's okay.
01:03:43.556 - 01:03:45.840, Speaker A: Let's talk about what's coming up next.
01:03:47.090 - 01:04:14.220, Speaker B: If I can retrieve it. Let's see. No, that's it. That's it. Click on the wrong one.
01:04:36.420 - 01:04:49.910, Speaker A: I think I've just about to have it here and I'm just going to have it open, hopefully in 54321.
01:04:53.960 - 01:04:56.390, Speaker B: Yeah. All right.
01:04:58.620 - 01:05:00.936, Speaker A: Just got to wait for it to turn up.
01:05:01.038 - 01:05:03.800, Speaker B: But anyway, I shouldn't share mine.
01:05:04.860 - 01:05:14.990, Speaker A: Yeah, actually that's not happening for me. You know how life is sometimes when you just want to look at something.
01:05:15.920 - 01:05:25.616, Speaker B: I can share it. That's okay. I've got the. This one.
01:05:25.718 - 01:05:48.872, Speaker A: Okay, that's enough. All right. Yes. So the merch store is open, so get your cool merch, which is good, including this t shirt here. Next slide. All right, so we've got a few talks coming up. So we've actually got a winter break for a few weeks now.
01:05:48.872 - 01:06:35.110, Speaker A: And then after we come back and for those in the northern hemisphere, a summer. So we've got Adeg Benga, who's on the call right now, is going to give a talk on Paymaster smart contracts, which is going to be awesome. And then the week after that we've got Jaden Windall is going to talk about ERC 65 51. And then Philip Zetner's is going to talk about the case for bridge aggregation. And then after that we're going to hear about forge development. So using forge for solidity development. And that's going to be August 30.
01:06:35.110 - 01:06:58.792, Speaker A: And then the last slide is this one. So if you're here today, there's a YouTube channel. If you're watching this on YouTube, you want to join live. There's the slack invite for the Slack workspace, and there's the meetup group. Sometimes we have example code. You'll find it on there. If you're interested in formal methods, there's a reading group that gets together reasonably regularly.
01:06:58.792 - 01:07:30.470, Speaker A: Join the slack workspace and you'll hear all about that. So thank you, Frank, for so look, thank you for a great talk. And yeah, hopefully people have learned stuff and yeah, I guess we'll see you all in about two or three weeks time. And Adeg Benga is going to be telling us all about paymaster stuff at that point, which will be cool. So see you all then.
01:07:32.120 - 01:07:40.730, Speaker B: You. Bye bye. Bye bye, everyone. Thanks, Frank. That was awesome.
