00:00:01.690 - 00:00:23.680, Speaker A: All right. Hello and welcome, everyone. My name is Peter Robinson, and this is the Ethereum Engineering Group. Meetup. Today we have Ben Edginton, who is going to tell us about Proto Dankard Sharding and an EIP in particular. So I don't know. Ben, tell us about what you're going to talk about and please introduce yourself.
00:00:24.890 - 00:00:44.294, Speaker B: Hi, Peter. Hi, everybody. Hello, world. So, yes, we're going to talk about scaling Ethereum and in the first instance, EIP 4844, which has also become known as Protodank Sharding. So we'll cover all of that. We'll talk about the naming and all of that. So I'm Ben.
00:00:44.294 - 00:01:25.350, Speaker B: Edgington. I have been at Consensus for just over five years. Not quite as long as Peter, but nearly. And I've been Ethereum obsessed since 2016 1st. Heard about it all seven years ago now, and I've been working pretty much continually on Ethereum since. After I joined consensus, I spent the first couple of years helping build the R D team in the protocols group Pegasus as we used to call it, and for the last three years have been leading Teku development. Teku is a consensus client on the Ethereum network, so we helped deliver the merge.
00:01:25.350 - 00:02:08.338, Speaker B: And nice to see some of the team TechU team are here, and other things. I wrote a fortnightly thing, what's new in ETH Two? For about four years leading up to the merge, just tracking the development of Ethereum, but I've put that aside now. I'm now working on upgrading Ethereum, which I grandly style as a book, but it's going to be the definitive go to guide for everything you ever wanted to know about the proof of stake protocol in Ethereum. It's very slow progress. I might be finished in a decade or so. So that's me. Wow.
00:02:08.424 - 00:02:15.590, Speaker A: Can't wait for them. I want to get a hard copy. So there's a bit of pressure on you and before Ethereum Three comes out as well.
00:02:15.660 - 00:02:17.800, Speaker B: Well, yeah, that's the challenge. Right.
00:02:19.690 - 00:02:29.820, Speaker A: All right, so, Q and A, what would you like to do the whole way through at the end? Combination of both? How should people ask questions?
00:02:31.310 - 00:03:10.970, Speaker B: So I'm happy to have questions throughout. I may be in the chat, I won't be monitoring the chat, but Peter can monitor, if you're okay with that, Peter. And if there's something that you think should be answered at that point, that would be great. I'm going to sort of tell a story, as it were. So often, questions anticipate things that are coming later, so the answer might be, hang on, we're getting to that. But if there's something you want clarified or a point you'd like to make, then feel free. Drop it in the chat, Peter can pick it up, and then at the end, we'll do a roundup and we can have open Q A.
00:03:10.970 - 00:03:12.220, Speaker B: Does that sound okay?
00:03:13.230 - 00:03:18.810, Speaker A: Sounds awesome. All right, well, take it away and please share your slides.
00:03:20.030 - 00:03:31.660, Speaker B: OK, you got them? Yes. It says zoom quit unexpectedly. Can you hear me?
00:03:32.990 - 00:03:42.890, Speaker A: Yes, we can hear you and we can see the slide frozen.
00:03:43.710 - 00:03:51.044, Speaker B: Oh, man. How?
00:03:51.202 - 00:03:56.404, Speaker A: Oh, isn't it just life? You do network checks, you do oh, no, you're back.
00:03:56.522 - 00:04:15.756, Speaker B: Am I back? Zoom crashed. There we go. Let me try again. Right. Okie doke. Right, we've done the intros. Here's the outline of where we're going.
00:04:15.756 - 00:04:58.840, Speaker B: So we're going to talk about how to scale Ethereum. Just an overview of our new roll up, Centric Roadmap. And then we're going to get into the meat of the thing, which is EIP 4844, also known as Protodank sharding. We'll talk about blocks and Blobs, we'll talk about polynomials, we'll look at the lifecycle of a Blob, we'll talk about when it's happening, and a special slot on the KZG ceremony, which may or may not mean anything to you by the time we get there, I hope it will. We can't really talk about EIP 4844 without looking beyond into the future. So we're going to do that. We're going to look at full dank sharding under the topic of data availability sampling.
00:04:58.840 - 00:05:49.608, Speaker B: I'm really going to give a very light sketch on that. We're not going to spend a lot of time on it because it's a huge topic, but we do need to take a quick look. So let's start how to scale Ethereum. And we need to start with some hard truths. Ethereum Mainet processes on the order of ten transactions per second. So when I talk about transactions per second, I am going to talk in order of magnitude terms. Mainet today can process maybe 25, ERC 20 transactions a second, maybe 50 or 60 simple Ethereum transfers, or fewer complex contract interactions, but on the order of ten transactions per second.
00:05:49.608 - 00:06:39.604, Speaker B: And when I say transactions per second, I'm using non BS TPS. Our colleague Pat McCorry has a great talk. I've got a link to it later where he talks about TPS's BS. The point being that any network can crank up their transactions per second to hundreds, to thousands, to tens of thousands, but they make fundamental sacrifices when they do so. So we are talking non BS TPS in this talk, so they're balanced throughout the system. There are absolutely no plans to improve on Ethereum Mainet transactions per second. Right? So this is what surprises people, but this is the actuality.
00:06:39.604 - 00:07:25.400, Speaker B: So let me just sort that out, change things so I can at least see a few of you, because that helps. I don't feel I'm talking to myself. There are no plans to change the improve on ten transactions per second or so. Over the time, we have had marginal improvements in, or marginal increases in block gas limit. With the merge, we got slightly more blocks per second over time. We got bit better at gas optimization, so you can put more transactions in a block. So there's been a few percent here and a few percent there improvement, but there's nothing on the horizon, which is going to give us two x or ten x or 100 x more transactions on main net.
00:07:25.400 - 00:08:04.500, Speaker B: And why not? Because running a decentralized network places severe limits on block size. As I mentioned earlier, we can crank up the block size to 100 to 1000 transactions per second. But if we do that, most of the nodes on the network will not be able to cope. They won't have the bandwidth, they won't have the processing power, they won't have the storage space. And so your network will be run by a small number of supercomputers. And there are networks out there who regard this as a feature, not a bug. But this is not the Ethereum way we enable decentralization, it's in our DNA.
00:08:04.500 - 00:08:52.028, Speaker B: So if we want Ethereum to be the global internet of value, it really needs to be able to achieve 100,000 transactions per second. That's one transaction per day for everybody on the planet. So a relatively modest goal if we are really going to achieve a human planetary scale embedding of Ethereum there. So we have this gap of 10,000, a factor of 10,000 between where we are today and where we want to get to. So how are we going to fill this gap? Right, some of the next slides some of you will have seen before, but we'll skip over them and get to the new stuff soon. But this is important background. So Ethereum has this roll up centric roadmap.
00:08:52.028 - 00:09:29.080, Speaker B: Let me give you my very hand wavy analogy for what that means. Here we have a picture of Lower Manhattan, probably some of the most valuable real estate in the world. And it's constrained in size. So like the Ethereum block space, it has a finite footprint. If it were populated with low rise buildings like we see in the background, then that would be a very inefficient use of the space. So what economic forces have dictated over time is that it's packed more densely. And the way that's achieved is by putting skyscrapers on it, building upwards.
00:09:29.080 - 00:10:03.896, Speaker B: So it's now extremely densely packed. And in a similar way, roll ups put skyscrapers on Ethereum and I will illustrate that. But they pack more densely the existing real estate that's available, we don't have to imagine this I came across while I was looking at this. This amazing YouTube video won't show it, but you'll be able to see it from the notes. Evolution of the Lower Manhattan skyline from 1900 to 2018. Recommended watch. So this is what Ethereum's looked like in the past.
00:10:03.896 - 00:10:39.856, Speaker B: Everything has happened on layer one. So we've got Ethereum layer one at the bottom. That's our block space and it can support about ten non BS transactions per second. And on top of that, we've built applications, we got DFI, NFTs payments, Dows, identity stuff and everything anyone can imagine, built on Ethereum layer one. But constrained by this block space. Where we are today is a little bit improved. So we've still got the same block space at the bottom provided by the L One, the base layer of Ethereum.
00:10:39.856 - 00:11:21.508, Speaker B: But on top we're building this L two, layer two ecosystem. And so what we've done is we've put a new type of application on top of the block space, which we call a roll up and roll ups like kind of virtual machines. They run their own applications inside them. So this is building up. So each of roll ups, I've got five here as an illustration, optimism, Arbitrum, ZK, sync and so on and so on. And they're available today on Ethereum, though they're not yet maxed out in capacity. When they're maxed out in capacity, we'll be looking at maybe 100 transactions per second.
00:11:21.508 - 00:11:59.160, Speaker B: So a ten times improvement in throughput of the Ethereum ecosystem. By using block space more efficiently, what these roll ups do is they compress their data so it more efficiently uses the existing block space like the skyscrapers. So that's available nowish. And basically what we've done is sort of modularize Ethereum. So we've got a separation of concerns now. So we've got layer one, we can focus on optimizing it for security and reliability. And now we've got a layer two, which we optimize for throughput and usability.
00:11:59.160 - 00:12:34.324, Speaker B: And this is where user applications will sit. And we still have sort of legacy applications competing for space on layer one as well. And the way rollups do this is they pack the data they need into the call data of an Ethereum transaction, which is a very expensive way to store data on the blockchain. And we'll come to that. So this is what the near future looks like. And this is the topic of this talk. So this is EIP 4844 Ethereum improvement proposal, also known as Protodank Sharding.
00:12:34.324 - 00:13:30.620, Speaker B: It's named after two characters in ethereum cordevs proto Lambda and Dankrad. And the idea is that we have lower Manhattan on the left, which is our existing block space, and we bolt on a Blob, which is like an extending lower Manhattan with a pontoon, and we build skyscrapers on top of this pontoon. And so we've got this extension to our block in the form of this blob, provides a huge amount more data, and roll ups can use that data and potentially scale by another factor of ten. So we're looking at 1000 transactions per second. And I'm going to unpack everything that I just said over the later slides and then the far future. This is what's called full dank sharding. So we have Sharded blobs, they're much bigger.
00:13:30.620 - 00:14:11.652, Speaker B: And this gives us another factor of 100 in throughput. And this is where we get 100,000 transactions per second that the Internet of value for the planet needs to sustain. And this is not to scale. Existing legacy applications are on the left, but they're invisible in this model. And you've got these applications running on roll ups, so that's the roadmap and we're going to focus in on the near term EIP 4844 thing. So what we need to talk about is blobs and blocks. Those are our currency.
00:14:11.652 - 00:14:46.532, Speaker B: And this is basically the takeaway from this whole presentation, the TLDR of the whole thing. If you remember one thing about this presentation, EIP 4844 bolts blobs onto blocks, okay? And it's alliterative you can easily remember. It bolts blobs onto blocks. And I've even put a visual aid in for you. So this is basically all you need to know about EIP 4844. Got to have a meme, right? Is it a block size increase? Because I can't talk to you very easily. I'm going to talk to myself for a moment.
00:14:46.532 - 00:15:11.212, Speaker B: So forgive me, dude. You said we can't increase the block size. Yes, I did say that, didn't I? You just increased the block size, kind of. But blobs are not blocks. What have we got? We've got block space and we have Blob space. So let's compare the two. So block space is what we have today.
00:15:11.212 - 00:15:49.944, Speaker B: Blob space is what we will additionally have after 4844. Block space is seen by all nodes, as is Blob space, but they differ in their longevity. So blocks hang around forever. Okay, we're committed currently to storing the whole history of the chain right back to Genesis. So in Ethereum terms, getting on for eight years worth of blocks stored. So anything we put into a block is a forever cost. For nodes that have to run Ethereum, blob space has a longevity measured in weeks.
00:15:49.944 - 00:16:29.456, Speaker B: In fact, 18 days is the plan. So we can forget blobs very quickly. The other big difference is that blobs are visible to the EVM. They contain transactions that the EVM is interested in, so it has to process the contents of blocks. Blob space is inaccessible to the EVM, so it has no computational cost for people running nodes. As a nicerty, we store blocks on the execution client kind of. I mean, Beacon blocks are on the Beacon client, consensus client as well, but the execution part of the block.
00:16:29.456 - 00:17:01.996, Speaker B: So transactions are stored on the execution client. Blobs are stored on the consensus client. Since everybody has to run both, this is just a nicety as for size. So this is the call data size. Currently, we can put data into blocks alongside transactions. If we filled an entire block with data and no executable transactions, we could have almost a megabyte as a target. It can actually go to double this, but that increases the price of blocks.
00:17:01.996 - 00:17:36.708, Speaker B: This is the cap for EIP 1559, but the on target size of a block. We can pack in almost a megabyte of data, but it's very expensive. You're paying 16 gas per byte to do this. Blobs are a little more modest. They're initially targeting 256. That may increase, but the point is they're super, super cheap because we don't have to store them forever and they are not visible to the EVM. So Blobs have a bounded storage cost to a node.
00:17:36.708 - 00:18:24.010, Speaker B: They have no execution cost effectively, but still because they're seen by all nodes, every node on the network bears a bandwidth cost because the blobs are transmitted around the whole network in every node season. So that's the distinction. So to recap EIP 4844, it bolts blobs onto blocks and blobs are ephemeral. Okay, going back, remember, this is all you need to know. This is it. This is the essence of 4844, right? And if ethereum design had gone in a different direction, we could have just stopped there. Basically, the rest of this talk is an exercise in massively overcomplicating things because that's what EIP 4844 does.
00:18:24.010 - 00:19:25.020, Speaker B: What is a blob? So all it needs to be is an opaque string of 128 binary data. We have two in a block, so 256 block. And as far as building a scalability system for ethereum, this would do, this would be fine. The EVM doesn't need to know about the structure of these blobs and the consensus layer doesn't need to know about the structure. We could just have this opaque binary blob, literally a binary large object and really not worry about it at all. But in fact, if we look into the structure of what a blob is, is actually 4096 elements of a field of primodulus this massive number, which is a 255 bit number. That's kind of interesting.
00:19:25.020 - 00:20:45.508, Speaker B: Can we look deeper? Well, in fact, it turns out that blobs comprise coefficients of a degree 495 polynomial over an elliptic curve of this group order. And to achieve our goals with 4844, this is completely unnecessary, but this is what it is in the design of ethereum. So what have polynomials got to do with it? So I've said that a blob is a coefficients of a polynomial and our data points are the evaluation of that polynomial at specific points in that field. So if we have a bunch of data, we need to find a polynomial that evaluates to that data at certain points and those points are the roots of unity in the field. So if we have a 1023 degree polynomial can encode 1024 data points and it will have be evaluated at the 11th roots of unity in the field. So it's a nice d. But the thing about evaluating polynomials at roots of unity is we can use Phosphoria transforms and inverse fastorial transforms to switch between the polynomial representation and the data.
00:20:45.508 - 00:21:29.540, Speaker B: So we call it polynomial representation and the evaluation representation and we can switch between them using fast Fourier transforms and that's order n log n. So it's really efficient and really well understood mathematically. So we're encoding the blob, we're encoding our data as a polynomial, we're storing the coefficients that polynomial in this blob. So what polynomials enable data recovery. So this is the big one. This is the whole motivation for using polynomials. So this is similar to Reed solomon encoding, and we'll talk about it a bit later when we do data availability sampling.
00:21:29.540 - 00:22:08.850, Speaker B: The bottom line is if we encode data in a double width polynomial, so we have n data points, and we encode it in a two n length polynomial or two n degree polynomial, then we can lose half of our coefficients and we can still reconstruct the original data. So this is the data recovery side. Do we use this data recovery property in EIP 4844? I ask you. We do not. So it's completely pointless. Do we use this data recovery property in full Dank Sharding? Yes, we do. It's fundamental to how full Dank Sharding works.
00:22:08.850 - 00:22:54.060, Speaker B: Okay, this is why we're doing this whole song and dance. And it actually says this in the EIP. EIP 4844 implements the transaction format that would be used in Sharding, but not actually Sharding, those transactions. So if you're familiar with the Agile methodology, the extreme programming methodology Yagny, you aren't going to need it. This is the opposite of Yagni. We're putting in today infrastructure in EIP 4844, which is not actually useful in 4844, but will be useful in future if and when we actually do full Dank Sharding. So that's why 4844 is so complex.
00:22:54.060 - 00:23:24.680, Speaker B: We're doing stuff now that may be useful in future. And that's a reflection of the fact that blockchains are really hard to change, right? If we did something different now and then changed it all later, it would be much more complicated. That's the expectation. So, in other words, EIP 4844 is way overcomplicated for what it does. Remember, this is what it does. It bolts Blobs onto blocks right before we do polynomial commitments. I need a need a drink.
00:23:24.680 - 00:24:03.664, Speaker B: I probably need whiskey. Okay, so commitments to bolt blobs and blocks together, we need a commitment scheme. And what this ensures is that nobody can come along and change our Blob data, can fake it. So that we know that when we're presented with a Blob and we're told it belongs to this Blob, that that Blob actually was what the user originally submitted. So it's a property called computationally binding. You can't fake it. It's simple, really.
00:24:03.664 - 00:24:38.320, Speaker B: We have our block within the block, we store a small commitment, 32 bytes, 48 bytes, and that links us, binds us to this Blob, which is this large data and examples of commitment schemes. We could just take a Shard two, five, six hash across the data. So we could take the Blob data, put it through a hash function, and then put the resulting 32 byte hash in the block. And that binds us irrevocably to that Blob. Others are Merkel roots. Merkel roots have nice properties. We'll talk about them in a moment.
00:24:38.320 - 00:25:37.092, Speaker B: And thirdly, polynomial commitments, and that's what we'll talk about in lot more depth, but effectively, they all do the same thing with different properties. So Merkel roots versus polynomial commitments in a Merkel tree and a Merkel root is just the root of a Merkel tree. We represent our data as a vector of values, just a list in a polynomial commitment. Our data points are polynomial coefficients. This is what we're dealing with, which, again, is also just a list of numbers at the end of the day, but they have more meaning. The commitments in a Merkel tree is a Merkel root, which is just a hash function. In a polynomial commitment, we use an elliptic curve point, so it's a point in our elliptic curve group.
00:25:37.092 - 00:26:33.584, Speaker B: The size of a hash is typically 32 bytes. The size of a compressed elliptic curve point is 48 bytes in the scheme that we're using. And the complexity of generating this commitment, and we'll look at it in a moment, is just order the size of our data. So if you have n data points, you can compute a Merkel tree in order n operations and the polynomial commitment. Similarly, the operations here are operations in an elliptic curve group, so they're kind of more expensive, but it's still order. N both merkel roots and polynomial commitments give us a way to prove a value so that we can take a single number from our set and we can prove that that number belongs to our original set without providing the whole set. And we do this with Merkel proofs.
00:26:33.584 - 00:27:20.640, Speaker B: Our Merkel trees the size of a Merkel proof is approximately log n in the sorry, it is log n. The size of a Merkel proof is log n in the depth of the depth of the tree. So you've got n items that you're committing to, and the depth of the tree is the length of the proof. You just provide a path, and the complexity of it is, again, log n. In order to prove a value with a polynomial commitment, we do what's called an opening, and I'll explain what KCG is in a moment. It's basically evaluate the polynomial at a particular point. So we said that our data is the evaluation of the polynomial at the roots of unity.
00:27:20.640 - 00:27:55.532, Speaker B: So we pick one, and we evaluate the polynomial at that point, and that's our data item that we've encoded. And we can construct a proof which is just an elliptic curve point, and it's 48 bytes. So it's constant, doesn't depend on the size of our data. So that's a very nice property. And the complexity of doing the proof is just constant, again, in the size of our data. But so far, the size of our data is not so large that Merkel trees are unfeasible. Merkel trees look actually pretty good for our purposes.
00:27:55.532 - 00:28:29.608, Speaker B: We can also do multi proof. So if we've got five values that we want to prove from our data set, then each scheme offers a way to do that. And Merkel trees, it's kind of five times log n, but we're sharing some values, so it's probably less in polynomial commitments. We can do a multi proof, again using a single elliptic curve point. So it's just 48 bytes, but they're still pretty comparable in terms of complexity. So what's the difference? Well, this is the big one. Merkel trees are not data recovery capable.
00:28:29.608 - 00:29:19.164, Speaker B: They don't have these polynomial properties. If we lose data from our set of our vector values, we've got no way to get them back using a Merkel tree. Whereas with polynomial commitments, we're using polynomials, we can encode it in such a way that we can lose data and recover it. And that basically forces us not to use our nice, friendly Merkel tree technology that everyone understands and is well proven, but to use this kind of funky, spooky new polynomial commitment technology to do full bank Sharding later. So that's why we are moving away from our kind of traditional cryptography and moving to this new polynomial commitment cryptography. And this is how we commit to a polynomial. I'll do the math in a moment.
00:29:19.164 - 00:29:41.010, Speaker B: We've got a section on that. But we have a Blob is 4096 field elements. Each field element is 32 bytes. It's a scalar. And so the size of a Blob is 128. Blobs are Blobby in this nomenclature. Alongside that, we have a trusted setup, and we'll talk about that a bit more.
00:29:41.010 - 00:30:42.100, Speaker B: We combine those. We run it through a problem, through a function called Blob to KZG, and it spits out a commitment, which is an elliptic curve point, and it's in an ellipse because it's an elliptic curve point. And when you serialize that and compress it, it's a 48 byte quantity. And for convenience, we then hash that serialized representation, and we come get the virgin hash of the commitment, which is just 32 bytes. I don't know what hash functions look like in your mind, but they're pretty spiky in my mind. So this is a hash function, and both the commitment and the versioned hash, which is just the digest of the commitment, they are binding on the polynomial and therefore on the Blob data itself. So given a commitment or a versioned hash, no one can change this Blob to anything different such that it still verifies.
00:30:42.100 - 00:31:32.630, Speaker B: If we take a Blob B Blob primed and run it through the process, we will get a different commitment and a different version to hash. We do the versioned hash thing just because the currency of the EVM is 32 byte words. So it's more convenient to handle 32 byte quantity in the EVM than it is to handle a 48 byte elliptic curve point. Why keep the commitment around? Well, the commitment allows us to do another operation, which is opening the polynomial, which is evaluating it at a point, and we'll come to that as well. Right, time for a little breather. This is the maths. Let me just check the chat, see if we got any questions before we get into this.
00:31:32.630 - 00:32:00.850, Speaker B: Right, we'll do. How many Blobs are bolted to a block? Yes, the polynomials are same size as the original data because you're just doing a fastware transform between them. Yes. So those are answers. Lovely. Right, here's the math. You don't need to know this, but I cover it for those who enjoy this kind of thing.
00:32:00.850 - 00:32:30.360, Speaker B: Doing mathematically equations in Google Slides is utterly painful, so I've kind of cut it down to a minimum. And it's really just an exercise in demonstrating you can fit the mathematics onto a couple of slides. It's not totally esoteric. So we'll do a bit of notation. So G is the generator of an elliptic curve group. It's an elliptic curve point. The elliptic curve group is of prime order.
00:32:30.360 - 00:32:53.648, Speaker B: So any point in the group is a generator. And by convention, we choose one. And everybody agrees. This is the generator of the group. A is a scalar coefficient, so it's just a number. And we're going to write A times G as A in square brackets. So when you see a quantity in square brackets, you know that it's an elliptic curve point.
00:32:53.648 - 00:33:34.460, Speaker B: So this is A times the generator. I'm using additive notation, not multiplicative notation. That's traditional in the circles in which I move. Sorry if that upsets you. So assume we're going to wave our hands now. Assume we have a set of elliptic curve points, and it's S times a generator, s squared times a generator, s cubed times a generator up to S to the N times a generator where S is unknown. Okay? And that comes from a trusted setup because this in square brackets is S times a generator.
00:33:34.460 - 00:34:17.370, Speaker B: Even if we know this point, which we do, we cannot find out what S is. That is the discrete logarithm problem or discrete division problem in additive Notation. So we can have this set of points, which we have here, without knowing what S is, and we'll come back to that. So what do we want to do? We want to commit to a polynomial. So we have this polynomial which is P sub zero P sub one P sub two P sub N. And we want to make this small digest which binds us to this polynomial. And all that we do is evaluate the polynomial at this elliptic curve point S.
00:34:17.370 - 00:34:59.400, Speaker B: And it's very easy to do that because we have all the points S, S squared, S cubed, S to the N, and we have the coefficients which are here. So we evaluate our polynomial at this point S, and S is unknown. And that's our commitment. It's a simple multiscalar multiplication. And it turns out that if you don't know what S is, this scalar value S, then it's very hard to come up with a different polynomial that evaluates to the same number. So it's like taking the hash of a polynomial. Finding a different pre image that has the same output is difficult.
00:34:59.400 - 00:35:28.896, Speaker B: So it is a commitment. Computationally binding. How do we open a polynomial? So we. Want to evaluate our polynomial at a particular value. So one way we could do this is if I want to prove to you that the value of the polynomial is seven at zero three, I could send you my whole polynomial and you could evaluate it and you could find the answer. But that's really inefficient. So I'm not going to do that.
00:35:28.896 - 00:36:15.676, Speaker B: I'm going to send you the commitment to the polynomial, which is this single elliptic curve point that came out of the previous process, and I'm going to send you a proof. So we're going to expand our notation because we've now got two elliptic curve groups. We've got g one and G two. They come from the BLS twelve three eight one curve. But it's a pairing friendly curve, and all pairing friendly curves have two subgroups that we use, and our notation is basically the same. But we have a subscript one for things in group G one and a subscript two for things in curve group G two. And we now have something called a pairing because it's a pairing friendly elliptic curve.
00:36:15.676 - 00:37:14.070, Speaker B: And pairings are kind of how they work is utterly mysterious, but actually using them is relatively straightforward. They're conceptually, not very difficult to use and to understand. But if you dig into how they actually work, then it's proper Moon math. So we can view this pairing E as a kind of multiplication operator. You can't multiply elliptic curve points normally, but with pairing friendly curves, you can take a point in G one and a point in G two and map them to a point in a group called GT, and it's kind of like multiplying them and ending up with a point in this group GT. And you can see the T subscript at the bottom here. So what we want to prove is that if we evaluate our polynomial at point z, then the answer is Y, our data point at that point z.
00:37:14.070 - 00:38:00.530, Speaker B: So we know Y and z P is a polynomial, and we previously committed to it, and we have a commitment C. So I'm going to send you a proof that this is true. But I'm not going to send you the whole polynomial, I'm only going to send you the commitment. Now clearly, if we just subtract Y from both sides, we can see that our polynomial P of X minus Y is zero with X equals z, so we can factor out X minus z. So this is fundamental operation in kind of polynomial studies. You can just take out a factor of X minus z and we're left with this second polynomial Q, which is equal. So Q X minus z is our original polynomial minus Y.
00:38:00.530 - 00:38:39.920, Speaker B: And our proof that we're going to send is just the commitment to Q. So it's Q evaluated at this secret S number. So we generate that and we can check that. So I'm going to send you two elliptic curve points the original commitment C, the commitment to Q, which is Pi. And I'm going to send you two data items, which is X and Z, and you're going to be able to prove using just that information that my claim is true, that P of Z is Y there. So we're going to evaluate the proof. So we do two pairings.
00:38:39.920 - 00:39:40.384, Speaker B: We pair our commitment to Q, which is Pi, with S minus Z in the group two, and we check whether it equals the pairing of our original commitment minus this point, and H is the generator of the elliptic curve group G two. And this is very hand wavy. And I'm just basically saying this is possible. You can look into the math on your own time, and it turns out when you run through it all, that it all falls out that this turns out to be true. So we have basically taken this unknown point S and used it. We've just checked that the value of our relation holds true at this unknown point S, and to fake this without knowing what S is computationally difficult. So nobody can produce a fake proof without knowing what this scalar value S is in the grand scheme of things.
00:39:40.384 - 00:40:43.700, Speaker B: So there we are. We have two fundamental operations. We have the commitment, which is evaluating the polynomial at this unknown point, or for this unknown value S, and we have an opening which is proving that it equals a certain value at a certain point. And in pictures on the left is what I give you a commitment to the original polynomial approve that evaluated at point Z that equals Y, and then it just spits out yes or no, this is true or not true, and just some end notes on that. The polynomial commitment scheme we're using is called KZG or KZG, after Kate, Zavarucha and Goldberg. It was published in 2010, so it's relatively new compared to a lot of the crypto that we currently use. We used to call these Category commitments, and you can see that around in various places still.
00:40:43.700 - 00:41:38.440, Speaker B: But Category actually wrote to Ethereum Core Devs and asked us to include the Z and the G in the name as default, which I thought was pretty cool because he wanted to acknowledge his co authors. So that's what we do. We are already using elliptic curve pairings in the consensus layer signature scheme. So this is not putting a whole load of new cryptography inside what we already have. This is building on stuff that we already use in the consensus layer. In particular, it's using the same BLS twelve three eight one elliptic curve that the consensus clients already use to do its operations. So we don't need to add any elliptic curve, primitives or fundamentals, into what we already have.
00:41:38.440 - 00:42:07.516, Speaker B: The libraries are already available and the interfaces and so on there. So that's one reason why we've chosen this scheme over other things, like inner product arguments, and so forth. We already have the infrastructure in place to do these polynomial commitments. I wrote a thing a few years ago if you wanted to dig into the BLS. Twelve three eight one curve a bit more deeply. This is my most popular thing I've ever written. It's had something like 45,000 views.
00:42:07.516 - 00:42:46.664, Speaker B: Who would have thought it? Which is still not as many readers as my daughter has on her Crochet instagram. So I need more Crochet content in my output, apparently. Right? Let me check the time. Okay? Time is running away with us, so I will go swiftly. Lifecycle of a Blob, right? Blobs are born when we submit them to the network. You and I are never going to do this realistically, okay? We're going to be moving ERC, 20s around or whatever, or interacting at L two. We are not going to be submitting Blobs.
00:42:46.664 - 00:43:28.350, Speaker B: This will be done by roll up sequences and roll up operators. But nonetheless, we can look into what it means to submit a Blob transaction. So you submit it to your execution client, just as you do for a normal transaction via the JSON RPC interface. Blob transactions have a new transaction type. We've taken the opportunity to start introducing SSZ SSZ into the execution layer. This is a sort of direction of travel. We've been using this on the consensus side since we started.
00:43:28.350 - 00:44:10.344, Speaker B: More or less. Normally, transactions on the execution side are encoded in RLP, but over time we're going to be migrating things. And Blob transactions have all the usual stuff value, nons cool data and so forth. But they also include a Blob diversioned hashes field. So you have one versioned hash. It's just a 32 byte hash of the commitment for each Blob in the transaction. And there's no particular limit on the number of Blobs in a transaction, but there is a limit of Blobs in a block, which we'll come to, and we gossip the transaction with its Blobs around the network as a new data type.
00:44:10.344 - 00:44:45.284, Speaker B: So transaction network payload and nodes are supposed to verify that the Blobs match the versioned hashes before they gossip them on what's it cost to send a Blob transaction. So this is also new. So we have gas pricing in EIP 1559. For normal transactions, Blobs use data gas, which is new. So we have two dimensional gas. In a sense, data gas pricing has a similar mechanism to 1559 that we're now familiar with. Basically, we target a certain number of Blobs per block, which is two to start with.
00:44:45.284 - 00:45:18.812, Speaker B: That may increase over time. If it's wildly successful, we might bump that up. We allow for up to twice that many. So Blobs are elastic or blocks are elastic, so we can include more Blobs per block, up to four. And the way we price it is, if the previous block had more than two Blobs, we increase the data gas price by a factor. If the previous block had fewer than two Blobs, we decrease the data gas price by a factor. The maximum increased decrease is 1.125.
00:45:18.812 - 00:45:54.024, Speaker B: So there's a doubling time, about six blocks. So if we continually have high demand, so we have a string of blocks with four Blobs each, then we will get an exponential increase in price doubling every six blocks. And as far as I can tell from the EIP, all data gas fees are burned. Which surprised me. I was not expecting that, but that seems to be the case. And I'd be glad if somebody can correct me on that if it's not the case. But yeah, as a node operator, I'm not entirely happy about that, but there we go.
00:45:54.024 - 00:46:38.456, Speaker B: As an Ethereum holder, I'm happy this Blob transaction has been submitted. At some point, it gets included in a block by a block proposer, and it's structured like this. You have a beacon block. Since the merge, beacon blocks have carried execution payloads, and we bolt on this Blob sidecar. So we have a number of Blobs, up to 428 KB each. And we have a single aggregated proof, which is 48 bytes, which serves as a proof across for all the Blobs in that block that they are correct. And then in the beacon block, we add a field, which is the actual KZG commitments, which are the elliptic curve points.
00:46:38.456 - 00:47:51.516, Speaker B: We use those to verify the aggregate KZG proof. In the execution payload, we have the Blob version, hashes, and the gas tracking stuff as well. And as for what we do with these things, well, beacon blocks are persisted on the consensus client, execution payloads are persisted on the execution client, and also sometimes the consensus client, though maybe not in the long term. And Blob sidecars are temporarily stored on the consensus client. On the consensus layer, we have a fork choice rule and a fork choice rule for a blockchain dictates which Blobs are allowed to be included in the chain. Blobs that fail the following test will not be included, considered for inclusion, so they will never make it onto the chain. And the test is, the new test is, is data available? So when we have a block, we'll look at that block and if it claims to have some Blobs, we will say, okay, are those Blobs available? Can I see that data? And it has basically two checks.
00:47:51.516 - 00:48:30.436, Speaker B: We try and retrieve the data and we try and validate the data in EIP 4844. Retrieving the data is trivial because Blobs and blocks are bolted together and travel around the network together. All we need to do is look in our local database and see is it there? We did have a design whereby they traveled around the network separately, and we might have had to go out to a peer and say, have you got the Blob for this block? And so on. But we've moved away from that. And currently the design is that Blobs and blobs travel together. So we just look it up and see if it's available to us. Validate blob.
00:48:30.436 - 00:49:03.776, Speaker B: Scicar checks the KZG proof of the blobs in the sidecar. It's got an optimization to check multiple proofs at once. I won't go into that. But it uses fiat shamir heuristics to randomly check a point that the opening matches. The interesting thing is that when we move to full bank sharding, we don't need to change this wrapper. We still have an Is data available wrapper. It's just that it's much more sophisticated and will involve data availability sampling.
00:49:03.776 - 00:49:51.220, Speaker B: So instead of just checking our local database, we will do complicated stuff, which I'll talk about in a moment, in order to check whether the data is available on the network somewhere, not just in our local database. So at some point we want to use this blob data in the EVM. That's kind of the end goal of it. And it turns out that in order to use the blob data, we only need to be able to verify openings. So we only need to check that certain values in the blob are what we say they are. So EIP 4844 adds a pre compile to the EVM called the Point evaluation Pre compile. This address 20 in decimal.
00:49:51.220 - 00:50:43.148, Speaker B: It will be about 50,000 gas, probably a little bit more. We're just doing benchmarking for it at the moment to do that. And you will give it the data that we said, you'll give it the commitments to the polynomial so that to the blob, you'll give it a proof and you'll give it the data points you want to prove, and it will tell you, yes, that data is correct or that data is not correct for that blob. We used to have a blob verification pre compile, whereby you'd give it the entire blob and it would recalculate the commitment, the polynomial commitment. But in practice, this is not actually needed, it turns out. So we only have one pre compile. Now, how do we use this blob data? What does it give us? I'm going to give one example, which is fault proofs in optimistic roll ups.
00:50:43.148 - 00:51:41.224, Speaker B: So today, all optimistic roll up data goes into call data on the blockchain. It takes up block space in cool data, very expensive. But once we've done EIP 4844, all that data will go into blobs and will never appear in blocks except when we're doing a fault proof. So if we want to challenge the optimistic roll UP's execution and say you made a mistake or you defrauded me at that point, then we take the data from the blob and we submit it to A block. So we put it into cool data at that point, and then we use this point evaluation pre compiled to verify that yes, this was the correct data that corresponds to that blob. And then we can run the fault proof using just that small amount of data. So fault proof should almost never happen.
00:51:41.224 - 00:52:35.800, Speaker B: I mean, in the happy flow, they never happen. So effectively, we've taken the whole data availability of Optimistic Roll Ups off main net and put it into Blob space. The 18 day retention period for Blobs is kind of driven by Optimistic Roll Up challenge periods. You've basically got a week or two weeks settlement period for Optimistic Roll Ups, which is to ensure that challenges are not censored. So we need to kind of keep the data available for a couple of weeks is a thinking so that it's available to make fault proofs within that window. For Optimistic Roll Ups, we've talked about making it available for 30 minutes and anyone who's interested in it, if you're a user of the roll up, you can download it and keep it yourself and whatever. But 18 days seems to be a reasonable sort of compromise, not too onerous for nodes.
00:52:35.800 - 00:53:12.192, Speaker B: So this is an immense saving in costs for Optimistic Roll Ups and then eventually Blobs die, unlike blocks. So we keep them for 18.2 days. Within that period, every node must store its Blobs. If it doesn't, its peers are allowed to downscore or disconnect d peer them, which is bad in a peer to peer network. But after 18 days you can just delete the Blob data, forget it ever existed. Currently, blocks are stored forever.
00:53:12.192 - 00:53:37.336, Speaker B: In principle, clients are moving away from that now. It's not formalized. EIP four four four is in the works and will mandate that you store execution payloads for only a year. That's not yet implemented. But even so, 18 days is much shorter than a year. So it's tentatively scheduled. Well, more than tentatively.
00:53:37.336 - 00:54:29.330, Speaker B: I mean, it's reasonably firmly scheduled for Cancun stroke deneb upgrade, which is due later this year, maybe Q. Three spec for EIP 4844 is mostly final, won't rule out some. A few Tweaks clients are nearing code complete. Some devnets are sort of running with subsets of the clients. This is not widely advertised, but core devs have an off site all next week in a secret location. One of our stated goals coming out of that offsite is to have a feature complete spec frozen EIP 4844 DevNet with all clients running on it. So that's potentially achievable and would herald 4844 coming soon.
00:54:29.330 - 00:55:35.220, Speaker B: Got to talk about trusted setup. Okay, recall we are kind of hand waved about this series. We assumed we had this series of powers of S times elliptic curve points available to us where S is unknown because anyone who knows S can forge commitments and proofs. How do we obtain such a series, such a magical object? Well, we use something called a trusted setup, which is a distributed multiparty computation. And basically what happens is that participants take it in turn to add a random contribution on top of previous participants contributions. And then so I add my randomness and I delete my randomness and the entire setup is secure as long as at least one contributor was honest. So as long as at least one person contributed actual randomness and also raised their contribution, then the whole setup is secure.
00:55:35.220 - 00:56:19.952, Speaker B: And here's a kind of sketch of how it works. The actual process needs to have extra stuff around, checking that each actor didn't break anything, didn't submit kind of fake powers of their randomness and whatever. But what we can do is we can start with a set of elliptic curve points. So let's start with the generator. This is one times the generator in the group. Then participant A, or the first participant, generates a random number A, and they multiply the generator by A, A squared, A cubed up to A to the 495, and then sends that back to the sequencer of the trusted setup. Sorry.
00:56:19.952 - 00:57:19.684, Speaker B: The next participant takes a's output, generates a random number B, and multiplies the first entry by B, the second by B squared, the third by B cubed, and so on, and then C repeats that, and so on. And eventually we end up with ABC for the first ABC squared and so on. So our random number S is the product of everybody's randomness. And as long as at least one of those bits of randomness is not available, it's been deleted and it was truly random, then the final quantity is the product of them all is random. You cannot go from ABC times, elliptic curve point times the generator back to ABC. That's a discrete logarithm problem, which is thought to be computationally difficult. So that's a sketch of how it works and you can join in.
00:57:19.684 - 00:58:08.064, Speaker B: So this is actually running now. It's running for two months. This is a setup process for EIP 4844 generating this trusted setup. So it's already the largest trusted setup ever run we've had, getting on for 8000 participants. I think you can visit this website and it will guide you through the process. There's a sequencer that sort of randomly chooses participants from the queue, adds their randomness into the mix, and it will give you a receipt at the end of the process and you can verify later that your randomness was indeed included in the transcript correctly. How can I trust Ethereum in the future? How do I know that it will be secure and this whole Blob mechanism will be correct? Participate in a ceremony.
00:58:08.064 - 00:58:33.150, Speaker B: Presumably you trust yourself. You trust yourself to put in some random data and to delete that data. You can then be confident that the whole process is secure. And if you want to be really paranoid, you can audit all the code, it's all public, and check the transcript as well. So hop along there and participate. You got a couple of months, a bit congested at the moment. There are long waits, but over the next few weeks it'll die down.
00:58:33.150 - 00:59:07.000, Speaker B: Right, we're basically out of time, but we have to do a very quick survey of data availability sampling. We're almost there on the home straight, just to make sense of why we have overcomplicated EIP 4844 so much. This is the direction of travel. This is full dank sharding. This is recap slides. So block space versus blob space. Bounded storage cost, no execution cost, but every node bears a bandwidth cost in EIP 4844 because these blobs are seen by every node.
00:59:07.000 - 00:59:49.860, Speaker B: What we're going to do in Full Dank Sharding is add. I've called them super blobs here, they're called Sharded blobs elsewhere, but we'll come to that. And the main difference is they are no longer seen by all nodes. So we'll talk about it in a moment. But most nodes on the network will never see these super blobs, and so they can be much, much bigger. So superblobs have no storage cost on the node side, no execution cost on the node side, and very low bandwidth cost, which is awesome for scalability. We can have thousands to hundreds of thousands of nodes participating in the network.
00:59:49.860 - 01:00:35.556, Speaker B: What we're trying to achieve is with the whole blob thing is data availability, providing usable data to the network for roll ups, which means that we are ensuring that no data can be completely withheld from the network. Data availability is trivial when every node sees the data, I just say, yes, seen that done. And that's where we are today, and that's where we are in EIP 4844. But this severely limits the scalability of the size of the blobs. We are constrained in the blob size. So for example, I'm talking to you now over an eleven megabits per second uplink, and my node is already using four megabits per second of that. And that's before EIP 4844.
01:00:35.556 - 01:01:07.164, Speaker B: It'll be using more when it has to process blobs as well. So I'm already approaching not far from not being able to run a node at home. And if we just increase the blob size, then there's no way I could run a node at home. I'd have to put it in a data center. And that kind of misses the point. So we need a way to check that data is availability without every node seeing all the data, without erasure coding. So this is where we are with EIP 4844.
01:01:07.164 - 01:01:43.720, Speaker B: We take n data points, we do an FFT, and we turn them into n polynomial coefficients. If we lose any of this data, then part of our initial data is scrambled. It's no longer available. What Dank Sharding full Dank Sharding introduces is data erasure coding. So in our superblobs, we are going to encode n over two actual data points and n over two zeros. We're going to just stick on the end. So we've got n points in total, and then we're going to turn them into n polynomial coefficients.
01:01:43.720 - 01:02:44.460, Speaker B: And what this means, to cut a long story short without going into all the details, is that we can lose half of our polynomial coefficients and we can still do the inverse FFT and recover all of our N over two data points. So half of our data can go missing or be withheld, but we can still recover from that situation. So that is very cool. How does this affect data availability? So we do sampling. If I'm a node on the network and I want to know, is this massive data blob available, fully available to me? Is all the data in it available? I don't need to assure myself that. I only need to assessify myself that half of it is available because I know that if I can get half of it, I can reconstruct the whole thing. So what I do, and this is the kind of brilliant insight is I ask the data provider, the block producer, for 20 random elements, which is much, much less than the whole data blob.
01:02:44.460 - 01:03:40.640, Speaker B: If 50% or more of the data is not available, if that block proposer is withholding more than half the data, then each inquiry I has has a probability of more than half of failing. So I'm asking for random points. If it's withholding the first half of the data, then I've got a 50% chance of hitting that. If all my random queries succeed, then I know with a probability of better than one in a million that all the data is available. Because if it's trying to withhold that data, some of those queries would have failed, half the queries would have failed. So this is really sweet. So I can make a very small number of random queries and I can satisfy myself that at least half the data is available with a very, very high probability.
01:03:40.640 - 01:04:07.284, Speaker B: Therefore I can reconstruct the whole data, and therefore I'm happy that the data is available at a network level. Yeah, memes. It's kind of misnamed these days. We've been talking about sharding for so long that people are kind of clinging onto this. So we call it dank sharding, but I think it's much better called dank sampling. So there we go. Full sketch of dank sampling.
01:04:07.284 - 01:04:47.236, Speaker B: So this is a recap. We separate. Most nodes no longer build blocks. We have specialized block builders who construct 16 megabytes super blobs alongside blocks that's proposer builder separation, which is a direction of travel for ethereum. So almost no one sees this entire super blob. Only builders, roll up operators, maybe blockchain explorers, maybe a few enthusiasts download whole 16 megabytes blobs every slot. But almost nobody sees them or has to see them.
01:04:47.236 - 01:05:56.796, Speaker B: Most nodes on the network, like my nodes sitting here, only sample the blob, as we mentioned, confirming that at least half of it is available with very, very high probability. If we can't satisfy ourselves that it is available, then our fault choice will say don't include it on the chain. So the load on each node is low and constant. So just by doing these 20 queries, which is a constant load, we can satisfy ourselves that data is available. And the really interesting thing is that the more nodes there are on the network, the bigger super blobs can be because more nodes are sampling the super blobs. So they've got individually, in aggregate, we've got many more points because this is the kind of key point is if we find later that the data is actually unavailable, we've said it's available, we've done our sampling and blobs available, but then suddenly find, oh, it's no longer available, somebody's deleted it. Then we can reconstruct the blob from all the samples that all the validators took.
01:05:56.796 - 01:06:56.152, Speaker B: We can get together and compare notes and reconstruct the original data. And this is really complicated and as a 2D erasure coding scheme has been proposed to facilitate it biggest unsolved, I'd say as yet challenge with it all is kind of network coordination, designing of subnets and so forth. And we also need to be able to run this proposer builder separation in a secure and safe way whereby we don't have to trust block proposers. We're not really there yet, we've got a kind of hybrid version at the moment. But we have to get comfortable with the ideas that there are specialized supercomputers on the network which are building blocks and blobs and everyone else is just checking their homework. So that's really a sketch a very quick and maybe I'll come back one day and talk about full dank sampling. So we're now just in the closing bits.
01:06:56.152 - 01:07:30.200, Speaker B: Let's recap where we've been. So ethereum scaling roadmap is roll up centric roll ups fundamentally just need lots and lots of data. EIP 4844 bolts blobs onto blocks. Blobs are stored temporarily, blobs are not seen by the EVM. Therefore blob space is much cheaper than block space. We could have stopped there, but we've also introduced much of the paraphernalia which is required for data availability sampling, which is Gank charting Gank sampling in future. So that makes 4844 much more complicated than it needs to be.
01:07:30.200 - 01:08:04.468, Speaker B: We're treating data as polynomials, we're using polynomial commitments, but we're not doing data recovery yet. But the bottom line is that full dank sampling will allow the ethereum ecosystem to scale to the population of the world. And here it is. I do apologize for the color scheme. This came from a different presentation. So today rather in the past we were at around ten transactions per second. Present ish if all roll ups start performing at optimal we'll get to maybe 100 after 4844, perhaps 1000.
01:08:04.468 - 01:08:43.788, Speaker B: So within a year, within this year should be and then some years out. Dank sampling looking at our 100,000 transactions per second. Just a few resources. Peter will share the presentation. So you'll be able to get the links if you want these slides, here's a direct link to these slides if you want to scan the QR code EIP 4844 linked to the EIP itself. The consensus specs has its own repo Vitalik's FAQ on. Protodank charting is excellent, goes into some good depth.
01:08:43.788 - 01:09:28.956, Speaker B: There's even a website for EIP 4844, which is a worrying trend. KCG ceremony, good stuff. There cole Bequeas and did a video deep dive a couple of days ago, which I would highly recommend if you want to dive deeper into KCG commitments. Dancrad did a terrific explainer, and the actual full math and proofs of everything cryptographic standard are in the original paper link there. And this whole data availability sampling thing is laid out by Vitalik in this nice article. I've done a few relevant bits and pieces just to shill those I mentioned already. My BLS twelve three eight.
01:09:28.956 - 01:10:05.512, Speaker B: One thing I wrote, a really absurd toy example of this data recovery thing, just to prove it can be done, is flipping between data and polynomials and losing half the data and recovering it. So it uses two data points. And so it's kind of absurd, but just walks through all of the methodology. And I did it to satisfy myself that, yes, it does actually work. And then I mentioned earlier the book and there we are. All right, thank you, everybody. Let's round up Q and A.
01:10:05.512 - 01:10:09.630, Speaker B: Sorry for going on a bit long. What have we got?
01:10:12.080 - 01:10:36.420, Speaker A: Yeah, I think there are a few questions in the chat. Why don't we start off the nice and simple one? How many Blobs are bolted onto a block? And so I heard at 1.2 Blobs per block in the initial release, two hundred and fifty six K and everything. But then at another point, it sounded like could be up to four Blobs.
01:10:38.040 - 01:11:10.080, Speaker B: Yes. So Blob space is a little bit elastic like block space with the IP 1559. So target on target is two Blobs per block. The price of gas, of data gas does not change. If we have two Blobs per block, we can accommodate up to four. But if you have four, then the data gas price goes up. And so if you continually have full Blob space, so four Blobs per block, then the price will exponentially increase to bring it back down to two.
01:11:10.080 - 01:11:29.830, Speaker B: So on target is two. You can have zero. Of course, we may change the target. If this works out and the bandwidth demands aren't too high, then we may increase that target to three or four Blobs per block with a cap of double of twice that. Does that make sense?
01:11:31.640 - 01:11:58.980, Speaker A: Makes sense to me. To whoever asked the question. Okay, let's go with the next one. So Hayden asked, how large is the polynomials Blob compared to the original data? And Tom said, almost the same without the erasure encoding. So that looks about right.
01:11:59.430 - 01:12:27.370, Speaker B: Yeah, that's right. I mean, it's 128 KB there's a little bit of wastage because we're using 255 bit encoding of the field element, so you lose 0.4% of the space, but effectively it's almost exactly the same size, 128 Blob.
01:12:30.270 - 01:12:39.600, Speaker A: Parathio asked if Blobs are not available to the EVM. Why is there a precompile needed? I think you actually answered that a little bit after.
01:12:39.990 - 01:13:29.630, Speaker B: Yeah, hopefully that example with the optimistic roll up explain went through it pretty quickly. But the point is that the way roll ups work is that you only really need the data on chain. At least optimistic roll ups, you only really need the data on chain if there's a dispute. At the moment, it's all put on chain because there's no alternative but to prove it's available. But you only really need the relevant bits on chain when there's a dispute. And then in order to verify that the correct relevant bits were submitted, you need the original proof from the block and you need the data. And then the pre compile will just check that that was correctly submitted before the fault proof is processed.
01:13:30.370 - 01:13:33.840, Speaker A: Okay, that makes sense to me.
01:13:36.630 - 01:13:52.600, Speaker B: Things are much more complicated with ZK rollups. I didn't even want to go there. So Fitalik's written a bit about it, but you need a proof of equivalence between polynomial commitments in your ZK EVM and KZG commitments and yeah, anyway, we won't go.
01:13:55.530 - 01:14:04.522, Speaker A: Was. So you mentioned there was some off site so someone said, how do you get an invite to the off site? And then it descended to how do you become a core dev?
01:14:04.656 - 01:14:07.340, Speaker B: Yeah, I'm seeing some good answers to that question.
01:14:09.090 - 01:14:24.130, Speaker A: And I said, essentially join the core devs call and do some good pull requests that are accepted for an Ethereum client and start doing that sort of thing. Do you have any other good ideas?
01:14:24.470 - 01:15:06.910, Speaker B: Yes, it's basically client teams. I mean, that's a level of commitment required is full time on a client team is the ticket to participating in these events. So how do you get into a client team? Yeah, I mean, a great place to start is get to know the team, submit some good work. I mean, Enrico here started with TechU, did some work as an external contributor and we snapped him up. Alternatively, look for job openings. You can apply for roles as well or start your own client. It's fully permissionless build a client.
01:15:06.910 - 01:15:17.298, Speaker B: Others are doing this. Paradigm is building re and others have done so as well. Yeah.
01:15:17.464 - 01:15:47.726, Speaker A: Okay. Cheeky Gorilla said, do I understand correctly that EIP 1449, so maybe they mean that we're dealing with today will increase bandwidth requirements for stakers due to every node needing to see all Blobs. But Full Dank Sharding will remove it by introducing super blobs, which don't need to be seen by all nodes, right?
01:15:47.828 - 01:16:29.740, Speaker B: Yeah, that's correct. Understanding. It's fair to say that 4844 may not actually increase data bandwidth requirements very much because we're expecting that instead of cool data, roll ups will use blob space. And so your block size will go down because they're no longer using cool data for this and they will put it into Blobspace. So it's not necessarily current size of block plus an extra chunk. There's a bit of give and take there, but likely because Blob space is cheap, demand will increase and there will be some more demand on bandwidth. But yes, exactly.
01:16:29.740 - 01:16:45.230, Speaker B: There's a limit to the number of Blobs we want to circulate now with 4844 because of bandwidth concerns, but those concerns go away when we do full dank sampling, because nodes don't need to see the entire Blob.
01:16:45.570 - 01:17:03.960, Speaker A: Yeah, the last one here that we've got in the chat is, is it expected that this and future Sharding changes result in reduced gas fees in terms of dollars, terms enabling affordability to one transaction for every human on Earth per day?
01:17:04.730 - 01:17:53.842, Speaker B: Yeah. Supply and demand is kind of difficult to judge, right. If you supply things very cheaply, then demand for really trivial things goes crazy. So it's always going to cost something? It depends what it's worth to people, but, yes, the idea is that if demand on Ethereum stays the same as it does today, then transactions will be effectively free. I mean, the cost will be so trivial that nobody will care about them, but of course demand will increase. What we will provide is the capacity eventually for every human on Earth to do a transaction per day. If every human on Earth wants to do two transactions per day, then the cost is going to increase.
01:17:53.842 - 01:18:33.490, Speaker B: Right. It's difficult to judge at the moment. I assume this is infrastructure stuff. Only really important stuff will be done on blockchains. Right. Most people use centralized databases most of the time, but really important life stuff will be done on blockchains, where it's where the resilience, permissionlessness, censorship, resistance and decentralization of a blockchain adds value, and that comes at a cost. And hopefully it will be a very small cost, but there will be a cost to it.
01:18:33.490 - 01:18:36.450, Speaker B: We are building the world's least efficient database.
01:18:38.150 - 01:18:56.150, Speaker A: Yeah, for sure. All right. We've got people who are anyway, no extra questions there. So does anyone have a question that they'd like to pose live rather than via the chat?
01:18:58.250 - 01:18:59.240, Speaker B: I should.
01:19:02.110 - 01:19:22.386, Speaker A: So I think we're all good. Could you just reshare your slides just for a SEC, so we can get what's happening next? And the next one? OK, thank you. So, thank you, Ben, for your talk. We should have all clapped whilst we had the video up. Anyway, thank you very much for an awesome talk. Fantastic presentation. Yes.
01:19:22.386 - 01:19:49.290, Speaker A: So that was awesome. And this is going to go viral, this one. You're going to have your 100,000 views, for sure. No problems. Yeah. All right, so we've got some talks coming up and hopefully that's is that yours? Hopefully that's Ben that had Zoom unexpectedly quit. All right, not to worry.
01:19:51.310 - 01:19:58.938, Speaker B: So there are a lot of talks. I'm back. Zoom crashed again. Let me reshare.
01:19:59.114 - 01:20:49.310, Speaker A: I'm almost certain that what's happened is Zoom has upgraded because some very strange things have happened on my Zoom that yeah, I don't yeah, I think Zoom's upgraded, so I probably need to reboot my computer or something anyway. But I can't because I've got in one window this your KCG or whatever thing's been submitted, and you're in the waiting room. And don't close this browser window. So I don't know what I'm going to do. Anyway, we've got a lot of great talks coming up. In particular, hermius, and it's going to be ermius and me are going to deliver a talk on the cross chain risk framework. So this is something that we've come up with to try and define quantify risks for cross chain and bridges and try and help people improve the ecosystem.
01:20:49.310 - 01:21:27.820, Speaker A: We've got daniel Britton is going to be talking about some work here's, some research he's done. Jolene, unfortunately, is not able to deliver her talk. So that talk is going to be pushed out by about three months. But we are in discussion, active discussion with someone who's going to do a talk probably on February the Eigth, and probably on proposal builder separation and some insights they have there. So we've just got to get that finalized. And from there, look, we've got a whole stack of great talks coming up. It's going to be exciting times.
01:21:27.820 - 01:21:56.770, Speaker A: Next slide, please. Oh, yes, cross chain workshop. So the call for papers is actually going to be extended. We're going to extend it by two weeks, so don't worry about that. January 23 deadline, though, please put your papers in. We've got some papers already, and it'd be great to get a whole stack of papers in. It's going to be published on IEEE Explore.
01:21:56.770 - 01:22:39.146, Speaker A: It's going to be a hybrid event. So if you can't afford to or can't get to Dubai for some reason, you would still be able to participate in IEEE ICBC. So more information, just see that link and the next slide. If you're here today, this is on YouTube, so just Google Ethereum engineering group. This talk should be up by this time tomorrow. There's a Slack Workspace where you can have discussion. Obviously, if you're watching this on YouTube, you too can turn up and ask the speakers tricky questions by joining the meetup.
01:22:39.146 - 01:23:00.790, Speaker A: There is some example codes for some of the talks, and as well as that, there's a formal methods reading group for those who are interested in that and join the Slack Workspace and then join the formal methods reading group channel. So and I think that is definitely so thank you again, Ben. I guess you can unshare and then we can see you.
01:23:00.860 - 01:23:01.478, Speaker B: Yes.
01:23:01.644 - 01:23:08.540, Speaker A: So thank you again for doing a talk. It took, what, four or five years to convince you that you've done a fantastic talk.
01:23:09.310 - 01:23:19.370, Speaker B: Thank you. Thanks for the invitation. I appreciate it. Peter, it's been good to see everybody. And it's warming up a bit here. The sun is out, so I will take the scarf off shortly.
01:23:20.190 - 01:23:22.682, Speaker A: Awesome. All right, thanks everyone.
01:23:22.736 - 01:23:23.050, Speaker B: Later.
01:23:23.120 - 01:23:24.038, Speaker A: Bye bye.
01:23:24.214 - 01:23:25.800, Speaker B: All right. Bye. Thanks a lot.
