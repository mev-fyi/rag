00:00:00.480 - 00:00:01.030, Speaker A: You.
00:00:03.080 - 00:00:36.190, Speaker B: Hello and welcome, everyone. My name is Peter Robinson, and today I'm going to talk about decentralized sequencer consensus algorithms. So I'm the head of blockchain research at immutable. Mutable is a web three gaming platform. So computer games, not so much gambling, and in is going to be web three gaming is 2024. So by the end of the year, everyone's going to be doing web three gaming. It's going to be awesome.
00:00:36.190 - 00:00:53.770, Speaker B: Okay, let's share some slides. Okay. So hopefully everyone can see my slides. And I've got chat. So I should be able to see things. And I should be able to see you. Okay.
00:00:53.770 - 00:02:01.608, Speaker B: All right. So I've been working at immutable for eight months, actually. And so in this past year or so, I've had a lot of conversations with people at Polygon who we're partnering with, and other people in immutable about this tech. So I guess what I'm saying is I'm standing on the shoulders of giants. So a lot of the insights I've got here are really insights that I've acquired from other people. So I'm going to talk about sequences and provers and roll ups, generally at a high level, and then talk about decentralized sequences, and then dive into how provers are designed, because there are some things that are interesting takeaways, and this is how provers, at least from some of the companies, are being designed and what they're moving towards. And then I'm going to get to the actual, well, what is a prover friendly sequencer algorithm? A consensus algorithm for a sequencer.
00:02:01.608 - 00:02:50.780, Speaker B: And then there are some compromises that I think we need to make in the short term. And then I'm going to talk about some attempts that I've done at standardization and where they've got to and why they've been waylaid at the moment. And I think they deliver some insights all by themselves. So if you stick around for the end, then you'll get some, hopefully good insights. So with a roll up, you've got a sequencer, which is taking transactions. You've got approver, which is creating a zero knowledge proof, and you've got a verifier contract, which is verifying that proof. You've got externally owned accounts that are signing transactions, submitting transactions.
00:02:50.780 - 00:03:45.660, Speaker B: And then the proverb gets the block header per transaction, state the list of transactions information about the bridge to do Zk bridging, and about the consensus protocol. And then it takes that and it generates a proof, and that's generally how things work. But as thus far, the sequences have been centralized, so people have been happy with a single node. And I do actually find it interesting that if you had a single node blockchain, everyone would just say, well, it's a database. And so all these roll ups though, we've all been sort of vaguely happy with a single node sequencer. And so I think that's something that people want to move away from. They want to try and somehow other decentralize that sequencer.
00:03:45.660 - 00:04:49.830, Speaker B: And so one thing to think about though is if you've got a single node sequencer, then the prover, it trusts that sequencer implicitly, and so it only has to prove the state changes that the transactions do. It doesn't have to worry about whether the transactions are in the correct order. It just has to trust that the sequencer has given it the transactions in the right order, so there's no check. And so when the verifier contract is looking at something, it also is implicitly trusting that sequencer. So again, it's just trusting that the order of transactions is right. And it's just verifying that if you have this transaction, then you'll go from this state to that state, and the proof of that is correct. So what about decentralized sequences? So some of the top level requirements is that.
00:04:49.830 - 00:05:30.272, Speaker B: So what is the right order of transactions? Well, that's a good question. I think it's up to the it. So Frank asked, what is the right, in inverted commas, order of transactions? And so it could be the time order that they arrive at the sequencer. But maybe people can say, pay more money to have a transaction turn up earlier before a batch of transactions has been posted to the verifier contract. It's really open to what that ordering actually is. Yeah, too philosophical.
00:05:30.336 - 00:05:58.716, Speaker A: Well, but maybe I can ask the question. What I mean is, where is the order and when is it defined? I don't mind about the definition, because you say here, oh yeah, the verifier or the ZK approver, they are not going to verify the order. But when you write that the sequencer gets a transaction, it gets a batch of transactions, right? Or does it order them? What do you assume?
00:05:58.908 - 00:06:11.520, Speaker B: I think it's up to the sequencer. So different sequencers will do different things similar to blockchain clients and how they will do different things depending on who's designed the blockchain client.
00:06:11.600 - 00:06:37.528, Speaker A: Okay, so instead of, I would say I'm trying to understand the idea, right? Instead of saying something negative that the prover is not going to verify the order, if I understand. Well, we think as the order, as being outside of the pop system, we're not going to care about the order of transactions. It's defined by the sequencer, and we'll prove that the execution is correct, but nothing about the ordering.
00:06:37.704 - 00:06:38.380, Speaker B: Yes.
00:06:38.530 - 00:06:39.230, Speaker A: Okay.
00:06:43.970 - 00:07:39.882, Speaker B: For a decentralized sequencer, you need to come to an agreement fairly on the order of transactions in a block and the order of blocks. And you need to be able to do this even if some of the sequencer nodes are byzantine. And so these are usually for non malicious reasons, say software configuration issues. Say one node has upgraded and the other one hasn't. Or maybe networking issues, there's a network outage, or maybe you have power or water issues, which mean your air conditioner fails, or maybe just the electricity to the data center fails. Or maybe there's some key compromise, or some other situation, and hence your nodes hacked. Or maybe the actual operator themselves are malicious.
00:07:39.882 - 00:08:25.120, Speaker B: So there are reasons why you might have byzantine failures. And so you need to be able to come to an agreement fairly, even with these issues. And that sounds a lot like a consensus protocol to me. And another thing is that you need to have deterministic finality of the transaction. That is, you can't start proving a transaction that's not final, so it needs to be deterministic. Or maybe if it's nondeterministic, you wait and make sure that the probability of reorg is really low. But deterministic finality is much better.
00:08:25.120 - 00:09:58.650, Speaker B: And so having a single block finality system consensus protocol would actually be really helpful, so that then, you know, once a block has been created, then the proverb can prove it and not have to worry about whether the block is final or not. So, decentralization, what does that mean? Does it mean 100,000 nodes? Does it mean 1000 nodes? Does it mean 20 nodes? People have sort of been happy with centralized sequencer nodes at the moment, and so you could go fully decentralized. But I would argue that having even say, 20 or 30 nodes that are completely independent, run by independent people, have independent DevOps, is probably pretty decentralized all by itself. And it's certainly a good stepping stone towards a fully permissionless system. Fully permissionless, decentralized roll ups aren't going to be suitable for everyone. So I think if we just think about at the moment, say, a limited system of, say, 30 different independently run nodes, then that's a good partial way towards decentralization. And so if we're talking about single block finality, say 20 or 30 nodes, then we're talking about a BFT consensus protocol.
00:09:58.650 - 00:10:37.750, Speaker B: And so that means that your decentralized sequencer could be running a BFT consensus blockchain. So essentially your decentralized sequencer is a blockchain. And a lot of people like POS as well. POS is really good. It helps with the utility of the token, helps people have control and ownership, and also share in the rewards depending on how much they're prepared to invest. So Pos is good. So it'd be great if the algorithm is actually a POS BFT.
00:10:37.750 - 00:11:41.100, Speaker B: So the ZK prover, it can only now trust blocks proposed by the validators from the BFT blockchain. If the consensus protocol shows the validators of the blockchain came to agreement on the block. So I think that's what we're, so hence the ZK prover needs to prove that the state changes have happened correctly and the validators came to a consensus on the chain of blocks. That's what it seems like, but it isn't quite right. And to understand why and that there's a subtle nuance there, you need to understand a bit about how provers are being designed. It's prover design for dummies. But I think I feel like I'm a dummy on this because every time I learn a little bit more about provers, the more I realize I don't know about prover design.
00:11:41.100 - 00:12:44.330, Speaker B: You can imagine you've got your BFT blockchain, you've got a couple of validator nodes, maybe 30 validator nodes. You've got RPC nodes, so remote procedural nodes, and they're talking with a prover system. You've got a leader that coordinates activities. You've got workers proof aggregators, consensus prover, and then you've still got that verifier contract. So the first thing that happens is the leader in the ZK prover calls to the RPC and know is there a new block? Give me information that I need from the block. So the per transaction state, the list of transactions, block header information and that information is returned. And then the leader farms out the transaction.
00:12:44.330 - 00:13:50.150, Speaker B: So it says to say worker one, here's a transaction, here's the state of, so you're going to access this state when you execute that transaction. And so here's a proof, a Merkel proof of that state. And so then the worker then executes that and comes up with a per transaction proof of this execution. Took the state from that starting state to this ending state. And so you do that for all of the transactions, and then you have, the leader will also probably in parallel with all of this farm out to a special consensus prover. Here's the block header, here's the merkel proofs of state for validator set information. And then you get a consensus proof saying yes, the consensus participants can agreed on the block.
00:13:50.150 - 00:14:47.990, Speaker B: And then the proof aggregator aggregates all of those proofs together, so that then you end up with that small proof in the end. And then the block proof is put onto the verifier contract, which verifies everything. The thing to think about with all of this is that the workers are stateless. The proof aggregator is stateless, the consensus proofer is stateless. And the leader, beyond needing to know the current block number, it's stateless too. And so the whole ZK prover is in fact stateless in this design. So you can think that you've got different blockchain clients running the sequencer, you've got this stateless stuff as the ZK prover, and then that verifier contract.
00:14:47.990 - 00:15:56.166, Speaker B: And so I said earlier that the ZK prover can only trust blocks proposed by validators from BFT blockchain if they came to an agreement on the block. However, really they can only create a proof if the block and correct blocks been proposed, and they'll fail to create a proof. So it's a subtlety. So it's not about the ZK proof of trusting anything, it's about what it can create a proof of. And the verifier contract will then verify that the proof is correct. But the proof can only be constructed if the state transitions are correct. All right, what about decentralized sequence consensus algorithms? So we talked about you're getting that information from the decentralized sequencer.
00:15:56.166 - 00:17:07.860, Speaker B: So those blockchain nodes, the big important piece of information is there must be finality. So you must have deterministic finality, and that's a requirement of the consensus protocol. So Adam Ma said, might have missed this, but does the proof of store l two state? So this is going back to this diagram here and in this model, which is where a lot of the provers are heading towards. So this is what scroll are doing, what polygon zero and Hermes are moving towards, and what consensus linear is moving towards is that the prover is stateless. And so all of that l two state sits in the decentralized sequence of blockchain. Does that make sense? Adam? Okay, cool. Okay, so finality is the first requirement.
00:17:07.860 - 00:18:27.052, Speaker B: Another thing that comes out is that you've got multiple evms running. So at the moment, if you're running a blockchain, you might have say a go Ethereum client and a hyperledger Besu client, and each of them have got their own EVM implementation. And so those two EVM implementations have to be the same, otherwise you end up not being able to create blocks. You have a disagreement about what's a valid block or what the state is, so your state route ends up being different, and hence you can't agree on the blocks. And so similarly with a roll up, you've got an evM, or at least one evm, but could be multiple evms running this decentralized sequence of blockchain. So maybe it's just go Ethereum, but you could have go Ethereum, Eragon and Besu, so all of them would have to be the same EVM. But additionally the ZK prover, its circuits, where it's proving that this execution results in this state change, it's also essentially got an evm in it.
00:18:27.052 - 00:19:16.536, Speaker B: And so the two evms between the sequencer and the prover, they have to work exactly the same. So they got to have the same opcodes, same pre compiles, same storage systems, same absolutely everything. And if they don't, then you're not going to be able to prove what's happening on that decentralized sequencer. So they have to be the same. And so an important upshot of this is that say you think I want to have my very own pre compile in my decentralized sequencer. So if you do that, and then you call that precompile, the prover won't know about that pre compile. So it should think that you should get a revert because you're calling a contract address that doesn't exist.
00:19:16.536 - 00:20:11.980, Speaker B: And so in the OD sequencer it'll have worked, and something will have happened. In the prover it will have reverted, and so hence the prover will not be able to prove the state from the sequencer. Hence these e two evms have to be in sync. And so if you hard fork, say the sequencer, say to support push zero, for instance, then the prover would need to, at exactly the same block, have a hard fork. So the provers need to have the ability to have hard forks in exactly the same way as the blockchain clones. So what are the ramifications of this EVM compatibility? So there's a few things here. So anything that's going to affect the state has to happen on chain.
00:20:11.980 - 00:21:32.040, Speaker B: And the reasoning around this is that the ZK prover, it proves transactions out of the box, no changes. And so if you do an action and it happens via a transaction, then the prover is just going to prove it and there'll be no special logic and it'll all be good. So when you go to Polygon consensus matter, labs scroll and say, hey, we've got this special extra feature, and they all say no, then you're in trouble. But if you say everything happens out of the box, plain Ethereum, then they'll be happy and they'll say yes, it's going to work, and hence everything has to happen via transactions. And so when you've got the validator set, you're going to want to be able to change the validator set in a way that you can agree on. And so really you need to have this being happening in a contract so that then the transactions which occur which change the validator set can happen via a transaction. And so that means that when the consensus protocol is executing, it's got to get the list of validators and say their amount of stake via a vehicle on a contract.
00:21:32.040 - 00:23:13.880, Speaker B: It also means that within the validator set contract, the location of the validator information needs to be in a deterministic storage slot, so that then the storage slot can be part of the proof, because you're going to need to pass those storage slots to the prover. And so delving into that, so in your validator set contract you're going to have something like this where you've got the node address maps to a boolean which is saying whether the node is active or not. So default is false, which would mean it's not active. So if the mapping rather is the first variable declared, then it's going to be in storage slot zero. And hence for the prover to prove that a validator is active, it just needs to calculate the storage slot as being kick hack zero and then comma the node address and then find that it's got the proof of that storage slot and then check whether the value is zero. So there's a methodology to work out whether a node is an active validator for the prover. Other ramifications of doing everything in transactions is that if you want to do balance changes due to block rewards, so things that aren't part of the EVM as such, then that has to happen via a transaction.
00:23:13.880 - 00:24:25.592, Speaker B: And so thankfully we've got Coinbase as an opcode and so you can pay the Coinbase account, but it means that every time you've got a block, you need to have a transaction which pays that Coinbase account. And that code is going to need to be super lightweight because you're going to be doing this transaction every single block. And everyone says what's your TPS, what's your gas per second you can do on your l two? And if you start saying, oh, but by the way, we've got this transaction we do once a block and it uses 300,000 gas, then you're not going to be very popular. So you want to really try and limit this transaction down to costing as little as possible. But of course you've also got to make sure that you haven't paid out the block reward twice. You're going to need to have a bit of security around the transaction. Another part of this is that it'd be good to pay block rewards to non block proposer validators that have been part of the consensus protocol.
00:24:25.592 - 00:25:57.372, Speaker B: So normally you want to pay people who you can prove have been part of the consensus, so that then you're getting some recompense even if you're not the block proposer. And in this way, if you say aren't the block proposer for an hour, you don't say switch your validator node off for say 50 minutes, then switch it on again when you're the block proposer, which is a lazy validator problem. And so the problem with trying to pay non block proposers is there's no good way, there's no cheap way of doing it, so you risk the lazy validator problem. Slashing is another thing which needs to be done on chain, but again, you want to do it cheaply and you need to be able to do it on events that are provable as well. So you don't want to just have it as a popularity contest where we slash someone because a lot of people sign something saying hey, they're bad. So the slashing condition that we're going to run with in immutable is non participation in the consensus protocol. And so essentially we're going to be saying, well look, that validator hasn't proposed a block in this time period and the probability of them not having proposed a block, given the selection process, is one in a trillion or something.
00:25:57.372 - 00:27:11.712, Speaker B: And so chances are they should have been proposing a block. So that's our approach and we should be able to do that quite cheaply. Other things to think about. So how do you have consensus signatures in a block header and how do they relate to the validator set contract? What about the signature algorithm? And when should the validator set change? Any time? Or is there a specific time? And so leveraging QBFT, there's an extra data field in the block header of Ethereum blocks. And so you could have this sort of structure where you've got some vanity data which really is just having an id field to say this is this consensus protocol. Maybe you don't need it, but maybe you do. Then have a list of validators that have signed the block header and then have the round number which is just used within the actual say QBFT consensus, and then have the array of signatures.
00:27:11.712 - 00:28:31.660, Speaker B: And so you can use that validator address data and that mapping of the validator set to then load up, then make sure a validator is real. And then it's just a matter of verifying the commit seal the signature. So what about signature algorithms? So the cost of an EC recover is a certain amount per signature. And if you do BLS aggregation, then you're adding the public points and the points that are the signatures together and then doing a pairing operation. But that pairing operation is going to take about 30 times as much proving cost. And so when I'm talking about proving cost, I'm actually talking about like the Amazon Web services compute cost. So how much does it cost to run that computer? And so if the computer is going to take 30 times as much, which is similar to the gas costs difference of 3000 to about 100,000, then if you're only going to have 20 or 30 validators, you're probably better off just sticking with ECDSA and not doing BLS aggregation.
00:28:31.660 - 00:30:03.864, Speaker B: So when should validators change? So you can imagine you're going to be actually proving batches of blocks. And so you're not just going to do one block. You might do say for us we've got a two second block time, maybe we prove blocks once a minute. I'm not sure it's just a number, but say we do 30 blocks in a batch, then you can think that if the validator set only changes once per block, then you might be able to, if block 103 is going to include the block hash of block 102 and further back. And so if you can prove the consensus is correct and so the validators signed block 103, then you know that they must also have signed 102 and 101. And hence if you just change who the validators are at the start of a batch, then you can just do this verification of consensus at the end of the batch and not for each and every block. And so that's obviously going to save like a lot.
00:30:03.864 - 00:30:56.650, Speaker B: So if I'm say, talking about two second block time and say 1 minute batch hosting time, or maybe it's even five minutes or ten minutes. So if I'm talking about that, then we're talking about 30 or maybe 300 times saving on that consensus proving cost, which is a lot of money. So I've talked a lot about. Okay, I've got a few questions. So Frank said, what is the impact on latency? There you go. And that was two minutes ago. So when you say latency, Frank, what do you mean by latency and in what context?
00:30:58.910 - 00:31:25.800, Speaker A: Okay, so what I mean is you're adding a certain amount of layers, like consensus round and so on. So compared to a centralized sequencer, of course there's going to be some cost and some delays. Right. But do you have an idea of what it is? Is it 100 times? Ten times, yeah.
00:31:26.570 - 00:32:17.458, Speaker B: So with a centralized sequencer often you'll get a soft commit, which will be maybe the centralized sequencer will sign the transaction hash and say, got it. Don't worry, I'm going to put it in a batch sometime soon. And so I think that's pretty much instantaneously. So down into the milliseconds, I imagine. But then you might want to wait until the batch is posted to Ethereum. And so then it comes down to how often you're posting the batches. And so some of the roll ups post every five or ten minutes, some of the roll ups post once every day or when there's enough traffic.
00:32:17.458 - 00:33:50.450, Speaker B: And so it really depends then on when they're being posted. If you've got a decentralized blockchain as your sequencer, then the reason why you're worried about when the batch is posted to Ethereum is you're worried about data availability and you're worried about the system dying and you want to be certain that the system hasn't died and lost your transaction. So if you've got a blockchain as the sequencer and you've got not just one node, but you've got say 100 nodes there, then once you can do an RPC call and you're seeing your transaction has been included in the blocks on the blockchain, then you've got your data availability. And so I would argue that your time between when you go from here's my transaction submitted to knowing about it is how long does it take for the transaction to get included. And so in a blockchain that's say using EIP 1559, even if you have to surge and go from half or whatever of your block size to your full block size. As long as you get into that block, you're good. And so if you're not got capacity issues, then chances are you'll get in in one block, period.
00:33:50.450 - 00:34:53.740, Speaker B: But maybe if there's a lot of congestion, maybe it's two. So certainly for our chain intermutable zkevm, it's going to be 2 seconds. And so I'd argue that actually the soft commit where you don't know whether you can use it or not, use that soft commit which is almost instantaneous, but then the real commit, which is minutes to a day later, is worse than having a decentralized sequencer where you're seeing that it's been included after 2 seconds. Obviously if you're doing a multimillion dollar transaction, maybe you want to wait for the proof to be posted as well. Maybe, but I think that the availability is solved by having a decentralized sequencer. So a blockchain, I don't know. What do you think? Do you agree with my answer? Does that make sense?
00:34:56.590 - 00:35:10.426, Speaker A: Yeah. Without any test bed or any practical implementation, it's hard to know. But we'll know. Maybe when you implement it and we test it, that's going to be I.
00:35:10.448 - 00:35:14.000, Speaker B: Agree, or mainet is soon, I can tell.
00:35:15.090 - 00:35:25.940, Speaker A: But I agree with your comment about it's probably better to have an instant finality rather than waiting for something. So I agree with.
00:35:30.150 - 00:36:36.780, Speaker B: Okay, there was another question around consensus rounds overhead. So for consensus rounds, that's part of the BFT consensus protocol and that normally everything happens in round zero. It's really only if a consensus node is offline. I did a talk on QBFT consensus back about two months ago. So if you have a look at the YouTube video on that, it talks about the consensus rounds and why that might happen. Roberto asked, what triggers a validator set change? So if you're in a PoS consensus, you might have minimum amounts of stake that the validators are supposed to have. And so if their stake goes below a certain amount, say they withdraw stake or they get slashed, then they would not be part of the validator set.
00:36:36.780 - 00:37:33.158, Speaker B: Other validators might come along. Another interesting one that we're working through is so we are allowing for multicoin staking. So where you're not just staked in one currency, but staked in many, and you can imagine there could be price shifts where maybe your token which had boomed, maybe plateaus a bit or maybe goes down a bit. And so maybe if you've got multiple tokens and one of your tokens goes down too low, then maybe you're not meeting some requirement. But yeah, you could imagine though primarily it would be related to new stakers coming along, new validators coming along, and validators either withdrawing their stake or being slashed. Does that make sense? Roberto? Yeah.
00:37:33.244 - 00:37:34.040, Speaker C: Thank you.
00:37:34.730 - 00:38:23.610, Speaker B: Okay. And the detail around the actual triggering happens in the smart contract. That's just some logic that needs to happen. Okay, compromises. Wouldn't the world be great if there was no compromises? But I don't think that's the real world. So at the moment I think it's fair to say that the L two prover, ZK prover companies are all working extremely hard to get rock solid workers. This whole piece of leader worker proof aggregator and getting it working really performantly.
00:38:23.610 - 00:39:21.550, Speaker B: Everyone's busily trying to get another 10%, another 20%, another factor of ten. And so they're really busy doing that. And so at the moment, the actual let's prove consensus isn't going to happen. So then the question comes, well how do you get around that if it's not going to be part of the actual proof? And so you can imagine that means that you're going to submit the proof and also the Merkel proof of the validator set storage locations and blockheader to the verifier contract. So essentially the verifier contract is going to do the verification in contract, which is going to cost a heap of gas. But I guess you're going to save on your AWS costs of not having to do it there. But you can imagine in the fullness of time that'll all be integrated.
00:39:21.550 - 00:40:20.590, Speaker B: All right, so I've done a few attempts at standardization and I think it's interesting to just dig into what they are and where they're up to. So the first one I had a crack at was EVM profiles for roll up. So as I said, that you've got these two separate evms. And one of the thoughts is that we often hear about type one, type two, type three types of ZK roll ups. And a lot of the differences are supposed to be around how the actual EVM works. So maybe the gas prices are different, maybe there's some opcode that's not supported or some pre compile or something. And so I started off by trying to say, well, let's try and state how this should work and have profiles that could say, well, this is what a type one actually means.
00:40:20.590 - 00:41:29.334, Speaker B: And if there's any changes, this is what it is. So this hasn't gained a bit of traction for, I think primarily because everyone's just busy trying to have it perfectly be ethereum. And so people want to be able to release and say, this is where we're up to, but they don't want to bother about standardizing, because when they're able to prove everything, then that profile is not going to be, any of the profiles aren't going to be important. However, there's talk of adding extra opcodes and pre compiles for things like account abstraction and just have them available on l two. And so if that was the case, then again, I think so that you could write some code that works against a certain profile, you'd want to know that the prover was able to work in that profile. And so I don't think this requirement or this need has gone away. I think that it's just that maybe we're not quite at the point where we're ready to standardize this.
00:41:29.334 - 00:42:59.060, Speaker B: So maybe give it another six months and then maybe people will be ready to come to the table. The other one is around the prover sequencer API. And so the first one was just generally, what does the per transaction state look like? How do you actually package that up? What are the transactions look like? What other information do you need to have in that information that goes from sequence at approver. And then another part of that was working on, well, what information do you need to prove consensus? So trying to have it flexible for a variety of consensus protocols, what should you have and what should it look like and what information do you supply and how do you supply it? And then the final one was related to bridging and trying to allow for what's the minimum thing that you could define and have a bridge. And so the idea with each of these a, b and C protocols was that you would have a range of provers, could work with any blockchain client. So you'd have go Ethereum and Aragon and Bessu, and they would all implement a single API that would provide all of this information. And then any prover could call that API and get the information.
00:42:59.060 - 00:43:43.394, Speaker B: And so the business people from all of the prover companies thought this sounds like a really good idea, because rather than them having to worry about the sequences and essentially the blockchain clients, they can just call a common API. So it should be less work. However, then there's a few things that come into play. And I think this is what the technical people pointed out. And one is that the prover companies are still optimizing and are still improving their own technology. And so really trying to work out what they need from a sequencer is changing subtly. And so they're not ready to standardize as well.
00:43:43.394 - 00:44:34.130, Speaker B: I think some of them probably view that they got some secret source, some special thing that they're getting out of their blockchain client, their sequencer, and that's making their thing better in some way, shape or form. And hence they're not wanting to standardize because they don't want everyone else to have their secret sorts. But I think that for all three of these, certainly this one here, the initial one, I think it will happen. But again, this might be six months or maybe a year off at the moment. The prover companies, they've had a little bit of a look, but they're not really ready to come to the standardization table yet. They're not ready, I think. And if anyone's from the prover companies believes differently, please reach out, Telegram, LinkedIn, whatever, and we can have a chat.
00:44:34.130 - 00:45:10.610, Speaker B: So in summary, the consensus algorithm needs to provide deterministic finality. The validator set must be in a contract. Changes to the validator set must be via transactions. Block rewards and slashing needs to be via transactions. And the validator set, if it's going to be 20 or 30, just use ECDSA. So there's a merch store up and running. Grab your merch.
00:45:10.610 - 00:46:18.858, Speaker B: We've actually got a whole stack of talks coming up. And so the next one is by Christian and he's going to talk about a new range of wallets, so similar to the passport wallet that I talked about a month ago, but other brands of wallets and just trying to compare. How do they go? We've got a distributed oracle protocol from supra and they're going to talk about how that works. We've got Diego, who's going to talk about the technology that he's involved with at Cartesi, where essentially you can run the whole Linux stack as a smart contract. And it's funny, it feels a little bit like hyperledger fabric running everything in a docker container. So anyway, it'll be interesting to talk about that. We've got push, which is cross blockchain messaging, and then we've got Dr.
00:46:18.858 - 00:46:52.040, Speaker B: Sandra Johnson and she's going to talk about proposal selection and EIP 7251. So if you're here live, this is on YouTube, there's a slack workspace. If you're watching this on YouTube and you want to hassle the speakers and ask them tricky questions. Join the meetup. And sometimes there's example code, and that's where it is. So I've talked for quite a while and answered questions live, but are there any other questions?
00:46:57.990 - 00:47:35.470, Speaker A: I have some questions, actually. Sure. Related to the. So at the beginning of the talk, you sort of stated that it was taken for granted that we have to decentralize the sequencer and it's somehow related to censorship resistance and so on. I've got a follow up question to this one, but the first thing is, how do you ensure that with the decentralized sequencer you've got something that is censorship resistant?
00:47:36.850 - 00:47:56.162, Speaker B: Good question. So I think the way to do that is to have a large number of nodes, whether they be RPC nodes or validator nodes. The more nodes you've got, the better.
00:47:56.216 - 00:48:02.146, Speaker A: So the censorship resistance is scaling with the number of nodes. It's a function of the number of nodes.
00:48:02.258 - 00:48:19.578, Speaker B: Yeah. And if they're all independent, because at the end of the day as well, if you start censoring stuff, your chain is going to get a really nasty name really quickly and everyone's going to start exiting the chain. The validators are incentivized not to censor, I think.
00:48:19.744 - 00:48:38.866, Speaker A: But let's say something simple, right? I'm a user and I'm posting a transaction somewhere. So maybe to a participant in the consensus, there's no guarantee that this participant is going to include it and be fair with me at any point in time. Right?
00:48:39.048 - 00:48:43.262, Speaker B: Yeah, that's true. But your transaction will sit in the transaction pool.
00:48:43.326 - 00:48:45.186, Speaker A: Yes. In the mem pool for a while.
00:48:45.288 - 00:48:45.746, Speaker B: Yeah.
00:48:45.848 - 00:49:04.982, Speaker A: The second thing is, I agree with this. It's not clear that it's censorship resistance. And if there's some censorship resistance, it's a function of the number of nodes. So you may need, the more nodes you have that are involved, the more randomness somehow the better. Right. For censorship resistance.
00:49:05.046 - 00:49:32.740, Speaker B: Yeah, that's the thing. So the more blockchain nodes you've got, because that's essentially what you've got. It's just you're back to where a blockchain, if you've got a blockchain and you've got two nodes, it's not as good as 20 nodes. And even at 20, even at ten independent companies, I think trying to get them to all agree to censor something is going to be pretty hard anyway.
00:49:33.910 - 00:49:55.130, Speaker A: So the other thing is, it's related to the order of transactions and something you haven't mentioned during the talk, which is mev attacks. So usually you want to be some censorship resistant. And if you can mitigate mev attacks. Right. So how does this approach mitigate mev attacks?
00:49:55.870 - 00:50:31.538, Speaker B: So if you want to start worrying about front running and all that stuff, then your blockchain that you've got, you can do all of the things that are being done in ethereum. You can have proposal builder separation, and you can go and do all of that stuff, can be redone at the l two layer as well, which obviously suddenly makes everything vastly complicated. But that's how you do it if you start to get sandwich attacks, front running attacks.
00:50:31.714 - 00:50:42.140, Speaker A: So maybe there's a simpler approach. But we've been working on what we call fair sequencing in the research team at mentor, and I'm happy to talk about it in a couple of weeks.
00:50:42.670 - 00:50:44.060, Speaker B: Okay, well, sure.
00:50:47.390 - 00:51:01.040, Speaker A: The premise is that censorship, resistance or fairness is not necessarily achieved by decentralizing the sequencer. There may be other ways of doing it and achieving it. Okay.
00:51:02.770 - 00:51:09.214, Speaker B: It'll be interesting. All right, well, let's set up a call and let's set up a meet up talk.
00:51:09.412 - 00:51:31.000, Speaker A: It's not a comment against what you're saying, right. Or you're doing. I think it's widespread in the industry at the moment that we don't really know what problem we are solving, but we are decentralizing the sequencer. That's why I would phrase it. And then we'll try to find out what we are actually solving. So the way we approached it in our team was a big difference. We say, what do we want to solve? And then trying to have a solution to it.
00:51:31.550 - 00:51:55.460, Speaker B: Well, and I think the decentralizing the sequencer, it does more than just censorship resistance, because you've also got to worry about your single key, and there's a lot of other things going on. And maybe we should do a talk just on that. What are the problems of single node sequences and what are the possible mitigations for?
00:51:55.830 - 00:51:56.580, Speaker A: Yeah.
00:51:58.950 - 00:53:23.310, Speaker B: Okay, Roberto, what is your view on shared sequences and what advantages disadvantages do they provide? So my understanding of shared sequences is that you've got, essentially a sequencer that does a certain rate. Say you said it does 100 tps, and you essentially farm out part of your block space to other subsequences. And so say you had ten subsequences and each of them had ten tps or something. And so the idea is that they can essentially be multiplexing. And because those subsequences are all in the same overall sequencer, the overall same block, then you can do fancy things with Merkel proofs to prove the various things between the subsequences. And so the advantage is simpler cross chain communication. The disadvantage is that your throughput is limited by the actual overall parent sequencer.
00:53:23.310 - 00:54:00.730, Speaker B: So if you're doing, I don't know, 7.5 million gas per second, then the sum of all of these subsequences is going to be 7.5 million gas per second. And invariably, in the shared sequence articles, it's something that they seem to avoid discussing, which I don't understand. Why not? Because it's obviously a disadvantage. They always talk about the advantage of how good the cross chain communication is and how you have single block or within one block communication and stuff, which is great, but it's a pretty big limitation. I don't know.
00:54:00.730 - 00:54:12.520, Speaker B: Does that gel with what everyone else understands? Getting a nod from Frank? Is that what you understand too, Roberto?
00:54:13.260 - 00:54:56.048, Speaker C: One of the things I'm not sure about is in terms of cross roll up communication, each roll up potentially has a different proverb, right? So you might have a kind of synchronization point with the shared sequences, but then water lab might finalize that proof and the other doesn't. So kind of looks like until all the proofs involved into cross roll up transactions are finalized, you really don't like. You only have certainty that this cross roll up transaction actually occurred. Does that match?
00:54:56.234 - 00:55:39.172, Speaker B: Yeah. I thought that for a shared sequencer there was just one proverb. All right, so anyone who's out there, who's watching this recording, who wants to do a talk on shared sequences and do a detailed review and be prepared to have people ask some tricky questions, please fire me a message. All right. Well, as always, the questions are always very helpful in doing a talk. So thank you, everyone, for coming along live and asking me lots of questions. I look forward to seeing you all in two weeks time.
00:55:39.172 - 00:55:41.060, Speaker B: Have a great fortnight.
00:55:43.320 - 00:55:44.580, Speaker A: Thank you, Peter.
00:55:46.840 - 00:55:48.768, Speaker B: Bye bye. Thanks, Peter.
00:55:48.864 - 00:55:49.280, Speaker C: Thanks, Peter.
