00:00:01.290 - 00:00:26.150, Speaker A: Okay. Hello and welcome everyone. I am Peter Robinson, and today I have Frank here who's going to tell us about a magical algorithm that helps the deposit contract for Ethereum to work. And so he's done, he and team have done analysis, or was it just you, Frank? Anyway, Frank was going to tell us all about it, so please introduce yourself and then do your slides.
00:00:26.730 - 00:00:52.650, Speaker B: Thank you, Peter. So I'm going to share my screen with slides, so if you want to interrupt me during the talk, please feel free to do so. So hopefully I'm in presentation mode now and you can see the.
00:00:52.720 - 00:00:53.740, Speaker A: Yes, you are.
00:00:55.330 - 00:01:43.450, Speaker B: Thank you, Peter, for inviting me to give a topic of formal verification of algorithms. So I'm Frank, and I'm a member of the trustworthy smart contracts team within consensus software R and D. And so today I'm going to talk about a formal verification work on an algorithm that's key into the Ethereum two beacon chain. And Peter was mentioning at the beginning that there was not much in the title that would refer to Ethereum two. So I should probably try to make sure there's some relation to it. And that's my first slide, actually. So this is the algorithm that is implemented in the deposit smart contract of the beacon chain.
00:01:43.450 - 00:02:40.640, Speaker B: So I'm going to explain a few things later on, but basically that's a very simple algorithm at the end of the day. But you can see that the developers, they wrote a comment at the end of this loop saying that, okay, as the loop should always end prematurely with the return statement, this code should be unreachable, and we assert false just to be safe. So there's a few things, I'm not going to elaborate too much on this, but writing an assert false just to be safe is debatable. And that means that actually that they think that this is correct. The algorithm that you should never terminate basically the loop because of this test turning into false, but they are not sure about it. So this is the kinds of problems that may arise in an algorithm like the deposit smart contract. And we would like to make sure that these kind of runtime errors that can have critical effects cannot happen.
00:02:40.640 - 00:03:34.830, Speaker B: So that's the actual purpose of the work I've been doing last year, and I've been trying to verify and provide a formal proof of correctness of this algorithm, which is part of the incremental mercury algorithms, and providing a formal verification of the algorithm to make sure that this can never be reached at runtime. So this is the kind of things that we can do. We can do much more. But among the things we can conclude is that, yes, that's true, what they were asserting here, we can prove it formally. And by formally I mean with rigorous, let's say mathematical reasoning. And on top of that, it's not only rigorous mathematical reasoning, but it's rigorous mathematical, mechanized reasoning. So the proof that we provide can be verified and checked by a program which is a verified.
00:03:34.830 - 00:05:00.810, Speaker B: So I hope it clarifies and relate the topic to ethereum two and the beacon chain. So in the rest of this talk, I'm going to focus on the algorithms that will pertain to incremental Merkel trees and describe how we can design them and verify them. So the first thing I'm going to do is to try to provide a statement of the incremental Merkel tree problem is, and then instead of doing what other people have tried to do before, which is taking the previous algorithms and trying to prove that it's correct, I'm going to try and design the algorithms in a functional style and try to prove that the imperative versions of the algorithm implements the functional style algorithm. And I will try to finish with some findings and lessons learned during this experiment. And also, if I have some time to give you an idea of what the code look like and also talk about related work. So what is the incremental mercury problem? So the basic thing that the deposit smart contract in doing is doing in the beacon chain is to record the list of validators and their stake. So I've given in the beacon chain there are validators in charge of proposing blocks and determining which block is going to go next, let's say running the consensus algorithm.
00:05:00.810 - 00:05:37.238, Speaker B: So the deposit smart contract is in charge of letting validators get into this game. And to do so, they have to make a deposit, they have to stake something. And you can imagine that the list of validators that have staked something is indexed between zero and a large number. So at index zero we have a validator that has staked something, indexed one another one. So you index the validators and you've got the list of validators with their stake. So as you can see here, I took some liberty already with the stakes. It's not actual ether and so on, it's a number, because it doesn't really matter.
00:05:37.238 - 00:06:44.350, Speaker B: It's just that with each validator you associate a number, and this is the stake in the deposit smart contract. But in principle it's a list and you would like to record this list, but of course this list can grow very big and you don't want to send this list to all the nodes in the network every time you need to check some properties of this list. So you would like to have a compact representation of the list. And this is done by, let's say summarizing this list into a hash number. And this hash number is computed using what's called a merkel tree. So if I have a list, let's say of five validators with their stake, the first thing I'm going to do to build a merkel tree is to complement this list to write padded with default values to make the list of length a power of two. And if I do that, I can then put all the elements of the list into the leaves of the tree from left to right, and I can compute a number.
00:06:44.350 - 00:07:28.486, Speaker B: So this is how merkel trees work. You compute a number associated with the root of the tree by synthesizing a value from the bottom to the top. And the way you're going to do it is to use a function that combines the values of the left and right siblings of each node. And the function I'm going to use in the next examples is a simple one. You're going to compute the value of a node which is not a leaf. The leaves are given values already, but the value of a node will be the difference between the left and right value minus one. So if we do that, we compute it bottom up, we get at some point the value at the root node, and this is called the root hash.
00:07:28.486 - 00:08:29.114, Speaker B: So how it is used in the, let's say, deposit smart contract, and in the beacon chain is that for instance, two nodes, if they want to check that they have the same list of validators, they're going to use the root hash instead of the actual list to check that this is the same number. And provided this is, let's say a good hash function, which is not the case for mine, right? It's just for the sake of the example. But if it's a good hash function, the probability that the root hash is equal for two different kits is close to zero. So you can basically check and send this root hash to other nodes and say this is the root hash. And if we can check that we have the same list, we just have to compare our root hashes. It has other applications in this Merkel trees framework, which is basically being able to prove that a given value is at a given index in the tree. And this is called a Merkel proof.
00:08:29.114 - 00:09:25.326, Speaker B: So I'm not going to talk about Merkel proofs in this talk, but just about computing efficiently the hash root of a Merkel tree. And under the circumstances that we have in the beacon chain, the computation is actually an incremental computation. So that's what I'm going to describe next. So if we add a value, for instance, in this list, so we had the list of five, and in the beacon chain the validators are coming in, so we assume that they can never go out, but they're coming in. So we'd like to append the validator with its stake at the end of the list. And in this case, of course, the corresponding Merkel tree has changed. So we have to recompute all the values on the leaves and on the nodes of this tree, and we end up with a new value at the root.
00:09:25.326 - 00:11:16.110, Speaker B: And you can see that in principle, I don't actually need to insert a new value in the tree and to recompute the root node, I don't actually need to use and to do the computation that have already been done on the left hand side in this orange triangle, and on the right hand side in this green triangle, they are not changed by the insertion of a new value. So in principle, the computation of the root hash on arbitrary tree can be done in exponential time. In the height of the tree, you have to go through all the nodes from the bottom to the top. But for the problem that we have at hand, we have a tree, and the tree is going to be built by gradually adding new values to the leaves from left to right. So what we would like to know is whether in this particular case that our tree is modified in adding a value from left to right every time, can we provide an incremental computation of the root ash for each tree? By incremental I mean if you have computed the first root ash, when you have one value inserted in the tree, can you use the sort of the results that you have already computed for r one to compute r two, and so on. So the question is, instead of recomputing bottom up in exponential times, the root ash of a tree every time you insert a new value, can we have an algorithm that can do that in polynomial time, the successive root hashes? And of course, a special interest here is, yes, we can do that. I think Vitalik Petrain proposed the algorithm that can do it, and that is implemented in the smart contract that I've displayed previously.
00:11:16.110 - 00:12:15.670, Speaker B: But what we are interested in here is can we also formally prove that the algorithm that we provide is correct? So I'll define what correctness means and how we can formally prove that this is correct. So any question on the problem. Feel free to interrupt me and ask questions anytime. Right. So there were a few attempts to try to prove these incremental mercury algorithms, let's say the deposit one that I've proposed, that I've shown at the beginning, but most of them were about finding bugs, and there was no proof of the absence of bugs. So instead of trying to do the same thing that's been done before, which is you take the algorithms and you try to check whether there is a bug or not, I'm going to use a standard, let's say academic style algorithm design principles. Try to get some properties of problem and try to design the algorithm.
00:12:15.670 - 00:13:41.922, Speaker B: So one thing that is important in the algorithms that are, or let's say the methods that are in the deposit smart contract, is the fact that the deposit smart contract has two methods, and there's one method that is getting the current root of the current tree. So you're in a given state, and you would like to know the root hash of a given merkel tree that corresponds to the list of validators. And another method is adding a new value in the tree. So it's broken into two for the reason that is that adding a value to the tree, let's say it costs some gas because there's some state modification, so you have to pay for it, whereas computing the root of the tree on a given tree, it's a read only operation, so it doesn't cost anything. So I'm going to follow this principle here to first try to show r given a tree and a summary, or a compact, let's say, state representation of this tree. How can we compute the root of a tree? So what they do in the algorithms of a deposit smart contract is that they consider one path to a given leaf. And you can see that actually to compute the root hash of the tree, you don't need to know all of the values in the other subtrees on the left and on the right.
00:13:41.922 - 00:14:46.242, Speaker B: But you just need, because you combine the value at each node, you use the values of the siblings of a given node. You just need to know at each node on the grain path the values of the siblings on this path. And of course the values on the grain path are going to be computed, so they're going to be updated according to the value that you insert. So if I insert four, I will compute three using the siblings of this node, and then I will compute three using the siblings on this node, and minus twelve using the new value and the siblings on this node. So to compute the root hash in a given tree, one simple result is that you just need the values of the leaf on one path and the values on the siblings at each level in the tree of the nodes that are on this path. So that's very similar to, you've seen that before, probably to providing a Merkel proof. But that's what you need to compute the result of the root path of the root hash.
00:14:46.242 - 00:16:13.006, Speaker B: So that's going to guide our ID to say, well, if we want a polynomial time algorithm, so we should represent the tree that has exponential size in a polynomial space, can we actually keep track of the left siblings and the right siblings and of the current path? And is it enough to be able to compute incrementally the root hash when we insert a new node? So that's the sort of the principle that we'd like to check. So, of course, if I need to maintain the values of, let's say, left siblings and right siblings, I can do it in a list for each level top to bottom, and the right siblings are for each level top to bottom as well. And you can see that if I maintain these two lists left and right at a given level, for instance, at this level, I don't need the right sibling. So it doesn't matter what's stored at this cell in the list. And on the green path for the left siblings, I just need the left siblings at the highest level. So I don't need the values of the left siblings at the lower level. So in the next slide, I'm going to show a simple algorithm to implement this computation, and it's based on also the representation of a path in a tree by its binary encoding.
00:16:13.006 - 00:16:54.770, Speaker B: So that's a standard thing that you do when you deal with trees. So in a binary tree like this one we have here, we're going to compute to encode a path using the direction you take from the root to the leaf. And when you go right, you use the one, and when you go left from a node, you use zero. So this path from the root to the sif leaf will be encoded as 10 zero. And now I can write a simple algorithm to compute the root in a tree. And I've written this algorithm using a generic type. So it doesn't really matter what you store in the leaves.
00:16:54.770 - 00:17:53.298, Speaker B: You take as an input a path which is a sequence of one and zeros describing the path to the leave, the left and the right siblings. A generic function which stands for the hash function that we have. It should be able to combine two values and returns a new value and the seed, which is the value at the end of a path. And this functional style algorithm computes the root starting from the end of the path and the value and the seed value, and it returns the value, the root ash of the tree. So that's to give you an idea what the sort of higher level specification is and what the algorithm, so this is a very simple algorithm, this one. And of course in the algorithm for the deposit, or let's say getting the rotation of a tree in the deposit smart contract, they don't use the encoding of a path with a sequence of bits. They actually use the index of the leaf that we are at.
00:17:53.298 - 00:18:51.670, Speaker B: So just to give you an idea as well, it's not very hard to do the same thing. And I can write another algorithm that uses instead of the path, the binary encoding of a path, sorry, of the sort of counter, the number of values that we have inserted, k and the height of a tree. I can encode this value k in binary using h bits, and this corresponds to actually the path leading from the root to the leaf. And I can prove later on, I'll tell you how I can do that. But I can prove that this algorithm, using the height and the index of the next leaf we are at, computes exactly the same as this algorithm on the left hand side. But basically it shows that it doesn't matter whether we use a binary encoding of a path or whether we want to use the counter to the current leaf we are at. We can have an equivalence between two algorithms.
00:18:51.670 - 00:20:10.042, Speaker B: So that's how we compute. Let's say if we have a state of the system that summarizes the current Merkel tree using a path, the left siblings on the path and the right siblings on the path, and the seed at the end of the path, we're able to compute the root hash of the current Merkel tree. So now, I hope many of you have heard of dynamic programming. So if not, I'm trying to give you a feeling of what that is. But dynamic programming seems to be really useful in this kind of situation when we want to incrementally compute a value, because it's the art of caching previously computed value and reusing them in order to make computations more efficient. So let's say in the previous slides, what I've said is it looks like if we can have the current path, the value at the end of the current path, and the left siblings and the right siblings on the current path, we are able to compute the root hash. So what I'm going to try and do now is to check whether I can maintain this property when I add a new value to the tree.
00:20:10.042 - 00:21:35.910, Speaker B: So let's assume that I start with this tree, and the first path in the tree is the path to the leftmost leaf. And if I'm able to get for the leftmost leaf, the values on the left siblings from top to bottom and the right siblings from top to bottom in two arrays, l and r, at the beginning, of course, there are no left siblings for this path. For each node and this path, they don't have left sibling, they only have right siblings. So this is the value of l. It doesn't really matter what l is. So if I can maintain it when I add a value three and I insert it in the tree, if I was able to compute the new values on the next path, but update these left and right siblings, and they would store the value of the left siblings and right siblings of the orange path, I would be in a position to recompute the root hash of this new tree, because I've got the left and right siblings of a given path and the value at the end of a path, which in this case would be the default value zero. So if I can maintain this property and make sure that every time I add a new value in the tree, I add a new value in the tree, the path to the next available leaf to be used to insert a new value, I've got the left siblings and right siblings on this path.
00:21:35.910 - 00:22:52.994, Speaker B: Then I'm able, every time, just using these two tables, l and r, and the current path, I'm able to compute the root hash of the tree. So the fact that we can store a compact representation of the tree in two arrays and make sure that every time we insert a new value in the tree, we update these two arrays. It's a technique that's related to dynamic programming. So I'm going to try and explain in the next few slides how we can actually do that and show that if we have the left and right siblings of a given path, how can we compute the left and right siblings of the path leading to the next leaf after we have inserted the new value? So this is the current state, we have a green path leading to the leaf with, let's say a default value, and I've got the left siblings in this table. So again, the two lower levels, we don't care about the left siblings because we don't need the left siblings on the node. Zero and one, sorry, minus one and one here we need only the right siblings to compute the root hash. And this is how it works.
00:22:52.994 - 00:23:38.274, Speaker B: So I'm going to insert a new value, replace the zero with the four. So I have to recompute the values on this path. And I'm using, of course, at each level, the values of the right siblings and left siblings that I can find in my table left and in my table right and on the next path. So the path to the next leave, because I will have inserted a four at this level. So next time I'm going to insert a value, I'm going to use the next node, which is here, and I would like now to update my array so that it contains the value of the left siblings on the blue path. So I can do that. Sorry, I can do that.
00:23:38.274 - 00:24:30.494, Speaker B: And actually, the value on the blue path, the left siblings on the blue path, will be given by this array where four has been inserted at the lower level. So how can we compute this new array, left prime, which contains the values of left siblings on the blue path using the values of left siblings on the green path. That was the one leading to the previous leaf. So the way you do it is to start computing the update from the bottom to the top. And the first time you encounter a left leaf, a left node, sorry. On this path, at a given level, you store the value that you compute in the left table. So you update the left table the first time, bottom up that you find, you encounter a left node.
00:24:30.494 - 00:25:17.230, Speaker B: So in this case, the left node is a leaf, but it works for any other left node. So that's how it works. So basically, to compute the new value of the left siblings from the old value, the value of the left siblings on the path leading to the next available leaf, using the value of the left siblings from the path leading to the previous available leaf. We just need to walk this path up, compute the hash of different nodes, and the first time we encounter a left node, we update the value of this array with the value that we have computed for this left node. And that's it. We don't need to update it anymore previously, because the prefix of this path is the same of a blue path, is the same as the green path. So the left and right siblings for the higher levels are the same as before.
00:25:17.230 - 00:26:29.366, Speaker B: So the takeaway is updating and computing the values of the left siblings of the next path where we have to insert the next value if we want to insert it, requires only a single update in the array that stores the value of the left siblings. So, of course, I'm making some claims here, and I haven't proved it, but that's the point of the next, let's say, phase, where I'm going to try and describe how we can formally prove that. So what about the right siblings? For the right siblings, it's even simpler, actually, because we have a property that we have built, a list, and the suffix of the list has only zeros, the default values. So basically that's the values we have inserted so far, and these leaves have not been used yet. We can prove that the right siblings of any node at a given level are given by a constant. Actually, the root ash of a tree, where the tree has leaves equals to zero for all of them. So the value of the right siblings doesn't change and depends only on the level you are in the tree.
00:26:29.366 - 00:27:19.450, Speaker B: So it can be stored in a table too, but it's the same as before. So this one is a constant in the algorithm. So this is the method or the function to compute the new value. So I'm not going to talk about it too long, but that's the functional version of it. This is, you start with the path, the left siblings of a given path on this path, the right siblings on this path, a new value that you insert at the end, and you return the value of the left siblings, which is a sequence, a list of values on the path that is next to p. So this algorithm computes this new table left prime that I described before. So these are the two main, I would say, components that we need.
00:27:19.450 - 00:28:18.058, Speaker B: So now we have these two things. So I should probably come back on this one. So we have achieved the following. We have an algorithm compute root that given the left and right siblings of a given path, and the value at the end of this path enables us to compute the root hash of a tree. Then we have another algorithm, the one that I described before. When we insert a value, there's a way of updating the left siblings and the right siblings, so that we keep track of the actual leaf we are at, and we have the left and right siblings of the path to a given node. So we are again able to recompute the root hash, because we always have the left and right siblings of a path in the tree after having adding some values up to a given point.
00:28:18.058 - 00:29:02.750, Speaker B: So we have two methods to compute the values of the root hash using left and right. Another method that updates left and right to make sure that we keep track of the values that have been inserted. So we have achieved what we wanted to do at the beginning, which is having incremental algorithm and incremental algorithm to compute the root hash of a given tree. And these two tables are of length, the height of a tree. So this is linear in the height of a tree and not exponential in the height of the tree. And the two algorithms I have presented before are linear two in the height of the tree. So we have two linear algorithm using linear space to incrementally compute the root hash of the Merkel tree.
00:29:02.750 - 00:29:46.890, Speaker B: That corresponds to a list that grows incrementally. Right? So now we have these algorithms, and they are basically the algorithms proposed by Vitalik Petrine a few years ago. We'd like to formally prove that they are correct and that they compute the right thing. So how can we do that? So the main correctness invariants are the following. So again, my reference is a Merkel tree. So I assume that I have a Merkel tree and I have a purple path here. I've inserted some values so far, and I keep track of a path to the next available leaf.
00:29:46.890 - 00:30:54.626, Speaker B: I've got two arrays, left one and right one, that keeps the values of left and right siblings of this purple path, and I'm able to compute the root value r one. So that's the claim. So what I'd like to show is that, yes, if in this tree, the left one and right one corresponds to the values of the left siblings of the path to the leaf at index c one, then when I use my algorithm, compute root up with index that I described before, it actually returns the same thing as what I would have obtained if I had built a tree. So I'm not going to build the tree, I'm just maintaining this state of the tree, which is with a counter to the current leaf, or if you wish, the path encoded in binary, the left siblings of this path and the right siblings of this path. And I claim that this is enough to compute the value of the root of the tree. So I should prove this property that indeed, if I use my algorithm, I obtain the same value that I would have obtained on the tree. And I've got another proof to make, which is an invariant.
00:30:54.626 - 00:31:55.554, Speaker B: So showing that if I insert a new leaf, then I should update my components in the state to reflect this update. So I go into another state, s two, I increment my current counter, I've got one more value inserted in the tree, and I update the left siblings so that they reflect the left siblings on this new path, the green path. And I used my algorithm before that updates the left siblings on the previous state I was in, and the right siblings, of course, they are left and change. And I should show that after this update, the same thing holds. I'm always in a position if when I use my algorithm, compute root with index on the given state, it always returns the root value of the tree. That would have been obtained by inserting values one by one, right? So that's what I'd like to prove. So there are some proofs of these properties using pen and paper.
00:31:55.554 - 00:32:39.046, Speaker B: And a few things have been, let's say, I would say mechanized. I'll describe later what that is. But in the current state, before this work, there was no fully machine checkable proofs of this fact. And that's what we contributed with this work. So how did we do that? We used a verification friendly programming language called Daphne. And in Daphne, what you write is you write programs with annotations that describes the properties that you would like your methods to satisfy. And Daphne has a verification engine, sort of an engine that can reason about all logics and programs and whether programs are correct.
00:32:39.046 - 00:33:30.674, Speaker B: And it can tell you using a backend solver, whether your program is correct with respect to the pre and post conditions, which are specifications or not. And in Daphne, you can also write proofs as programs. So if I need some in my proofs of the previous correctness criteria, if I need some lemmas and so on, I'm also going to write them in Daphne. And this logical reasoning will be encoded as a program and check with Daphne. So I want to write a full proof of the algorithm, including the intermediate limas that are needed, such that this proof is a program and this program, and the correctness of this program can be checked by a verification engine to avoid any mistake that I could make using a pen and paper proof, or things like that. So this is the example of the code that we have in Daphne. So there's two methods.
00:33:30.674 - 00:34:15.490, Speaker B: This get deposit route, which is the imperative version of a compute route. So instead of using function style algorithm, it uses a while loop and the deposit function, which is the inserting, adding a new value in the tree. That's the algorithm that we have synthesized from our specification. So, we'd like to prove that these algorithms do not have any issues. So what are the potential issues that these algorithms can have? As you can see, they use arrays. So the branch array in the algorithm of the deposit smart contract is the equivalent of the left array that I used before the left siblings. And the zero hashes array is the right siblings that are constants.
00:34:15.490 - 00:34:50.430, Speaker B: So these are the two arrays. So when you use arrays and you dereference an array, you should make sure that the index that you're using is within the bounds of the array. So these are the kind of questions that we want to answer. There's also some increment of values in the deposit smart contract in the algorithm. So you would like to check that there's no overflow happening when you compute with those values. There's while loop in the algorithm as well. So you'd like to prove that these loops always terminate.
00:34:50.430 - 00:35:57.170, Speaker B: And of course, as you can see here, there are two arrays, branch and zero hashes, and in the deposit smart contract they are allocated on the heap at the beginning of the execution of the smart contract. So you'd like to know that, or to prove that modifying, for instance, the value at index I in one array doesn't change the value at any index in the other array. So there's no potential harmful aliasing in the system. So how do we do that? So in Daphne we write programs, let's say the get deposit wrote before, and we annotate the program with assertions so they are not, let's say runtime assertions that you would have in a language like Java. But assertions are mandatory things or predicates to prove before you run the program. So we can specify properties of a program using assertions. Like for instance, if I dereference the branch array using index h, I would like to show that h is less than the length of the array.
00:35:57.170 - 00:36:46.230, Speaker B: That's an assertion. And if I write a program like that with Daphne, Daphne will check that this is true, or will try to prove that this is true. And if it can't prove that this is true, it will ask me, or give me some feedback to give me a chance to add some hints in the proofs to be able to prove it in the next round. So it's sort of an iterative way sometimes of proving that the programs are correct. So in this algorithm, there's basically the algorithm that I had before, the get deposit route algorithm that computes the root of given tree with a while loop. And to prove that programs are correct, we usually use invariants. So loop invariants for instance, and in Daphne you can write these loop invariants, and Daphne is going to try and prove that they are correct mechanically.
00:36:46.230 - 00:37:18.974, Speaker B: And of course to prove that, let's say loops terminate. A standard way to do it is also to provide what's called a ranking function to make sure that there's some measure that's decreasing strictly and bounded from below. So we are in a well funded order and that's the way you prove termination. So definitely supports this as well. So I can propose a ranking function and definitely can try to prove that the ranking function has the right properties. That would imply that my loop terminates. And last thing I can do is functional correctness as well.
00:37:18.974 - 00:38:26.086, Speaker B: So that's the actual proof that what this algorithm is computing, it computes the result in a variable, let's say R. And this specification in Daphne says that this is the same thing that you would have had if you had built a tree, a Merkel tree with a list of values. These are the values that you have inserted so far of height, tree height, using a hash function s, and a default value for belief zero. So this is the functional correctness criterion that is used to establish or to specify the correctness of the algorithm. And in Daphne, what we have to do, I'll try to show you some code later on, is to add some annotations in this program to make sure that the verification engine can conclude that this holds at the end of the computation. So these specifications, I will skip these two things, the valid thingy. But this specification says that at the end of the computation of this algorithm, the value of r is the same as the value that you would have had if you had built a Merkel tree with the set of values that you have received, the list of values that you have received so far.
00:38:26.086 - 00:39:01.590, Speaker B: So that's the specification that we have. And of course, we use also pre and post conditions to write test and specifications of programs. So what we have achieved is to write this kind of programs in Daphne. And we have proved some properties, basically some sort of safety properties, that there's no runtime errors. So for instance, there's no array out of bounds, which is using an index. Dereferencing an array at an index, that's not allowed. So that's an array out of bounds.
00:39:01.590 - 00:39:39.370, Speaker B: We have proved that this doesn't happen, that there's no underflow or overflow in the programs. In the two algorithms get deposit, root and deposit, that all the loops terminate. There's also memory safety, so there's no harmful aliasing between the two arrays left and right in the algorithm. And more importantly, these are the sort of standard safety properties that you'd like to prove. So we have proved that it's not, that we have tested that it doesn't happen. We have proved that it cannot happen. So the absence of these problems are proved using our technique, and we have also proved functional correctness.
00:39:39.370 - 00:40:31.006, Speaker B: So that's the property that I described before. The functional correctness is that what the algorithm computes is the same as what we would have obtained if we had built the Merkel trees and computed the root hash on these merkel trees. So that's the basic properties that we have established. And there's a GitHub repository with the code, and I'm going to show you a bit of a code later on. So what are the main findings? I would say in this project we've used a verification friendly programming language, Daphne, that has been used before on other rather large scale projects. So it's not mainstream programming languages. I would say you have to learn a bit of formal verification, program verification before you can use it.
00:40:31.006 - 00:41:14.426, Speaker B: The verification effort for this type of contract. So I was doing this verification was around twelve weeks, and it involved actually writing the documentation, designing these new proofs that was not provided before, and mechanizing these proofs, or writing the proofs as a programs. A few CRMs have been proved in the code. It's 3500 lines of code approximately, and quite a lot of documentation too. I'll show you in a few seconds. And the end of the day we have proved that there's no runtime errors and of course that there's some sort of functional correctness to in the deposit smart contract. And we have highlighted some.
00:41:14.426 - 00:42:13.722, Speaker B: I would say we have proved that the error I showed, or the question I showed at the beginning in the comments by the developers of the solidity code, the assert false statement, we have proved that this is actually correct, that the assert false is never reachable, and we have proved that runtime errors did not exist in the current version of the algorithm. So there's sort of a glimpse of, let's say the different files that we have built. So it's a software project, so it's got packages and different programs too. So this table is just to give you an idea of, let's say, the difficulty of the proofs that are in each file. So if it's green, it's relatively easy. So by relatively easy I mean that when I write the code, the verifier in Daphne can most of the time figure out the proof without any hints. Orange is you need to provide a few hints, so a few lemmas and proofs that you write as programs.
00:42:13.722 - 00:43:18.050, Speaker B: And red is when you need quite a detailed specification of the lemmas that you want to prove. So you need to show Daphne step by step what to do to build the proofs. So overall there's only a few files that require, let's say extensive details in the proofs, I must say that for these ones, for instance, the siblings one, it's unclear to me why some steps are actually needed, and maybe some of them, if I remove them, Daphne can figure them out, but it would take a long time. So it's better to speed up the verification and to add these hints in the proofs and in the deposit smart contract version. What's hard as well to prove is because we have loops and also pointers to arrays or arrays on the heap. There's some reasoning on frames and so on, and that's expensive in terms of formal verification. So overall there's around 100 lemmas that have been proved, and that's enough for the smart contract.
00:43:18.050 - 00:44:08.338, Speaker B: So again, I will highlight only one result related to this, apart from correctness. The question that the developers were asking whether this is safe or not, we have actually proved that, yes, you never exit the loop because this condition becomes false. You exit the loop because inside the loop this condition becomes true. And this is a simplified algorithm that we have. Instead of having, let's say a loop and sort of a break statement in the loop, you can have a single loop. So that makes it a bit clearer and still prove that at the end of the computation. So you see here that we iterate basically on the size counter, which is decreasing, dividing by two every time.
00:44:08.338 - 00:44:48.480, Speaker B: So we have to make sure that at the end, and we increment the index in an array. So we have to make sure that at the end this index is within the range of the array branch. So it's actually what the developers wanted to write here is that because you're going to dereference branch at height, you want to make sure that this is within the range of the array. And to make sure that this is the case, you write it in the loop. So what we have done is to prove that this is correct. This algorithm, which is a simplification, is correct. It has one test less than the previous algorithm, and there's a correctness proof in the code.
00:44:48.480 - 00:44:59.646, Speaker B: So I'm checking the time if I can, actually. So yes, I've got a few, you've.
00:44:59.678 - 00:45:01.814, Speaker A: Got about another 15 minutes.
00:45:01.932 - 00:45:37.530, Speaker B: Yeah. So I'm just going to quickly go through to show you some code to give you an idea what we write. If I can find my window. That's this one, right. So this is some example of Daphne code. And this one is, I can probably show you the structure of. So it's a standard against software project.
00:45:37.530 - 00:46:28.982, Speaker B: There's a few packages, algorithm helpers, path, path, have sequence of bits, computing a synthetize attributes and some trees, some things on trees, and the deposit smart contract. So let's say the definition of a tree in Daphne. So I've defined an inductive data type with a parametric type for the values stored in the leafs and the nodes. And you can define some functions as well, like what's the height of a tree, the nodes that are in the tree, and so on. And we can also define in Daphne what Merkel trees are. So the central concept in our proof, and you see here that I've used in Daphne a function. So a function, Daphne doesn't need to be executable.
00:46:28.982 - 00:47:37.878, Speaker B: So it provides a definition. And this function is tagged with the axiom attribute, which means I don't want to prove anything about it. It's a definition, and it defines what a merkel tree is when you built it from a list of values that you have at the leafs of the trees, a given height, a given function to compute a given hash function, and the default value that you use for the leaves that you write pad with. So this gives the definition of this buildmerkel function, and it ensures that the return tree has certain properties. So for instance, that the number of leaves in this tree is two to the power of h, that the height of this tree is h, that this tree is a merkel tree. So there's a definition of merkel just right after, and that the leaves of this tree, when you look at them from left to right, they correspond to the list that you have given at the beginning to build this merkel tree. And another, for instance function that we can define, which is predicate.
00:47:37.878 - 00:48:32.340, Speaker B: A predicate is a function that returns true or false, is whether a tree is a Merkel tree, and a merkel tree is a tree that has all the levels filled with all the leaves up to a given depth. It's a binary tree, and if it's a Merkel tree, the values that are associated with each node should be computed according to the sort of given node. You use the values of left and right siblings to get the value of a node. So that's what this is decorated with says, and that the leaves of the tree from left to right must match the given list. So that's the type of definitions we can have. And in Daphne we can write everything these logical definitions using quantifiers like for all, and exist as programs. So that's what I've done here.
00:48:32.340 - 00:49:52.466, Speaker B: The computation of the root path, I would say that I described at the beginning, which is computing a value using the left and right siblings. That's this algorithm. The fact that this algorithm is correct when you use it on a tree is expressed by the following. It says if the left and right siblings, so these are the left and the right siblings here, and the path p of I is the direction of the path at the depth from the root index I. So it relates the siblings value in the tree r that we give to this left and right arrays that we have in the state that we are maintaining. So this formal statement says that in my tree, this tree is a Merkel tree, and the left siblings of the given path, left corresponds to the left siblings that are in the tree, and the right siblings, let's say the right sequence. Well, the right array corresponds to the right siblings on the given path.
00:49:52.466 - 00:50:49.040, Speaker B: So that's how we express this. And we can conclude that in this case, that's the conclusion I was showing before. The algorithm that we have, which is compute the left right up algorithm using the path, the left and right siblings will return the value of the tree that we would have built if we had had to build this tree. So this is the kind of computations that we can have. The sort of transformation of the algorithms that use, the algorithms that use path encoded as binaries, one and zeros and counters, they are given in this file index based algorithm, some proofs as well. There should be a proof later on that the algorithm is correct. Finally.
00:50:49.040 - 00:51:19.174, Speaker B: But the deposit, smart contract itself. So this is given by this file. So there's a tag here, which is auto contracts, and it doesn't have anything to do with smart contracts, that's the definite attribute. But the smart contract is an object. So it's defined by a class deposit. It's got some constants, which is the default values that you assign to the leaves of the trees that have not been used so far. The hash function that you want to use.
00:51:19.174 - 00:52:09.602, Speaker B: So it's a non interpreted function. In this case, the tree height, the values of the left siblings of a given path to the current leaf that's available, the right siblings, and so on. And there's a constructor as well. So the daphne code is executable, you can compile it and run it. So there's a constructor to build the initial values of the tree of the left and right. There's a method in it, zero ashes, to fill in the values of the right siblings given the height of the tree. So all these methods have been proved to compute actually the exact same values that the right siblings would have in a given tree, the deposit method.
00:52:09.602 - 00:53:18.990, Speaker B: So just again, just to give you an idea of it, looks like there's the algorithm which is buried here within a loop, but most of the code is invariants, which are specifications for the verifications and also in Daphne calculations and let's say inductions, but verified calculations that are steps that the verifier can take to try and establish some properties. So in this one, you can see that we need to detail quite a lot what has to be done to build the proofs that this algorithm is correct. And that's how we write, let's say the programs. So what the computation is here and the proofs here. So this is a special construct in Daphne, for instance, that describes a proof statement that's going to be checked by the Daphne verifier. And there's a few lemmas that are needed as well to build these proofs. So it's not a program, it's a reasoning that you write as a program, and Daphne can check that your reasoning is correct.
00:53:18.990 - 00:54:04.986, Speaker B: And this one establishes that two different computations use the same thing. And the get deposit root algorithm is a bit simpler. So basically, what you can see here is that there's a while loop to compute the current root ash and this while loop to prove the invariant and correctness of this while loop, we use the actual functional version of the algorithms that we have proved correct previously. So we know that this one is correct, and we're going to show that this imperative version computes the same thing as the functional version of it. And we can conclude at the end that the algorithm is correct. So all these, again, it says compute root is correct. It's a call to a lemma.
00:54:04.986 - 00:54:57.054, Speaker B: So it's going to be checked by Daphne. It's not a statement that's taken for granted. So everything is checked by the Daphne verifier, and I run it to make sure it would verify. But we can verify all the files by running Daphne of them. So you run Daphne on the file, checking, saying verify, and it takes a few, I think it takes 1 minute to verify all the files in this project. So it's relatively efficient, I would say. Of course, writing the code takes more time than checking it, but if you modify the code as well, the code that you have, so you try to do some optimizations and so on, it's easy to recheck, you just rerun the verification, and then you can check whether your optimizations were correct or not.
00:54:57.054 - 00:55:57.444, Speaker B: So it's also an easy way to check new IDs and to reverify, because it's mechanically verified and checked by machine, you don't have to do it by hand. And you see the effects of one change in other parts of the code too, when you rerun the full verification. So that should be the last thing. So everything has been verified, all the different packages of project, and that proves, that gives a reproducible proof that the deposit smart contract algorithms are correct. So, just to finish up, so we went through the code. There's some related work that's been done by runtime verification. So they published two papers, and one of them is called an end to end formal verification of a deposit smart contracts.
00:55:57.444 - 00:56:33.712, Speaker B: But this work has some gaps in it. So we fill these gaps. So basically is that some proofs are not machine checkable and some steps are with pen and paper more, I would say. So these two results have been published at conferences. The main differences compared to our work here is that the paper by runtime verifications, and another one, I will ignore this one for now. That's not by runtime verification, it's remotely connected to Merkel trees. They haven't proved functional correctness, but rather tested it.
00:56:33.712 - 00:57:12.800, Speaker B: And the rest of the proof is a pen and paper proof with some holes. It's written in the paper. They haven't checked memory safety. They have tested it too. Termination is not easy when you test a program. One thing that they've done is to analyze the bytecode, and this is something that we haven't done, but also in this respect, the bytecode is tested against, not against the high level specification, but against the specification that's been extracted manually from the top level one. So there's still a gap between what's proved on the bytecode and the top level specification using mercury.
00:57:12.800 - 00:57:57.576, Speaker B: So in contrast, what we have provided is a full functional correctness property with memory safety and termination. And we have a mechanized proof that is reproducible, so you can use it from our git repositories. There's a docker container, you can run it yourself and check that the proof is correct. And because it's machine checkable, all the steps of the reasoning must have been encoded properly. You can't skip a step and say, yeah, I think it's okay, I'll deal with that later. So, a breakdown of the results is that we have provided the first machine checkable proof of the algorithms of deposit smart contract. We have proved the absence of runtime errors.
00:57:57.576 - 00:58:45.192, Speaker B: This is a reasonable effort for a person to verify it. And we have identified two optimizations. The first one is an optimization and sort of a safety check that this assert false that I mentioned a few times, can never be reached. So we have got a proof of it, and also that the initialization of one array, the left siblings, do not really matter. You don't need to initialize them. So there's ample documentation on the GitHub repo with the deposit smart contract in Daphne, and the paper summarizing our, let's say, experience using Daphne that's available in archive. And finally, these are some resources that you can consider if you want to go deeper.
00:58:45.192 - 00:59:05.670, Speaker B: So the first two are the ones I mentioned before, and the three other resources are the deposit smart contract written in solidity and end to end verification, and the reports of it by runtime verification. And that's it from me. So I'm happy to have some questions.
00:59:14.470 - 01:00:05.922, Speaker A: So I have one, Frank. So I was thinking, as you were proving that that assert false wasn't needed, and I was thinking, would I get rid of know? Frank has proven it beyond reasonable doubt that the solidity code would not execute it. However, we know that we're not executing solidity code. There is a compiler between us and the bytecode, and the bytecode is what actually gets executed. And I know the bytecode is probably going to be pretty simple as well for this contract. And so you could almost look at the bytecode yourself manually to convince yourself that the solidity turned into the correct bytecode. But I'll put it to you.
01:00:05.922 - 01:00:08.210, Speaker A: Would you remove the assert statement?
01:00:09.350 - 01:00:48.526, Speaker B: So I think that there's two things in your question. So the first thing is, in the solidity algorithm, you can remove it, and you should probably remove it. And you should remove this for loop, because it's conceptually a wrong one. It doesn't go to the end of a loop. So you should modify the code and use the code that we have proved correct, because it's formally probed and there's a proof that there's no array out of bounds. And so on. The second one, which is about the bytecode, there's no proof that the compiler, from the solidity code to the bytecode is certified, is correct.
01:00:48.526 - 01:01:18.182, Speaker B: And actually what you're mentioning, which is, yeah, it's easy to see that blah, blah, blah. There were quite a few problems discovered during, let's say, the early phases of a verification by runtime verification using the Viper compiler. That was buggy. So the bytecode was producing some code that was not, I would say, consistent with the high level solidity code. To me, actually, just to say, frank, correction.
01:01:18.246 - 01:01:20.780, Speaker A: The high level viper code, though.
01:01:23.710 - 01:01:24.038, Speaker B: It'S.
01:01:24.054 - 01:01:32.782, Speaker A: A big difference because we don't want people walking away from the talk going solidity compiler is not good. So that was the viper which was in.
01:01:32.916 - 01:02:26.094, Speaker B: We don't know whether the solidity compiler is not good. And actually that's another answer to your question. It's why I think it's the main problem not to adopt a new version of a smart contract, is that the bytecode that's been produced by the solidity compiler. We assume that the solidity compiler is not trusted, so it computes some bytecode, and what runtime verification have done is to check that the bytecode satisfies some properties. So again, they synthesized, let's say an equivalent of the correctness criteria that I mentioned. They synthesized manually, the correctness criteria on the bytecodes. They started with some manually crafted correctness criteria for the bytecode that are inspired by the correctness criteria from the high level source code.
01:02:26.094 - 01:03:28.580, Speaker B: So there's no proof that they are actually correct. And they analyzed the bytecode to check that it was consistent with this manually crafted, let's say, specification. And I would say the main hurdle in adopting a new smart contract is that you would have to recheck the bytecode. So I think the developers of smart contracts are quite happy that the bytecode has been somehow analyzed, so they have some confidence in it and they don't want to modify it. So even if it's not optimal, they'd rather continue to use the old one because they think it's. Again, I'll be honest with you, I don't think it's been verified, it's been analyzed, but there's no verification. I wouldn't bet that it's bulletproof, but they trust more the code that's been analyzed, the bytecode that's been analyzed, compared to regenerating some new code and running it.
01:03:29.990 - 01:03:41.880, Speaker A: Fair enough. Speaking from someone who's analyzed a fair bit of bytecode, I can say it's really hard to be 100% sure you know what's happening exactly.
01:03:42.570 - 01:03:51.100, Speaker B: You would have to build a proof by refinement, for instance. And that's not trivial, because the structure of a bytecode is sometimes different to the structure of a high level code.
01:03:51.550 - 01:04:27.880, Speaker C: Just like to add a comment on the would you remove the debug assert, even if it's proved to never hit it? Just because the code today never hits it doesn't mean that this is a case where it's probably not going to be changed too often. But in general, coding, coding in the future, might someone could introduce an accidental bug that would then hit your debug assert or whatever. An assert is generally not to protect your current code, which you may approve is correct, but to protect future changes that someone might accidentally make.
01:04:29.610 - 01:04:56.442, Speaker B: Yeah, sure, but the way it's written, right, I agree with you. But if you write an assert statement in the code, this is not the same assert statements I had in my Daphne code. In the solidity code. In the Daphne code, it's a construct that says this should always be true. In the solidity code, it's a runtime check. So you haven't proved that it can never happen. You can test that it doesn't happen on some input tests.
01:04:56.442 - 01:05:25.000, Speaker B: So I think it's much better to go and prove the absence of bugs. This statement will never break. And again, if you change it and you want to be resilient, as you were saying, it's sort of a contract that you write in the code, and if you modify it, you'd like to test that it's not broken. So. Yes, but we do the same in Daphne. We write this verification, this specification and the verification, and if we modify the code, we recheck everything. So if something goes wrong, we'll find it.
01:05:25.470 - 01:05:38.170, Speaker C: That's the other way. You could obviously put it into your build process to say, hey, every time you change this, rerun the proof to make sure you haven't accidentally introduced a bug that is going to reach.
01:05:38.240 - 01:05:49.330, Speaker B: Yeah, but if it's a test in the solidity code, right, tests are not complete. So even if you modify the code somewhere else and it breaks the code, you may not discover it using this test.
01:05:49.400 - 01:05:53.730, Speaker C: Whereas rerun the proof.
01:05:56.470 - 01:06:10.690, Speaker B: If I run the proof, I've got 100% guarantees that this cannot happen. But if I have a test, I've just tested it. So I said my test, they didn't discover any problem. So that doesn't guarantee the absence of problem, I suppose.
01:06:10.850 - 01:06:14.520, Speaker C: Can you integrate the proof in with a build?
01:06:15.450 - 01:06:52.482, Speaker B: Yeah, that's what we do. When I run it in front of you, that's what you do. You change the code and you rerun the proof. But once you've proved that the code is correct, all these assert statements that you do at runtime, for instance, or the checks of the preconditions and so on, once you've proved that this is correct, you can remove them because you have already proved that it's correct. So in the deployed version, you could remove all of these checks because you've proved that they can never be. I mean the preconditions of each function can never be falsified. So that's also a hint that if we use that technique, you can probably speed up the computation too.
01:06:52.482 - 01:06:56.866, Speaker B: You don't need to check at runtime, you check before runtime, you check offline.
01:06:57.058 - 01:07:11.306, Speaker C: I guess in this case we're not necessarily making what the changes and redeploying the contracts. And it doesn't really make sense to put it into a deployment pipeline, because we're not going to repeatedly deploy it.
01:07:11.488 - 01:07:31.790, Speaker B: No, you're going to run it. So when you run it, there are lots of checks of preconditions, like the requires in the solidity code. And in our code we have checked that they can never be violated, so you can remove them. You don't need to check them at runtime, they've already pre checked.
01:07:33.170 - 01:07:48.360, Speaker A: Which is why that, like the concept of a Daphne to solidity transpiler or something, could be interesting as well, because then you'd write the source in Daphne, create the solidity, which would then go to EVM, I guess.
01:07:50.250 - 01:08:20.260, Speaker B: Yes. So there's still a gap that you hinted to Peter, we would need a certified compiler from Daphne to solidity, or from Daphne to the EVM. And building a certified is really hard. There's only two in the world, and it takes not only twelve person weeks, but probably twelve person years. That's something we can. No, but Frankie can do it as a weekend project. Yeah.
01:08:22.310 - 01:08:23.554, Speaker A: See you in 20.
01:08:23.752 - 01:08:46.620, Speaker B: That would be ideal. And I think what you can do too is to start. Right. Because you may have still more confidence. If you build a translator which is without optimizations, there's less chances that you mess up with the translation. Right. And the translation from Daphne to solidity, it's a structural translation, it seems to be.
01:08:46.620 - 01:09:23.030, Speaker B: So I shouldn't say that because I'm a formal method person, but it seems to be okay to implement it. And there will be some glitches, but it should be almost correct. It's not compiling into bytecode, where you have really different structures of the two programs and optimizations and so on. Compiling to bytecode, for instance, with optimizations, it's really hard to prove that you compute the same thing as the high level program, but for a translation which is source to source at the same level of languages like Daphne and solidity, that's probably doable.
01:09:23.850 - 01:09:56.240, Speaker A: The solidity to EVM compiler, when I've looked at it, and I don't know, maybe I wasn't having every single optimization on, but there aren't that many optimizations so the actual, you can look at some code, a line of code, and you can look at a set of statements. And generally it's not like some C plus plus compiler where you've got a function and it goes and mangles the whole thing together. It's just not like that at all.
01:09:57.250 - 01:11:07.110, Speaker B: So it's good in some respect, because if it's a sort of translation that maintains the structure, so it's a sort of a compositional translation. You translate a loop into something and you build blocks by saying, if I've got a while loop with a body, I'm going to translate the body into bytecode and then build the translation of a loop. So it has some good properties that you can use. For instance, you don't need to build a certified compiler, but you can say, I'll take a program as a source code in solidity, take the bytecode and try to prove by refinement, which is a formal verification technique, that the two programs compute the same. So because the structures are the same, you can use a technique which is called refinement. Because if you use optimizations, and that's what happens with compilers like the Arm compiler and so on, or Kylie four C and so on, they generate some code that's with the levels of optimizations, like two or three, the code that you get is completely unrecognizable compared to the source code that you started with. So they have very clever mechanisms to unfold loops, to do some linear optimizations and so on.
01:11:07.110 - 01:11:27.760, Speaker B: You can't recognize the code that you get at the end of the optimization. So there are really useful optimizations. Of course it speeds up. Sometimes it's orders of magnitude faster. But the drawback is that you don't have a proof that it computes exactly the same result as the high level program.
01:11:28.530 - 01:12:28.020, Speaker A: Yeah, could you show the slide which shows the forthcoming talks, please? Thank you. Okay, so in the next forthcoming six weeks or so, we've actually got quite a few things happening. So next week, so it's not two weeks away, it's just next week, there's going to be a networking and recruitment event. So it'll be the same time of day as this talk was. And so I think at the moment we've got five companies that are going to give a quick five minute presentation on who they are and what they're doing and why you should want to work for them. And then we'll have breakout rooms, and we might even have an extra breakout room for people who just want to chat and people will be able to hopefully come and go from breakout rooms. So that should be interesting just to meet some people in the ecosystem.
01:12:28.020 - 01:13:14.020, Speaker A: In two weeks time, I'm going to talk about roll ups and side chains. So talk about l two technologies for Ethereum and my take on how all of that works. In four weeks time, Simeon is going to talk about CDBCs and the regulation of them. So he is a PhD student, so it's his research into that. Felicity Dean is an associate professor. Sorry, I'm mangling that. So she is someone from Qut and she's going to talk about the tax implications of crypto and NFTs in Australia.
01:13:14.020 - 01:13:44.060, Speaker A: And then we've got the supply chain and blockchain conference is coming up, actually the day before Felicity's talk. And the registration is free and we're due to announce the agenda in about a week and a half's time. So feel free to click on that link or go to SCOBC. Net and register for the conference. And that's all. Are there any final questions for Frank?
01:13:46.980 - 01:14:18.836, Speaker B: I have one. So, regarding the verification of the smart contract, was there any consideration paid to gas, gas costs, or is that totally your problem? That's a good question. Yeah. No, but that's something you can. Right? So there's two things. No, because it's the source code level and the source code level. There's no notion of gas.
01:14:18.836 - 01:15:14.410, Speaker B: It's at the bytecode level, but it can be done. So you can reason about. So for instance, if you add some, let's say, notion of gas for the source code, like an instruction, like a while test, for instance, a test cost two or three and so on, you could integrate it into the verification in Daphne and prove that the cost of executing the algorithm is less than something. So the question of checking whether the cost is less than a given bound could be something that you could prove in Daphne. And there's a paper, actually, I think I lined it up for the reading group that we have. There's a paper on computing gas costs on the source code and make sure that it's conservative measure of what's going to happen on the bytecode. Thank you.
01:15:14.800 - 01:16:01.140, Speaker A: And if you're interested in being involved in that formal methods reading group, you should reach out to Raghavendra, Ramesh or Frank Cassette, and I'm sure either of them can help you become part of that group. There's a discussion chat channel on the Ethereum engineering group, meetups slack workspace as well. So if you go to meetup from there, you can get a link to join the Slack workspace, and then you could join the channel so you can join the conversation about the formal methods as well. All right, thank you, everyone. Have a great rest of your day and talk to everyone later. And thank you again, Frank.
01:16:01.640 - 01:16:02.920, Speaker B: Thank you. Bye.
