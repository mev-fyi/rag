00:00:02.970 - 00:00:15.920, Speaker A: Hello and welcome everyone. My name is Peter Robinson and today we've got Dr. Frank Cassay, who is going to talk about fair sequencing. So, Frank, before you dive into your slides, why don't you introduce yourself?
00:00:16.530 - 00:00:46.374, Speaker B: Yeah. Thank you, Peter, for inviting me to talk today. So I'm Frank. I'm the head of blockchain research at Mentor, the mentor network, which is a roll up, a layer two, formerly Bitdao. And my research interests are in roll up technologies and more broadly Ethereum and blockchain. My focus is probably more on formal verification and so on. But the talk I'm going to give today is not in this topic.
00:00:46.374 - 00:00:56.058, Speaker B: It's about roll ups and providing some fair sequencing in roll ups. So if it's okay, I'm going to share my screen and start the presentation.
00:00:56.234 - 00:00:57.360, Speaker A: Sounds good.
00:01:05.980 - 00:01:37.300, Speaker B: So feel free to ask questions during the talk. If you want to interrupt me, I'll be very happy to ask the questions. Right. So this is a talk on fair sequencing, and there were two other people involved in the work I'm going to present today. So on the left hand side, I'm trying to get my aegon. Gleason is at mantle as well, in the research team, and Andreas Penskoffer. So we did this work altogether.
00:01:37.300 - 00:02:38.356, Speaker B: So what's the problem? So first I'm going to tell you what's the lifecycle of a transaction in a layer two, and then what are the issues that can arise. And the talk about fair sequencing is to try and mitigate these issues. So usually when users submit a transaction, they submit the transaction to a mem pool. So in a typical roll up or layer two, it's a centralized mem pool. And then the main component of the roll up is the sequencer. It's got a huge amount of power, and the sequencer is going to extract from the mempool some transactions, bundle them in a batch. So here you take some transactions from the mem pool, a number of transactions, let's say k of them, and then you execute them in a batch and you compute the next state on the roll up.
00:02:38.356 - 00:03:28.836, Speaker B: The roll up, next state, right. So I'm not talking about the layer one here, but just the behavior of the roll up. So what can happen here is that the sequencer is a centralized sequencer. So of course it's got full control on the transactions. It's going to extract from the mem pool and on the order of transactions when they are executed in the module that executes the sequence of transactions. So one thing that can happen, of course, the sequencer can censor some transactions, so never take them out of the mem pool and never execute them. Another thing that can happen, which is related to the fact that the sequencer can order transactions as they wish, is that they can create what's called never tax or toxic attacks or manipulation and so on.
00:03:28.836 - 00:04:23.364, Speaker B: So I'll describe what never attacks are in the next few slides. But there are two things that can happen, which is censorship and attacks that can be detrimental to users, farmers and so on. So what can we do to fix this problem? On a typical layer two, to be able to mitigate the censorship problem and also the mevatax problem. So in the next few slides, I'm going to give you an example of what a MeV attack is and how it's related to ordering transactions. So MEV stands for maximal extractable value. It used to be minor extractable value, but with the move to proof of stake in Ethereum, it became maximum extractable value. And it's a scheme in which builders or proposers of blocks try to, let's say, extract the highest benefit from building and proposing blocks.
00:04:23.364 - 00:05:14.564, Speaker B: So I'll describe here a simple example of what can happen. So this is the timeline from top to bottom of what's going to happen. So assume you're on your own and you're Alice, and you want to swap some ether for mental tokens. So the exchange rate here is one ether, one mental token. This is actually completely wrong, right? One mental token is not worth a lot at the moment, but Alice would probably send this ten ether to sort of a decentralized exchange with a pair, a pool offering the exchange of ether for mental and mental for ether. And if the current exchange rate is one to one, she sends ten ETH and she gets back ten mantle tokens. But then the price of mantle tokens in the pool has changed because it's not balanced anymore.
00:05:14.564 - 00:06:01.530, Speaker B: So that's the mechanics of the pairs in the decentralized exchange pools. So the fact that you can change the exchange rate by buying some tokens can be exploited by malicious attackers. So here assume that we have Bob. And Bob is going to sort of make a transaction before Alice because he knows that Alice will buy some mental token, some mental tokens. He's going to front run Alice by first buying ten mental token at the exchange rate of one to one. And then of course the exchange rate has changed. And when Alice does her transaction this time she won't get ten mantle token, but only 9.1.
00:06:01.530 - 00:06:57.930, Speaker B: And then what Bob is going to do is to just use ten e, get ten mental token, let Alice do her transaction and convert his mental token back to ETH. And doing this front running and back running, which is called the sandwich attack, he will get, let's say in this example, two E. And of course it's at the detriment of Alice who didn't get ten E. So that's called sandwich attack, and it can be created by manipulating and inserted transactions before certain other transactions and after. So if you're able to manage a pool, a mem pool, like the sequencer in the rollup, you can actually do that. You can explore transactions in the pool and say, all right, Alice is going to swap tenny for fundamental token. I'm going to front run and back run and make some money out of it.
00:06:57.930 - 00:08:04.378, Speaker B: So it's usually considered as harmful, and of course it's detrimental to users. So we should try to find a way to mitigate this Mev attacks in the network. So in the next few slides, I'm going to present sort of a modular solution that tries to isolate the issues of censorship and ordering and manipulation of ordering of transactions and describe a high level architecture that could help to solve these problems. So, any questions so far? Okay, so I'll continue. So one solution to try and mitigate these issues has been proposed. It's decentralizing the sequencer. So by decentralizing compared to the previous picture I had, instead of having a unique component here that's going to pick up transactions from a main pool, you're going to run a consensus protocol to try and select the transactions and build a batch.
00:08:04.378 - 00:09:30.694, Speaker B: So this doesn't really solve the problem of censorship because the leader of, let's say, BFT protocol controls the actual choice of the transactions and what's usually called, let's say, censorship resistance, even at the ethereum level, is based not on the use of a consensus protocol, but the most important part is to have diversity of the participants if you want to improve censorship resistance. So using decentralization in itself doesn't solve the problem. Another thing that can happen in most of the architectures I've seen so far, including decentralized and shared sequencer, is that there's, let's say, some decision on which transactions are selected, and then a proposal of an order of the transactions, and it's then sent off to an executor that's going to execute the batch, but there's no verification that the module in charge of executing the transactions has executed them in the order that was sent off to it. And it's usually an assumption in the designs on shared and decentralized sequencing that yeah, the executing module will not alter the result of the batch. So that's a strong assumption as well. So I'm going to try and show you how we can do that, fix these issues with our design. So the goal of fair sequencing, so it's not entirely new, it was introduced a couple of years ago in 2021 by Chainlink.
00:09:30.694 - 00:10:28.190, Speaker B: They even offered what they call a fair sequencing service. But the fair sequencing service is based on using a BFT consensus algorithm as well. And they improved the technology in the last few years by having what's called the protected order flow system, which is a way of collecting transactions on different nodes and on different nodes. All the transactions are ordered, totally ordered. And you can build from this totally ordered on each local node a total order of the union of the transactions of the node. So that's the algorithm they are using. So what we are after is to build a system that is resilient to censorship and never tax that is secure so you can prove that the transactions have been executed in the order that was decided by the agent in charge of the decision of the ordering.
00:10:28.190 - 00:11:19.422, Speaker B: And also we would like to have what's called attributability. If something goes wrong in the system so the transactions are not executed properly, there's censorship happening and so on. We would like to be able to blame the agent in charge of this issue and slash them if they have staked some assets and so on. And on top of that, we'd like to make sure that the solution we propose is rather efficient and simple enough to be implemented. So I would emphasize that running a BFT or consensus algorithm to compute the order of transactions, it's not a light solution. It can take some times to come to an agreement. So fair sequencing without consensus, that's what we are after.
00:11:19.422 - 00:12:23.646, Speaker B: So first thing here is the main architecture of our design. So censorship resistance, we're going to try and let's say tackle it at the level of the main pool manager component. So we're going to split our design architecture into three different steps. There's a main pool manager in charge of making sure that transactions can arrive to the main pool, that once a transaction is in the mempool, it's going to be executed in a bounded number of blocks. The fair sequencer is in charge of providing a fair ordering, and I'll define what fairness is in a couple of minutes. And the executor as well will be subject to the verification that it didn't alter the sequence of transactions that was given to it by the fair sequencer, so it has to execute the transactions in the order they were given to it without any modification. So how does it work? So the mempool manager is going to work as follows.
00:12:23.646 - 00:13:13.506, Speaker B: So we've got a mempool, and every time a transaction like Alice is going to submit a transaction, we fill in some sort of buckets, batches and so on, that you can identify them with blocks. Basically. That can be the same thing, to simplify. So some of the buckets have been fully loaded and they've been processed as well, and some of the buckets are ready to be processed, and some of the other buckets are not fully yet. So when Alice submits a transaction to the main pool, her transaction is going to go in the first bucket. That's nonful. And at the same time Alice is going to receive a receipt that a transaction will be in batch K or block K in the processing of the layer two.
00:13:13.506 - 00:14:17.506, Speaker B: So she gets a promise that a transaction will be in block K, but it doesn't know the order of a transaction in block K. So if you're a malicious attacker, the only thing you can do is to submit your transaction. We'll tell you you're in block K, but we won't tell you that you're at position or index three or four or whatever. So now, how does it work for Alice, and how does it work in terms of censorship resistance? So what we offer is that provided the transaction reaches the mem pool, Alice will be able to verify that her transaction has been executed. So one simple thing, for instance, is if Alice can see batch or block number K plus one, and she hasn't seen her transaction, or she hasn't received any acknowledgement that her transaction has been processed, she knows that the promise was not kept, and she can blame the pool manager. And if there is a mechanism with the proof of stake and the slashing mechanism, we can slash the pool manager. So that's how we address the problem of censorship resistance.
00:14:17.506 - 00:15:16.730, Speaker B: And the batch. When we extract a batch, it's a list of transactions, right? A sequence of transactions. We can also provide a proof, well, not a proof, but a recording of the sequence of transactions as a Merkel hash root of the list of transactions, and publish it as well to a data availability layer. So notice that in this setting, right, I can still have a centralized mempool. And what we define as censorship resistance is if the transaction reaches the mempool, then you get a receipt and we guarantee or will be slashing the mempool manager later on if your transaction doesn't make it. But what we don't guarantee is that your transaction can reach the mempool. So there are other mechanisms, I may talk quickly about them later on, broadcast protocols and so on, to try and ensure and guarantee that it's easier for a user to make sure that the transaction reaches the main pool.
00:15:16.730 - 00:15:23.770, Speaker B: Any questions on that first module, the one handling the mempool?
00:15:29.910 - 00:15:39.418, Speaker A: I think that makes sense. So you know that you're in a particular batch, but you don't know where you are within the batch.
00:15:39.534 - 00:16:12.762, Speaker B: Yes, but it's block bonded, right. Because you're going to, assuming that blocks are produced on a regular basis and are published. If you see again block K plus one, and you haven't seen your transaction in the previous blocks, you know that something went wrong. So you can determine it's not okay. The other thing you can do is to examine for each block. I'll talk about it later. Each block that's going to be published, each block, sorry, that's going to be processed at the layer two level will be published, or a hash of it will be published.
00:16:12.762 - 00:16:31.750, Speaker B: You can check each block, whether your transaction in it, and ask for a Merkel proof that your transaction is in it. So there's two ways that you can check that the promise has been kept. So it's a simple mechanism, right, a ticket, and it's not heartbreaking.
00:16:33.210 - 00:16:33.960, Speaker A: Yeah.
00:16:35.770 - 00:17:46.374, Speaker B: But I think the main takeaway from this is to have a separation of concern between censorship, resistance and the other things that are happening later on, separating the different modules and aspects of the processing of transactions. Right. So now how can we provide some sort of fair sequencing? So that's where actually I should probably try to convince you that a definition of fairness is not easy to find, but also that if you run a consensus mechanism to come up with some agreement on the ordering of transaction, it may not be fair, because the participants of a consensus algorithm, they decide what's fair for them, right. So they could decide altogether or the majority of them to run sandwich attacks and so on. So in our setting, we want to have something that's simple to use and completely fair. So when you think about fairness, you may think about a coin or a die or whatever that you toss. So tossing a coin, fairness, it's the typical example of a randomness.
00:17:46.374 - 00:18:32.250, Speaker B: So when you toss a coin, there's 50% chance of heads or tails. So in our setting, what we'd like to do is say, all right, we've got a bunch of transactions that arrived in the same bucket and we ordered them, let's say, in a centralized name pool with the first, well, with timestamps in the order they were received. And what we would like to do is to shuffle them, but in a way that's not predictable. That's also one of the main features of randomness. So, to do so, we would like to use a random function. The thing is that of course, this random function could be flowed or manipulated as well. But turns out that you can use random functions and verify that a given random function has been used to compute some randomized numbers or sequence of numbers.
00:18:32.250 - 00:19:10.038, Speaker B: So that's what we're going to do here. We're going to extract a batch and then query an oracle. So supra, or chainlink, they offer these kind of services to get a random permutation. Let's say if we have a batch of 100 transactions, we'll get a random permutation of 100 numbers. And the nice thing is that this random permutation comes along with a zero knowledge proof or a proof of randomness. The proof that you have computed the permutation using a given random function. So it's called the verifiable random function.
00:19:10.038 - 00:20:04.010, Speaker B: So what we do is we extract a batch. We've got a sequence, we've got a random function that describes the permutation of the transaction. And then we're going to compute the permutation of the transactions using the randomization that was provided from the verifiable random functions from a batch b, and a sequence of transactions in B. We arrived to a randomized sequence in B prime. So that's the computation that we perform. And again, what's happening here is that we get a batch, we can archive the order of the transaction, sorry, in the batch with the Merkel root hash, or whatever you want, on the data availability layer, we can also publish the randomization and the proof that the randomized sequence has been computed using a given random function. So, zero knowledge proof.
00:20:04.010 - 00:21:13.530, Speaker B: And then we can also compute a proof that the permutation of the sequence of transactions, so B to b prime has been computed properly according to the random permutation. And we can also commit to batch to the new batch b prime at the end, so everything can be committed to a data availability layer. So that's the main aspect of it. So now why is it, let's say, efficient? Is that computing a permutation? There's an oracle involved, but it should be as effective at least as running a consensus algorithm. But the nice thing with some verifiable random functions like the supra Oracle one, is that you can request a private computation of the randomization so nobody will see it except the agent who asked for it. So you could pre compute a randomization. So have some randomizations in your store already pre computed and kept private, and reveal them only when you extract the full batch.
00:21:13.530 - 00:21:54.350, Speaker B: So here I'm actually referring to the issue that if there was a possibility that someone knew the future permutation, they could of course use it to try and manipulate the order of transactions in the batch B. So we don't want to reveal the permutation before the batch b is actually created. So that's why it may be important to be able to pre compute, but keep secret these permutations and use them very quickly. If they are pre computed, it's very easy to compute the permutation. Any questions on this?
00:21:58.660 - 00:22:00.048, Speaker C: Sorry, Peter, you.
00:22:00.214 - 00:22:02.850, Speaker A: No, no, you go, James. You had your question first.
00:22:03.220 - 00:22:20.100, Speaker C: I was just thinking, are there instances where this proof could be invalid and could be failed to be verified? I'm not sure if there's any form of verification. Which proof, either the zk proof here or the proof of permutation that was computed correctly.
00:22:20.760 - 00:22:49.004, Speaker B: Right. Okay, I'll try to answer the question and you tell me if I answer your question. Right. So assume that this proof beta, well, a proof is sent, but it's not valid, right? That could be the case. So if it's not valid, you may have to ask for a new random function and so on at this level. And you could also say, well, we're going to stop until we get a good random function. So of course the service could be down and you never get one.
00:22:49.004 - 00:23:06.310, Speaker B: So there are liveness issues, but you can check whether the permutation has been computed properly. So you can check in real time, or you can check, let's say, later on you just publish the proofs, and later on you could scan and detect whether everything was computed properly, similar to a sort of a fraud proof system.
00:23:07.080 - 00:23:08.390, Speaker C: That's how it's going.
00:23:08.840 - 00:23:25.944, Speaker B: Okay, you can do both. You can try to detect it. It's really similar to a ZK roll up where you publish the proof and you check the proof, and if something goes wrong, you stop or you publish the proofs, and then we can go back at some point and scan all of the proofs that have been published.
00:23:26.072 - 00:23:27.548, Speaker C: Perfect. Thank you.
00:23:27.714 - 00:23:45.024, Speaker B: Okay, so that answers your question. Yeah. Okay, so all of the things. That's a good question. Thanks for the question, James. All of the things that are published to the data availability layer? Yes, it's asynchronous. So they are published and at some point they will be available.
00:23:45.024 - 00:23:53.040, Speaker B: So there may be still a lag, which is a standard in rollups when they communicate to a DA layer.
00:23:54.440 - 00:24:30.508, Speaker A: Yeah. So my understanding of how VRFs worked was that essentially you gave them a value and it's got a private key. You've got their public key, and then they essentially, like a PRNg, essentially they come up an output and then they sign it. So the proof is like a signature, I think maybe more than a. So what's the value that you pass to the super, is it like the batch number or something like that?
00:24:30.514 - 00:24:55.972, Speaker B: Yeah, exactly. You could compute the hash of the batch or the seed. Could be anything. You could compute on the batch or whatever, if you have a batch. But if you don't have the batch, you could pre compute random values from the previous values you had and so on. So have a hash of a hash, like a shamir interactive proof. You collect all the hashes you had and you compute the next one and you use it as a seed.
00:24:55.972 - 00:24:57.748, Speaker B: So you need one at the beginning.
00:24:57.924 - 00:25:01.112, Speaker A: Yeah, that could be a way of doing it. All right.
00:25:01.246 - 00:25:46.890, Speaker B: There's also something that's interesting. I thought I would talk about it later in the talk, but is that the verifiable random function thingy can be decentralized. Supra, they offer a DVRF, or actually the computation of a randomized value is going to use a multiparty computation scheme where every participant will contribute. Every participant will contribute to the randomization. There could still be some, let's say decentralization in this computation as well. And you could also reward participants in a way that they are rewarded for the work they're doing.
00:25:48.540 - 00:25:50.010, Speaker A: Yeah, that makes sense.
00:25:53.020 - 00:26:07.356, Speaker D: The main issue here is the ordering of when these things are produced. Why not make B? An input to the sigma is coming after the b. Yeah, so that's what.
00:26:07.378 - 00:26:18.800, Speaker B: We want to do. Yeah, exactly. So we could compute a hash of the bash or whatever, and feed the randomized, the oracle with the value that depends on B. Yeah.
00:26:18.870 - 00:26:19.844, Speaker D: Okay, that's good.
00:26:19.962 - 00:26:27.830, Speaker B: But if we want to do it in an asynchronously, so we want to pre compute the values, we may need another way of doing it.
00:26:31.900 - 00:26:45.550, Speaker A: If you hashed the actual batch, the actual transactions, then it's going to come down to the transaction ordering, whereas if it's just, say, the batch number, that's unchangeable. So it's nice.
00:26:46.080 - 00:27:46.670, Speaker B: Yeah, this is a few things that we haven't actually set. It's very modular, so you can choose right. It's the same here with the verifiable random function. You can plug in anything you want and you can extend it as well to a distributed verifiable random function if you want as well. I think the main takeaway, I would say, in this business is to try and see whether we can track every step of the way whether something is going wrong and provide some guarantees on some sort of fairness. So what we guarantee basically right in this business as well, related to MeV attacks, is that the probability. I'll talk about it later in the conclusion, the probability of a sandwich attack is pretty low because you can't really manipulate the order of transactions that's given by the random function.
00:27:50.870 - 00:28:25.450, Speaker C: Frank, just to that point about not being able to manipulate the order of transactions, does that also remove this whole other sort of paradigm? Say I'm in a very congested network and say there's a primary sale going on, but I have a particular transaction that's not related to that primary sale. But I feel like it's a very important transaction that I need to come through quickly. It feels like there's no priority ordering based on the receipt for the batches. So how does that extra dimension of.
00:28:25.600 - 00:28:54.050, Speaker B: Yeah, that's a good question. So there's two things you can do. So first thing is, let's say what you're mentioning. For instance, you can have a mechanism similar to, let's say, time boost in arbitram to try and boost your transaction and prioritize it. Entering a batch before other transactions. That's one mechanism for urgency. You could pay extra fees, for instance, to get into the first available batch, but there's no mechanism to jump.
00:28:54.050 - 00:29:43.334, Speaker B: And maybe we could probably do that. But let's say take out a transaction from a previous batch and put it in a later batch because we have sent a receipt already. So that would change the commitment. I was going to say another thing, and another thing you can do is to somehow have an overlap with, let's say, in the process I've described so far. You take a batch and it's fully randomized, but sometimes you would like a few transactions in a batch to be in the same order that they were at the beginning. So you can still do that. And the way we treat it is basically to say, right, if you submit a pair of transactions that you would like to be in a given order, let's say two transactions, and you say, I want this one first and the second 1 second.
00:29:43.334 - 00:30:46.380, Speaker B: What we're going to do is to say, right, we need two slots in the randomization for these two transactions. So we've get transaction one assigned to index 32, transaction two assigned to index 46, and if they are in the right order, then that's fine. If they are not, we can commute them, or we can compute, let's say two slots and assign the transactions in the order you want it to keep at the beginning, because otherwise we run some experiments actually on some real transactions. Otherwise you get a lot, of course, reverted transactions, because they are not meant to be shuffled. So you can add a mechanism to make sure that you preserve some ordering. So let's say from a user point of view, I could say I'm a user, I submit three transactions and I'd like them to be in this order, but you can interleave with other transactions in between. So the sort of a mechanism is, I provide a local order, a partial order, and I make sure that the permutation is an extension of this partial order.
00:30:46.380 - 00:31:32.720, Speaker B: So this can be done. You can even make them, I mean two transactions next one to another, if you treat them as a single transaction. So there's lots of modularity in the design, right? So what's the last part of it? So we've extracted a batch, and it's been fairly well, there's a fair ordering. So the transactions in the batch b have been ordered into B prime. And this b prime, we publish it to the data availability layer. We publish a digest. So for instance, Merkel root hash, and then we send it to the executors going to execute the transaction.
00:31:32.720 - 00:32:24.720, Speaker B: And this transaction will result in a block. And in the block you usually have a sequence of transactions, the list of transactions and the order in which they've been processed. So we'll publish this digest of a block and a digest of the list of transactions that are in the block. So you can compare these digest of transactions that have been processed and the block published compared to the initial B prime, and check that they are the same. Right? So that's the mechanism to make sure that you. And again, because we check everything one at a time, if everything went well, until, let's say, the computation of B prime, so nobody cheated previously. And then at the end, we detect that the list of transactions in the block produced by the execution of the list of transactions in B prime is not the same as B prime.
00:32:24.720 - 00:33:13.510, Speaker B: We know that the module in charge of executing the batch is malicious and should be blamed and slashed if they have staked something. But we separate the processing into three different phases and three different actors. So in terms of, I would say decentralization, the first module that was. I've got another, the mempool manager. So you can decentralize it as well, right? You could have some like Ethereum for instance, a peer to peer network with local pools and so on to build a global view of the pool and stuff like that. So that increases. Well, that reduces the likelihood of censorship, but that doesn't eliminate it.
00:33:13.510 - 00:34:00.290, Speaker B: The first sequencer, there is no real need to decentralize it because the sequencing is given by the verifiable random function that itself can be decentralized. If you want, you can use a decentralized version of the VRF. And the executor doesn't need to be decentralized even because there's no choice. It just has to execute the transactions in the order it was given. So that's the overall view of the design. So we ran again some experiments on some transactions from our own roll up and the rejection. Let's say the reverted transaction rate was close to 20%.
00:34:00.290 - 00:34:50.132, Speaker B: The overall result, it's hard to interpret because you can't really tell whether it's good or bad. Because if the reverted transactions are the transactions that would prevent never tax, that's good. If the reverted transactions are the transactions that should have gone through, that's bad. We haven't been far enough to try and extract the different types of transactions. So we can't really tell the source of reverted transactions. But that's something that would be useful to measure. So in practice, when we simulate, do we in practice reduce mevatax or not? But if anybody knows a good way of measuring and detecting mevatax, we could try to implement it in the simulator and see how it goes.
00:34:50.132 - 00:35:32.194, Speaker B: So that's the overall design. So the takeaway is there's three different modules and they're all in charge of, let's say, different aspects of the fair sequencing. One is in charge of censorship resistance, the other one of mitigation of nev attacks. And the last one has no choice but to execute what was given to it. So what can go wrong? Right? So of course what can go wrong is we've got the mem pool. So it could be spammed. Because if you spam the mem pool and you try to fill in the batches of the buckets, you increase it.
00:35:32.194 - 00:35:36.260, Speaker B: Yeah, I see that. There's one question, so I can stop there.
00:35:37.450 - 00:35:48.390, Speaker C: Sorry, me again, Frank, just on the last slide similar to how we were talking about a fraud proof. Does the executor just optimistically trust the fair sequencer?
00:35:49.630 - 00:36:03.974, Speaker B: So the executor doesn't have to trust anything. The executor gets the batch as an input, and its job is to execute and build the block with the transactions executing in the order given by this batch.
00:36:04.102 - 00:36:06.830, Speaker C: What if the proof of B prime is invalid?
00:36:11.410 - 00:37:09.218, Speaker B: We publish B plus we get B prime here. Right? So the order that was given in B prime is committed to the data availability layer. There's a hash of some sort that's committed here. And the executor, its job is to take the current state of the layer two, to execute the sequence of transactions as given by the sequence in B prime and publish it. And once it's done, we check that it has done it properly using the transaction list in the block description. Right, okay, does that make sense? Every phase, you can think about them as three separate phases. And when we start the executor, we assume that, well, we assume, or we could check later on again that b prime is given, and the job of the executor is just to execute in the order that was given to it, and we can verify that it was the case.
00:37:09.218 - 00:37:31.254, Speaker B: The job of the fair sequencer is to compute the permutation of the batch B and prove that it was fairly ordered. The job of the mempool manager is to mitigate censorship resistance. So build batches, but not deal with fair ordering or execution of the transaction.
00:37:31.382 - 00:37:38.426, Speaker C: Right, but you were saying that there's a potential for B prime to be invalidated at a later time. Right.
00:37:38.608 - 00:38:13.734, Speaker B: Okay. So it depends how you want to process. What you can do is to publish or to check in real time every step. And once you discover that something has not been computed properly, you stop or you revert and you have a backup mechanism. But the way it usually works for most roll ups is that you publish the data and you can, with the lag, check that something went wrong or not. So for instance, with optimistic roll ups, you have a certain time window to check that what you have published is consistent. You could do the same.
00:38:13.734 - 00:38:22.010, Speaker B: And if it's not consistent, you have to revert to a previous state or someone can challenge the misbehavior.
00:38:22.430 - 00:39:03.462, Speaker C: Cool. Yeah, that makes sense to me. And I guess my last question is just for my mental model. In a typical network architecture, for example, what we run is a set of RPC nodes and then a single validator that acts as a single sequencer. Do you see each of these pieces of the architecture being all in a single node, or how do you see that sort of responsibilities broken up across, say a sequencer or an RPC nodes that would say that the validator or the single sequence that doesn't have any RPC endpoints exposed, and the RPC ones only have the RPC endpoints exposed. Like how do you see all of this fitting into something like that?
00:39:03.596 - 00:39:38.290, Speaker B: So they should fit on the side of the mempool manager. So that's how you can refine the mempool manager. So that's where you try to deal with, again, having mechanisms like, I can't remember what it's called, but I've got the name later on, some broadcast protocols where you try to increase the chances that the transactions arrive, you try to mitigate the fact that some users are close to some nodes and others are not. And things like that, that happens on this side, I would say the main pool management.
00:39:39.510 - 00:39:40.754, Speaker C: Yeah, that makes sense to me.
00:39:40.792 - 00:39:41.490, Speaker B: Thank you.
00:39:41.640 - 00:39:55.094, Speaker D: Okay, just another question. The hash of the B prime looks to be redundant to me. You could just remove that and all the information is still available. Anybody can recompute that.
00:39:55.292 - 00:40:46.200, Speaker B: Yes, I agree. I agree on. So you could omit and try to recompute. Yeah, I agree with that. So again, the takeaway is there are three different modules. They are in charge of three different things, and if something goes wrong, we can clearly identify the module that was malicious. So if there's no more question on this slide, the previous slide, I'll try to finish with a few things that can happen and see how it works.
00:40:46.200 - 00:41:36.036, Speaker B: So we've got this mem pool, even if it's a centralized mempool and so on. So again, malicious users can try to spam the mem pool, because if you increase the number of transactions, you can send similar transactions to a mem pool to increase your chances of net attack, let's say, to mitigate these kind of things. So making sure that, for instance, we are censorship resistant and we mitigate mev attacks. Again, there's lots of broadcast protocols that can be put in place, and they are simpler than a consensus mechanism. They focus really on broadcasting something from one place to another. And that's it. That's the only thing you have to do in the mempool to protect from spamming.
00:41:36.036 - 00:42:29.752, Speaker B: There are a few solutions already, I think, because it's not something that's specific to our design, it's something that, of course, every chain and roll up has to deal with. You can have adjustable fees, priority fees, or a mechanism like time boost. So for instance, pay some fees or making sure that the price of submitting, let's say 100 of similar transactions is the same as the price of submitting one transaction doing the same. So you can try to identify these kind of things and make sure that there are disincentives for user to spam the pool. But this spamming effect, again, it can be dealt with at the entry point level, domain, pool manager level. So I think overall you get a mechanism where you've got different modules. So it's very modular.
00:42:29.752 - 00:43:33.984, Speaker B: You can pick and choose and refine your different modules, making them centralized or decentralized if you wish, and attribute any fault to the agent responsible for misbehavior. There was something else I wanted to say, but I may remember later on. So the conclusion to this in terms of let's say decentralized sequencer or sequencer that's not centralized, there's the decentralized sequencer approach. So it doesn't really provide censorship resistance. The agents in the BFT or the consensus protocol, they define their own agreement. It doesn't protect from MeV because they could also say you're going to front run and back run these kind of transactions. The latency in terms of delaying the transactions reaching the execution goes up.
00:43:33.984 - 00:44:21.512, Speaker B: You've got communication and a few rounds of BFT protocols going around and attributing the fault. It's also not something that's very easy. There's some research at the moment on not only slashing, but rewarding the participant in a decentralized sequencer approach. That's not something that's clear at the moment, the shared sequencing approach. So it builds on top of decentralized sequencer. And the idea is instead of having, let's say building a sequence of blocks for one roll up, you want to build sequences of blocks for multiple roll ups. So the main feature, if I'm correct, is to say, well, I would like these two transactions on roll up one and roll up two to be executed atomically.
00:44:21.512 - 00:45:14.210, Speaker B: So both of them are not. So this is the kind of things you can do with a shared sequencer, but it's not trivial to implement. And again, there's no guarantee that it's censorship or MeV resistance. And the latency is also high and it's also a bit tricky to design a mechanism for rewards and slashing. So the fair sequencing mechanism, it's probably very simplistic, I agree with that, but it provides some very well defined measure of fairness. So the random function that you're using you know it at the beginning and you can prove that the randomization has been computed with this function in the design we have proposed. I would say something that's a bit new is to separate the different concerns and be able to attribute the misbehaviors to each of the participants every step of the way.
00:45:14.210 - 00:46:40.250, Speaker B: We haven't implemented it, we have a prototype in a simulator, but there's now some traction on base sequencing and pre confirmation, so it seems to be the way things are going. There's also another approach by polygon on aggregation layer. Let's say we're not thinking that this design will make it through to the layers, but maybe some ideas of this design will find some applications in the current discussions about base sequencing and pre confirmations. So I've put the link here, it's the name of the link, but we've got two articles, a blog post on the Mantle website that is a short version of the article in the HackmD, and the hackmD version of it has a bit more technical details and some reports as well on the experiment. So again, that's some work I've done with Egan Gleason and Andreas Penskoffer and it's a work of the research team at Manton. So I encourage you, if you're interested, to read this blog post. Hopefully we explain what is happening in more details and that's the conclusion of the talk.
00:46:47.800 - 00:47:24.140, Speaker A: Thank you. Thank you Frank. That was really interesting. Certainly got us a different way of thinking of things, which is what I love about these talks. So I don't have any questions, but does anyone else? I know we've had lots of questions during the talk, but does anyone have any final questions for Frank? Well, looks like. Oh no. Do blobs change anything? Question?
00:47:24.750 - 00:47:55.060, Speaker B: No, that's why I've got this sort of parametric data availability layer. I haven't said it changed something in the way that it's cheaper to publish something to the data availability layer or layer one in this instance, but it doesn't change the design. So you could publish to Eigen DA or to a layer one with blobs and so on. It's going to be a different price to publish your transactions, your data, but it doesn't change the design.
00:47:58.950 - 00:48:07.800, Speaker A: All that information is just temporal, isn't it? It's not like after say an hour. It doesn't matter, does it? Or maybe it's a week.
00:48:08.330 - 00:48:38.560, Speaker B: Well, you can decide, but it depends. James was asking the question whether you check in real time or you want to be able to recompute everything from the beginning. So if you're happy with the time window saying, yeah, well, we only keep in blobs the information for is it two weeks or something like that, then that's your choice. But you should still at least publish it for a minimum amount of time to be able to verify it.
00:48:42.480 - 00:49:11.992, Speaker A: Yeah, that makes sense. I know I only did it at the last second, but did you have time to integrate those final. Yes, you're a star. All right, so if you're watching this now live, it's on YouTube. If you want to be able to ask tricky questions of speakers, please join the meetup and come to the meetings directly. There is a slack workspace as well. Okay.
00:49:11.992 - 00:49:14.760, Speaker A: And next slide. Or maybe previous.
00:49:15.500 - 00:49:16.016, Speaker B: Yep.
00:49:16.068 - 00:49:41.872, Speaker A: And there's a merch store. Get your awesome t shirts there. And next slide. There are a whole stack of talks coming, so it's going to be great. So please come along and learn something new. All right, so, Frank, thank you again for doing a great talk. Have a great rest of your day.
00:49:41.872 - 00:49:47.250, Speaker A: This will appear on YouTube probably tomorrow morning, I think that's my guess.
00:49:48.740 - 00:49:49.810, Speaker B: Thank you, everyone.
00:49:52.180 - 00:49:54.228, Speaker A: All right, bye bye bye.
00:49:54.324 - 00:49:57.140, Speaker B: Take care. Bye.
