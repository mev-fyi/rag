00:00:01.530 - 00:00:20.186, Speaker A: Hello and welcome everyone. Today Min Feng is going to do a talk on essentially self sovereign data. That's my understanding of it. And so Min Feng, before you present your slides, could you tell us a bit about yourself, your topic?
00:00:20.298 - 00:00:57.630, Speaker B: That's good. So before we start this presentation, just I just have some brief introduction by myself, as Peter said. So hi everyone. My name is Ming Feng and currently I'm a PhD candidate starting at Swimbung University of Technology. So my base campus is currently at Morbin. It's really my honor to be here to give you a brief live presentation about the research project that we've done recently. So the title the topic of this presentation is basically about the data showing.
00:00:57.630 - 00:01:39.146, Speaker B: We have implemented platform which about the data showing that we call it DeFi enabled data showing and the trading system. So we also got this work have been published in a conference. I think it's a pretty good conference and I also put it in the last slide. If you are interested in it, you can just have a look at it after. Yeah, I think it is a brief introduction for me. Okay. I just forgot my third interest is basically about the blockchain privacy and the smart contract security.
00:01:39.146 - 00:02:03.650, Speaker B: And currently I'm being cooperated with an industry company who is doing digital wallet and I'm basically to protect their security. So that's one of my interest. So, without further ado, let's just get started. Is that okay, Peter?
00:02:04.890 - 00:02:06.646, Speaker A: Yes, most certainly. Please go ahead.
00:02:06.748 - 00:03:12.906, Speaker B: Yeah, thanks. So in this presentation, I will firstly introduce the background of the project and illustrate the contributions made in this work. Then I will briefly discuss the system model and the model evolution part. Okay, so the background I believe that there's a fact that we are living in the big data era. And every moment you can imagine, there are massive amount of data being generated and collected from various data sources, right? Ranging from our mobile to the IoT devices, from business logs to the social networks and et cetera. So the data has become an indispensable part of our life. And this collected data, I think they are very valuable assets because there are various knowledges and patterns hidden inside the data and just waiting for us to own machines to mine and learn.
00:03:12.906 - 00:04:06.186, Speaker B: So that's the key point. However, we have to realize that the data is either difficult or expensive to get in the reality. Because for me personally, when I doing some research and maybe I need to build some machine models and I realized it's really hard for us to find a really interesting and valuable data. So that's a true part. I'm not sure whether you dig it or not, but for me, yeah, it's true. So while there are many reasons for that and we just found that there are some following barriers like the limit and discourage the data sharing. So I will just introduce one by one.
00:04:06.186 - 00:04:58.954, Speaker B: So the first limitation that we found like in the real life is the lack of discoverable data. So the data owners don't know how to describe or publish their data for sure. On the other hand, they don't have a uniform data category or repository to retrieve the required data. And the second reason with that we found this is lack of incentive and fair data. Surely while the data the open data are always welcome and valuable but they are streamed by their size, their coverage and access control. So some companies and individuals may collect data in various domains and resale it higher price which make it more affordable for people like me to use the data. So that's the second reason.
00:04:58.954 - 00:06:31.720, Speaker B: And the last reason that we found is the lack of open and trusted data market. So that's one of the motivation that we want to implement this project. Even though people are willing to compute their data or purchase data at a reasonable price, it's hard for them to fund such an open and trustable marketplace to allow people to trade their data in an end to end way. So that's the motivation to drive the above those challenges that I mentioned, we just proposed or designed a centralized ecosystem where people can donate or trade data at the end to end for the public goods or max mapping the data value. So we just leverage blockchain as a decentralized infrastructure and the ideas always come from, I mean it's come from a DeFi because in the DeFi people can autonomously manage their assets. So something like that we just leverage this concept and we assume like our data can autonomously manage their data assets. So that's the linkage between the DeFi concept and our project.
00:06:31.720 - 00:07:37.930, Speaker B: So I know the people come for today is pretty much familiar with the concept of blockchain the DeFi. So I wouldn't spend too much time on expanding ways of blockchain the DeFi. So basically I'll just skip it and go to the next slides. So the contribution we just built a POC DeFi enabled data sharing and trading system. So that's one of major contribution. And the second in our system we build innovative elects data pricing schema because we know the data pricing is a very important component in the data marketplace. So we design a schema that can dynamically adjust the data price with the demand of the data demand and the time decrease.
00:07:37.930 - 00:08:48.440, Speaker B: So that's another contribution and the third we got it, I mean we conduct some evaluation there and this is the third contribution. So let me just so right now, let me just quickly go to this part like to introduce our citizen architect architecture. We present our proposed model and describe the model working process from a high level perspective. So I won't go to too details like how it implemented. So that's the reason I just asked Peter why the background of the people coming today there are five entities in our system implementation. One is the data providers. Those are the people who are looking for to commercialize their data.
00:08:48.440 - 00:09:55.798, Speaker B: So the users can pause the data as a product and by just creating a smart contract and filling the metadata according to the provided generic metadata template. So in our system we already have a template for metadata template and the people who want to seal the data, they only need to fill this template and this quick and a very convenient way to publish their data. And the second component that we call is a data consumer. So those are the people who want to purchase a data that meet their needs. They can explore and compare multiple data for this collected, collected in our platform and purchase data via various Q and a faster method payment channel. The third component where we code is a smart contract. So basically the smart contract will implement the business logic and the rules to govern the data transaction flow.
00:09:55.798 - 00:10:56.270, Speaker B: So there are four modules written in the smart contract that I will introduce later which is about the price, delivery, transaction and discovery part. Also the blockchain. We just employ the blockchain as a decentralized infrastructure to substitute the centralized architecture in the conventional data sharing system. So that's one of the characters that we know with the blockchain as a decentralized infrastructure. And finally, the first component code is a distributed file storage system. It's an off chain storage approach, they're usually used to store and receive the data. So the reason that we don't want to store all the data, all the data information on the blockchain because we know the storage cost on the blockchain is quite high.
00:10:56.270 - 00:12:26.650, Speaker B: So that's why we choose the off chain and on chain combination way to store the data. Okay, so over product, over data will connect the end users which are seller and buyers, but while not holding any data itself in this graph, in this figure we just illustrate the end to end introduction of the data transaction and basically it can be divided into the five steps. Firstly, about the data seller, they can do action like to publish their data product. So the data product can be published by anyone who are able to connect to the web. Three, they only need to fill the metadata as I described. So in the metadata we have a couple of categories so they need to choose one of the option like weather data category and what's their image, the price to submit. I mean there are lots of forms they have to fill out and to describe their data products and once they're done they can just post it, they just click the button and the data products will be published.
00:12:26.650 - 00:13:17.466, Speaker B: So any transaction will be written into a blockchain and once the information on the blockchain will be immutable and can be traced. And the second is about the data product browse. It's sort of for the data users. So the data users can browse various data sources and they can search for specified data sources by using some keywords. They also can obtain some data sample data. So that's basically the functions in the data browse. Also the users once the users determine to buy a data product they can just check out.
00:13:17.466 - 00:14:59.530, Speaker B: So it's quite similar to the transaction we made whatever in the Amazon or the ebay it's quite familiar with us check out but different from that because we're using blockchain. So basically we can now using some real credit card or other things instead. We can use the cryptocurrency to pay for that. So there are three basic pricing options that we provided in our platform which is about the one off purchase and the ongoing subscription and the usage based license. So there are three options they can choose when they want to check out. The fourth part is about the data delivery. Data delivery can be deemed as another important component because the things about data delivery is quite big because you have to prove like the data sources can be skew or can be data privacy issue so the things connect to the data delivery complicated but we are just to define another secure way about the data delivery.
00:14:59.530 - 00:16:05.410, Speaker B: So we are now trying to use the security key or private key to do that but later on we can have a discussion about data delivery. So the last part is about data updates. As I said, we had implement a schema about the data price. So I will do more details about that later on. Basically data updates. I will say it's kind of important because the price like the user as I said, the user first, they set up their price when they publish data but it's not reasonable the data price will be upchanged with the time or with the data values. So the data price to be reflected the data value.
00:16:05.410 - 00:17:46.290, Speaker B: So that's why we have implemented a schema about the data price. It's also a quite big aspect. Well, there are still a couple of things we cannot go to details due to time limit the remain time I would expand more about the data price this schema that we implemented. Okay so as I said, the data price is a rather completed problem in order to reflect the true value of the data in the marketplace we just proposed a data price schema which include two parts. One is the initial data price estimation method and another that we call is an elastic data pricing model used to dynamically to update the initial price set up by the users. So I would like to show like compare with the data pricing method that take various factors in the current method like they always take the data value, the number of features, the predictions into consideration when they estimate data value so it's kind of complicated but in our initial data price. We try to avoid such a complicated estimation.
00:17:46.290 - 00:19:28.718, Speaker B: We just let the user data providers, we provide such a simple way like to let them to roughly to get a data price just using a data cost plus a margin as a price. So we have a formula that we made here, the P is the calculated initial data price and the percentage m is a margin and the s is a parameter corresponding to the expected number of the transactions and the c is the cost of the data. So by just using such a very simple, very clear formula that the users who want to populate data can just employ this formula to calculate the initial price. But we know that the initial price I said cannot like the real data value in the market, it's just estimated by the user itself because the rival that we use is just determined by the user. But the price is largely depends on the subjective estimation, depends on the estimation of the data providers and it may not accurately refrigerate the market conditions. So let's just take as example. So if the data price is much higher than the market price, it's unlikely to attract the potential users.
00:19:28.718 - 00:20:26.050, Speaker B: So therefore, we have to design another way, which we call it elastic data price to dynamically adjust the initial data price to a real price that can reflect the true data value. So in the second method we consider two factors. One is the transaction and another is a time. We call it entropy based data dynamic data Price Estimation method. So there are two parts I said, the one is the transaction based value increase. It basically describe the impact of the transaction on the value of the data. So the overall trend between the two rivals is positively correlated and as the number of the data transaction increase, the value of the data also will increases.
00:20:26.050 - 00:21:39.926, Speaker B: And the second record is the time based value attenuation as the time goes by, the value of data is constantly decreasing while the data entropy continues to increase. So, taking into account the different time values, we create a nonlinear status model to describe the trend of the data over time. So you can see actually from the figure at the bottom, it just described the value of the data over time. So overall, just combining these two methods, first they set up the initial price and based on the time and the data demand which is about the transaction, the total number of transactions, the initial price will be eventually to present a stable price that can truly reflect the data market and the data value in the market. So that's our design. Yeah.
00:21:40.048 - 00:22:21.500, Speaker A: So just looking at that formula, if I can ask a question yeah. So sum of s is the number of times people have accessed the data. So as you go through with time, if people want that data more and more then price would tend to go up. However, that entropy function is that thing there that with time it goes higher, which given it's on the bottom of the equation, means that the overall price would tend to go down. So is that essentially saying the older data is the less valuable it is? Essentially. All right, that makes sense.
00:22:24.670 - 00:24:12.074, Speaker B: As I said, we implement our data marketplace and we also do some evaluation about that, about the evaluation part, we just test the performance of the system services and we just visualize the testing result in a various plots. So overall, most of the functions like we tested is satisfy our objectives in terms of the responsible time, the response latency in the bias report and we also tested the functionality of the Smart contract that we implemented. Basically it's about the gas consumption and also we got some outposts which are also satisfied. So I think that's all for my presentation and I really want to thank Peter to give me such a good opportunity to present our work, to present our ideas about the data sharing. And I would like to talk to give a minute, there are some other future talks in the coming years and you can see actually from December 7 until the beginning of next year, we got more interesting topics and I would like to show those topics to you guys, if you can just join us. Also we will publish our yeah, I think that should be talked by the Peter, right? That's okay. Yeah, I think that's all for my prediction.
00:24:12.074 - 00:24:55.900, Speaker B: And as I said, we just got our work published in some conferences. One is on the ASM Internet Symposium on blockchain sq crypto infrastructure, bscci is HRCC's conference and another which is more details about the data delivery. As I said, we just use the databox services to deliver for the data delivery part and we got it. Publishing the ICWs is also a good conference. So if you're interested in this work, you can just have a look, as I said. Yeah, thank you guys. Any question?
00:24:58.840 - 00:25:00.790, Speaker A: I think you're going to have a few questions.
00:25:01.180 - 00:25:02.810, Speaker B: Yeah, I've got a few.
00:25:07.980 - 00:25:16.380, Speaker C: Hi Min Fang. Just as a first question, can you talk more about the delivery model module and for instance, is it on chain?
00:25:17.680 - 00:26:06.990, Speaker B: Okay, delivery module, I think it's a good caution. So it's another on chain delivery method, but the transactions made, for example, you are the buyer and I'm the seller. So the transaction we made will be recorded on the blockchain, but it will not happen in the blockchain. So only the details of the transaction will be recorded. But definitely we can have another way to build a skew and faster channel to make a payment like that.
00:26:08.320 - 00:26:11.180, Speaker C: So where is that user data actually stored?
00:26:13.920 - 00:26:19.890, Speaker B: You're talking about the transaction data or the delivery data?
00:26:21.620 - 00:26:25.136, Speaker C: Not the delivery, sorry, the actual transaction data.
00:26:25.238 - 00:27:06.350, Speaker B: The transaction data, if you are to pay it by the cryptocurrency. Definitely the transaction made will be stored on the blockchain. But as I said, basically the common way and the easier way is just using the cryptocurrency to pay the data product that you buy. But if you would like to do another way, like the transactions, the data of the transaction can be recorded on the.
00:27:08.160 - 00:27:12.284, Speaker C: And delivery data. Is it on IPFS or something?
00:27:12.322 - 00:27:39.480, Speaker B: Like yeah. So the real data actually will be stored on the IPFS or the MongoDB other databases, but there are some mapping with the real data and the data stored on the blockchain. So if you want to retrieve the data, actually the data can be linked into the IPFS and you can retrieve the data from the IPFS.
00:27:41.500 - 00:27:55.150, Speaker C: And what stops the person from then reselling the data as well? Sorry, does anything stop the buyer from reselling the data potentially on other platforms or anything like that?
00:27:56.900 - 00:28:51.600, Speaker B: That's a really caution. So if the people want to resell the data well, we haven't figured out how to prevent the users or the buyers to sell data, but it's a very good topic. So I would like to discuss with the guys if you have any discussion or any ideas to prevent from reselling data, because I think it's sort of complicated topic, but it really happens in our real life. Maybe what I know, like, some methods will pull some IDs into data or something like that. So in our work, we haven't done such a thing, but it's really good topic.
00:28:53.300 - 00:29:16.580, Speaker D: Hey, Min Fang. I've seen companies use things like rate limits on API calls and that kind of thing to limit the volume of information on there. Is that something that you've taken into consideration? Or once the user has made a purchase, are they able to pull all the entire data repository?
00:29:16.740 - 00:29:49.590, Speaker B: Yeah, definitely. That's one of the options. So using the API, as I introduced in our data delivery, we got three options. One is just buy us once so you can get all the data. And another is the license, for example, the one year license. Or another way, like you can just based on the usage, like using the API way to control the data that you used.
00:29:50.760 - 00:31:05.870, Speaker D: Okay, I have one more question about the pricing formula that you have. So how does it take into account the volume of data? So, for example, the value of the data goes down over time, but if the data was collected over 40 years, 50 years, I think the size of the data or something. For example, I was reading something about mortgage information. If you had like mortgage insurance information and over time, you could kind of see certain trends with different counties and that kind of thing. How does this formula take into account the volume of information? And context like that where the newest information wouldn't necessarily be maybe the last year would be valuable, but collectively, the collective information in and of itself would provide more insights, especially for forecasting or seeing outliers within the data.
00:31:09.040 - 00:32:02.092, Speaker B: Yes, it's a very good idea. In our formula, we only consider two factors. One is the time and another is the demand or can be deemed as the transactions. So let's just take a user, for example. I have a data protocol, let's say in this year or in this month. My data prototype was before, I mean, was bought like by 10,000 toms. So the demand that we can see is quite high, right? So we can deem as the value of the data will be high, but after months, suddenly nobody come to buy the product.
00:32:02.092 - 00:32:28.390, Speaker B: So definitely the number of transactions will become variable, but the time goes up, so the value of the time will goes up. So the value of data will significantly decrease. So that's why the data value itself will be decreased as well. Did you get what I mean? Yeah, that makes sense.
00:32:36.020 - 00:32:42.960, Speaker A: I've got a question related to it was a slide before this. Maybe there was a sequence diagram.
00:32:43.460 - 00:32:44.210, Speaker B: Okay.
00:32:44.580 - 00:33:10.600, Speaker A: And I think it's going to be sort of indirectly related to that one there. So we've got this delivery module and it I believe is on chain and anyway, we go deliver private key and so I assume that I'm not sure is that private key for encryption?
00:33:12.320 - 00:33:13.070, Speaker B: Yeah.
00:33:14.960 - 00:33:48.804, Speaker A: That looks like a very bad idea. If the data consumer publishes a public key, say on the blockchain, they could say, hey, this is my public key. And then the data provider caught in data against their public key. So the data provider controls their private key. Data consumer controls their private key. We don't have a private key in the cloud or on the blockchain or anywhere. Does that make sense?
00:33:49.002 - 00:34:51.496, Speaker B: Yeah, definitely true. Because I have been talked to this project, I mean, this overwork in many things and this is quadrant often just asked by the others. So the reason that we want to explore the data delivery in another work, because we just found this way, I mean, just deliver the private key this way definitely is insecure and it's not supposed to be happened like just deliver a key for the data delivery. But in this work, because we have published this work and in that work we just partially implemented this data delivery module. So we haven't considered too much details about the security. So that's why you got this figure in this size. But definitely in the real implementation, we haven't done that.
00:34:51.496 - 00:34:56.764, Speaker B: We haven't do such a thing. Like just yeah, okay, all right.
00:34:56.802 - 00:35:17.830, Speaker A: And I think that then that covers Lecky's question as well. At least question one about IPFS is public. So that you're going to need to at least encrypt the data though. Even encrypting data that has long term security ramifications. If it's anywhere that people can access.
00:35:18.280 - 00:35:19.030, Speaker B: Then.
00:35:20.840 - 00:36:09.024, Speaker A: Say there's some as importance from now until say, 2035 and if the encryption methodology is breakable, say in five years, that data would be exposed. So if you thought quantum crypt analysis was going to be a thing, then that'd be a problem. So even storing encrypted data on IPFS could be a problem? If it's information that you need to keep. Yeah, Lekki also said I suggest could also tap into did so decentralized identity. That way people could retrieve access to the data. All right. And then Hayden's talked about encrypting data.
00:36:09.024 - 00:36:12.070, Speaker A: OK. Are there any other questions?
00:36:14.600 - 00:37:26.830, Speaker D: I have one more question. So using let's say there was a decentralized mortgage, just going back to that as an example, and I wanted to be able to show the payments that I've made. Let's say the mortgage is a 30 year we do 30 year mortgages in the states. So let's say I wanted to prove that I made those payments on time, over a period of time. But then the person who's purchasing the data is looking for maybe there's some demographic information, some location information, like where the property is located and that kind of stuff. But the person who's purchasing the data is again looking for collective information related to so they want to see all information related to mortgages within a certain geographic area. Is there a way to aggregate that? So that way when they're making the purchase of that information, they're basically purchasing in bulk, but paying out to every single individual who provided that information.
00:37:26.830 - 00:37:46.130, Speaker D: Is there a way to batch purchase the data from a collection of users that fit a certain criteria that that purchaser would be looking for?
00:37:48.820 - 00:38:24.900, Speaker A: You'd have to have metadata then wouldn't know associated with the encrypted data, so that then you could group data into a data set. I don't know. Min Feng. I'm answering. I'm just thinking out I mean, that'd be the only way to because if you've got an in, everyone publishes essentially encrypted data, then you're not going to be able to dive into it's.
00:38:33.260 - 00:38:43.150, Speaker B: I cannot figure it out, but I think it's really good ideas or good terms, like we can think later on.
00:38:46.240 - 00:40:05.424, Speaker D: Yeah, I think it would be interesting to be if there was a way to be able to batch, because right now this is something going back to the states. There are data brokers in the United States and they sell our information without our permission all the time. And you can buy them from these marketing firms and that kind of but like, I understand the need for the companies on the other side to want to collect that data, to better understand their customers, to better understand trends and that kind of thing. But if there was an opt in way for individuals to say, yeah, I'm comfortable sharing this information because I know one where it's being used and why it's being used in such a way and that kind of thing. So if people could pull together, not necessarily getting together and say, hey, let's pull our data and add it to the marketplace, but individually selecting what works for them and what information they want to share. So that way, the person or entity purchasing that data is able to then, through different filter criteria, pull that information in and say, okay, now I have a nice data set of maybe 100,000 individuals with these different parameters. Maybe it's shopping patterns.
00:40:05.424 - 00:40:26.910, Speaker D: Maybe it's bill payments, maybe it's whatever it might be. And then from there, you could run different types of analysis in a way that doesn't violate anyone's rights but still allows the user to retain control of the person contributing the data to retain control of their information.
00:40:47.130 - 00:41:23.600, Speaker A: Hayden's, a great little thought in the chat. And for those people on the video who won't see it, completely agree. Rude people are willing to sell their medical data for search, but not for insurance purposes. For instance, you can probably think of as like situation for almost any personal data or for research. Medical data for research. Yeah. So, look, Ming Feng, thank you for your talk.
00:41:23.600 - 00:41:42.402, Speaker A: Generated a lot of questions, a lot of thoughts, so a lot of things to think through. So good luck with the rest of your PhD. We look forward to hearing about you being a doctor. And so thank you again and talk to everyone later. Bye bye.
00:41:42.546 - 00:41:43.718, Speaker B: Bye. Thank you, guys.
00:41:43.804 - 00:41:44.886, Speaker A: Thank you, everyone.
00:41:44.988 - 00:41:47.240, Speaker C: Thanks so much.
