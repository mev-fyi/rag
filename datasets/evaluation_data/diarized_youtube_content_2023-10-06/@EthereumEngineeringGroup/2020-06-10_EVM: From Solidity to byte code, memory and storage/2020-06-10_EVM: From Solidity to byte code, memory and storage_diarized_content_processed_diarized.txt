00:00:01.450 - 00:00:27.240, Speaker A: Okay. Hello and welcome, everyone. This is the Ethereum engineering group meetup. Today, David and I are going to talk about the EVM solidity and what solid solidity looks like as a program in the EVM and how it uses code, storage, stack, memory, and other things. David, do you want to introduce yourself?
00:00:29.930 - 00:01:04.450, Speaker B: Hi, David Hollandwood from consensus. I work with Peter and the Pegasus business unit on TeamX, and all of us here at TMX are pretty much present, I think. So if you want to speak with any of the others doing this area of work, please either ask questions or introduce yourself to people that you may not know. Peter.
00:01:06.230 - 00:02:00.740, Speaker A: So thank you. So before I start, I'll say that David and I were brainstormed this talk and we were going to work on it and do about equal parts, but David's got super busy, so you won't hear David talking as much as I am, but he still knows a lot about this. All right, let's share my. Let's do this. So if I go play there just a sec, as Google Slides catch up, catches up, and so does let's move that over there and go present on that screen. Sorry, just getting my screens to work. And now what we'll do is we'll share screen and we'll share desktop too.
00:02:00.740 - 00:02:57.800, Speaker A: Okay, so you should all now be able to see my slides. Can people see the slides? Yes. All right, brilliant. Thank you. Okay, so this is a talk on the EVM, and we've put together the talk and we've had a lot of feedback from these people, which has been very much appreciated. We're going to walk through an introduction to the overall topic and then talk about Ethereum transactions and how they relate to it all, and walk through slowly but surely. If we get to about one and a half hours in and we're not finished, what we'll do is we'll reconvene this time next week, so we'll see how we go.
00:02:57.800 - 00:03:59.190, Speaker A: So there are some examples in this talk, and they can all be found in the Ethereum engineering group solidity examples. The bytecode output that you'll see was generated using some tools that we've created, and the stack analysis was created using some static code analysis tools. Create. So when you think of the solidity language, you've got a file that something or other soul, and that's your source file. You put it into the compiler, and normally you get out the ABI in the bin. You can specify parameters to say what you want to get, but those are the two files you normally end up with. And your binary file is your output of the actual bytecode.
00:03:59.190 - 00:04:50.090, Speaker A: And the ABI is an interface file which tells applications how to interact with the code. So essentially what functions your code supports and what parameters they take. Though it's worth remembering that solidity is but one EVM language. So for instance there's a language called Viper which is in alpha at the moment, and it's compiled with the Viper compiler. All of that said, we've done some recent analysis of all of the deployed contracts on main. Net, and the numbers are indicating that it's well over 90% of the contracts on main. Net were created with various versions of solidity.
00:04:50.090 - 00:06:19.350, Speaker A: I'm not going to give you exact numbers because our analysis needs just a little bit of tweaking. So once you've got that binary file and the ABI, you then put it into some sort of solidity wrapper generator, and that will generate say a Java wrapper, which essentially is an interface that you can call from your Java code to call the solidity code inside of the Ethereum node. If you were using JavaScript, obviously you end up with a JS file, or maybe you can just use the library interact directly. And so once you've got your application code and your wrapper code, you use a library, some sort of web three library, and then that talks to your solidity code via the Internet, and your solidity code will be running inside the EVM, inside an Ethereum client, inside of the Ethereum network. So an Ethereum client is an instance of software running on a node in the network. And a client, given we're in a peer to peer network, is both the server and the client. So it's essentially the machine that's operating the Ethereum network.
00:06:19.350 - 00:07:27.530, Speaker A: So you've got to think your application is interacting with the solidity code that is now bytecode inside of the EVM. Okay, so I think I've just skipped. Yep. So in this talk we're going to focus on that bytecode in that code bin. So that's the code that gets executed by the EVM. So in ethereum you have transactions, and these are the things that are sent over the wire between the application and the ethereum node and also then around the network. And so you've got a nonce, and so that's a value that starts at zero, you've got the gas price, which is how much you're prepared to pay for gas in terms of ether.
00:07:27.530 - 00:08:31.150, Speaker A: Or in particular we, where we is a unit of ten to the -18 of ether. You've got the gas limit, which is the amount of gas you're prepared to use the to address. So who you're sending the transaction to and the value and the data to be sent to the two address and the signature. So if you're just transferring money between accounts, then you have it set up like this, where you always specify just 21,000 because that is how much gas you need to use for a base transaction. And the address you're sending the ether to and the value and the data is usually empty. But it's actually worthwhile pointing out that the data doesn't have to be empty, it's not mandatory. And so if all you want to do is record a commitment onto the blockchain, you could put some data there and then you'll have that as part of the overall blockchain.
00:08:31.150 - 00:09:36.550, Speaker A: So it's important to remember though that it's not going to be part of world state, so it's not going to be part of the state tree, but it will be part of information that gets linked into the overall blockchain, because the summary of all the transactions. So the transaction route is part of the block header. If you're going to deploy a contract, the two field has to be empty. And that indicates to the Ethereum client that you want to create a contract and the data field contains the initialization code. So essentially a code fragment which is going to set up the state of the contract and deploy the contract. And if you want to do a function call, then the data field are the parameters. So what function do you want to call and what are the parameters? And it's really up to the EVM code to process that.
00:09:36.550 - 00:10:23.310, Speaker A: So code that conforms to the ABI specification takes a function selector and parameters. However, EVM code does not have to conform to the ABI spec, it could do anything. And as such that data field is just a blob of data that the EVM code can do stuff with. Yep, and there's a link to the ABI spec. So stack memory storage. So all of the locations you can do stuff with in the EVM. So the EVM.
00:10:23.310 - 00:11:20.710, Speaker A: So Ethereum is a stack based processor and what that means is that there aren't registers, you've only got the stack. And so when you have it do an operation, you use stuff from the stack. So you take input from the stack and you pop that input off and then you push onto the stack. The result, so add is popping off the stack, stack zero and stack one being seven and two, it's adding them together and pushing the result nine onto the stack. And so inside of the EVM, you can access and store information from six different places. So there is the stack. So you can push stuff onto the stack and keep it there for a while as temporary storage.
00:11:20.710 - 00:12:10.920, Speaker A: The stack isn't persistent, so at the end of the transaction, the stack is gone. You've also got call data, so remember the data field that there was in a transaction, so that information is available to the contract. So when that transaction comes in, that call data is a read only memory area. Memory is a temporary place of storage where you can store stuff just for the duration of the transaction. So it's like a scratch pad. Then you've got storage, which is a persistent data store. You've got code, and that's the code you're executing, but you can also use that for static data storage as well.
00:12:10.920 - 00:13:03.000, Speaker A: And then you've got logs. So this is a writer only output area. And so when you're thinking about storage and how it all works, you've got to think you've got a block header, and it's got the parent hash, the Omas, the block number, and all that sort of stuff. And it's also got something called the state fruit. And so this is the root hash of a Merkel Patricia try of all of the accounts. And if you don't know too much about this area, have a look at the YouTube video from two weeks ago where Harathio and I did a talk on stateless. And we do talk about the Merkel Patricia tries and how they store information.
00:13:03.000 - 00:13:46.580, Speaker A: So anyway, within this try, you've got a key, which is the 160 bit account address. And so then within each account you've got account state. And so that consists of, say, the nonce and the balance and the code hash and a storage route. And so within the storage area, you've got another Merkel Patricia try. And this try has got 256 bits of key, so each key is 256 bits. When we talk about storage, this is what we're talking about. We're talking about something stored against the account.
00:13:46.580 - 00:13:53.960, Speaker A: Okay, over to David to talk about the yellow paper.
00:13:57.210 - 00:14:34.754, Speaker B: Thanks, Peter. So the Ethereum virtual machine is defined in the Ethereum yellow paper. It has been kept up to date since the first version, and so there have been changes to the yellow paper along the way. But if you go into GitHub at this URL, or just do a search for the Ethereum yellow paper, it'll pop right up to the top. The paper was originally written by Gavin Wood at the beginning of the Ethereum project. But since then, many people have contributed to this paper. Next slide, please.
00:14:34.792 - 00:14:35.410, Speaker A: Peter.
00:14:38.650 - 00:15:10.494, Speaker B: Toward the back of the yellow paper you will see that all of the Ethereum opcodes for the Ethereum all the Ethereum virtual machine opcodes are defined. This is the canonical location of this information. So what's here is the definition of the Ethereum virtual machine. It's a virtual machine that implements the totality of these opcodes and the way they're defined. Next slide, please.
00:15:10.532 - 00:15:11.150, Speaker A: Peter.
00:15:15.590 - 00:16:10.610, Speaker B: So if we look at the first few here, you'll see that you've got values in hex that start at zero and they just increase. So the very first opcode is zero x, that's stop. Next one is one, that's add, et cetera. On down the list, all the arithmetic is modulo two to the 256, which is the same 256 number that you saw in that merkle. Patricia try on Peter's last slide and the description of each opcode goes through. You'll see that stop doesn't take anything off the stack, doesn't take any input, it just halts the execution. Add.
00:16:10.610 - 00:17:15.666, Speaker B: Peter has already shown you in a previous slide where it takes two items off the stack item zero and stack item one, adds them together, then pushes back onto the stack. Opcode two mul or for multiply, does exactly the same thing as add, except the operation. The computation is to multiply things together instead of add. This is pretty much the way this level of any machine code looks like, and it's going to be very familiar to all of you who had this level of computer science. It's pretty straightforward stuff. If you looked through all of the opcodes, you would find that it's all the typical sort of things you'd get in a compiler class with the addition of some things that are very specific to the Ethereum virtual machine. Peter already mentioned that they're stack operations.
00:17:15.666 - 00:17:26.090, Speaker B: It's a stack virtual machine and the higher opcodes are the ones that are more specific to Ethereum operations.
00:17:27.230 - 00:18:26.000, Speaker A: Peter thank you, David. Yeah, so now I'm going to talk about contract deployment constructors at init code fragments. So this is some solidity code. So we've got a constructor which is public, and we set one of the storage variables to three. And so the init code includes code to deploy the contract plus set up the state. So in other words, setting val two to three is part of setting up state. And so you compile the code using that to produce the bin file and you get something like this.
00:18:26.000 - 00:19:11.550, Speaker A: And so in this code here you've got the init code fragment and the actual code to be deployed. And so it doesn't have to necessarily be this arrangement, but for solidity it is. So as I've said before, contracts are deployed using transactions where the two address is not specified and the data is the init code Fragment. And so the init code is not stored on the blockchain. That makes a lot of sense because it's the constructor. It's only ever used right at the start to set things up. So it doesn't need to be stored on the blockchain.
00:19:11.550 - 00:20:01.930, Speaker A: And for this particular type of transaction the data is actually treated as code. So you might remember I said that you can access the call data. So for a contract deployed transaction the call data is actually treated as if it's code and executed as code. So if you look at that binary there, you start out with 68 and so 60, if you looked at the yellow paper, is the opcode for push and pull. Push one in particular, meaning push just one byte. And so that's pushing zero x 80 onto the stack. Oops.
00:20:01.930 - 00:20:37.430, Speaker A: Yeah. And so if we decompile a bit more or turn it into opcodes, this is what you've got. So we're pushing eight here. And so push the word onto the stack. So the stack has a depth of 1024, so you can have 1024 things on the stack. And so when we use the term stack zero, that's the top of the stack. And stack one is the one, just one below the top of the stack.
00:20:37.430 - 00:21:18.840, Speaker A: And the stack items are one word wide where one word is 32 bytes. So when you're pushing one byte onto the stack, push one. Then the top 31 bytes are set to zero. And so after this has happened, you have 80 as the top of the stack. And then we push 40 onto the stack. And so the stack now looks like 40 at the top and 80 is the next one. And then we have something called m store.
00:21:18.840 - 00:22:16.610, Speaker A: So m store is store a word to the temporary storage memory, so to that memory area. And so it takes two values off the stack. And so stack zero is the location to write to and stack one is the value to write. And so what this is going to do is it's saying we'll set memory location zero x 40 to value 80. And so what this is doing is setting up what's called the free memory pointer. And so the idea of the free memory pointer is that the solidity code that runs after this can load it up and it'll know that anything after zero x 80 is something that it can use. And so if it allocates some information into memory, then it will need to update the free memory pointer.
00:22:16.610 - 00:22:56.050, Speaker A: And so that's the idea. It's interesting to note that zero X 40 is a byte offset and not a word offset. And so though we are storing whole words, it's actually a byte offset. Call value puts onto the stack what the value field of the transaction was. So how much we has been sent. And then you have dupe one. So dupe one duplicates the top of the stack.
00:22:56.050 - 00:23:36.914, Speaker A: And so now we've got the stack looking like this, where both the stack zero and stack one contain the value that was sent with the transaction. And so there's also dupe two, three, four, all the way up to 31. And what they are doing is they will duplicate different levels into the stack and put them onto the top of the stack. And that can be used for stack manipulation. So is zero pops a word off the stack. And if the word is zero, it puts one onto the stack. Otherwise it puts zero.
00:23:36.914 - 00:24:51.546, Speaker A: And so what this is saying then is, well, if the value was zero, put one, but if the value wasn't zero, put zero. So then we push two bytes onto the stack. So push two means push two bytes and then jumpy is jump if. And so what that is all about is it's saying that if stack zero is, oh, jump to stack zero if stack one is not zero. And so it's going to jump to a location or go straight through, depending on the value of stack one. And so yeah, so if that value wasn't zero, then jump, otherwise don't. And then it pushes zero onto the stack and then duplicates zero.
00:24:51.546 - 00:25:24.580, Speaker A: So then you end up with a stack that looks like that stack zero and stack one both being zero. And then we have a revert instruction. And so revert halts the execution and indicates that a revert has occurred. And it uses stack zero as a memory location and stack one as a length to indicate the revert reason. So the reason why the code reverted. So the stack had zeros at the top. So in other words, we're saying there is no revert reason.
00:25:24.580 - 00:26:51.680, Speaker A: So all of that code together is really saying, look, this constructor wasn't payable. And so if someone sent value to the construct as part of the init functions, part of this contract deployment, then we should revert, because the contract is not supposed to have ether sent with the contract deployment. So when you have a jump, whether it be an absolute jump or a jump if jump I, you always jump to a jump destination. And so the program counter. So we had push two and pushed zero x 10 onto the stack and then we jumped to it if the value was zero. And so that's going to get you to program counter zero x ten. And so yeah, if you jump to an invalid jump destination, then your code will stop and you'll have an error, pops the value that's on the top of the stack off and just throws it away.
00:26:51.680 - 00:27:54.990, Speaker A: And so now we're, it's worth recalling that we were setting val two, which is the second storage location to the value three. So what we can see here is we push three onto the stack, we push one onto the stack and then we store it. And so s store, stores a value into storage. And so it takes the stack zero is the location to write to, which is going to be one, and it's going to take the value which was three and it's going to store it there. So storage location one becomes three. And so that matches what we're seeing in the solidity code there. So now what we do is we push some values onto the stack and what we're doing is we're getting ready for code copy.
00:27:54.990 - 00:28:53.560, Speaker A: So you might remember the code, the runtime code followed the init code fragment. And so what this is doing is it's getting ready for the code copy operation so we can copy the code from where it is at the moment to the right location. So we set up 24, and you can see that here on the screen after the invalid operation. At PC 23 we've got PC 24, which is push 180. And so that put from zero x 24, that's the start of the runtime code. And so code copy copies from the code that is currently executing to memory. And so stack zero is the offset to write to.
00:28:53.560 - 00:29:39.170, Speaker A: Stack one is the offset to read from, and stack two is the length of the bytes to be copied. And so the stack ends up looking. And so the stack looked like this. So what we're saying is let's write to memory offset zero and start at offset 24. Program counter 24 offset, and do it for zero xc one bytes, which is the length of the runtime code. And the thing to note is, you remember how we set up the free memory pointer, so we're just wiping that out. So we set that up in memory, but now we're just going to copy straight over the top of it and that's not going to matter because we're going to exit.
00:29:39.170 - 00:30:38.310, Speaker A: But it shows you that setting up that free memory pointer just for this piece of code didn't actually help, and it was a bit of a wasted few operations. Yeah. So we copied the code. Yeah. Okay. So overall, that block was copy the contracts code that's going to execute on the blockchain that we want to have on the blockchain to memory, and then we return, and when we return from an init function. So a contract deployment transaction, stack zero is the start offset in memory where the result is.
00:30:38.310 - 00:31:34.380, Speaker A: And so stack one is the end of the result. So that's the same for any return function, but the result that's returned with the return function for a contract deployment is the code that should be copied into the blockchain. All right, which is what that just says there. So store this code into the blockchain against the new contract's address. So there's an invalid opcode there too, which is unreachable because you've already returned. And so the return followed by invalid indicates that this is the end of the init code. So in summary, we set up the free memory pointer, which in this case wasn't used.
00:31:34.380 - 00:32:30.620, Speaker A: We caused a revert, if any, we were sent with the transaction because the constructor wasn't payable. We set up a storage location to a non zero value, and so we could have arbitrarily set up lots and lots of them and complex structures as well. And then we returned a pointer to the code and the length of the code that needs to be stored into the blockchain. Okay, so what about function calls? So when we look at this code quiz question, Lucas, how many functions are we going to end up with? One. We're going to end up with three.
00:32:32.030 - 00:32:37.978, Speaker C: Oh, yeah. Gotcha. Good one, Pete. You got me the fields of functions.
00:32:38.154 - 00:33:29.280, Speaker A: Yep, the fields are. So when you're writing code, solidity code, you want to have a very close look at this. So if you've got variables, you can have them private, and then you're not going to have accessor functions in your solidity code. So if they're private, and you don't need to access them programmatically, then just have them private if you want to access them. If you just have them public, then essentially getter functions will be automatically created. So what this means is that we actually have three functions that we should expect to be created for this code. One to fetch val one, one to fetch val two, and one to execute set.
00:33:29.280 - 00:34:44.950, Speaker A: So at the start of the code, we're going to set up the free memory pointer again, and then, because none of those functions are payable, it does the let's check to see that the value is zero again. And if it isn't, let's revert. And then we jump to the next block of code. And so the code automatically copies the value onto the stack. And I think that's just in case any of these functions were payable and you need to use that value so it pops it off. So call data size is an opcode that tells you the size of the transaction data field and less than is going to tell you whether stack zero is less than stack one. And it's going to pop the values off and then push on, push one onto the stack if they're less than or zero otherwise.
00:34:44.950 - 00:35:55.160, Speaker A: So what that push one four down to the jump I are all telling you is if the transaction data field has less than four bytes, then we want to revert. And the reason why it wants to do that is related to something called function selectors. So a function selector is a truncated message digest of a function signature. So you can think we had a set brackets uint 256 function. And so if you message digest that and then have truncate that down to 32 bits, then that's the function selector for that function. And so when we're doing this, revert if less than four. What we're really saying is if we don't have a function selector, so if we don't have enough bytes to at least have a function selector, then we should revert.
00:35:55.160 - 00:36:39.748, Speaker A: And so then if you did have enough more than four bytes, then you go to a jump table. And so you execute call load. And so call data load only loads 32 bytes at a time. So one word at a time onto the stack and it loads starting at offset pointed to by stack zero. So you can see we've pushed zero and then call data load. So we're loading up the start of that call data. So load up the first 32 bytes, sure is shift right.
00:36:39.748 - 00:37:37.000, Speaker A: So shifts the stack one to the right stack zero times and then pops them off the stack and then puts the result back on the stack. So we've had stack one. So stack zero had the, sorry, I should have had a little diagram here saying what was on the stack. So call data load has loaded up the value of the call data 1st 32 bytes. Then we're going to shift by zero xe zero to the right, because that's the top of the stack. Essentially what that's going to do is it's going to shift the top 32 bits from the top of the word to the bottom of the word. And so what this is really doing is it's loading up your function selector.
00:37:37.000 - 00:39:13.640, Speaker A: And then once we've got the function selector, we duplicate it. And then we say, let's push a value onto the stack, essentially a function selector. And we say, well, does the function selector that was supplied in call data equal this? And if it does, well, let's jump to offset 41. And so if you see in code function selectors, you can go to this website here and paste it in here and search and it'll tell you what it thinks that function selector might mean. Now, because it's a 32 bit hash or truncated hash, you're going to have collisions. So just because you see that it comes up with a certain thing, it might not necessarily be that, but if you have your set of functions that you think it might be, then it could help you differentiate between say two or three different functions. Okay? And then with this next bit of code here, it's going to jump to 5d if the function selector is that, and another jump if the function selector isn't one of the ones that we know about.
00:39:13.640 - 00:40:08.410, Speaker A: So if the function selector passed it in as a parameter isn't one we expect, then we don't know about this function. So we revert. Ben, mention something that I didn't realize. So when you're looking at the offsets in the bytecode, they're put in ascending order. And so Ben was, you know what you could do is you could try and work out a function name that produced a function selector with a low value. So it was going to be higher up. So your important and oft used functions would be higher up.
00:40:08.410 - 00:40:52.010, Speaker A: But an extra fun fact from myself is that you've got to be careful with that, because if you have more than about four functions, so maybe eight functions, it starts to do a binary search. So essentially it says if it's greater than this, jump to here. Otherwise go straight ahead. And I think I'm seeing a few that have got, say 32 different functions in a solidity contract. And I'm pretty sure they actually do a binary search until they have only about four functions or so. So yeah, you'd want to actually look at your code before you started doing micro optimizations like that. Okay, let's focus in on this set function.
00:40:52.010 - 00:42:33.450, Speaker A: So we've got a parameter that we are expecting to be passed in via call data, and we're going to set it to that very first storage location. So the first thing code does is it sets up a return address or an end address. And so what this is, is that the code invariably has some code that it's going to call right at the end to exit, whether it be just to stop or maybe to return a value. And so invariably the way the compiler is written is such that when you enter a function, you push this address that you're going to go to at the end onto the stack. And then what we do is we work out the call data size minus four. So essentially, how much call data do we have? If we exclude the function selector? And before we jump into analyzing the next block of code, something to understand is, so if you ever see Lt followed by is zero, you've got to think that what we're doing is so less than would set one if it was less, and then is zero, will set it to one if it's zero, or set it to zero if it's not zero. So essentially Lt followed by is zero is going to give you not less than or greater than or equal to.
00:42:33.450 - 00:44:04.080, Speaker A: So what then? So we had the call data minus the size of the function selector, and then we push 20, which is zero x 20, which is one word, onto the stack, and we say, well look, if we don't at least have so greater than or equal to one word, then we should just keep on going straight ahead and revert. But if we do go to location 55, so then we pop a value off the stack, get rid of it, and call data load. So what we want to do is we want to load the word at offset four from the transaction field onto the stack, and then we push seven b and jump to there. And so that's essentially seven b and then jump is just a hard jump. So yeah, what you can think of this is saying is push underscore param onto the stack and then jump. And then once we get to seven b, we push zero onto the stack and then s store. So we're storing at location zero the value that was next down on the stack, which was the param.
00:44:04.080 - 00:45:25.676, Speaker A: So store underscore param at location zero, and then we jump to the location we've set up earlier, five b, which is there, and then we stop. So stop means end execution, indicating that the execution status is successful. And don't return any data though. So this is stop is what you'll get if it's a transaction which doesn't return any data. All right, what about Val two, which was returning un 256? So focusing on that. So it's just a view call that's auto generated and its signature is just Val two brackets, all of the view returns and brackets, you went 256 is not part of the function signature. So if we jump to there, first thing it does is it sets up its return address, which is 63, and then it loads the storage location one and duplicates it.
00:45:25.676 - 00:46:09.630, Speaker A: Actually dupe two, which is so dupe two is give me the value from not the top stack, but the one below the top stack. So if you recall, 63 was the return address, essentially. So s load will have pushed Val two onto the stack. And now we want to get 63. And so we put that there and then we jump. And once we get to location 63, now we've got this on the stack, and you'll notice that the stack one is never going to be used, but fell two will be. So stack zero.
00:46:09.630 - 00:47:14.560, Speaker A: So we load up the free memory pointer, and then we manipulate the stack, and then we duplicate that free memory pointer. So we swap the free memory pointer down and then duplicate it back to the top of the stack. And then what we do is we store Val two to that location pointed to by the free memory pointer. And then what we do is we load up the free memory pointer again. So we load the free memory pointer again. So we've got a copy of it, and we do a bit of stack manipulation. And then what we do is we say, well, essentially creating how big is the return value that we're going to be creating and what's its offset.
00:47:14.560 - 00:48:13.608, Speaker A: And then after all of that manipulation, we're then ready to return. And so remembering return says stack zero is the offset of the result, and stack one is the length of the result. And so what you're doing is you're saying, all right, where the free memory pointer was pointing. So zero x 80, I've got one word, and please return that, which is the value. So Val two is at that location. So putting all of that together for that block of code, what it was really doing was it was returning a uint 256 that was put into memory. So let's focus on, actually this is Val one.
00:48:13.608 - 00:48:51.590, Speaker A: This is an error in the slide. We're now focusing on Val one, not Val two, because we've already looked at Val two. Oh, sorry. So what this is showing you, rather, is that what we were trying to do is return Val two, which is what we've just done. So now focusing on Val one. Okay, so the first thing we're doing here is we're setting up the return address again. And something to note is zero x 63 is the same return address that we used for returning val two.
00:48:51.590 - 00:49:33.040, Speaker A: So we're reusing the same code between two different functions, because that piece of code, all it was doing was saying I've got a UN 256 and I want to return it as the result. And so because it's common code, it can be reused between the two functions, the two view calls, and you notice that when it's getting to zero x 87, we're doing an s load from location zero. So very similar code, and it's just essentially reusing code, which is interesting. It means your bytecode will be smaller.
00:49:36.900 - 00:49:51.540, Speaker B: So what about Peter, I have a question for you. Is that the result of the compiler optimization? If you would turn optimization off, would those have been separate? Would the duplication have been present in the bytecode?
00:49:54.300 - 00:50:30.470, Speaker A: That is a fantastic question. I suspect the answer is correct. Yes. A true statement is that the code has been generated with optimization set to on. Yes. So you're probably correct that there will be things that duplicate code if you don't have optimization on. I suspect though, reverse engineering or like getting the opcode from the bytecode would tell you for sure.
00:50:30.470 - 00:51:39.370, Speaker A: So what about if you want to be able to have people send ether to the contract? And so what you do then is so you could have a function which accepts is payable, but if you just want to have normal transfers, normal non function call transfers, you need to have what's called a fallback function and it needs to be payable. So as well, a fallback function is the COVID all. Someone's done a function call to your contract, but the function didn't match any of the ones that it's got. So you still want to accept that function, call and do something. And in particular you want to accept the ether sent to it. Yeah.
00:51:42.300 - 00:51:58.000, Speaker C: Didn't they make some breaking changes on v six around fallbacks? Now I know your compilers four, but I think they've changed the fallback into two functions now on version solidity. Yeah, I'll post a link on the chat.
00:51:58.740 - 00:52:22.840, Speaker A: Yeah, please do. I compiled this using version five point something, or rather, yeah. All right. Interesting to know. I know that there have been a few breaking changes with respect to fallback functions. So fallback functions are one of these things that I know well. When I first saw them, I thought, oh yuck.
00:52:22.840 - 00:53:28.190, Speaker A: The idea of someone doing a function call to your contract and the function not matching anything that you've got but you accepting it anyway seemed a bit dodgy. But yeah, having a payable fallback function makes sense if it's just to accept a direct ether transfer, but a function call that didn't really work should just fail. All right, thank you, Lucas. Yeah. So if you do have that callback function, how does stuff change? So now if the call data size is less than four, then we're going to jump to zero x 30, which is now going to, rather than being a revert, is just stop, which means that the ether will be accepted. And additionally you've got your normal function block. But if none of them functions match, you'll just fall through.
00:53:28.190 - 00:54:19.550, Speaker A: And again, you'll just stop and accept the ether. Another side effect of it is because you've got a payable fallback function, but the other functions are non payable. Now, each and every function needs to check whether the call value is zero or not. So you'll end up with more code. Okay, so the next area of the talk is about storage. So, storage is set up as 32 byte words. And variables that are smaller than 32 bytes will be stored in the same word.
00:54:19.550 - 00:55:23.040, Speaker A: And so you end up with something like this, where the byte is stored by itself in storage location zero. And then you've got a whole word. So that's in storage location one, and then you've got storage location two and then three. And so because they perfectly match the right size, the UN 32, 64 and address are all pushed in together. If you had a bit of spare size, though, you could imagine you could have moved your storage around your ordering of your variables. So the byte actually could have been part of that word, that storage location too. So remembering as well that I'm setting up each of these memory locations to 1011, 1213, 14 and 15 hex.
00:55:23.040 - 00:56:04.188, Speaker A: So this first block of code is going to set that initial byte to ten. So what we do is we load up the location. So even though the only thing stored in this storage location is that byte, the code isn't smart enough to work that out. So it's still going to assume that this is just one byte of a word that is being used. And so it loads up. Just one byte pushes the zero x ten, the value that we're going to store there. And then it loads up ff and does not.
00:56:04.188 - 00:56:58.690, Speaker A: So that'll end up giving you zero x fff with the bottom. So essentially a mask, which you then, and with the current value in that storage location and then, or in the value that you want to put there and then you store it. So because it's only one byte, you're masking it in. If you compare that, if you had have had that same value as just a. You went 256, then you would have only had these three opcodes here. So you would have used up less, fewer opcodes. I mean it should be of course noted that push and swap and other opcodes like that only use tiny, tiny, tiny amounts of gas compared to s store.
00:56:58.690 - 00:57:54.210, Speaker A: So it has very little actual effect because s store is 20,000 or 5000 depending on if the memory or storage location has been used before. Whereas some of these others are only say three or five or something like that. So for storage location two, we've actually got quite a lot of code. And so what's happening here is that we load it up at the top of this code block. We mask in each of those fields, each of those values and then we store it. So this is important because every time you do an S store you're using 20,000 gas or 5000 if the location has already been used. So just doing one s store for all that is important.
00:57:54.210 - 00:59:02.980, Speaker A: Having these three variables in one location means if they're being manipulated at the same time you're going to use fewer s loads and s doors which is going to affect the amount of gas you're going to use quite substantially. And again, this one here is just a word. So what this is telling you is that well laid out storage variables will result in fewer locations and fewer locations. Well, I say will, but it's probably could save you money. It will depend on your usage patterns because it really comes down to how many times do you call that s store and s load operation. So in the example there, there was no way of having that valve byte, that first byte in with the other storage locations. But if there had have been a spare room, you could have pushed it in with them so that you would have had fewer storage locations.
00:59:02.980 - 00:59:53.990, Speaker A: Okay, bytes and strings. So bytes and strings are variable length byte arrays and they behave in the same way. So you can do something like this where you're pushing a byte onto the end of an array. And so the bytes is allocated a storage slot given the code. And we only had. So here we've only got one value that we're in having in storage. It'll be stored in storage slot zero.
00:59:53.990 - 01:00:51.560, Speaker A: And you'll have to excuse this diagram. A few people have pointed out that byte zero off to the far right here should be as big as byte one and all the other bytes. So it's not that byte zero has got more bits than byte one. It's just that it was harder to draw it. But so if you've only got zero to 31 bytes, then your byte zero has the bottom bit set to zero to indicate this is a small number of bytes and it has a length field which is zero to 31 and with the top bits being zero. And then it allocates bytes starting at the top of the overall word and going across. And so all of this is stored in the actual storage location for the variable.
01:00:51.560 - 01:01:07.580, Speaker A: However, if more than 32 bytes are stored, then the marker. Sorry about that. Susie's obviously got some thoughts on variable length byte arrays.
01:01:09.840 - 01:01:10.924, Speaker B: As well she should.
01:01:10.962 - 01:02:07.250, Speaker A: Peter. Yes, so the marker bit indicates that you've got 32 or more bytes in it. And so what happens now is the storage location zero just holds the length of the overall bytes array and then the storage location which is given by the message digest. So KcaC 256 of the storage slot number. So we're in storage slot zero. So that's going to be kecac of zero and that is going to be where the bytes are stored. And if you've got, say, more than 32 bytes, then byte 32 and more will be stored at kickac of the storage slot plus one.
01:02:07.250 - 01:03:11.764, Speaker A: And so if you had two bytes arrays, then one of them will be, say, in storage slot zero and another one will be in storage slot one. And so if you do a kick at 256 of zero and one, you'll end up with a completely different part of the number space and so you're not going to have collisions. So what does this actually look like in bytecode? In opcodes? So we push that return address onto the stack. To start off with, we work out, get rid of the function selector from the call data size. And then what we do is we check the length of the variable that's been passed the parameter. So you might recall we're only after one byte. However, in solidity and in the ABI, all parameters are passed in as words.
01:03:11.764 - 01:04:12.890, Speaker A: So 32 byte words. So even though we've only got one byte, it's passed in as a whole word. And so what we do is we check that we've got at least 32 bytes, and if we don't, then we revert. And so then what we do is we load up the call data, the word, and we create a mask. And so you could go push 32 and zero x and lots of f's, or even push 31 and just lots of f's. However, these op codes here, 44 c are, we should be right now. Everyone, my kids are home.
01:04:12.890 - 01:05:23.296, Speaker A: Yeah. Yes. So if you look at that, the fact that we're from 44 to four c, we're setting up a mask. And the reason why you're doing it that way, rather than push 31 all f's, is because this ends up being fewer bytes. And so again, to David's point earlier about optimization, I am pretty sure if you had the optimizer off, you would be pushing zero x fff. So we mask off using that end operation to mask off the parameter, and then we jump to 53 hex, okay? And then we do some stack manipulation and we load up storage location zero, and we mask off the length part of storage location zero. So three f.
01:05:23.296 - 01:06:46.808, Speaker A: So three f is 31 shifted by one and with the one extra bit and you and that in. And so if you can think, I'm going to go back a few slides here. If you can think of this length field, we're trying to mask off the length and the bottom bit, then what we do is we push three e onto the stack and we say, well, is this length plus that bit perfectly equal to three e? Because if it's perfectly equal to three e, then that means that the actual length is one f and the bottom bit is zero. And so if it is, then we jump here and then we've got the stack looking like that. And then we store the byte, or we store the, we set up the memory location to be zero. M store the memory zero to be zero. So we're using the location zero as a scratch pad.
01:06:46.808 - 01:07:44.270, Speaker A: So you remember how we had zero and then we had 40 hex was the function, the free memory pointer. So location zero is a scratch pad. And so we use that scratch pad to record essentially the storage location that we're going to use. And then we use kick at 256, which is denoted as Sha three. So though the opcode says Shah three, it's actually doing a kick at 256. And so what that's doing is it uses the memory location pointed to by the stack to do the message digest. And so it's going to do a message digest of offset zero in memory for 20 bytes or 20 hex bytes, which is that one word.
01:07:44.270 - 01:08:54.602, Speaker A: And then what it does is it gets rid of the length byte that currently exists, and it stores the current information that is currently stored in the bytes into that kekac of zero. And then it stores the updated length. And so the reason why it's doing this is that now we're in a good state to actually process the bytes. All of the work that we've done before, this has just been saying, all right, we've gone from having 31 bytes, now we're going to have 32 bytes. So we need to change the way the length field operates. And so you can see there's an awful lot of code there to manipulate and get bytes in just the right location. It so that's bytes.
01:08:54.602 - 01:09:57.380, Speaker A: Another thing we've got is variable length arrays. So storage arrays, and so you can have Uint 256 arrays, byte arrays, you can have arrays of pretty much any of the types, including complex types of. So values for an array are stored at a memory location, which is given by the message digest of the storage slot number plus the offset. So I say key, but it's really the offset in the array. And so you store the value into that offset. And so the number of elements in a dynamic array is stored using that formula. So storage and then slot number.
01:09:57.380 - 01:11:11.750, Speaker A: So as you make the array bigger or smaller, you put it into that value there into the storage slot. Zero, say, for the array, for mappings. Mappings is a way of mapping between one type and another type in a sort of key value store where the value can be arbitrarily complicated. So for mappings, what we do there is you have the key and the storage slot number and they're concatenated together. And then you do the kekac of that and you set the value to that location. So you should note that for mappings, nothing is actually stored at the storage location, slot number itself. So you can have mappings of mappings and mappings of arrays, arrays of mappings, et cetera.
01:11:11.750 - 01:12:24.670, Speaker A: And so what you do there is you do recursively do that Kicak 256 to work out the location where the actual data should be stored and have a look at that link for details. So we've talked a little bit about memory as being a scratch pad. And you might be thinking, well, I could write some arbitrary code, EVM code, and I could push, I don't know, a huge number onto the stack and I could say store zero at that location and go m store. And then these ethereum clients would have to try and allocate that memory and then they'd crash because they'd run out of memory. So each time you use an extra memory location, it costs gas. So you have this memory expansion thing. And so if you memory, you can probably run or you will run out of gas.
01:12:24.670 - 01:13:40.314, Speaker A: And so before you cause an out of memory error on the Ethereum client, you're going to run out of gas. And so I guess what this is saying is that you want to be a bit careful about say, allocating huge arrays or huge other variables as memory because they cost gas. So the layout of memory is so the first 40 hex bytes are a scratch pad and they're used for hashing methods for when you're doing things like creating, working out the location variables should be stored in storage for mappings and arrays. The next location is that free memory pointer. Then you've got zero, and then you've got your location that your actual memory starts at. So if you say create a Uint array in memory, then it'll be stored starting there, for instance. So Peter, can I ask a quick.
01:13:40.352 - 01:14:03.200, Speaker B: Question on memory before you go on? Yeah, so I don't see any protection in Ethereum unless it's implemented at the client level that I'm unaware of. Or maybe, I guess potentially at.
01:14:05.110 - 01:14:05.474, Speaker A: Maybe.
01:14:05.512 - 01:14:59.060, Speaker B: The web three layer. If you were to create a client and you had your own implementation of soliditywrapper Java or web three library, you could inadvertently or advertently create a situation where memory just was terribly corrupted and it wouldn't be hard to do that. I guess Lucas is about ready to find out how hard that to the, it's really up to the client to keep any memory corruption issues happening I guess, is my point. Right?
01:15:00.550 - 01:16:04.600, Speaker A: No. So memory, it's the solidity compiler, so very much in Lucas's domain to make sure it's okay. One thing that would be interesting, I think if you tried really hard, I reckon you could get two storage locations to overwrite each a. I haven't tried it, and so maybe there's some protection against it. But if you can imagine that you've got a kick app, 256 of say storage slot zero is an array starting at that location, and then you've got another one starting at a different location in the overall storage. So if you did a certain offset into that array that just happened to match the other value, I do wonder whether you wouldn't overwrite it. So that's storage, but you would have to try hard.
01:16:04.600 - 01:16:09.000, Speaker A: Sorry, go ahead.
01:16:09.530 - 01:16:51.430, Speaker D: Actually there is an attack, a known attack like that. Turns out that there is no protection against underflowing the index of an array. So even though it can be, I mean, in principle they are very far away one from another because of the hashing of the addresses. But still it's doable. And I think it was done even in some known attack. So yeah, the summary is it can be done not because of hash collision, but just because of using the way in which the index of the array is moved.
01:16:53.850 - 01:17:38.594, Speaker A: So I guess what that's showing is that if your solidity code essentially took an array index via the API and then fed that in then. Yeah, that would be an interesting attack to do. We should do a talk at some point where we specifically write the code to do it. Yeah, I'm sure we could get that to work. Shouldn't be too hard at all. All right. It'll take a day, take a day to actually get it to work and write the slides.
01:17:38.594 - 01:18:52.290, Speaker A: How's that? But as far as memory goes though. No, look, I used a fair bit of Thursday and Friday on these slides, so I've done my slide writing anyway. I've got another talk in two weeks time, so that's going to burn time. But now we'll do that as another talk for sure. But yeah, I think in memory though, you don't have dynamic sized arrays and so it's all fixed size. So I think for corrupting the actual memory compared to storage, I think you're going to have to have a bug in the solidity compiler do it. Actually, before I go on, there is one extra point to make, and that is you might recall how each account storage is completely separate.
01:18:52.290 - 01:19:48.778, Speaker A: So what that means is even if your contract or someone else's contract is a bit buggy and allows people to do all sorts of nasty things, their contract cannot trash your contract because you've got a completely separate storage area, which is really important. Okay, so yet another location that you can access stuff from is code. And so you can use code copy or external code copy. So code copy copies information from your contract into memory. And we've seen it was used in the init code fragment, but it can use it for other reasons. And you can use external code copy as well, which copies from a different contract. And so I've seen this used for revert reason error messages.
01:19:48.778 - 01:21:34.750, Speaker A: And so I dare say if you've got any static strings, then they would likely be stored in code and not say in storage. Also though, if you wanted to have your contract as a contract that deploys another contract on demand, you can imagine you would load up all of that from your own space. And the reason why you might say have some static data stored in a contract rather than S store and s load is because S store and s load are dramatically more expensive than code copy and writing to code. Okay, before we end, there is one extra area to look at, and that's Aux data or auxiliary data or metadata. And so modern solidity, and I'm not sure which version this is from, but has optionally has metadata added to it and it's bytes that are added to the end of the contract in seabore encoding. So that's a binary encoding format, and what it does is it indicates the swarm or IPFS message digest. So it's the message digest of the source code and it indicates that this code is stored on swarm or ipfs at this location.
01:21:34.750 - 01:22:41.030, Speaker A: And of course you've got to upload the code yourself. But the idea is that in combination with the compiler name and version which are also there, and whether it's an experimental version indicates to you how to compile the source code to get this bytecode. And so you can use it to automatically verify that the deployed contract matches the source code, because you can have these tools automatically scan. So if you have a look at the orgs data, you've got something like this where you've got a map and then you've got a length and so BZ R zero with swarm, but you could also have BZR one or ipfs and then you've got a length of your message digest and then you've got some characters which might say Soul C and then the version of the compiler. So there you are, Lucasol is using five point ten. Nice. Yeah, and have a read there of the docs.
01:22:41.030 - 01:23:59.034, Speaker A: So in summary, the EVM is a stack based processor that has access to all sorts of stuff, including the stack and so call data read only. It's the transaction parameters memory, it's a temporary storage area and it's not persistent. Storage is persistent and it holds the world state or is part of world state code. You've got your source, your actual code, you're executing, plus it's got static data. And also output wise you've got a write only event log and there's so much more to cover that wasn't covered. And like cross contract calls, what do they look like? And why do you use the various types of cross contract calls? What about error handling? So exceptions have been added recently, and we've also got asserts and requires and reverts. So how do they all interact? I didn't really talk about logging.
01:23:59.034 - 01:25:24.940, Speaker A: And what do they look like at the bytecode level? Are they efficient, how do you make them efficient? I didn't talk about bytes 32 or bytes one, and compare that to UN 256. And so probably there's more work to be talked, more to be talked about about memory variables. So we've been doing a project on code mercury, and so we've been analyzing all of the contracts on main net, and there are some really weird and wonderful contracts out there and you could almost do a talk, just looking at and trying to understand what was the developer thinking and what does that contract actually do and why? It might be interesting to look at, say, a real contract that people use a lot like ERC 20 or maybe some of the other important contracts. And we've already added another bullet point to the list. Try and write a contract that has a storage collision or a storage overwrite. But it'd be interesting for people to have a think about what else they would like talked about. So this will appear on YouTube soon enough, and when it does, please tell me if you've got any ideas, put comments in.
01:25:25.470 - 01:25:28.250, Speaker C: Is there any decent decompilers?
01:25:31.550 - 01:25:35.406, Speaker A: Oh, from bytecode to solidity, do you mean?
01:25:35.508 - 01:25:42.800, Speaker C: Yeah, or some kind of more readable markup language. It could get messy, right?
01:25:43.330 - 01:26:27.886, Speaker A: Yeah, I haven't seen any, but I haven't looked. So I've written some code that does static code analysis, which at least will tell you which of the bytecode is part of which function and gives you the call path. It also shows you what the stack looks like at every point of the code execution. So it's not a decompiler, but it certainly helps you understand what's going on. And it also takes bytecode and gives you that printout that we've seen during this talk, but not solidity. But yeah, I don't know, I'd do a web search, maybe someone's got something, but I think it'd be pretty hard.
01:26:27.988 - 01:26:38.340, Speaker C: Yeah, you couldn't get any function names or that kind of thing, so you have to kind of make them up like function one or something. Function two. Anyway, something to think about.
01:26:38.950 - 01:27:47.014, Speaker A: Yeah, I don't know. I think given it's only a 32 bit space, you could scour the Internet for all of the solidity function names, for all the open source projects, and I reckon you would get a pretty big data set that could be used for reversing what the function could be, and then based on the parameter usage, that would give you an idea as well. Okay, so we're going to be doing a few more talks, and in two weeks time I'm going to talk about code localization. And then in four weeks time, Ragavendra is going to talk about polynomial commitments and point proofs. So code localization is part of Ethereum one X, and it is related to stateless. And so essentially, how do we have the ability to not have to send all the state? So part of the state is the code that's going to execute. So how do you work out which bits of code to send.
01:27:47.014 - 01:28:44.022, Speaker A: And so that's what code merklezation is all about. Polynomial commitments is a thought that maybe there's a way of replacing the Merkel Patricia try and to do proofs and so they offer the promise of much smaller commitments. Then on the 13th, which is a Monday, we've got a conference coming up. And so we've just finished the call for speakers just yesterday. And so the early bird registration which ends on this Friday, is australian dollars 20 and standard registration after that will be 40. So we've got a whole stack of potential speakers and David Centre and I have got to work out who the speakers are actually going to be sometime tomorrow. But it looks like it's going to be an awesome world leading lineup.
01:28:44.022 - 01:29:26.082, Speaker A: So please consider registering for the conference in August the fifth. We've got Tim is going to give a talk on EIP 1559, which is all about fee markets. And so he's going to talk about the proposal and the various alternatives and just how this can be approached. So Tim actually lives in Canada, so we're going to do the call a bit earlier than normal. So 3 hours earlier. So please consider coming to those talks. Are there any questions how that Lucas has dropped?
01:29:26.146 - 01:29:34.060, Speaker B: We can probably safely say it'd be nice to have a future talk from Lucas on his compiler if he gets some progress made on.
01:29:34.670 - 01:30:16.440, Speaker A: Sure. So was that talk? Oh, I can see we've got a ton of stuff that's gone flying by in the chat. Ah, anyway, hopefully my audio didn't break up too often. Yeah, that's good. Thank you. All right, cool. Great talk.
01:30:16.440 - 01:30:32.860, Speaker A: Thank you. All right, well everyone have a great day and I'll talk to you all later. Bye bye. Thank you. Bye. Platform.
