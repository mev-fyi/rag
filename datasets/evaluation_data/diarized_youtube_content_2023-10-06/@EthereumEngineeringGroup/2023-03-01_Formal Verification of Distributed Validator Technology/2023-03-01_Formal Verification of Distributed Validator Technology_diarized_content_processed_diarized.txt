00:00:02.730 - 00:00:20.430, Speaker A: Hello and welcome everyone. I'm Peter Robinson and this is the Ethereum engineering group meetup. Today we've got Roberto and Tan Hai who are going to talk about the distributed validator protocol. So before we dive in, Roberto and Tanhai, why don't you introduce yourselves?
00:00:22.690 - 00:00:49.770, Speaker B: Sure. Hi everybody, I'm Roberto. I work for consensus. Specifically, I'm in consensus R D in a team that uses formal verification applied to distributed protocols to analyze the security. And we are mainly currently focused on protocols as they relate to the Ethereum blockchain over to Tanai.
00:00:51.550 - 00:01:10.066, Speaker C: Thanks, Roberto. Hi everyone, I am Han Hai and I also in the same team with Roberto and I focus on homo verification of distributed protocols. Yeah, it's me.
00:01:10.248 - 00:01:11.522, Speaker A: Okay, cool.
00:01:11.656 - 00:01:12.580, Speaker B: Well, welcome.
00:01:16.070 - 00:01:29.020, Speaker A: I'm interested to. You'll probably tell us when you dive into the talk, but how does this distributed validator protocol relate to Ethereum too? Or should we just watch? You'll do the talk and you'll tell us.
00:01:32.190 - 00:01:37.382, Speaker B: Yeah, it's probably. Let's leave this question to the end if it gets answered.
00:01:37.526 - 00:01:38.220, Speaker C: Okay.
00:01:39.950 - 00:02:38.238, Speaker B: All right, thank you. So the agenda for today is to try to answer to five key questions. One is why do we need a distributable data to start with? And why do we need to formally specify and verify the distributable data technology protocol? How does our formal specification look like? How does our formal verification look like? And finally, what we've actually been able to achieve so far and what is further ahead in our plans. So let's dive directly into the first. Oh, sorry, before I do this, there is an acknowledgement I need to do. This project is actually supported by a grant by the Theory foundation bubble and SSV network and was done by consensus RnD. So let's dive into the first question.
00:02:38.238 - 00:03:36.830, Speaker B: Why do we need a distributor validator? Well, let's look at how a normal Ethereum validator, which we call centralized Ethereum validator and the reason why we call it centralized will be clearer sooner. So an ethereum validator, as you can see, is composed by the architecture, usually splits it into a bitcoin node and the validator client. However, this is not really a distributed architecture. Architecture is just a separation. This architecture is subject to a single point of failure as bitcoinode and validator client play two different roles. And problem that you can have are software bugs, network failures, or our outages, just to mention a few. And also you have another problem, which is that the key that is held by the validator client can be hacked.
00:03:36.830 - 00:04:38.050, Speaker B: So someone and hacker can potentially steal the key. And we'll see soon in the next slide what this means. So let's have a look at, okay, basically now we know that this is a system that can fail. There is no redundancy if you want to it, but let's look at what the consequences are. Let's look initially at the problem of having an unresponsive node. So we have the blocks at the top, they symbolize the slot in an Ethereum protocol, and B is a slot where a node is supposed to propose block A is where it's supposed to create attestations in the Ethereum protocol the entire time divided into slots. For some slots, one validator is chosen to be the proposal, and a set of validator is chosen to be basically voting for the current head of the chain.
00:04:38.050 - 00:05:26.870, Speaker B: So when everything is okay, when it's our turn to propose a block, the Ethereum validator is alive. It proposes a block to the Ethereum network and therefore it gets rewarded for that. Similar when it's the turn to create attestations, the Ethereum validator create a testation, submit them to the network, it gets rewarded. Now assume that the Ethereum validator fails. So any of the two components, for example I showed you before, fails. What's going to happen is that when it's our turn to propose a block, well, we don't do that and therefore we don't get any reward. Worst is when it's actually our turn to create attestations.
00:05:26.870 - 00:06:10.926, Speaker B: So we failed. We don't create attestations, which means that we missed the reward by worse incorrect penalties because we didn't create an attestation. So the consequences of having a failure in the node is that we can miss rewards and we can also incur penalties. And now let's look at what happens actually indicator that validator key is compromised. One thing before diving into it, I want to remind that in Ethereum there are two type of keys. One is the withdrawal keys, which is used to withdraw when this will be available to potentially withdraw the stake. Another one is the signing, which is used just to sign attestation.
00:06:10.926 - 00:07:19.690, Speaker B: So let's assume that just the validator key is compromised, not the withdrawal key. In this case, what can happen is that the malicious node can create, can sign two attestations that are slashable. We'll see later what this means, but let's assume that basically they are violating some fundamental rule of the protocol and the node can then create a slashing proof, can package all of this into a slashing proof using their own key. So using the malicious validator key to sign the slash improve, submit this to the network. As a consequence of this, the honest validator gets slashed. So basically some of their stake gets taken away, and some of this stake is actually transferred to the malicious node. So in a way, even though the malicious node does not steal the withdrawal key, it's still capable of taking some of the funds away from an honest node and increase their own balance.
00:07:19.690 - 00:08:25.210, Speaker B: So these are the two main consequences of what can happen if an Ethereum validator fails or the signing key is stolen. Now, one might think of a solution, just simple solution, which is let's run multiple validators so that we have redundancy. Okay, we still don't solve perhaps the problem of the validator key being stolen, but at least we have the redundancy. Well, this is perhaps what someone did at the beginning when Ethereum bitcoin chain was launched. But this proved not to be a direct solution, as this is actually almost the right recipe to get slashed. Because if these multiple validators instances, they don't communicate with each other, they can create conflicting blocks or node or attestations, which then causes them to get actually snatched. This is what the distributed by data technology protocol addresses.
00:08:25.210 - 00:09:28.690, Speaker B: This is the problem. And to be clear, this is a protocol that was we didn't design, it was designed by the theorem foundation. And the basic idea is to create a middle layer here, which is called distributor validate, which is represented by the distributor validator client, which connects the BICO node and the remote signer. The remote signer, you can think of it as an equivalent of the validator client I showed you before. The only difference is that the remote signer accepts requests to sign a block or an attestation, where validator clients do that on their own. They don't accept request to sign, they just sign blocks and attestation. Because of the architects of the protocol, in this case, there was a need to replace validator client with thermo Signer, but they basically fulfilled the same role, which is to sign blocks and attestations.
00:09:28.690 - 00:11:11.490, Speaker B: So essentially what we have is that we have a distributor validator signing key, and this is split into different shares, each share goes to a different signer, and then we have the disputability data protocol, which is responsible for coordinating the various components and making sure that the overall system is seen as a validator. And essentially, if you can think of this as the component of DVT, are threshold signatures, a consensus algorithm to get agreement and to coordinate everything put together with additional logic to fulfill the requirements of the protocol. So let's look at the different components. As I mentioned, we have threshold signatures. The way threshold signatures come into play into the protocol is that where the validator signing key is split into n shares, and these n shares can be used to reconstruct the signature as if it was signed by the original validator signing key. One of the properties is that you need at least m out of the n shares in order to be able to reconstruct a valid signature. If you have less than m, you can't.
00:11:11.490 - 00:12:07.320, Speaker B: It's highly improbable that you can construct a valid signature, and this is what essentially provides key theft resilience to the protocol. We'll see later how this interplays with the overall the other component that we have is a consensus algorithm, which is in this case is a byzantine fault tolerance consensus algorithm, which means that it can operate correctly despite less than one third of the nodes potentially being byzantine. And the properties, the main properties that this consensus algorithm has to ensures are agreement validity and termination. So agreement is depicted on the right hand side. You can see four nodes. One is malicious. The objective is that all the honest nodes, they reach the same decision despite the dishonest node trying to confuse them.
00:12:07.320 - 00:13:28.922, Speaker B: The validity property says that there is some validity condition on the decided value, and the protocol must ensure that any decided value satisfies these validity conditions. Termination says that the protocol must eventually, any honest node must eventually reach a decision. Now, in the DBT protocol itself, it's kind of agnostic of which VFT protocol is used, and as long as it satisfies these properties in a way, the idea here is that people can choose different protocols, perhaps protocols that work under different conditions, as long as these properties are satisfied. So I just want to give a brief illustration of how DBT works in the case of attestations. So in Ethereum in theorem validator world, we have the concept of ADu. Adu is something that is created at the beginning of an epoch, and an epoch in the theorem protocol we have, as I mentioned before, the time is divided into slots, and every 32 slots you have a new epoch. So at the beginning of an epoch, a new duty is created.
00:13:28.922 - 00:14:53.206, Speaker B: And this duty basically says when the validator is supposed to create attestations and what its position in the committee of validator is. And this index is something that must be included in the attestation that is created. So, okay, we have a new epoch, a new duty is created when it's the time for the validator to create a new testation what the different distributed validator client composing the distributed validator do they communicate with the consensus layer, consensus layer to start a new instance. And of course the malicious node might try to start earlier, might try to start later as it doesn't necessarily follow the property. What's going to happen then is that the consensus protocol follows the ensures the property we saw before and performs attestation validation on whatever is decided value. Eventually a value is decided. So basically all the honest nodes reach the same decision on which is the value of the attestation that needs to be created.
00:14:53.206 - 00:15:59.918, Speaker B: At this point, each distributable data kinds the decided value with their share of the sharded t and sends this decided value together with the signature. It broadcast it to all the nodes in the network. And this is what in the dispute the validator technology protocol are called attestation shares. Now these honest nodes, they don't need to follow this. They can try to send attestation shares before they try to send them later. Of course they can try to disguise the other nodes to try to get to the wrong conclusion. Now, once a honest node receives enough attestation shares to be able to reconstruct the signature, can reconstruct the signature, and can create and submit an attestation, and each node can do that.
00:15:59.918 - 00:16:47.990, Speaker B: And the idea here is that even though we can have multiple attestations created, they will be the same. So there is no issue in terms of being slashed. Now let's look briefly about what are the expected behavior of the disputeable data technology protocol. We have a property about safety. So safety, when we speak about safety in general, we want to say something bad never happens. That's high level concept. So one property that it's important for the distributable data technology protocol is that a distributable data staking private key is secure unless security is compromised at more than two thirds of the distributable data client.
00:16:47.990 - 00:17:34.594, Speaker B: Another property is that distributable data never commits slashable violation unless more than two thirds of the distributable data clients are byzantine. Okay, there's an error in the slide. It should be more than one third of the. So this condition is guaranteed in this case under full network asynchronous, meaning that we allow any possible message delay. So message delays are arbitrary. We consider message might be delivered immediately, never delivered, deliver in 10 seconds, 100 seconds. We allow any type of scheduling and liveness.
00:17:34.594 - 00:18:33.240, Speaker B: Liveness says that a distributed validator eventually produces new valid attestations and blocks. And this is proven under a different network condition. This mastered under this network condition, which is a synchronous network, meaning that message delay is bounded and known. So before I hand over to Tanai, I just want to close this section talking. Who actually benefits from this protocol? Well, of course, we have stakers, store stakers, staking products and staking pools, but most importantly, the theorem protocol itself. This is because Ethereum network is only as secure as Ethereum validators operated by honest participants. And the reason for this is that the security of the theorem protocol relies on the fact that two third of the stake is in honest hands.
00:18:33.240 - 00:19:01.390, Speaker B: However, if honest participants are hacked and their stake can be slashed, or even worse, some of the portion of their stay can be diverted to malicious participants, then this balance starts to tipping over towards more favorably towards malicious nodes and therefore decreasing the security of the net. Okay, now I'm ready to end it over to Tanai.
00:19:14.310 - 00:19:25.860, Speaker C: Hi Roberto, I think the section why do we need to formal verification and specification is on you wanted to present it on me?
00:19:26.330 - 00:20:43.040, Speaker B: Okay, I can do this. Okay, so why do we need to asynchrony what is important to formally specify and formally verify such a system? Well, the good news about this is that we have removed any single point of failure and therefore we have a higher resiliency. However, we've also increased the complexity, which means that we also increase the chances of design bugs. So how can formal verification help us? So I just want to give you a feel about this and compare formal proofs to testing, which is a technique commonly used in software products to try to reduce the chances of errors. So the overall key takeaway is that formal proofs are exhaustive, whereas testing is not exhaustive. So what does it mean? Let's look at the network size. Consider, with formal proof, we can consider any network size, any number, finite size, you name it.
00:20:43.040 - 00:21:49.870, Speaker B: We can potentially build a formal proof that shows that the system guarantees certain properties against that size, whereas for testing, we need to use most sizes. Otherwise, the testing time taken by the test is going to grow exponentially. Also, in terms of byzantine time behavior considered, we can consider any type of byzantine behavior. Any type of byzantine behavior means that we can code it. Of course, we exclude things like changing an honest node states, but we don't have limitation in the terms of specifying what the type of behavior we want to specify. Whereas in testing you can only deal with simplistic byzantine behavior, like for example, on an order, stopping or sending conflicting messages, but you need to pick and choose the type of present time behavior you want. Also, if a property, if a given property is not true through the formal proof will be detected.
00:21:49.870 - 00:22:33.150, Speaker B: Basically, we will never be able to put together a formal proof that checks. Of course, it's up to the experience of the people putting together formal proof to actually reach a point where they understand that there is a problem. But at least you never get to the point where you think that something is true, but it's not true. And whether in testing, all your test might pass, but the property might still not be true. And if the property is true through formal proofs, we can prove it. With testing, we cannot prove it. We can only show that whatever test cases passes, but we cannot generalize this property to all the possible cases.
00:22:33.150 - 00:23:34.894, Speaker B: So let's look briefly at what formal verification consists of. We have a formal specification, which is a mathematical definition of the behavior of a distributed protocol, which essentially says when message x is received, that message y should be sent. We also have a definition of the properties that we are interested in. And this again is a mathematical definition of the properties that the protocol is expected to guarantee. Like never to meet a slashable fence, for example, altogether is linked by a formal proof, which is a mathematical proof that the protocol specification guarantees the property defined. Now what we want to do, we want to take this a bit a step further and have proofs that are machine checked. So proof that are checkable for correctness by computer, which provides higher degree confidence in the correctness of the truth.
00:23:34.894 - 00:24:24.850, Speaker B: And this is what we target with our work. So, concluding my section, staying for real, our target is have formal proofs of the safety property against slashing. So, want to show that a visibility validator is never slashed, can never be slashed as long as less than one third of the visibility validator clients are by and time and do have a threshold signature scheme and a consensus algorithm that work correctly. You can do this by verifying two properties. Which is one is about the fact that, honest, this will be the validator of clients never submit slashable attestations and they never submit. Now I can hand it over to Tanai.
00:24:31.490 - 00:25:22.406, Speaker C: Thanks Roberto. So I guess that everyone can see my screen. And now I'm going to show you how our formal specification looks like. And instead of a big single file, we want to split our formal specification into smaller files. And here you can see that on the slide, there are many boxes and each pop becomes one module in our specification. The first pop is about distributed validator. So this pack describes the behavior of a global system.
00:25:22.406 - 00:27:07.210, Speaker C: Is it a conjunction of the behavior of submodules. Here we have four submodules, network consensus, adversary and correct node. In our model, the network is asynchronous and for the module consensus, we don't specify the details the algorithm, we only assume about the properties that a consensus algorithm needs to satisfy. And in our model, the failure nodes are byzantine. So it means that a byzantine node can send different messages to different nodes and they can behave as they want. However, a byzantine node cannot create a signature from other node and it cannot modify the state of other nodes and correct note here is for honest node that follows an algorithm and we have n is a number of nodes and f is the number of failures. In addition, we have one more module that is for executable reference implementation of correct nodes, and our specification is written in Daphne.
00:27:07.210 - 00:28:21.680, Speaker C: Daphne is a programming language and its syntax is very similar to C or Python. Therefore, it's very easy for developers to understand the code written in Daphne. Moreover, Daphne is designed with a formal verification away and let's start from the executable reference implementation. This is the code in Daphne for a method called attestation consensus decided. And you can see that this code is very similar to C or Python. So I guess that everyone can understand what is written in this code and there's something new in this code. For example, here you can see that we have two keywords required and modifies after the method declaration, and the developers can ignore these keywords because they are only used for formal verification purpose.
00:28:21.680 - 00:29:30.150, Speaker C: In addition, we have operator for assessment propagation at the bottom of the slide. And now I'm going to show you how our distributed verdict formal specification defines a transition or move in a system. So first we need to define the initial state. The initial state is a conjunction of many states for submodules. Here we have the initial state s zero. This is the global state, and we have n minus f styles for honest nodes, and we have another state for the network, one for adversary, and the last one is for consensus layer. And the next thing we need to specify is system transitions.
00:29:30.150 - 00:30:31.478, Speaker C: In our specification, we can move from one state to another state by trigger by using the event. An event can be a new set of duties, or this is when or not need to execute some duty, or when a message is delivered. Importantly, our formal specification captures nondeterministic behavior in our module. In our model, there are two sources of nondeterministic behavior. The first one is more than one event can be triggered in one state. For example, here you can see that if we are standing at a state s. So then from state s we can move to s table by using two different events.
00:30:31.478 - 00:31:41.130, Speaker C: The first one is that note, one receives message s from node two, and the second is that not one receives message y from the three. This feature allows us to capture the semantic the behavior of a network. And the second source of nondeterministic behavior in our model is from the same event can lead to more than one state. For example, here we have an event that officially sends message s two, and why this event can lead to different state. The reason is about the behavior of byzantine node. So a byzantine node can send different messages to different nodes. And now I want to show you more details about our specification written in Daphne.
00:31:41.130 - 00:33:01.780, Speaker C: Assume that we have a transition from state s with event x. And now we can specify this transition by defining a function dvnest event whose return value is boolean. And we can move to state s prime from state s because in this function we have db nest event x s bram it is evaluated as true. However, we cannot move to state t because its evaluation is false. Now let's look at the Daphne code. So the first thing we have is a transition for honest node. This is the predicate node nest, and we have three more predicates to represent the transitions for network, adversary and consensus, and the conjunction of this predicate that presents a transition of a global system.
00:33:01.780 - 00:34:16.472, Speaker C: And here you can see that we have 25 variables like message receive messages, ten and decided value. And because there are many values for these variables that can make this predicate become true. So that's why we have a nondeterministic behavior. And in our specification, there are some part that we trust and another part that we don't trust. So you can see that here the behavior of network consensus and adversary, they are what we trust and also the definition of properties. However, we don't trust the behavior of single node and also we don't trust the reference implementation. And in our work, a manner of proof is not enough.
00:34:16.472 - 00:35:20.936, Speaker C: So that's why we want to check our proof with Daphne. So this is a formal proof. And how can we increase the likelihood that what we trust is correct? We apply for following steps. First, we keep trustic specification part as simple as possible, so the number of code lies for trusted path is very small compared to the path of non trustic. Moreover, we apply the peer review to have the feedback from other researchers and the tool we use. It has a strong response for formal verification. The last step we apply is to formal proof of safety and likeness properties, and the formal proof will highly reduce the chances of errors in our assumptions.
00:35:20.936 - 00:36:15.100, Speaker C: So let's consume a scenario that a network is completely rushed, so then no messages are delivered, and it means unit nodes cannot make any step. So it's very easy to see that in this case the safety property is guaranteed. However, we don't want to have this scenario in our application. So by proving likeness, something eventually happens. Something would eventually happen. So it means that every unished note we eventually can publish an establishation. So by proving the flatness, we can avoid unexpected scenario.
00:36:15.100 - 00:36:58.750, Speaker C: And now I'm going to show you how our formal verification looks like. So the first thing we need to do in our proof is to formalize SEMs. The SEMs are for the behavior of our trustees pass. So for example, here you can see two SEMs. They are related to threshold signature functions. In Daphne, we specify the assembs by using two keywords, demo and SCM. And the first assembly is a property of the hash root function.
00:36:58.750 - 00:38:25.490, Speaker C: So it means that if the hash of two data, d one and d two is the same, so d one must be d two. The second SCM says for every root, every signature s one and s two, and one public KPK. If we can verify that a signature of d s one and pk, and also a signature of d s two and pk, so it implies that s one must be s two. Another thing I want to mention here is that for the axiom so they don't have any demo spotting. So in our proof we use negative invariance. And here I want to show you the definition of variance and why we need to use an industry environment in our proof. So you can see that from an intestine environment provides an over approximation of reachable style.
00:38:25.490 - 00:39:15.538, Speaker C: Here, for example, we have an inertial style, and from this type we can go to other states. But this set of sties is the subset of states satisfying the thief invariant. And our proof has three steps. First we show that in need the initial state implies in the thief environment. And the second step we prove that if the current state satisfies an inductive environment, so then the next day must satisfy an inductive environment. And the last step is to show that an inductive environment implies a safety property. Sorry.
00:39:15.538 - 00:40:12.950, Speaker C: So by using an inductive invariant, so we only need to reason about two consecutive states and also for uninitial state. So they will help us to simplify the proof plot. And the next thing in our proof is about quantum intersection. So in the DPT protocol there are two important rest tone. The first one we have is that three times f plus one less than or equals to n, and f is the number of foes and n is the number of nodes. And the second restaurant is about the quorum. A quorum is a maturity of nodes and we need to have at least two n over three of nodes in the quorum.
00:40:12.950 - 00:41:18.130, Speaker C: And we have an important observation that given two arbitrary quorums, q and q prime, we know that each intersection contains at least one on its node. And before showing you more details in our proof, I want to remind you about the definition of non taxability for attestations. So our target is to prove the safety property against lashing for attestations. And to do that we need to prove for every pair of attestations I and ipram submitted by an honest note. I and ipram are non latchable. The definition of non latchability is technical and you can find the definition in the link before the image here. I just want to show you some integration of latching conditions.
00:41:18.130 - 00:42:15.590, Speaker C: So we have two cases, two conditions. The first one is that authoritator cannot publish two different votes with the same target height. This is in the left picture. And the second condition is that a validator cannot publish a vote in the span of its other votes. And now I want to remind you about the construction of an attestation. So the step one is that a consensus instant needs to decide an attestation data for an attestation duty. And the data must be validated by owners in some quorum.
00:42:15.590 - 00:43:43.730, Speaker C: Second, after receiving after knowing attestation data, T Shiran knows it to send attestation shares with the signatures and the value of D. The final step is that an attestation can be constructed by a quorum of attestation says, and this is the sketch of our core proofs. So we assume that an DVCN submits to attestations A and a prime such that a is submitted before a prom and when to prove that a and apram are non logical. So how can we do that? So let's look at the timeline diagram. So in this diagram we have two important events that are submissions of I and I prom. And let's see what happens before the submission of Iram. So we know that to submit a prom a decided data, the prom of a ram must be decided by some quarter of and moreover this data, the bram must be validated by owners in Qpram.
00:43:43.730 - 00:44:53.770, Speaker C: And now let's look at the submission of a. And because to submit a, an is not n needs to have attestation says for I from a quorum q. And if we make an intersection of q and q prime, we know that there is an is not w in both q and q prime. And because w in q prime, so we know that the blue predator is a prime. Moreover, because I is mated before a prime, so we know that an n is not the blue. It must know the value of I when it validates a prime. So then, because w is not, so we know that they are not touchable.
00:44:53.770 - 00:45:08.000, Speaker C: Yes, this catch up our core proofs. So now Roberto will present what have we achieved so far and what is left to do? Roberto, please.
00:45:14.690 - 00:46:23.318, Speaker B: So we are the last section of the talk, first out by speaking, what actually we're able to do by October 2022, which is when presented initial work at Devcon. So what we're able to do was to have a proof of this non stashable testation property, and using various assumptions, most of these assumptions, we've already mentioned them. One that we haven't mentioned so far is that this property ODs, even if the disputeable client, the bitcoin nodes connected to the disputable client, are on different fork or different forks of the Ethereum chain. And I'll explain a bit more later what this means. And they are also able to prove that the reference implementation is a correct implementation of the specification. However, there is actually a liveness issue with this protocol. Not that they have proven liveness, but by reasoning about it, we saw that about this issue.
00:46:23.318 - 00:47:20.898, Speaker B: And the issue is that what we had in this version that we formally proved around October 2022, is that each disputeability client had a queue of duties. This is because there is a similar scenario where there is with the current duty to be acted upon, it was to have a queue. Now, when a new duty is when it's the time to act on a new duty, for example, create a new testation, assume that the consensus layer is still busy. What we were doing, we are not changing the current duty and queuing the new duty in a queue. And then once the consensus layer reached a decision, we're popping a duty from the queue. We're serving the part. However, what is the issue with this scenario? Well, let's assume that we have a chain, and at some point we have a fork not in the non funnelized part of the chain.
00:47:20.898 - 00:48:13.994, Speaker B: And assume that we have four validators. Two validators are on one side of the fork, two validators are on the other side of the fork. And this is the same situation where you are where the new epochs done at that point the new duties are created. But because there are two different chains, the commit index for the same validator could be different in the two different folds. And the value of the commit index is something that is checked by the validation performed at the consensus layer. Which means that basically you end up in a situation where the consensus never terminates, and because you are using a queue, we are waiting for the consensus to terminate before acting upon a new duty, you end up in a situation where basically no new attestation will ever be produced. So the way this was fixed was with this version two.
00:48:13.994 - 00:48:59.830, Speaker B: So we don't have a queue anymore when we have a new duty. And for example the consensus layer busy at the time when a new duty is time to be served. What we do, we just start a consensus instance on the new duty so we don't wait for the old consensus instance to finish. And of course this doesn't create the doc situation I talked before, but was important to actually prove that non slash about the station. Is it correct? And that's what it does. So we have updated the format specification reference implementation to implement this new protocol. We have updated the proof of nostalgia testations for the new spec.
00:48:59.830 - 00:49:51.490, Speaker B: We've also written the format specification and reference implementation for the part about signing blocks. What is left to do is to update the proof that the distributed station reference implementation ideas to the specification, to do the formal proof for non slash word blocks, and also to do the proof for latents. And this is the last one. Thank you. Sorry, do you want to go to the next slide before the questions? With the talks.
00:49:56.170 - 00:50:09.500, Speaker A: We could do questions now. I can see Frank's got two questions in the chat. The first one was asked early on and then the second one was later.
00:50:10.910 - 00:50:54.790, Speaker B: Okay, I'll answer them in opposite order. So is there a reason to use for all post conditions instead of using parameters into the axioms? Not really that we chose to do in that way, just to present dilemma we could have done with the parameters. Personally, I find that sometimes having the for all allows to ever show the proof. Sometimes it's counterproductive as actually Daphne never terminates. And having the parameters helps definitely verify things quicker.
00:50:55.770 - 00:51:07.442, Speaker D: That is actually my point. If you have universal and quantified formulas, you may not be able to get through, whereas with parameters, if you know when you need to apply it, it's much easier.
00:51:07.606 - 00:51:28.690, Speaker B: Yeah, I think what we end up doing is that then we have a lemma using this for all parameters, I think to do exactly what is so that's the question I'm not sure. I think it refers to one of the initial slides.
00:51:30.790 - 00:51:31.202, Speaker C: Yes.
00:51:31.256 - 00:51:33.830, Speaker D: Slide 1111.
00:51:41.270 - 00:52:13.100, Speaker B: Okay, so when the decision is made, the decision is made on the, on the expect. The consensus protocol ensures is that the same value would be decided. And a property of the threshold signature scheme is that unless you have m out of n signature for the same value, you cannot construct the signature. Does it answer the question?
00:52:15.330 - 00:52:33.410, Speaker D: So it doesn't matter which subset of the validators submit? They don't submit actually, something that depends on the parameter or the number of valid, the identity of the valid validators within the distributed validator.
00:52:34.150 - 00:52:43.298, Speaker B: Oh yeah. Okay. I think the dependency comes into play when the key is split between.
00:52:43.384 - 00:52:46.742, Speaker D: Yeah, that's the sense of my question.
00:52:46.876 - 00:53:04.240, Speaker B: Yeah, that's what happened. But that splitting is not something we assume happens outside of this protocol. So we consider that the key splitting has already happened and the nodes already have their shares. That algorithm is correct.
00:53:12.350 - 00:53:14.860, Speaker A: Ron, do you want to ask your question next?
00:53:16.110 - 00:53:33.790, Speaker E: Thanks. I've got a couple of questions. I mean, one is relating to the actual representation that you're using of the crypto. Are you using sort of term algebra or are you going deeper than that into the representation of the crypto?
00:53:36.930 - 00:53:56.582, Speaker B: What we do, for example, we assume idea crypto. I don't know if this answered your question, but for example, for the ashing tonight, I showed you, for example, we assume ideal ashing, the two ashes are the same. That means that there is no resume, no collision, for example.
00:53:56.636 - 00:54:34.020, Speaker E: Yeah, I'm worried specifically with respect to the structure of the messages that the byzantine nodes might be cooking up. It's very typical in reasoning about cryptographic protocols that your adversary might be sort of cutting bits out of messages that have gone across the network composing together in complicated ways, and do attacks based on that sort of constructing of messages that end up deceiving the good players. So you're doing any of that sort of representation. Does that not come up?
00:54:35.770 - 00:55:24.100, Speaker B: No, not in this case. If there was a message containing multiple signed messages, we'd allow by and time validators to reuse these signed messages inside the more complex. Just in this case, we assume that Byzantine Ecto is not able basically to send a message signed by an honest validator unless it has already received it. That's kind of the assumption we rely on.
00:55:25.910 - 00:55:41.660, Speaker E: Okay, so then the second question relates to you're doing this in Daphne. So I'm just wondering how you're representing an arbitrary number of nodes which has some complicated code attached to it.
00:55:43.230 - 00:55:50.650, Speaker B: We basically have it as a parameter on one instance.
00:55:56.590 - 00:55:59.390, Speaker E: Each of your proofs is for a fixed number of nodes.
00:55:59.730 - 00:56:09.726, Speaker B: It's for any finite number of. What we have is just me or.
00:56:09.748 - 00:56:12.930, Speaker A: Is everyone lost audio from Roberto?
00:56:14.630 - 00:56:17.000, Speaker C: No, it's just you.
00:56:21.050 - 00:57:03.826, Speaker B: So we have what we do. In the initial state, we pass a configuration parameter which contains the number of nodes in the system. And our proof now approved, we let that number be an integral. Don't know if this answers the question. And then we use basically for all, we have a lot of, for all statements to say something is true and for all, then honest notes. For example, in the initial set, I.
00:57:03.848 - 00:57:21.082, Speaker E: Say basically that an environment object which is representing the states of all of your nodes, but then a single copy of the code for the nodes that just runs on the selected component of the environment state. Is that what you're saying?
00:57:21.136 - 00:57:21.740, Speaker B: Yeah.
00:57:22.270 - 00:57:29.390, Speaker E: Okay, that's my questions.
00:57:38.610 - 00:57:39.918, Speaker D: I've got two more questions.
00:57:40.004 - 00:57:41.940, Speaker C: If no one has a question.
00:57:45.830 - 00:57:56.242, Speaker D: Okay, so one is related to the structure of the proof. So maybe I didn't pay enough attention.
00:57:56.306 - 00:57:56.920, Speaker C: But.
00:57:58.730 - 00:58:42.226, Speaker D: If I had to do the proof, I would probably try to have sort of one validator and prove a standard validator and prove that this is correct. And then prove that a distributed validator implements or refines a validator. Right, because you already have a proof for one validator. It was designed in cock, for instance, let's say by the runtime verification team. And in your own work, you said that it was parametric, your proof. So it holds for a zero or one validator in the set. So I would have probably tried to have a proof showing that assuming that I've got a validator that's honest and I've got a distributed validator, the two, let's say, notion of being honest, one was implied by the other and have very fine and proof like that.
00:58:42.226 - 00:58:45.640, Speaker D: So is it what you did or not?
00:58:47.210 - 00:59:00.330, Speaker B: No, that's not what we did. I don't think we could use the work from on time. Verification. Verification.
00:59:03.630 - 00:59:58.910, Speaker D: No, I don't mean reusing their work, but that's the proof engineering side, which is restructuring the proof, right. You're basically building a distributed validator to emulate a validator, but with more resilience. So I would expect to be able to prove it in a layered manner, saying, okay, I've got the proof for a validator, a single validator, assuming it's honest. And I'm going to show that if I've got a distributed validator and I have a notion of honest, for distributed validators, it implies that the observable behavior of this set of validators implies that it's honest, as if it was seen as a single validator. Because at the end of the day you reconstruct the key at the end and everything. So it's equivalent to one validator. So that would make the proof probably easier to manage.
00:59:58.910 - 01:00:20.820, Speaker D: And the second question I had is that's what Ron was alluded to as well. What are the assumptions? Because there's lots of things like in liveness and so on. What are you going to assume to prove liveness? What are you assuming in your profile at the moment about cryptographic primitives and so on?
01:00:22.870 - 01:01:13.700, Speaker B: Yeah, thank you. For cryptographic primitives. Essentially we assume that ideal general, no collision threshold signature is correct. If you have less than the shares, you can't reconstruct the signature unless the shares, they sign the signature for the same data, you can't construct it. So we don't assume any probabilistic behavior, just assume that things works perfectly, as I said. Then also for Byzantine nodes, we assume that they can't forge signatures. They can send basically messages signed by honest nodes unless they've already received them.
01:01:13.700 - 01:01:26.040, Speaker B: And for liveness, which we haven't started yet, we plan to assume synchrony so that there is a known bound on the messages to be on the.
01:01:28.730 - 01:01:29.094, Speaker E: Night.
01:01:29.132 - 01:01:30.920, Speaker B: Is there anything you'd like to add?
01:01:33.050 - 01:01:55.576, Speaker C: Yeah, thanks, Tom. And we also need to have an assumption on the speed of processes. We also assume that they are synchronous. And. Yeah, I think for other assumptions Roberto already mentioned. Yeah.
01:01:55.598 - 01:02:22.520, Speaker B: And for example, the one that I showed in the axiom, we assume that uniqueness in the signature of the signature scheme so that you can have two different signature for the same value and public key. There's another question from Ron.
01:02:26.430 - 01:02:30.480, Speaker E: All right, I'm done. Didn't lower my hand.
01:02:32.530 - 01:02:34.080, Speaker B: Yeah, thank you.
01:02:34.610 - 01:02:40.926, Speaker C: Well there you go. That was a good talk and thank.
01:02:40.948 - 01:02:46.690, Speaker A: You for getting to. I know it's a complicated thing putting a talk together. So very much appreciated.
01:02:47.910 - 01:02:49.620, Speaker B: Yeah, so thank you.
01:02:51.190 - 01:02:52.894, Speaker C: There are a whole heap of talks.
01:02:52.942 - 01:03:52.822, Speaker A: Coming up in the coming weeks and in fact next week's talk is by Babu Palal, which is about his thoughts on MeV. And then we've got Barnabay Mono and he's going to talk a bit about proposal builders. There's my talk on inline assembler. There's also another talk that is going to be on April the twelveth, which is some Philip from Lefi talking about his thoughts on why it's a good idea to have aggregators and not just aggregators for value transfer for crosschain, but aggregators for arbitrary message communications. So I think that should be interesting. And on May 3, Leo has called it introduction to. But I think this is really going to be a deep dive into amms and really cover a broad spectrum of stuff.
01:03:52.822 - 01:04:02.706, Speaker A: So that should be a very interesting. So thank you again, Tanhai and Roberto. That was really. Yep.
01:04:02.738 - 01:04:04.920, Speaker B: Yep. There's all the socials and stuff.
01:04:05.370 - 01:04:15.918, Speaker A: Assume everyone knows by now, but if you don't, please join the meetup or watch this on YouTube. So anyway, thank you again and see you all next week.
01:04:16.084 - 01:04:20.170, Speaker B: Thank you for being here. Bye bye. Thank you. Bye.
