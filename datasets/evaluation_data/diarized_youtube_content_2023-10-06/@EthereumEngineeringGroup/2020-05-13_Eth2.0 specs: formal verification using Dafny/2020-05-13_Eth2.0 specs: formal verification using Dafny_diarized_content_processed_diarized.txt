00:00:00.890 - 00:00:39.740, Speaker A: According to the cloud. Okay, so hello and welcome, everyone. This is the Ethereum engineering group Meetup online, also shared with the Ethereum developer Alliance Meetup. So today we've got Frank, Joanne and Roberto, and they're going to tell us all about the form verification of Ethereum two and the work they're doing. So look, I don't know, Frank, Joanne and Roberto, maybe in that order. Do you want to introduce yourselves before flipping to the slides? Just very quickly for everyone?
00:00:41.870 - 00:00:56.430, Speaker B: Yeah, so I can start. So I'm Frank. I'm an applied researcher in the Pegasus protocol engineering group in consensus, and my expertise is in formal methods and software verification.
00:01:02.160 - 00:01:19.620, Speaker C: I'm Joanne. I'm in the same team. I've been working for Pegasus for about seven months now. My background is as an academic and sort of maths and cryptography. My areas of expertise.
00:01:24.840 - 00:01:47.150, Speaker D: Hi, everybody. My name is Roberto. I'm in the same team of John and Frank. I've been working at the beginning when I joined Pegasus, been working on consensus protocols, mainly focused on the ibtu protocol, which is implemented in Bezu. And most recently I moved into the formal verification stream for focusing on what you see today.
00:01:51.640 - 00:01:57.350, Speaker A: All right, well, thank you. So, Frank, do you want to share the slides and take it away?
00:01:58.040 - 00:02:41.924, Speaker B: Sure. Right. Um, so, yes, today we're going to talk about some work we've been doing on the Ethereum 2.0 specifications, and it's related to some formal verification using a programming language called Daphne. So you're welcome to interrupt me at any time if you have some questions. So we already mentioned that some people are going to talk during this presentation, Roberto, Joanne and me. There's another member of a team.
00:02:41.924 - 00:03:11.324, Speaker B: We are in the consensus protocol team, David. But David is focusing on the phase one, the next phase of Ethereum 2.0 specification. So he won't talk today. Right. So the agenda for today we'll have a short introduction to the actual formal verification topic that we are discussing, what it is and how is it formalized. So that's, let's say bullet .1
00:03:11.324 - 00:04:00.632, Speaker B: and two, and then we'll show how we have applied these techniques to the verification of the Ethereum 2.0 phase zero specifications. During this talk we'll be having. So I'm going to start talking about, let's say, section one and two and a bit of three, and then Joanne and Roberto will take over to cover some verification topics and results. Right, so what's, let's say, the benefits of formal verification versus what's known as conventional testing. So here is a simple example, a code snippet we've used many times, but it's actually taken from the actual specification of Ethereum 2.0 specs.
00:04:00.632 - 00:04:41.470, Speaker B: So there's a function which is called get next power of two. It takes an integer, and it should return another integer, which is the smallest power of two larger than the input parameter. So the way you would probably develop this program is to write the code as it is here, run some tests, and if the tests pass, then you're happy. So according to what's happening on my screen at the moment, I've run a few tests, three, maybe more, and nothing went wrong. So I could assume that the function seems to be okay. However, it's not factually buggy. It doesn't return the correct answer for x equals zero.
00:04:41.470 - 00:05:34.412, Speaker B: So this example is just a very small example of how bugs can creep in your code. It's not only when you write very complicated code that you can write buggy code, but it can happen in very small code snippets, and it also shows the limitation of testing. So actually, you saw that I wrote a few tests, they could be more. But among all the tests you're going to write, you may miss corner cases that would reveal bugs. So this is nothing new, and this was already pointed out by a famous computer scientist, Dykstra. And this is one of the limitations of testing. It can reveal bugs, but can never show the absence of bug, or only in very limited cases, when you can prove that you can do an exhaustive test.
00:05:34.412 - 00:06:58.644, Speaker B: And in this case you may be able to prove the absence of bugs, but most of the time you can't. And even if the state space, I would say you have to range over for the input is finite, it may be far too large to exhaustively test. So testing is limited. And to make sure that you develop some code that's somehow correct, you may need some other techniques. And that's exactly the purpose of formal verification, right? Just to give you an idea, before we dive into the formal verification definition of the results, actually we got related to this work, we already reported some bugs to the Ethereum spec repo also found some bugs in the implementations, and we reported them as well. So all these bugs were not revealed by any testing, and we found them doing this verification work, right? So how can we perform some actually formal verification, what it is and how do we do it? So the main, I would say, framework to check that programs are correct is derived from what's called all logic. And this is what I'm going to explain in the next few slides.
00:06:58.644 - 00:07:49.732, Speaker B: And then we can use some programming languages that are somehow verification friendly and can help you to implement this all logic verification proof style. Right? So how does it work? So we still get the same function? I've written it in a different style, I would say it's not in python, but in the Daphne syntax, so it still gets an input, which is an integer. My get next power of two and I'd like to define whether this function is correct. So again, correctness. I have to specify correctness with respect to some specification. And that's the purpose of this work, is to write a proper specification of what the function is intended to do. So that's the properties of, in this particular case, the properties of a result we want to compute.
00:07:49.732 - 00:08:41.592, Speaker B: And this should be different to the implementation, which is how the result is computed. So the basic, let's say styling or logic style proof is you write your code, your implementation, the body of a function, and you write a specification which constraints the input. So that's called the precondition, under which condition are you expected to compute the correct result? So this is called the precondition, and that's it called the requires. And if you compute the result of real function on a parameter that satisfies the precondition, what can you guarantee? This is the post condition. So this is really an input output relation. It has nothing to do with how you compute the result and how you compute the result. You write it in the body of a function.
00:08:41.592 - 00:09:28.484, Speaker B: And the actual job of, let's say, Floyd or logic is to say I've got an implementation, which is how I get from the input to compute the output. I've got a specification, what the input should satisfy and what the output is supposed to satisfy. When I compute from parameters satisfying the input. And I would like to check, using mathematical logic, that my code, the body of the code, actually satisfies this pre and post condition. So if I start with the parameter n larger than zero and I compute, I execute the body of this function, I end up with the result satisfying the logical predicate. That's the post condition. So that's the sort of basic of a framework of Floyd or logic.
00:09:28.484 - 00:10:45.516, Speaker B: You write specification that are distinct to implementation, and you want to mechanically check that your implementation satisfies a specification in terms of a pre and post condition. So to do so, these two guys, Floyd and Orr, they wrote somehow some sort of logical rules that you can apply to any program, and it depends on the structure on the program, and it enables you to derive a result whether a body, an implementation, satisfies a specification using some mathematical reasoning. So how is it related to the work we're going to talk about today? The tool we're going to use and the programming language we're going to use, Daphne, is actually a verification aware language, and it helps you to do the proofs that you would do, let's say manually 20 years ago, that your implementation satisfies the specifications. So the Daphne language enables you to write the specifications, write the proofs as well, and then check. It can check that your proofs are correct. So that's the way Daphne works. You write your program with pre and post conditions, and you may write some proofs.
00:10:45.516 - 00:11:41.132, Speaker B: And the interesting thing in Daphne is that the proofs are written as programs. So we'll show you how it is in the real code. And the Daphne compiler will actually take the program with the proofs. Check that built from this program, and the proof, what's called the verification conditions, sends it off to a backend tool, which is called an SMT solver, and check whether the program is correct and the proofs are correct. So there are two cases, either actually it's correct and then you're happy with that, or there's a flow, a bug. And in this case, Daphne will give you some hints. So I won't go into the details, what hints can be given? It could be in the form of a counterexample, but it could also be in the form the reasoning engine in Daphne was not able to prove your property, your program, because there are not enough hints in the code you've written.
00:11:41.132 - 00:12:22.284, Speaker B: So basically, we use Daphne to help us to support the Floyd or logic style proof system. And Daphne automates quite a lot of tasks to perform the checks that the program satisfies an implementation. Right. So how do we apply this to the Ethereum 2.0 specifications? The Ethereum 2.0 is, let's say, the next generation of ethereum specifications for the clients in ethereum. And it's supposed to be rolled out in three phases, phase zero, phase one, and phase two.
00:12:22.284 - 00:13:27.036, Speaker B: And in this work, we focused on the initial phase, which is phase zero. And phase zero is basically composed of three, let's say big packages, the serialization deserialization stuff, the mercurialization stuff, which is so serialization deserialization, it's about being able to communicate some data from one client to another. So you serialize some objects and you send them across the network, and at the other end of the network, someone can pick these packets and rebuild the data structure that was sent at the beginning. The mercurialization is about building short, let's say summaries of what has happened on the chain. And there's the beacon chain, which is the proper, let's say, package that implements the logics, the core logics of how to create blocks on this beacon chain, which is not the full chain. But for the purpose of our talk today, this is the scope of our talk. So the problem with the specs is they are quite complex and sometimes ambiguous, and they are probably buggy.
00:13:27.036 - 00:14:19.172, Speaker B: Actually they are. We found already a few bugs in the specs. So the idea is to try and use formal methods to try and do a thorough analysis of the specifications and detect inconsistencies, bugs, and when we find bugs, try to fix them and prove that after the fixes, the program is correct. So we may note that this work comes after quite a few audits were performed on the code and testing, and there are still some bugs in the code. So that justifies this work. So we have already mentioned that we reported some issues and we've got a repository with the Daphne code we have written. So I'll exemplify a bit what we've been doing, right, so we've been writing some Daphne source code.
00:14:19.172 - 00:15:32.300, Speaker B: So it's a programming language in which you can write specification. And we are not only providing proofs, and at a theoretical high level, we are actually writing proper, let's say specifications of what every piece of code in the specification is supposed to do. We are writing implementations as well, to show that these specifications, what it's intended to do, admit they do admit some implementations, right? If you write a pre and post condition and there's no way you can compute the result, it's not good. So we show actually that the pre and post specifications that we write are consistent and we can implement these. We write some proofs, I'll show some examples of proofs that are desirable from the spec, but that are not under the form of pre and post conditions, but that are more elaborate. And during the course of this exercise, we also document our code. And the intent is to have a sort of a self contained piece of code as a Daphne code, that you can get the code, the spec and the documentation, and you can extract the documentation to make it browsable.
00:15:32.300 - 00:16:18.040, Speaker B: Another thing we can do as well is to generate some code automatically from the Daphne source code, C sharp, go or JavaScript. And Roberto will exemplify this with the generation of go. And what we can do with it is for instance, build certified test suites, and that will be one of the facets we'll explore in the next few slides. Right, so I'm going to start on the proper, let's say, specification, implementation, proofs, and documentation of part of Ethereum 2.0 specifications, and then I'll hand it over to Joanne. So this is a very simple example of, let's say, one of the libraries in the spec which is serialized, deserialized. It's called SSZ.
00:16:18.040 - 00:17:12.796, Speaker B: And I'll try to demonstrate how we can go from the specification in plain English to something written in Daphne, and to some desirable proofs that we can mechanize in the Daphne code. So here is the problem with serialize and deserialize. You want to take an object, which is data structures, that is defined to be an Ethereum 2.0, let's say data structure, and you want to transform it into a sequence of bytes, send it over the network so that someone can at the other end, get it, grab it, and decode it back into the original data structure it started from, let's say. So I'll take a simple example, not too simple, but simple enough so that we can follow what's happening. This is the example of an object which is called a bit list. So it's a list of bits, as you can see here, it can be of arbitrary length, and it's a list of zeros and ones.
00:17:12.796 - 00:18:02.768, Speaker B: And when you want to sequence it somehow and send it, serialize it as a sequence of bytes, that's called the serialize operation. Of course, a byte is eight bits. So what you have to do is if your bit list has not a multiple of eight bits, you have to add some padding. This is something that you can't avoid because you're going to send multiple of eight bits, you're going to send bytes. But if you pad, right, if you pad with something, let's say with zeros, you may miss the end of the original list. So the way people in the Ethereum 2.0 specs have defined the serialization of a bit list into a sequence of bytes is to say you're going to take the initial list of zeros and ones and pad it with a one followed by some zeros.
00:18:02.768 - 00:18:43.304, Speaker B: So the zeros are padding for nonexistent values. And this one is just a marker of the end of the list, so that you're able, when you have to deserialize, to retrace the end of the list. So the serialize operation works as follow. You take a list of bits, you add a one at the end and you pad with a number of zeros to make it a multiple of eight. So that's the result that's here. And when you deserialize, you're going to take the list, try to find the list of bytes, right? So I've got two bytes here and retrieve the largest index with a one. And I know that this one is the marker for the end of the list.
00:18:43.304 - 00:19:56.690, Speaker B: So I will take the prefix to reconstruct the initial list. So one thing that's noticeable is if you've got a list of eight bits, it will be encoded over two bytes, because you need to pad with the marker of the end of a list, which is a one. So there are some slight, let's say, difficulties in this key. What we've been doing is to write the specification and implementation of serialized and deserialized for bitlists and other data structures. But for bitlists and one operation that you would like to, let's say, proof or property that you would like to get, which is not a sort of a pre post condition property, but a more elaborate one, is that when you serialize an object and you deserialize it, you get the same object, you get the initial object. So I'm going to show you some sort of the Daphne code we can write to perform this serialized deserialized operation, and how we can prove this theorem, which is called involution, that the deserialization of a serialized object is the identity. Are there any questions at this stage? So I assume no.
00:19:56.690 - 00:20:40.110, Speaker B: Right. So I'll share my screen with the code. So this is sort of a part of the Daphne code on the left hand side related to the serialization of bitlists. So just to exemplify a few things I was mentioning before, we take the opportunity during this work to try and collect the documentation on the Ethereum 2.0 spec and to write it and embed it into the code. So this one is the documentation of how a list of bits is encoded, and we write as well the sort of specification of how to encode a list of bits into a sequence of bytes. So there's an example here.
00:20:40.110 - 00:21:19.188, Speaker B: This is the encoding of the serialization of bitlists, and there's the corresponding operation, which is the deserialization of a bitlist. So what's noticeable in the code as well. So the code has been written. In this case, it's not the same as the code in the Ethereum 2.0 spec. We've written it as a recursive, let's say encoding, and there are three cases, but you can see that this is a recursive call to this function from bitlist to bytes. So we just encode the first eight bits and then recursively encode the next eight bits and so on until we have less than eight bits.
00:21:19.188 - 00:22:10.248, Speaker B: So that's how this function works. So what Daphne helps us to do, and the results are basically in this problem window at the bottom, where there's no problem for bitlist serialized deserialize. But the proof we do with Daphne is that we prove that two post conditions are satisfied. One is related to the length of the sequence of bytes we get as a result, and another one is related to the fact that the last byte of the encoding will have a value which is larger than one. So as I explained, the encoding, and as you add an extra one bit at the end of initial list of bits, you must end up with a sequence of bytes. The last one must have a value larger than one. So that's an example of a pre and post condition for this encoding, and what's remarkable in Daphne as well.
00:22:10.248 - 00:23:00.432, Speaker B: So Daphne is able to prove that these two properties hold without us doing anything. So the logical machinery embedded in Daphne is able to figure out a proof of these two facts, and it also proves something which is important in verification, and in particular this one. I've got a recursive call to the right. Sorry, I've got a recursive call. In my definition it proved that this recursive function always terminates. So we have to provide a sort of a hint for Daphne to do it, but it's based on ranking functions and so on. But Daphne is able to prove, one, that some post conditions will hold after you execute the code, and two, that the code snippet always terminate, which is called, when you combine both of them together, partial correctness and termination.
00:23:00.432 - 00:23:41.044, Speaker B: You enforce what's called total correctness. So your code computes the correct value and always terminate. So this is the example of how we can write the encoding. The decoding is written in similar way. Interestingly, this is the sort of theorem I was talking about, which is if you take a list of bits, you encode it as a list of bytes, and then you decode it to get a list of bits again. Then the operation is identical to the identity function, so it returns the same list. So the proof can be written as code in Daphne.
00:23:41.044 - 00:24:22.470, Speaker B: So you write lemmas, which are special types of functions, but this is written as a program. So the proofs are archived in your code as a program and the Daphne compiler will take everything, build a logical condition and check that this condition holds. And if it holds, your programs and your proofs are correct. So there's another proof which was actually a claim in the Ethereum 2.0 spec, which is that the serialization is injective and that we can easily prove as well in Daphne. So we give this proof, Daphne builds a verification condition and check that this condition is valid. So that's the basic mechanism of how it works.
00:24:22.470 - 00:24:30.730, Speaker B: Right? So I'll hand it over to Joanne and stop sharing my screen.
00:24:37.280 - 00:24:52.960, Speaker C: Thank you. I'll just set up my screen. Can everyone see the slides?
00:24:55.380 - 00:24:56.130, Speaker A: Yes.
00:24:56.600 - 00:26:04.068, Speaker C: Great. Ok, so here we see the structure of the libraries and their functions, and from the work that Frank has done on the Ssz library, we can jump to the green box and now have a look at the mercuryzation library to see how it builds on SSz. In particular, I thought today I would go through some of my more recent work to generalize the merklization function. Initially we started by considering specific objects separately, but this new work allows us to merklize basically any SSz object. So the work on the Merklization library is based primarily on the spec contained in simple serialize md and Merkel proofs md, and the PI Ssz implementation is also used when clarification is needed. The libraries comprise a number of helper functions and uses functions such as serialize from Ssz. Okay, so ETH two merklization is based on a binary Merkel tree.
00:26:04.068 - 00:27:07.948, Speaker C: Binary trees provide simplicity and efficiency. Any data structure can be translated to a binary tree with minimal effort, and their use enables a wide range of other binary tree specific optimization. While packages such as Pysz have used optimized implementations for their Merklize function as a starting point, in Daphne we code just a simple basic recursive definition to get the hash tree root. This has the advantage of then being able to be used as a check for optimized implementations such as Pysz. And from a project perspective, it also means that we can start to call the mercurialization function from higher layers such as the beacon chain, and therefore continue to link pre and post conditions of the various functions to check for bugs. This building and linking of pre and post conditions between functions is something that we'll see in the example that I'm going to go through in Daphne. So the Merkel proofs md file within the spec.
00:27:07.948 - 00:28:30.148, Speaker C: It describes the generalized Merkel tree index, whereby a generalized index of a node is two to the power of the depth of the tree plus the leaf index. This means that the generalized index has the property that the two children of node k are two k and two k plus one, respectively, and it provides for a linear representation of the merkel tree as calculated by the Merkel tree function shown here, which returns the entire tree and hence the value at position one of the output then represents the hash tree root. Using this definition, we can see that a recursive version of the function can also be written to return just the hash tree root. This recursive form is perfect for our daphne project, as it comes directly from the definition of a binary merkel tree and is straightforward to implement. Of course, as I mentioned previously, it can then be used to check optimized implementations such as in the PI Ssz library, as we have a proof of what the output should be. So just a little bit more detail. As defined by the spec, the ssz object being mercalized is packed into 32 byte chunks which represent the leaves.
00:28:30.148 - 00:29:30.936, Speaker C: If there's only one chunk, then that chunk represents the hash tree root. Otherwise, the root is calculated from the hash of its children. Each child is the hash of its children, et cetera, and the recursion stops when we reach a leaf node. The number of leaves needs to be a power of two, and as defined in the spec, if there are less chunks, then we pad with the empty chunk, which is just 32 bytes of zeros, also as defined by the spec. If the input to merklize is empty, which for example can happen for a bitlist of length zero, then the empty chunk is returned as the hash root. Okay, so I'm going to switch over to Daphne now, so I can show you how we are able to establish a generalized mercalization function to return the hash tree root. Ok, can you see my visual studio? Yes, thank you.
00:29:30.936 - 00:30:52.544, Speaker C: Ok, so just a couple of things that I'm going to mention before I have a look at the code. So as we're working in Daphne, we can see in the problems tab if we have warnings or errors, if something's not verifying when they're blue. So for example, you can see up here, that's just information, it's not an actual error, Daphne can work with it, it just gives us the hint as to what it's having to do to process that. And the other thing I should say is, given that we are working on the documentation aspect, I have only just pushed some of these functions to the repo, and I haven't as yet added the documentation, but that is not too far behind. So if I start with the merkalize function, I'm just going to run through and basically explain how it works and just sort of show you the power of Daphne and how it was able to help with this. So we can see that it receives a sequence of chunks, and those chunks, they represent the leaves, which of course hold the SSz object. Now generally these chunks are created by serializing the Ssz object, but there are some exceptions to that, including a bitlist that just uses a basic encoding.
00:30:52.544 - 00:31:46.960, Speaker C: And so my apologies for jumping around the screen, but just when we call the merklize function from within, get hash tree root for the different types of ssz objects, normally we pack, and that pack involves serialization. But for bitlists we just do a basic encoding. Okay? So the function returns one chunk which has been given the type alias hash 32. And in terms of preconditions and post conditions, the length has to be positive. So our sequence has positive length or zero, and ensures the post condition. We check that the result of the Merkley's function will just be a one chunk. So just the 32 bytes which we've called hash 32.
00:31:46.960 - 00:33:04.508, Speaker C: Now in line with the spec, we say that if the length of the chunks is zero, then we return the empty chunk as the hash tree root. Now as I said, that can happen if we have a bit list of length zero, and then otherwise we're basically going to mercalize the leaves. Now I haven't commented it out, but if we can just ignore this lemma that I have in here for the moment, basically, if our chunk length isn't zero, we go into this branch where we're using two helper functions. We've got a merklize power two chunks function that's going to require the sequence of chunks to have a length that is a power of two. And so to make that happen from our chunks input we have to pad if we don't currently have that power of two for our leaves. So if we go up and have a look at those helper functions again, so we're linking, we're able to specify the pre and post conditions for each of those helper functions. And basically what Daphne is going to do is it's going to check that everything still flows.
00:33:04.508 - 00:34:01.404, Speaker C: And in a moment I'll show you what happens if I didn't have my lemma. But so for the padding, basically we take in a sequence of chunks and it will return a sequence of chunks. We can say that we need at least one chunk to call this function and that fits in that if we have got zero, we would have gone down the if branch anyway. And in terms of the output of the pad power two chunks, we're going to ensure that the number of chunks we have is at least what we started with. And in fact it will be the next power of two of the length of chunks. So if we happen to send in a sequence of chunks that's already a power of two, well, the get next power of two function will just return that power. And so that's going to ensure that we have the correct length of the output.
00:34:01.404 - 00:34:39.180, Speaker C: I've got a commented out post condition. I am planning to turn this into something a little bit more readable in terms of is power of two predicate on the length of that output. But for the moment, the getnext power of two is sufficient. So that's my pad power two chunks. And that ensures that when Merkali's power two chunks is called, it's operating on a number of leaves. That is a power of two. So if we have a quick look at that function, again, we've got more pre and post conditions.
00:34:39.180 - 00:35:21.150, Speaker C: And so in Daphne it's checking that all of these are compatible with each other. We've got that we must have at least one chunk. We've got that the number of chunks we send in, in fact, must be a power of two. So it has to be the same as if we called next power of two on the length of the chunks. And in terms of the output, we're going to get that one chunk output. So we check for that with a predicate. Now, when I first did this, so if you were coding it, you would sort of set up your functions, you might be thinking about doing a little bit of error checking that, oh, I've got at least one chunk or whatever.
00:35:21.150 - 00:36:33.800, Speaker C: But where Daphne is so powerful is that if I hadn't included this lemma down here. And I'll just wait, I'll just save. Okay, so without that extra bit of information to help Daphne, basically, and this is what happened when I was starting to create this, I set up my specifications and then I said, okay, well, I'm pretty sure that it is doing the right thing, but Daphne can't actually prove it. And what I'm told, as you can see in the error down here, and they always show up in red, is that I've got a violation of a precondition. And what that violation is is that when we create the output from pad power two chunks, even though that function tells me that the output the length of that output will be the next power of two of the length of the input. When I call Merkali's power two, I've got a precondition that the length of what goes into that must be the same as the next power of two of the input length. And it can't see that that logically follows.
00:36:33.800 - 00:37:13.532, Speaker C: So I had to set up some extra properties to help with that. And you can see up here that I did that. I need to think of a better name for the first one. But basically I had a look at what I wanted Daphne to know. And I wanted Daphne to know that the length of the pad power two function was going to be the same as the get next power of two of the length of that output. Because that related directly to that. Sorry, this one here that I've got highlighted that precondition.
00:37:13.532 - 00:38:06.956, Speaker C: But with the chunks being created from pad power two now, setting up that proof wasn't sufficient. I had to do a little bit more work. And so the idea is that I'm sort of helping Daphne understand properties that it hasn't been able to sort of conclude on its own. Sometimes it's a little easier. So I haven't given this one very nice name, but I got to a point where I needed to show that because the getnex power of two function will return the input if it's already a power of two, I had to show that. Therefore applying the get next power of two function twice was actually still going to give me the same as next power of two over an integer. So in that case, when I started to set this up, it was instantly proven.
00:38:06.956 - 00:39:11.750, Speaker C: And that's you often see, we write thanks, Daphne, where Daphne is able to construct that proof itself based on what it already has, it doesn't require anything extra. Whereas here I've had to do a little bit of work just to help it with the logic. Now, the lemma that we've got here is exactly what we needed to ensure that the precondition for the Merkley's power two function was correct. And so I just take that lemma and I insert it down here before I actually call the function. And then Daphne is able to establish that yes, indeed it does verify. Now, I suppose as someone who's newer to formal verification, I think it's just so powerful that we can set up these functions and we can keep building on them and everything has to relate with those pre and post conditions and sort of the flow from one to another. It's very easy to miss things when you're working through the logic yourself.
00:39:11.750 - 00:40:16.760, Speaker C: And so this can give us so much confidence in what we're creating. Okay, so at this stage, we've got our Merkley's function, it's verified. And as I said, that one of the next things I'm planning to do is to set up a different implementation of mercalization based on what they do in Pysz, where they optimize the process. And I'll be able to compare the output of their implementation to the output from this sort of very basically defined Merklize function. And I can do that with as many different implementations as I like. Okay, I'll just switch back. Okay, so just in terms of some next steps of where I'm at with the mercuryzation, I still have a little bit of work to do, the documentation to put in some other properties of the hash function that gets used.
00:40:16.760 - 00:40:51.190, Speaker C: Planning, as I said, to check some of the optimized methodologies in libraries, and also then to focus on other aspects of mercuryization for ETH two, such as the Merkel multi proofs that authenticate that a set of nodes are actually part of a Merkel tree with some specified route at a particular set of generalized indices. So it's all about building from this point, and Daphne is just so good at helping us with this. So I'll hand over to Roberto, and I think I'll be back again a little later just to talk about some of the stats of our project.
00:40:54.200 - 00:41:56.236, Speaker D: Thank you, John. So I start with a little detour from mercury in a little while. What I want to show you is a very interesting if you want common programming error that definitely can catch. So I'll use this function which calculates the integral square root of a number, and it's used in the theory specification to calculate how the rewards depict over time if a block proposer doesn't propose blocks. So what you can see here is the method as defined by the specification. The two ensures causes the post conditions and they define the specification. And essentially, given an input number n, this function must turn a number x such that the square of x is less than the input number given.
00:41:56.236 - 00:42:59.570, Speaker D: And this is what this ensures. Close guarantees and not only must be actually the largest number that has this property, and this is what this second insure close or post condition guarantees. Basically, it says that there does not exist another number larger than the result, such that the square of this number is less than the input number. So these two ensure clauses capture in full which is the specification of the integral square root. Now, what is interesting about this one is that I initially proved that these two post conditions hold for natural numbers. So Daphne can call natural number in the sense of an infinite set of number, as we know. So I did work out the proof, and this is more or less what you can see on the screen.
00:42:59.570 - 00:44:07.940, Speaker D: However, the film specification doesn't specify the integral square root for natural number, but it does specify it for a finite set. Specifically the set of integral that can be represented with a 64 bit or UN 64, which compared to the natural number is not closed under multiplication and addition. So just to give you an idea, given this integrate querul for natural number, I want to see whether it actually verifies for you in 64. The first steps are to change the types as you would imagine, and we need to just give Daphne a bit of time to verify the changes and basically it will come back to us. And you see here a red underlining. What it's telling us is the result of the operation might violate a new type constraint 64. That's definitely a way basically to say you might overflow, or my underflow.
00:44:07.940 - 00:44:54.336, Speaker D: If we investigate a code, we find that this can happen. This can happen because if the input number is all one and you add one, then you overflow. The way to fix this is basically to add a precondition. The precondition is represented by. This requires where we say that the number must be less than a number with 64 ones. Now we let Daphne run its own verification and we see that we don't have any more an error here. So basically this tells us that there cannot be any overflow condition.
00:44:54.336 - 00:45:55.720, Speaker D: Given these specifications. We still have a red underline here. This is just because we need to give a bit more hint to Daphne to verify that there is no overflow. In that point, it's just about adding an invariant dilemma. And as you can see now, Daphne verifies, you can see down here there are no underlining, red underlining. So Daphne verifies that this implementation is correct implementation according to this specification for UN 64, ensuring that there is no matter which Un 64 you give in input, as long as this required condition is verified, the function will compute correctly. Okay, so if there are no question on this one, I'll move to the last bit of the talk, which is about the code generation.
00:45:55.720 - 00:46:43.812, Speaker D: So Daphne allows to generate from Daphne code. You can generate C sharp go JavaScript code. And I did have a sneak peek at the code currently on master, and there is an option actually for generating Java code. I haven't tried it out, but I expect that it might be included in next release of Daphne. So it'll be very interesting. So why is this important? What is the point of the code generation? Well, the point of the code generation is basically that you can use the generated code which is generated from a verified application to either complement existing implementation. So for example, let's assume that we have an implementation that doesn't have the serialization of bitlist.
00:46:43.812 - 00:47:35.640, Speaker D: We can take Daphne verified implementation of the serialization of bitlist, generate GoSh code, and use it to complement, let's say, an existing client which doesn't have this feature. Also, you can use this code, as we'll see soon. This is just basically as reference testing and to test existing implementation. So basically assume that you have an implementation and you want to verify whether this implementation, you can test cases. Basically for this implementation. Before I move to the next slide, I want to briefly go through the flow. So basically, as Frank mentioned at the beginning, we have the program with pre and post condition Daphne compiles where the verification with the SMT server phase.
00:47:35.640 - 00:48:34.520, Speaker D: And then this is important. If and only if the program is correct, then we have the code generation, meaning that if the verification fails, the code is not generated, ensuring basically that you generate the code only if the specification and the implementation is formally verified. So as I was mentioning before, this can be very useful to build certified reference test. So this is an example on this slide for the next power of two. So basically, on the left hand side we have what we call untrusted next power of two, because it doesn't have precondition, post condition is not verified. On the right hand side we have a verified implementation, basically with all of the preconditions, both conditions and verified by Daphne. And we can build test vectors.
00:48:34.520 - 00:49:44.630, Speaker D: Basically, given a set of inputs, we can verify that the output of the untrusted version matches the output of the trusted version. But interestingly, and I will go now back to Mercury to cover what we can do in practice for Ethereum two. So Ethereum two, there are a few different clients. One of the clients is written in Go. It's a client written by prismatic lab. So what I'm going to show you now is how we can use the Daphne code that Joanne showed you before and from this Daphne code, generate go code and use this go code basically to test, to have a few test cases and test the prismatic lab implementation of the mercury of bitlist using the correct by construction code generated by Daphne. So Daphne generates go code.
00:49:44.630 - 00:50:48.440, Speaker D: What we need to do, just to give you an idea, we need to basically write a sort of bridge in go. And this bridge, basically this is the bridge that allows us to invoke the bitlist rule, which is the function of the prismatic lab code that calculates the ash tree root of a bitlist. We need just to do some adaptation on the types as Daphne has its own type, its sequences, whereas the go implementation by prismatic lab is expecting an array of bytes. So all that we need to do is this conversion. And then we essentially call the function that is implemented by the prismatic lab guys. And then we do conversion back to sequences and we alter it. And we can use in building tests using Daphne code.
00:50:48.440 - 00:51:47.290, Speaker D: So for example, just to show here, that's what we do here, this is a Daphne function. It's a predicate. Basically it's a function that returns a boolean. What we do here, we basically given a bit list, a sequence of Boolean. We calculate the ash tree root using what Joanne showed you before, and then we go over this one later, and then we calculate the h three root using the prismatic mercurization function. So we do this basically by, basically the adapter is done, is split in two. We have the go code that I showed you before, and then we have a Daphne bit for it, which basically just declares some of these functions as extra.
00:51:47.290 - 00:52:43.420, Speaker D: And what we do here, basically this is just a trick. I'll show you later. Basically I'll show you how we can verify this test. And just to show that, to have some failure inject into it for any sequence that is longer than 1000 bit, basically I feed to the prismatic lab implementation a different bit list. So the expectation that for any bit list longer than 1000 bit, the test should fail because you give a different bitlist, you expect a different hash tree root. And basically this code does is a main and we have a parameter which is the number of tests. And all that it does is basically building test suite with this number of tests, and where the input bit list is randomized, is selected randomly.
00:52:43.420 - 00:54:10.084, Speaker D: So if we want to generate code, what we need to do is this Daphne command to generate code, it generates all the code. And we need to specify some of the go, if you see in the command line, some of the go adapters that we need for ten flow level stuff. One interesting thing I want to show to you, we can verify even this adaptation if you want this structure to run the test. And if we put n one here, for example, and we try to compile, Daphne won't compile it because this post condition is not verified, as you know basically this function, you can see it takes a random number and takes the mod for 2000, which gives basically the size of the run by size for the list between zero and 1009 nine. And so if you put wine as a lower bound, this post condition won't be verified. So going back to generating the code, we generate the code. Now that the post condition is fine, we build it and then we can call it for five cases.
00:54:10.084 - 00:54:50.870, Speaker D: And you can see basically for all of the bit list where basically the size is more than 1000. Tests fail as expected, because I injected a bit when calling the prismatic lab implementation for all of the tests where the bit list size is less than 1000, it passes. And basically this verifies that for those test cases, the prismatic lab implementation is correct and returns the correct result. There are no questions on this one. I'll end it over to Joanne to go over some statistics that we've collected on our.
00:54:58.440 - 00:55:44.688, Speaker C: So could you move to the next slide please? Thank you. So the project is gaining quite a bit of momentum, sort of as we're building up functions and linking everything. And it got to a point where we really wanted to be able to have a more accurate tracking of our progress. And so that meant collecting some stats on the various components. Now we wanted to automate that. So we've set up a python script at the moment in the form of a notebook, and basically it can go through and collect all sorts of information for us. So we've put in some of the sort of the current status for the different elements, but in terms of things like implementation, we have more than 2000 lines of code proofs.
00:55:44.688 - 00:56:45.080, Speaker C: We've got 82 lemmers now approved. Our documentation is growing and just to be a little bit more specific, if we can switch to the next slide. So the Python script basically goes through it, identifies all our Daphne files, automatically finds where they are and goes through and sort of counts all the information. And we're able to check the number proved by sort of compiling the code and checking that it matches up to the number of theorems that we have. So that's been quite useful. Just we can run it sort of once a week now and just be more accurate in terms of our progress and understanding, sort of the scope of what we've done to date. So that's the stats.
00:56:45.080 - 00:56:51.070, Speaker C: Frank or Roberto, did you want to say anything else to finish off.
00:57:01.430 - 00:57:04.142, Speaker D: Any question? That's what I have to say.
00:57:04.296 - 00:57:17.420, Speaker B: Just comment. There's a link in the slides to our git repo with all the source files and documentation. So you can go through it and have a look if you're interested.
00:57:23.000 - 00:57:28.944, Speaker A: Yeah, I have a question, Frank.
00:57:28.992 - 00:58:26.404, Speaker B: You mentioned that there were a couple of audits that failed to find some bugs. Does that mean that it would be better to directly go through the route of, for example, Daphne, or does that still leave some value for other kind of objects? Well, so as we mentioned earlier, with testing, you can find some bugs. So I think it's sort of, when we write the daphne code, for instance, sometimes we write tests as well. And when we are confident that the code we have written and the tests pass, then we build a proof that the code is correct. So it's still useful to have some tests. And for some existing implementations, I think, well, they are tests. What we offer is to have.
00:58:26.404 - 00:59:09.120, Speaker B: So they could be bugs in the test as well. The thing you test might be, it's the same for our specification. They might be, let's say, not invalid, but they might be missing something. So I think the route, which is writing some tests first, and then when you're confident that all the tests pass, you prove something, is the usual way to go. So I wouldn't really discard the testing phase. And for the code where there's only testing available, there's no support for any proofs. For instance, if you write it in C or C sharp and so on, you can use our implementation to test your implementation.
00:59:09.120 - 00:59:25.230, Speaker B: So you will probably find bugs. If you find the bug, you are happy. If you don't find the bug, you still have more confidence in your implementation compared to the beginning without test. Thank you.
00:59:29.560 - 00:59:59.390, Speaker A: So are there any other questions? All right, I'll quickly run through the talks we've got coming up. So in two weeks time, Harathio is going to tell us all about the witness specification. So this is something that's needed for Ethereum, stateless one X, and, I don't know, Harathio, are you doing that by yourself or are you and I tag teaming on that one?
01:00:01.120 - 01:00:03.876, Speaker B: We're teaming, I think teaming.
01:00:03.928 - 01:01:09.910, Speaker A: Okay, cool, good to know, right? Yeah, it'll be good. This is an evolving spec, but this is the basis of Ethereum one X. And the witness spec you can imagine will also be used with 2.0. So this is a key part of stateless and defines how you prove that some information that's been given to you is valid. So moving along, two weeks time after that, David and I are going to have a bit of a talk about. So when you've got some solidity code, you get to bytecode an EVM bytecode and memory and storage. And I think pretty much everyone who's had been dealing with Ethereum for any length of time has this abstract, vague idea, but what does it really look like? What does this code really look like and how does it work? How is it structured and what is memory and what is storage and what do they look like and how are they used? And so we're going to walk through a bit of that in reasonable detail.
01:01:09.910 - 01:01:52.290, Speaker A: It's very interesting, and it's actually a prerequisite for the next talk, which is really on code compression techniques. And how could you go about doing code mercurialization? So this is, again, if for stateless, you need to compress the information that you're going to send, because if you send the information you need, plus a ton of proofs, then it's going to be ginormous. And then coming up, we've got our conference, the supply chain, on blockchain conference. So that's only the link to tickets. And I should have put in the link to the website as well. So SCOBC. Net.
01:01:52.290 - 01:02:12.360, Speaker A: So we've got our keynote speaker locked in. And so this conference is coming up. We've got our call for papers open. And David, you can probably talk to who the speaker is much better than I can. So do you quickly want to talk to the keynote speaker?
01:02:13.180 - 01:03:23.970, Speaker E: Sure. The keynote speaker will be air voice marshal John Blackburn AO. He's retired deputy head of the Royal Australian Air Force, and in his retirement, he has created a think tank called the Institute for Integrated Economics Research Australia, based in Canberra. With fellows around the nation. From around the nation, John and his team have been working on a set of workshops, articles and proposals to build Australia's national resilience in the face of some form of exogenous shocks, like, I don't know, just to pull one out of the air. Global viral pandemic or a global financial crisis. And so the idea is not to be isolationist, but to be engaged with the world economy in a way that still allows Australia to operate effectively during those kind of shocks to the system.
01:03:23.970 - 01:03:42.310, Speaker E: So he'll be speaking particularly about the vulnerabilities on the supply chain. And he's very much looking forward to participating in this conference because he's interested in how blockchain can help. He's a fantastic, very dynamic speaker. We're very lucky to have him.
01:03:45.320 - 01:03:56.976, Speaker A: Thank you, David. Okay, well, thank you again, Frank, Joanne and Roberto. That was an awesome presentation and to everyone, have a great week. Bye bye.
01:03:57.168 - 01:03:58.032, Speaker B: Thank you, Peter.
01:03:58.096 - 01:03:59.096, Speaker D: Thank you. Bye.
01:03:59.248 - 01:03:59.692, Speaker B: Thank you.
01:03:59.746 - 01:04:00.440, Speaker C: Thank you. Bye.
