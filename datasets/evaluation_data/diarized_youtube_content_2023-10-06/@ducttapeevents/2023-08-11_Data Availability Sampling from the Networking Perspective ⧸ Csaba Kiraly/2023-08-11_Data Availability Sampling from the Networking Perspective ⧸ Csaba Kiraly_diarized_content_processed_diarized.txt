00:00:08.119 - 00:00:47.334, Speaker A: Okay, yeah, we are all set. So, yep, things settle. Okay. So my talk will be about the networking aspects of data availability sampling specific to the Ethereum context. This is work that we jointly done by Leonardo and Dimitri, who are my colleagues at status status, you might know from the status hub, from keycard, from the Inimbus client. And we are all in the codex team, which is working on decentralized storage technologies. And this work is funded by an Ethereum foundation grant.
00:00:47.334 - 00:01:44.640, Speaker A: So thanks to the Ethereum foundation, we can work on this. So what is the teraway of the sampling and why we need it? As I said, we are focusing on the Ethereum context. So data availability sampling is a generic technique, but in the Ethereum context, it's really the key, the new key to scale Ethereum. So the goal, what we have here is really increase the block size beyond what a single node can handle without really making the nodes heavy, so without changing the requirements on the nodes themselves. So the question we should pose ourselves is how can we go and increase the block size so that put ourselves in a situation where nodes cannot download the block and yet have the system working. And thinking of this, that was sharding, dank sharding. There are some underlying realizations I was listing here three.
00:01:44.640 - 00:02:12.736, Speaker A: One is that there is data which is sitting in data blobs, which nodes do not have to interpret. So if nodes do not have to interpret it, they actually don't need to download it to interpret it. They are really downloading it for something else. Currently they are downloading it. And what they are downloading for is really to know that the data is available, that the data was released that is not withheld. So that's the only reason they are downloading. So you can do something else.
00:02:12.736 - 00:02:56.856, Speaker A: And something else that you can do is sampling. It's probabilistic sampling. So the idealization is you don't have to go deterministic and download order block. You can do a probabilistic sampling, a statistical test, and based on that, you can hope to be lucky enough to figure out if data was not actually released. And thus you know that the data is not available and you can disregard that block. So we arrived to sampling, and as I said, we have a statistical test. The data, the block is segmented, and there's a sampling process going on, picking a piece and or maybe more pieces of the many pieces.
00:02:56.856 - 00:03:43.194, Speaker A: And based on that, deciding whether we believe the data is available or we don't believe the data is available. So the first thing we do is segmented sample so the data is in some state, it's either available or not, and it's segmented and you're asking for one of these pieces. And if there were k segments, and you are asking for one of these k segments, you have one over k probability that you figure out that it was not available. If you're on the right side, if you're on the left side and it's available, you're fine. You will always find it available. And you can ask for more. Simplest, you can ask for s samples or the many KPC, and you have this linearly growing probability of figuring out that it's not available.
00:03:43.194 - 00:04:17.994, Speaker A: If it's not available. The problem is that this is linearly growing. So you're not really gaining anything with the sampling. Statistically, this is quite long. So what you do is you are extending the code, you are using lead summary coding, coding and you are extending data. You are adding some encoding which has the base property that if any k, in this example, four of the pieces are available. Of the extended eight pieces, then you can reconstruct the data.
00:04:17.994 - 00:04:56.612, Speaker A: So when it's all available, it's fine. But what you see below is also repairable, right? So you have four pieces missing, the red ones, four pieces are there, two which were generated, parity data, two of the original pieces. So you can reconstruct the data. So you can look at it as the data being still available. This interesting part is when five pieces are missing and then the data is not available. And in that case you have kind of one over two, in this example, a bit more probability. But if there are many segments, it's one over two probability that you figured out that the data is not.
00:04:56.612 - 00:05:28.184, Speaker A: There is one little single sample. And the nice thing is that if you have two samples, you have zero, four. And if you have s samples, you have one minus one minus one over two to the k probability of figuring out. So it's kind of managing probability with the number of samples. If you have many segments that you figure out with a statistical test that the data is not available. And we are happy with this. No, we are not, because there is a problem.
00:05:28.184 - 00:06:02.044, Speaker A: There was at least someone encoding, we think, but actually we don't know that those pieces which were there can be used to regenerate the original data. They can be just random data. We will sample it, we will find out that the piece is there, a piece is there, something is there. And we believe that the data is there because our test is showing that. But they cannot actually be used to deconstruct the data. So in the example you see there actually there are five pieces missing. We just don't know.
00:06:02.044 - 00:06:49.718, Speaker A: We sample, we get something, we believe it, and that's wrong. So that's why we have to segment, we have to extend the data, then we have to prove each single piece and then we will sample. So what's happening is that we are proving the lead someone code, and this is going with a KCG commitment. So there is a commitment to the data. Each single sample proof has, each single segment has a proof attached. So if you have the commitment, a single segment and the proof, these together enable you to verify that the sample is actually coming from the collectively someone encoding. So your mass is working.
00:06:49.718 - 00:07:33.442, Speaker A: Again, if you sample and you get something, you know it can be used to deconstruct. So once you segment, extend, prove and sample, you are having something which can be used to guess whether the data is available or not. Now this would be very nice, but this is not what is being planned in Ethereum. That's a slightly bit more convoluted. It's just a nice little step. So this is what it is. So this is how block encoding is planned in the next evolution, where the k that you see there, the 64k, is not the k that you've seen before.
00:07:33.442 - 00:08:37.134, Speaker A: This is kilo. So this is 64,000 segments. So the date block, which is 32 megabytes, is divided into 64,000 segments, which is half a kilobyte per segment. And then over this there is a lite someone encoding, but it's not a normal Litzwill encoding, it's a two dimensional leads someone encoding, which has a nice property that if you have k PCs, so it's in this case 256 pieces of an individual row, then you can reconstruct the row, or if you have 256 pieces of an individual column, then you can reconstruct the column. So it is extended to 256,000 segments, each one being half a kilobyte. And we have one that 28 megabytes of data. And this is kind of our new block that we are going to sample on.
00:08:37.134 - 00:08:41.214, Speaker A: And of course there are KZG commitments. So you see the numbers there.
00:08:43.874 - 00:08:44.210, Speaker B: For.
00:08:44.242 - 00:09:32.982, Speaker A: The 128 megabytes of data, you have something like, I don't find it now. Yeah, twelve megabytes of KZG commitments added. So you have this extended set of data of which the top left corner is the original data, and sampling is going over this. Now let's have a little quiz. So is data available if these segments are there? The blue ones and the white segments are not there. And you would say, yes, this is the original data. This is data from which the extra data was generated.
00:09:32.982 - 00:09:50.518, Speaker A: So obviously this is repairable. And this is available. And with rows and columns you can easily repair it. You can see, I repaired the row, you can repair all the rows from those. You can repair all the columns. And yes, it's repairable. Let's take another example.
00:09:50.518 - 00:10:43.748, Speaker A: Is this repairable or not? And this doesn't seem that obvious, right? But if you see that you can repair that column, for example, you see that you can repair all those that row, you see that you can repair all those rows and you can repair the columns. And at the end, yes, that was repealable. What about this one? Maybe notice that in these, going back a little bit, only 25% of the data was available. So what about this one? Well, we have almost 75% of the extended block available. Sounds like repairable, right? But actually it is not. It is not because you cannot repair anything. There is no row or column where you can start repairing in every row or column.
00:10:43.748 - 00:11:32.354, Speaker A: You either have the row or column already there, so you cannot repair it because it's already repaired, or you have only three pieces, so you cannot start repairing. So this is a no, right? What about this one? So there are different reasonings here. One is, we just simply don't have enough data to have it repairable. It's less data than the original data, it can't be repairable. You can extend it, and you get back to the previous one, and you see this is not payable. So we have this nice data structure, and we understand how the Plymouth works on this, and we understand how sampling into this could work. But actually this is not done locally, this is done over the network.
00:11:32.354 - 00:12:20.724, Speaker A: So let's see the networking perspective. Let's see what happens on the network actually when we are sampling. So how we check data availability over the network in the context of Ethereum. So notice that in the context of theory, there are different nodes and they have different goals. So validators and full nodes have different goals that they want to achieve from this extended set of data validators. Their goal is they should either generate many attestations if the data was available, or they should generate very few attestations if the data was not available. And that's the system level goal.
00:12:20.724 - 00:13:32.450, Speaker A: We don't like things in between for notes, it's a bit different. They want to find out with a very high probability during the statistical sampling if the data is not available, because if it's not available, then it can be a block which maybe it was even validated, but maybe there is some super majority malicious among the validators, or maybe for other reasons it should not be accepted. So if it figures out that it is not available, then there might be data withhold. And depending on the use of this data, whether it's used for an optimistic roll up or whether it's used for ZK roll up, there are different use cases. But for each one of these, if the data is withhold, it can be a huge problem. So we have to get the data from the block builder to the validators and to the nodes in a way that they can satisfy this. So it's actually a process which goes through many steps, the individual pieces here.
00:13:32.450 - 00:14:22.314, Speaker A: So there is, the block is being created, extended as I was showing. Then the work proposal, or the builder is disseminating the rows and the columns to the validators, and the validators do the job of validating or not. And then there is another thing, which is the data is pushed into a neutral structure, a DHT, from which then the nodes can sample. And then this structure might also need repairing because nodes are leaving, that is churn. So there are all these processes going on behind the scene. Now today I will focus on the dissemination of samples to the validators. So I will not go into the DHT part and I will not go into the actual sampling over the DHT.
00:14:22.314 - 00:15:04.364, Speaker A: But let's see what happens between the buck producer and the validators. So from here there's two validators. Each validator is checking a number of rows and columns, and by default we are speaking of two, because it's an easy example, it's a valuable parameter at the moment. So each validator is selecting randomly two rows and two columns and they are checking that. And that's the check. Is it dead or not? There? And just to give you a bit of dimensions of the problem. So we have 500,000 plus validators, around 8000 plus beacon nodes behind which these validators are sitting.
00:15:04.364 - 00:15:44.824, Speaker A: You have those 260k segments which are sitting in 1020 4000 columns. And we have something like 4 seconds to do this. So how do we do it? There are different proposals out there. The base proposal that I'm speaking of now is to use a gossip sub or a gossip sub like protocol, I would say, because I don't think it will be the gossip sub as of now, which I will version. So we use a gossip sub like protocol, and we knew a number of topics in gossip sub. So if you know gossip sub, you know what it means if you don't know. Gossip sub is building mesh network like you see there.
00:15:44.824 - 00:16:48.704, Speaker A: Those are beacon nodes behind which you have validators which are interested in the 243rd column. And they form a small mesh network with a given degree in which they are sending the column so that everyone has it, and you have 1024, all of these networks built. And we like gossip sub and we like gossip sub like protocols, because they are robust, they have different reliability mechanisms. So one is that they are building this mesh to push, they are sending gossip messages so that nodes can pull information. And they have also using the library transport, they are using TCP doses, has downsides, they send lots of duplicates and they are not yet ready for large messages. But you see, we are not just using dossips up here we have two more technologies. Here you have the azure coding and we have 2d structures.
00:16:48.704 - 00:17:27.124, Speaker A: So we are adding two more layers of reliability. We have the lead Solomon code, which we can use SF, or rather the collection code from communications, if you know that. So basically if you have halved the row, if you received it, you are node, you receive the half, which I will have half number of segments, you can repair it. And the topics are cross connected. So if you receive something on the row, you have it on the column. So we can use these two techniques as well. So we have five reliability mechanisms there with which we can play to have a system which works well.
00:17:27.124 - 00:17:50.614, Speaker A: So what we did is a simulator for this. It's in Python, which is nicely accessible. We hope for others it's very much work in progress. And everything I show here is preliminary. So don't take the exact numbers. And we do many simplifying exceptions. From the networking perspective, we don't care about TCP slow start.
00:17:50.614 - 00:18:42.968, Speaker A: We have 100 milliseconds rounded time at the moment. And this is what you see on the right side is one single dissemination with a builder which has 1000 megabits per second link. This is a conservative estimate, but we are not counting with some factors. We have 5400 nodes which are kind of solo stickers, one validator with smaller bandwidth, and we have 1600 nodes which are having their beacon nodes, handling a bunch of validators and having a better network connectivity. And what you see on the right side is kind of the progress, the evolution. So on the top below there's the time. So we are arriving to 1 second, and in 1 second nodes are receiving samples and validators start to validate.
00:18:42.968 - 00:19:12.762, Speaker A: Validators validate when they have all the rows and columns. And then you also see a bunch of network statistics. What is the transmission bandwidth, the reception bandwidth, and how many duplicates you have. Now how we use this, let's see one example. This is about message size. So I said we are sending those in columns, but you can do it in many ways. So on the left side what you have is using gossip sub, with messages being rows and messages being columns.
00:19:12.762 - 00:19:53.974, Speaker A: So message is 280 kb. This is the big message case. And what you see there is that to validate, so that for the green curve to go up, we need something like 1.61.7 seconds. On the right side we are using small segments, so small messages, so every segment of those 260,000 is a message, and then we are much faster. Another example of the simulator is checking boundary conditions. So what you see on the top is something which we know is dependable and available.
00:19:53.974 - 00:20:42.150, Speaker A: What you see at the bottom is something which we know is not dependable and thus should be not available, should not be validated. What you see on the right side is validators, basically all of them validating what's on the left, what's on top in a second. And something below, what you see is that validation progress is stuck at a low percentage. Obviously no one has more than more than 75% of the chunks of these segments. And this is what we wanted to achieve. So we have the simulator now, as I said, it's quite working progress. We are using it to kind of figure out the right combinations, to figure out what gossips are like means.
00:20:42.150 - 00:21:12.334, Speaker A: So we are experimenting with different variations. We will improve the simulation fidelity. We will do parameter space exploration curves. I've shown you were single dance. So they are defined in statistically relevant exploring attacks and then dealing with the other parts of the dissemination to DHT and of the sampling. So thank you. If there are any questions.
00:21:15.354 - 00:21:28.374, Speaker C: Thank you, Taba. Super interesting. Again, very in depth. Any in depth questions from the audience over here.
00:21:30.474 - 00:21:43.434, Speaker B: So I wonder if what the blocks that are, are meant to be available for the period that is under consensus are constantly sampled by the validators.
00:21:44.214 - 00:21:45.414, Speaker A: Why they are constantly sampled?
00:21:45.454 - 00:22:00.994, Speaker B: No, I'm asking if block availability should be like two weeks, then all the two weeks worth of blocks are constantly, each of them are sampled from the DHT by nodes.
00:22:01.764 - 00:22:34.244, Speaker A: Why notes are constantly sampling it? So the validators are doing the verification because they don't validate. If they don't see it available, the notes are doing it because they also want to be protected against the case when the validators themselves have a super majority of malicious misbehaving notes. So it's kind of a case where they just don't believe everything to do.
00:22:37.264 - 00:22:47.844, Speaker C: Okay, another question from the audience. No. Going once. Going twice.
00:22:49.784 - 00:22:50.120, Speaker A: 2.52.752.
00:22:50.152 - 00:22:56.636, Speaker C: .8 okay, very good. Thanks again.
00:22:56.700 - 00:22:57.504, Speaker A: Saba.
00:22:59.524 - 00:23:02.044, Speaker C: We have Chuck.
