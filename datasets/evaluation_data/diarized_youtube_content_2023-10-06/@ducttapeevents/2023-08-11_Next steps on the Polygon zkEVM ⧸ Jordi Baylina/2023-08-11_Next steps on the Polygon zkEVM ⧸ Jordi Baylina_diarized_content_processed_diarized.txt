00:00:11.680 - 00:00:48.016, Speaker A: Well, hello everybody. I'm Jordi Baillina. I'm the technical league at Polygon ZKBM. As you know, we have been developing for the last year and a half, I would say one, two years from now on, ZKBM. For me, ZKBM is, well, it's a ZK roll up type two, where it works exactly the same that Ethereum does. EVM is ethereum virtual machine. That means that you should be able to take any smart contract the way it is and just throw it to the network and it should work forward.
00:00:48.016 - 00:01:53.030, Speaker A: You should not need any special tooling, you should not need any special compiler, you should not need any special wallet or anything like that. It's another Ethereum like network. And this is what's CKVM, that this compatibility is what makes the thing so cool, that we can reuse all the ecosystem, all the tooling and all the developments that are happening in the Ethereum space. We launched it three weeks ago. Network is up and running and is the first of the kind, the first of a kind in the space. So what I'm going to explain in this presentation is mainly what are the next steps? So what is going to be doing the ZKVM team inside polygon for improving and doing the ZKVM actually to scale and to make it better? So let's start for some of the things. The first thing is that we want to be a type two roll up.
00:01:53.030 - 00:02:18.918, Speaker A: Right now we are a type three roll up. That means that we are Ethereum equivalent on anything except that right now we need to implement some precompiled smart contracts that are not available right now. These are the Shadow 56. This is probably the most torsion. Then it's Blake and pavings. We're already working on that. But that's the thing that we want to.
00:02:18.918 - 00:03:12.364, Speaker A: The main differences between Ethereum, Ethereum and ZKVM. Okay, then we want to remove that ZK contours more on that a little bit later. Second thing is, we want to improve the prover. So right now if we see the structure of the prover that we have right now is big proof where it has different state machines. We have the main processor here. It's just, that's not problem anyway. So we have a processor, we have some binary arithmetic, some catch acts, some auxiliary state machines where you do most of the heavy operations.
00:03:12.364 - 00:04:17.802, Speaker A: The problem that has while this is working, well, the proof right now takes about two minutes in a big cpu machine to compute about 10 million gas. Okay, but the problem. So if you want to improve that, the big problem is that depending on the, so the quantity of catch acts that, for example, we can do in a proof is a fixed number, or the number of arithmetic operations is a fixed number. So the idea is that we need, when we are, when we are processing some transactions, we need to count how many of these resources we call these zigga counters we are using, so that if we are getting out of the limits, we can stop. The proverbs say, okay, there is no space for the provers, and then we just do a no operation state transition. So we can prove that in our system, we can prove that transaction cannot be proven, so that this allows us, for example, to have forced transactions in the system. Okay, but this limit, having this limitation, what happens is that you are not using a lot of the times, you are not using other resources.
00:04:17.802 - 00:05:16.174, Speaker A: So there is a lot of, so maybe in a batch, if you have 2000 ketchups, and maybe you just use 1000, but you still have polynomials for 2000. So here there is a lot of waste in resources. So the idea is, instead of having a big monolithic proof, the idea is to create using recursion, the idea is to have proof with many soup proofs. So the idea is to create a proof for the main processor, maybe a proof for the binary, and a proof for arithmetic. And this proof of the binary, the idea is that you can choose, so you have like three different circuits, one binary that's small, another it's a little bit bigger, and another is a little bit bigger. And then the prover, the recursion proof, the one that packs them all together, can decide which circuits you want to use. So with this we, we can fine tune very much.
00:05:16.174 - 00:06:00.304, Speaker A: How much fine tune very much. So depending on the transactions that you are processing, you will use one circuit or the other. So we will waste less resources according to that. The other thing, so this is good for when you have less resources. But the other thing that's interesting is that if there are some transactions that requires, for example, a lot of binary operations, the idea is that we can aggregate. So we can, instead of creating one single proof for binary operations, we can have many proofs of binary aggregations and we can aggregate all them together. So this allows us to have an infinite number of resources inside the proof.
00:06:00.304 - 00:07:01.396, Speaker A: So the advantages of this is there are many. The first is that this will allow us to remove, if not all, most of these ZK contours. This is the equivalent of the gas somehow is the limit, the resources that you are consuming. So we don't have to do this accountability of these resources and then the system is going to be much easier at the ROM level. And the other advantages there is, advantages is that right now, one of the big limitations in the system is that the prover requires a lot of memory. Requires right now we are using five terabytes memory in a server with five terabytes, which is big server on there, because the proof is too big, because we are splitting, if we are splitting the proof in smaller proof, the memory requirements are going to be much smaller. And this will reduce also the cost of generating the proof.
00:07:01.396 - 00:07:56.442, Speaker A: The other thing is that because the proofs are going to be smaller, then it would make sense to compute the full proof inside the GPU, okay? And this will also accelerate a lot the generation of the proof. So this model of variable degree composite proofs allows us to be more flexible and to improve the timings and the latency. So how much long it takes to generate a proof, especially when we go to the GPU. Okay, next thing is, in order to support these things, we need to expand the plural language. The PL is the language that we use to build ordinary meditation of the ZKVM. So here we have some plans to extend the PL language. I'm going to give you some highlights of the things.
00:07:56.442 - 00:09:05.646, Speaker A: The first thing is to support these variable degree composite proofs. So while we adapt the language, so that each state machine, we just say which is the soup proof, and in each subproof we can specify different sizes, and then we can say that, so that this soup proof can be aggregatable, so we can build pill, and then the proving system should support automatically, so generate the proof or the composition of proof automatically from the pill language. So that's important. Next thing is the constructive language. Right now, writing pill is very linear language. You just, you need to put all the identities one after the other, okay? And what happened is in some complex state machines, for example the arithmetic state machines, we end up doing a secondary program that's writing pill. It's like a kind of a script, JavaScript script that generates pill.
00:09:05.646 - 00:10:16.624, Speaker A: So it's just so, and then this is hard to audit because you need this secondary program, the one that generated pill. So the idea is to integrate, instead of having a separate script, the idea is to integrate that in pill. So we'll have a programmatic, so you will be able to define constraints in a loop or with conditions. So this will allow us to do more complex pills in general. One of the learnings is that when you are working with Starx, it's more optimal if you go to wider, to wider, so to many polynomials, so wider proofs and shorter degree, so small polynomials, many polynomials, smaller polynomials. In general, you get better performance mainly because you have much less copy constraints in general. But when you work with many polynomials, that means that the number of constraints gets more complex and then it's more necessary this programmatic way of writing these, these circuits.
00:10:16.624 - 00:10:58.304, Speaker A: Okay, another thing is the, well, conditional constraints. A lot of this is more for a readability thing, but sometimes there is a lot of constraints that only applies to certain lines. So the idea is to use this structure, which is much more readable than the one that's in the old way. We also allow to define challenges. So if you check how the proof is done, mainly it's many steps. So you commit to some polynomials, you send a challenge, then you commit to other polynomials, you send a challenge back. And there are different stages.
00:10:58.304 - 00:11:55.904, Speaker A: Right now in pill, the idea is that you just define the first stage and the others are automatic when you are using block apps or when you are using permutation checks. So you have access in the to pill only to the first stage. Here we are extending pill so that you can define your own challenges and you can define polynomials in different stages. So this will allow to generate other schemas for proving different things, things like equivalent, like blow caps, but different schemas that they would not need to be required to be implemented in build. In this case, for example, this is an example how blue cap will be implemented using this. Actually in current build you don't need that because you just used in directive and it's much shorter. But if you want to implement a different schema, then this should be possible.
00:11:55.904 - 00:12:36.548, Speaker A: This is not constant. Here is the constant generations. Right now, when you are writing a polynomial schema, actually you need to do like three things. One is generating the constants, the precomputer polynomials, generating the pill, which is the constraints, and then it's generating, actually it's generating the weakness or computing the trace when you are computing the proof. So you need a program to compute the trace. The idea is to put this step of generating constants inside peel. So instead of having a separate program, you can put it inside peel.
00:12:36.548 - 00:13:11.680, Speaker A: This has a couple of advantages. The first is it's much more readable, and when you want to audit some of the circuits, you just need to go to peel and that's it. But the second thing is that a lot of the constraints are cyclic. And when you have cyclics, when you detect cyclics, then you can do some tricks to improve the performance of the prover. So having in the definition language makes a lot of sense. Okay. Also in pill right now, you can access only to the current row and to the next row to do the references, because mainly what you're doing is just state machine.
00:13:11.680 - 00:13:49.900, Speaker A: But here we will allow to access to maybe many rows in front and many rows before, so you can have access. And this is much more flexible and much more readable. And that of course, if you are accessing too many values, you will need to do more openings for each polynomial. But this is very convenient in some cases. Finally, we also have a special notation for range checks. Actually we are doing range checks with a normal blue cap. But if you can define that in the circuit and you know, that's a range check.
00:13:49.900 - 00:14:30.974, Speaker A: There are other constructions you can use, other constructions that are more efficient. And a lot of the lookups that we're using are for just rain checks. So we can optimize also a lot here in the rain checks. Okay, so this is very much, one of the line of works is this pill. The other important work in the ZKVM context is about compression, okay? So right now the roll up is just sending the transactions in Rao in the, in the blockchain. But the idea is to compress this data, that availability is going to be one of the bottlenecks in the least in the short run. So the idea is to, instead of putting the transaction, is pushing a compressed version of the transaction.
00:14:30.974 - 00:15:26.526, Speaker A: This is perfectly possible. The thing is defining what's the best compression for compression. Well, in general, if you see how compression works, in general, a compressed file is just a set of commands and parameters, and each command is just writing part of the file. Good example, for example of these comments is for example, write, if you want to write just some uncompressed data, you just, okay, just print this or just output this and put this in the file. You can for example, do a rewrite, just reuse part that, just go back in something that you write before and just write this again many, many times. But when you want to compress transactions, you can do some special compression. For example, give you an example is for example, for the values, the values in general, values are typed by humans and humans, the significant digits used to be quite small in general, so you are sending one ether or three ethers.
00:15:26.526 - 00:16:06.536, Speaker A: You are not sending 1.54,567,348 ethers. So you can use techniques to compress decimal values very well. And this will allow us to reduce a lot. Okay, but okay, this is, but what else can you compress? The big important part is that in a roll up you can remove the signatures of the data compression. The idea is to substitute, the idea is to substitute the signatures by a proof. So you are proving that you have, or the person that generates a proof have the signatures that things that.
00:16:06.536 - 00:16:55.190, Speaker A: So you can replace all the signatures with a single proof that aggregates all the signatures that you are validating. This proof is, can be, if it's a Planck is less than 1 kb, okay. And the signatory is 64 bytes. So if you have like 16 signatures, it's more worthy to more than 16 signatures it's more worthy to just put this, put this proof instead of a single signal signature. Okay. But if you see how a transaction is on. But there is one thing that's more interesting, and the idea is that for compression, if you want to compress the information, one thing that you can do is use here you can use the state, so you can use the current state to access the state to do some compression.
00:16:55.190 - 00:18:21.644, Speaker A: But you can also have a system to access all transactions. For example, if you are saying an address, an ethereum address is very difficult to compress by itself because it's just some 20 bytes and they are quite random. But if you can do a reference and say ok, use the address that's in. So use this data that was in this transaction that was three blocks ago and in the transaction number ten and it's in the position, whatever this can compress, this can be compressed as data compression very much and you can have access, and here the access to all the transactions, to the raw data of the previous transactions, of the history of the roll up, this has the advantage not only for accessing addresses, which are very much compressed, they have also the access. For example, if you are accessing to the hash values of NFTs, or you are redeploying smart contracts that have very similar parts that you want to recycle, or you are accessing some mercury where the hashes are the same, so many users are using the same mercury. This is for example for the claim system. So this allows us to compress the data very much.
00:18:21.644 - 00:19:11.856, Speaker A: We did some estimations, we expect that the, we can have a compression ratio of five x. This is a little bit pessimistic, but so I think we can do it a little bit better. But this is the numbers that we are targeting. Okay, and if we add that, if we add that to the EIP for 844, that will give us ten x in the cost in the cost. That means that we expect that by the end of the year we can be a 50 x scalability factor by the end of the year when you compose both things. Of course, working in this EIP 4844 is a priority. We have a special people that's dedicated to that from now on.
00:19:11.856 - 00:19:58.840, Speaker A: And this is very important for scaling. For your information, this is the proto dank sharding, but dank sharding. So when sharding happens, we expect that at least it's going to be an extra zero here in the data availability. So this is a little bit the roadmap for this. Of course, having this, when you are scaling, it's not only about generating the proof, it's also you need to process these transactions and you need to have all the tooling around, the sequencer, the synchronizer, the database. So all the system itself needs to scale. So here is a lot of work also in improving the pieces to process that.
00:19:58.840 - 00:20:49.310, Speaker A: I expect that the biggest bottleneck after data availability is more or less solved with blank charting is going to be this. The transactions in Ethereum are quite sequential. That depends one on the other. So having a way to paralyze these transactions, there are some ideas and there is some people that are working in that direction, but this is probably going to be the next bottleneck after that availability happened. One thing that actually we are working, but we are not working. And one of the cool things of ZKVM, something that's opcode compatible and things is that you have things for free. I mean we have the EAP, so we have that, we have account abstraction for free.
00:20:49.310 - 00:21:24.974, Speaker A: Account abstraction does not require anything in l two. So you can take all the account smart contracts, deploy it in the CKBM. You don't even need to recompile them, you just throw them and you have these account abstractions for free. So this is one, this is a ZKVM, it's Ethereum. Okay, so if Ethereum is improving, the ZKVM is going to be improving. Is the Ethereum ecosystem works? Well, we're just going to get the benefits. We will have the benefits of the Ethereum itself and we will keep the full compatibility.
00:21:24.974 - 00:22:01.164, Speaker A: Just a couple of notes for these EIP. There are some optimizations. Knowing that this is happening, there is a couple of optimizations that you need to take in account. One is that the sequencer, if it's a centralized sequencer, it becomes very much, the sequencer becomes very much like a miner. So you need to have some API for the CIP that you need to implement, but we will implement. And the other thing is also, it comes in more in the, for the, for the compression. Okay.
00:22:01.164 - 00:22:47.864, Speaker A: If you want to remove the signatures of an account abstraction, you need to take in account that in the design protocol. So we are taking accounts that we can also remove the signatures from the account abstraction, user operations or transactions that are happening here. But besides that, I mean, this is just optimizations and everything should be in place on that. There is a full topic. This is something, well, we are start working on that is decentralizing the sequencer. Just giving you some ideas here is to, you can understand the sequencer as a consensus. So you can replace the sequencer with a consensus layer.
00:22:47.864 - 00:23:28.922, Speaker A: Right now a centralized sequencer is a kind of a dictatorship. Consensus is whatever the sequencer is, the consensus. Okay, so it's a consensus there, but you can replace this with another consensus layer. It's a temporary consensus, because the sequencer only makes sense until the transactions are in the layer one. But during this time, the idea is to change the consensus parameters in the trilemma. So the idea is that in general you want a consensus that have a high finality with maybe sacrificing a little bit of decentralization and security somehow. And this is the idea is to adjust and having some consensus in there.
00:23:28.922 - 00:24:12.980, Speaker A: So not in our team, but in Polygon, there is other teams that are working in that direction. And this is important also to mention, of course, there is a topic of security. I mean, it's the main topic, it's a priority in Polygon. So we are still doing audited bug bounties, rowdy things, auditing the new code. And this is something important that we are strongly committed to this. So it's a nonstop work. Last thing that I want to mention is that we are thinking also in start increasing the trailing wheels.
00:24:12.980 - 00:25:00.382, Speaker A: The idea is going to trailing wheels, very, going to trailing wheels very soon. Trailing wheels, the type two, according to Vitalik, type two very soon. That means in this stage, what it will mean is that if the system works, okay, so the central lights, so nobody will be able to lock or of course steal the funds. So the system is going to be decentralized. If everything works well, if invalid proof is detected, or in some situations, but needs to be proven in the smart contract, then the system may become, may halt, and then that can be upgradeable. Okay, for this, we need to enable force transactions. We hope to do that very soon.
00:25:00.382 - 00:25:21.134, Speaker A: We need to remove the security. Well, this is the force transactions are already programmed it's just a matter of hitting a button. But we want to maybe in two, three weeks. I hope we can remove that. And then we have remove the security council that's removing this kill switch. That's another thing. And the last thing is increasing from ten days to 30 days.
00:25:21.134 - 00:25:48.464, Speaker A: The upgrade time block. Okay, this is a little bit the planning for the end of the year. We are very optimistic as always. This is not a commitment. When you are doing these deployments, you can find things. But more or less, I think that this is where the timing is good. I think it's good to give times for the people to know what internal timings that we are working on.
00:25:48.464 - 00:25:52.244, Speaker A: So that's very much. Thank you very much for listening.
