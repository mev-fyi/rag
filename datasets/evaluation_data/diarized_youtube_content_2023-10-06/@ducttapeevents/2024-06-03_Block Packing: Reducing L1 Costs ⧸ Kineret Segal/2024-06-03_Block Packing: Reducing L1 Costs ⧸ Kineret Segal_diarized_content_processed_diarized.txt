00:00:06.520 - 00:00:33.584, Speaker A: And we are already back with the next speaker. I am really excited to welcome Kinnaret on stage. She is proving group manager at Starkware and today she will be talking about applicative recursion. That's an upcoming startnet feature which will deliver three big benefits. I'm sure we will hear from her what these benefits are. So yeah, you will get an engineer's insight into all of these upcoming features. Please welcome her on stage.
00:00:41.284 - 00:01:05.006, Speaker B: Hello. Hi. So thank you for the introduction. My name is Kinaret Segal and I'm working in the engineering team in stockware. I'm leading the proving group. And today I'm going to actually talk about block packing. So applicative recursion was the previous name, but we've changed it lately because we think blockpacking is more informative name.
00:01:05.006 - 00:01:42.354, Speaker B: So I'll be happy to hear after the talk if you think blockpacking is indeed a good name. And block packing is a very cool feature that we are all going to add to Starknet very soon. This is going to reduce l one cost and do some azure magics that you will hear in this talk. But first, just make sure you all know stockware. So we are a company building validity roll ups for a few years. Two main roll ups we have, one is StarcX and the other one is Starknet. Both are validity roll ups using stark technology.
00:01:42.354 - 00:02:21.620, Speaker B: And our roll up is based on the Starc protocol which is implemented in sharp. Sharp is our shared prover system and it runs the stone proverb. Stoneproover is a prover we just released like more than a year ago. And actually now we are working on a really cool the next gen prover, which is stooprover. But this is for another talk. If you're interested, you can reach out to me or to any other of the Starkware team members here and we can tell you about it. So Starkx and Starknet, we have up to $1.3
00:02:21.620 - 00:03:03.136, Speaker B: trillion in community trading, up to more than $2 billion in TVL. And from the beginning until now, we have over 800 million transactions. So that's kind of a lot of traffic there. So now let's dive in into understanding what this block packing feature is. But before I would like to explain you about the new feature. First I would like to walk you through step by step how the system looks now from the moment the transaction enter into the system up until the state update on layer one. So we have transactions entering into the system and the first step is the sequencer.
00:03:03.136 - 00:03:45.446, Speaker B: So the sequencer takes the transaction and it forms the new potential blocks that are going to be the next blocks for Starknet. Then before we update the state of the system with the new blocks, we need to validate them, right? This is a validity roll up. So we send the blocks to Sharp. And Sharp is the, as I said before, it's the shared prover, it's the system that is in charge of proving a valid statement. So let's do a few minutes of dive into what's happening in sharp. So we use Sharp to promote a complex state. It doesn't have to be necessarily starknet.
00:03:45.446 - 00:04:32.202, Speaker B: Sharp is a shared prover, so it gets input from many different applications and systems. But for now let's focus on the interaction between starknet and sharp. So we have starknet generating blocks and they send these blocks to starknet backend, sorry, to sharp backend. And the backend creates the proofs for these blocks. Then the proof is being sent on chain to be verified on chain using the solidity verifier. If the proof is valid, each of these blocks are going to be registered in some special storage that is called fact registry. So each block is going to have its own fact.
00:04:32.202 - 00:05:16.346, Speaker B: The fact is a unique identifier that identified this block. And the fact that the fact is written there signals that this block passed successfully both the prover and the verifier on chain. And then starknet can do fact polling, it can check that the fact is there. And if the fact of the specific block is valid it can update its state on layer one. And we do it block by block on layer one. So each block has its own footprint of the fact written in layer one. And we update the state block by block in layer one in the startnet side, an extra step in understanding what's happening in sharp.
00:05:16.346 - 00:05:41.224, Speaker B: So maybe some of you know it. We have a recursive flow. So this is a little bit complex, so bear with me. We have blocks arriving to sharp, you see the blocks to the left. And first we generate a proof for each single block. It can be a really small block or a big block, but we generate a proof for a single block separately. Then we don't send these proofs on chain.
00:05:41.224 - 00:06:26.074, Speaker B: This wouldn't give us the scale that we need because it can be a really small unit of computation. So what we do, we verify the proof using our Cairo verifier. So Cairo is the language that our prover knows. If you don't know Cairo, please check it out. This is a very cool language, but anyway, this verifier is not off chain, it's not written in solidity, it's written in Cairo, and it verifies the stark proofs. So now the Chiro verify verifies two proofs together. And then we send this chiroprogram again to the prover to prove that the verification of the two proof, of the two original blocks are valid.
00:06:26.074 - 00:07:02.522, Speaker B: So we get two to one, we squash two to one, and now we have a single proof for two different blocks, and we can do it again. Again we iterate this process and that's how we build the recursive tree of sharp. So what you see here is the recursive tree. The leaves are the original blocks. But as I mentioned before, sharp is a shared prover. So the leaves can be not only of starknet blocks, it can be of other applications, it can be a different blockchain operating on top of sharp. It can be stark x, for example.
00:07:02.522 - 00:07:50.530, Speaker B: But any logic unit can be plugged into the leaf. And sharp eventually generate a single proof for all of them. The last proof is the root proof. And the root proof is sent eventually on chain to the solidity verifier. And this proof is actually a proof that the verification of the proofs of the verification of the proofs of the verification of the original proofs of the original leaves are okay. So sharp sending the proof on chain, and as I said before, it's write a fact for each of the original leaf to signal that these leaves were proof and verified successfully on chain. Okay, so zoom out.
00:07:50.530 - 00:08:36.868, Speaker B: Again, we had the transactions, then it went to the sequencer. In starknet, the sequencer forms the blocks, then blocks go into sharp, this recursive tree in sharp, and we send the last proof, the proof of the root on chain. Then for each of these leaves, for each of the blocks, we write facts on layer one to signals that these blocks are valid. And now starknet can update its state on layer one based on these facts. So we check that the facts are there and then it promotes the state one by one for each block. We also send the state diffs on layer one for data availability. And we use the blobbing mechanism, the new mechanism that was introduced lately in Ethereum.
00:08:36.868 - 00:09:20.754, Speaker B: So we send the state if through there, we use a single blob per a single block, no matter if the block is full or not, we have a blob for each block. So that's the end to end. Wait, can I go back? Okay, so that's the end to end flow. And now let's think of what we are paying for in this flow. So I would like to focus on the block footprint, the fixed per block cost on layer one in that flow. So to the left you have the different operation we need to do for that flow. And to the right, for each unit we do it.
00:09:20.754 - 00:09:38.430, Speaker B: So we do a state update. It costs eighty six k. And this is done, as I said before, per each block separately. Also, for the blobbing mechanism, we need the KZG precompilation. It costs fifty k. And we do it per each block separately. Per each blob separately.
00:09:38.430 - 00:10:09.400, Speaker B: But we have a blob per each block. Then we have the factor distortion, this identifier that we run on chain for each block. This is twenty three k per each block. And then there is something which is called memory page transaction, which is the output of the block. And again, we pay thirty six k per each block. So all in all, we have almost one hundred fifty k per block, plus fifty k per blob. And this is done per each block separately.
00:10:09.400 - 00:10:48.264, Speaker B: There is also the cost for sharing the proof. But I didn't get into that one. Because what I want to focus on today is that we have here a linear cost in the number of blocks. And this is not very scalable, right? Because if we have more traffic, we have more blobs. We need to pay a linear cost in number of blocks. So one of the motivation of the new feature, the block hacking, is to decouple this connection between the blocks and the fixed payment per block. Okay, but if you are worried, don't worry because we have a solution.
00:10:48.264 - 00:11:19.494, Speaker B: But first, I would like to introduce you how we decrease the l one cost in the current architecture without the new feature. So what we decided to do is to make the blocks bigger. Okay. We used to operate to process around 10 million Cairo steps per block. But we gradually make it bigger lately. And now the blocks are of size around 45 million Cairo steps. So this is more than four times more.
00:11:19.494 - 00:11:49.674, Speaker B: And this is good because now we have a better blob utilization, right? Because the blob are fuller, there are more stay diffs. And also the data for bigger blocks can overlap. So it's also more efficient. Also we write less fact and do less state update, right? Because we do it for each block. But now blocks are bigger. So more transaction can fit into a single block. But this solution has also many disadvantages.
00:11:49.674 - 00:12:38.344, Speaker B: Because now all the units we process, the blocks are bigger, right? It takes longer to run them, longer to prove them. We need bigger memory, we need bigger machines. And I think some really bad, not bad, but something that we really didn't want is that now we have longer l two finality, right? Because on the same traffic we need to accumulate more transactions to close the block. So the frequency of the blocks went down. And this is not a good ux property of this improvement. And also probably some of you already noticed that we are still linear in the number of blocks, which is something that we want to avoid. Okay, so I'm sure you can guess now that we have block packing.
00:12:38.344 - 00:13:22.104, Speaker B: So I would like to walk you through again the flow that we have in Starknet and sharp. But now with the new feature of block packing. So we have transaction, it's the same, right, transaction enter into the system and they go to the sequencer and again they form the block. But now as the name indicates, we do some kind of a packing. So we treat the blocks in some sense and multiple set of blocks as a single block and we send them all together to sharpen. So let's do another dive into sharp. But now with the new feature of the block packing.
00:13:22.104 - 00:14:25.508, Speaker B: So what do you see? To the right is the original tree of recursive tree that I've mentioned a few slides before. So we have the leaves and we have proofs and we create the verification and then prove them again and so on and so forth. But now some of the leaves are not a single block, but they are root of what we called the blockpacking tree. So to the left you have the blockpacking tree and you have the original block, the set of blocks that were sent to sharp as a set. And we generate for them exactly like before, a proof and a verification and a proof of the verification and so on. But eventually in the last, in the root, in the last proof, we squash the output of the blocks and the root symbolize the entire tree. So the root is like a block that is, that represents the entire set of blocks.
00:14:25.508 - 00:15:32.968, Speaker B: And this is like the original leaf of the sharp tree. So how does it help? Because now when we're writing, when sharp, sending the proof on chain, we don't write a fact for each of the original block, but we write a single fact for the leaf, for the leaf of this tree to the left, sorry, for the root of the tree to the left. And also the data is much more efficient because there are many state ifs that some of them cancel each other. And the root there is, is the most compressed state diffs of all the original blocks. So if we do a zoom out again, we have a transaction, they are going to the sequencer, the sequencer forms the starknet blocks. But now the blocks are a set of blocks and not a single block separating. I mean with respect to starknet, they are single blocks.
00:15:32.968 - 00:16:42.032, Speaker B: So the l two frequency, we don't harm it, but from the perspective of the prover, it is a multi set of blocks. And as you see here to the right, sharp has now the new structure of the tree. So it has the tree, but it also has a subtree which is the block packing tree. And now we send a proof of the root of this entire tree, but we also send a single factor for all the leafs of the subtree. We don't send four factors before, but one single fact for all of them. What about state update? So we save here as well, because now as I said, we treat these blocks as a set of blocks and we do a single state update for this all set of blocks. Also for the state ifs we can have a few blobs that we use in order to send the statifs and to do the data availability solution on layer one.
00:16:42.032 - 00:17:26.442, Speaker B: And we close the tree based on the blobbing, how much the blobs are full. So now we can utilize the blob much more and we can gain from that. Also a decrease in the cost. So eventually we do a single state update to write single fact and the state divs are much more efficient. So this introduced a really high improvement in efficiency and in scalability, just to sum up what we've said. So let's go over the same operation that I mentioned before, because we do the same operation, but now let's see how this new feature helps us. So we have state update.
00:17:26.442 - 00:18:12.414, Speaker B: Again, we paid the same, we didn't change the state update itself, the logic it's 86k. But before that, before the feature we paid it per block and now we paid it for the entire block packing tree. And this tree can be really big. I mean here in the slides you see four blocks, but it can have like many blocks. KZG pro compilation again we pay fifty k per blob and the same we pay per blob. But now blobs are going to be much fuller and eventually it will be more efficient. Then fact registration, the fact that we were used to write per each block, we now write it per the entire block packing tree.
00:18:12.414 - 00:18:50.746, Speaker B: So again we pay the same but we pay it on a much larger set of blocks and memory page. Again we have a single memory page for this entire block packing tree. So we paid the 36k, but for the entire block packing tree. All in all the same amount of gas. But we paid for much more blocks. And this gives us the scalability and the decoupling trade that we wanted. So yeah, I started by saying that we want to decouple the stagnant block frequency from l one cost.
00:18:50.746 - 00:19:11.560, Speaker B: And this feature actually does it. Because now we have this entire tree. And this tree can be bigger and bigger. And we don't pay like a fixed cost for each block separately. But we pay for the entire tree. Again, also blob utilization. It's much better because as I said, we can close the tree based on the utilization of the blobs.
00:19:11.560 - 00:19:38.798, Speaker B: And also when you have many more blocks together state ifs are more efficient. There are more chances to overlap between the states. Between different divs of the blocks. And then we can utilize the blob much better. And then eventually it reduces l one cost, which is what we wanted. And we can go back to small blocks, to big blocks. I mean we decoupled the frequency of the blocks from the cost.
00:19:38.798 - 00:19:53.674, Speaker B: And now we can play with the block size without harming the user experience with the l two finality. So we are happy. Okay, thank you.
00:19:56.814 - 00:20:16.720, Speaker A: Thank you so much. On the question on whether that is a good name. I think that is a better name. Much easier to pronounce. Do we have a question from the audience? Please raise your hand if that is the case. I can't see you otherwise. Does not look like it.
00:20:16.720 - 00:20:22.128, Speaker A: I mean you can also find Kinderad around if you have any questions off the stage. Thank you so much.
00:20:22.176 - 00:20:22.424, Speaker B: Thank you.
