00:00:46.154 - 00:00:57.774, Speaker A: Hello. Hello. Are you ready for the next one? So, next one is Caddy. Hello. Let's talk about ZKML. Yeah, let's do it.
00:00:58.394 - 00:01:38.784, Speaker B: Is there a. Cool. Just let everyone getting seated before I start. Yeah. Hi, everyone. So today, this is a miscellaneous talk per the schedule. So I guess it's very hard to fit this topic into anywhere, because it talks about AI, it talks about ZK, and it also talks about optimistic systems.
00:01:38.784 - 00:02:21.194, Speaker B: So I'm the chief scientist at Aura. My name is Kathy, and we are really what we work at. Aura is trying to put AI and crypto together, and we have a quite unique approach, which I will explain today. But first, I will talk about how we are achieving verifiable machine learning on chain. Most of you would probably heard of CKML, serial knowledge, machine learning. And then we'll talk about something that aura has invented, which is optimistic machine learning, OPML, as well as our latest research, which is Opai. And I'll explain that term in a little bit.
00:02:21.194 - 00:03:13.966, Speaker B: So at Aura, we believe that verifiability is a spectrum, right? A lot of the times when we hear about verifiability, it's all about ZK, zero knowledge. And that is often considered kind of the universal, just like the only source of verifiability. But then, actually, optimistic systems are also a form of verifiability, right? Through a different security consideration, a different security assumption, and also a different consensus system. So actually, here are two pieces of our research. If anyone is interested into looking into the paper, it's actually both on archive. So optimistic machine learning is something that we have introduced as like the op counterpart of ZKML. So you have CK rollup and op rollup.
00:03:13.966 - 00:04:16.322, Speaker B: So why not Ckml and Opml then? However, the beauty of having machine learning, and also not really an on chain ledger, is you can have also the best of both worlds. So for a roll up, it has to be either Zk or either op. But now there is also a mix of it, I guess with chains like Taiko, but then with machine learning system, you can actually mix the two. So we can have op and CK at the same time. So for those of you who are not familiar with ZKML, just a quick overview I'll run down. So when we say CKML is actually meaning running machine learning inference in either a CK circuit or a CKML, and we want to do that for a few purpose. So first, of course, for verifiability, we want to prove that we have done some computation, we want to prove that we have run the model but then, apart from a verifiability, often when you hear about CKML, it has two promises, and those two promises I've put with an asterisk for a reason.
00:04:16.322 - 00:05:06.724, Speaker B: So often you are promised input privacy and also model privacy. Not only you could prove the machine learning computation, you can also hide either the input to your model or the model weights. Actually, this is not unique problems unique to ZKML, but just ZK proof in general, which is there are some known issues. Right now we're kind of limited by its performance and hardware, so it's memory consuming, time consuming, and hence is only practical for very small neural networks. There will be more concrete numbers in a little bit. So let's discuss those two points in asterisk. That kind of lead me to think about maybe CKML is not as private or as CK as we thought.
00:05:06.724 - 00:06:04.964, Speaker B: So first of all, improve privacy. Most of the time people talk about input privacy is saying that, hey, you distribute the prover to whoever is generating the proof. So that person only needs to put their input signals or input data into that model, into that prover, get a proof and submit the proof to you. So this mental model, when you think about it, that actually means that we are handing over the model to the user, to the client side. And that also means that the client or the user, if they want, they can actually make as many local inferences as possible. So the problem with machine learning is that once you have a model, even though the model is sort of like a black box, it's private, you don't really know how the proof actually works. You can actually have free inference to generate data that can train a model that mimic this model.
00:06:04.964 - 00:06:47.054, Speaker B: So essentially, I don't need to know even how that black box work. I just need to have ability to generate input and outputs. So actually, in this case, it's actually very easy for the local users to train a model to inverse the compute. So in machine learning it's actually called reconstruction attacks. Um, but in summary, what I'm trying to say here is actually that input privacy is not as obvious. Um, uh, well, it's not as obvious as, okay, let's, let's just construct a zero knowledge proof and you get input privacy immediately. Uh, with machine learning it's actually tricky because there are statistics that you can compute that can potentially reconstruct everyone else's input.
00:06:47.054 - 00:07:51.356, Speaker B: If you know how the model works so well, let's look at the other promise, which is model privacy. Well then I don't distribute the model, I keep the model to myself. I keep the model weights to myself, and whenever a user submit a request gives me some input data, I will perform the computation and then I will give out the proof along with the output. That should be safe, that my model is solely safe. And actually it also goes with the argument that I made just uh, before, is that you can even just with the input and output, and with a black box model, you can still heuristically compute a model that can mimic this computation. Right? So actually the c, in this case, what guarantees the model privacy is not the serial knowledge proof itself, but uh, the scarcity of these inputs and outputs. So the more your model is used, the more input and output pairs you get, the easier it is to reproduce what your model is trying to do.
00:07:51.356 - 00:08:54.053, Speaker B: So actually, you still need some sort of scarcity as well as high inference costs to prevent people from doing so. So in this case, CK alone is not enough to protect model privacy. You actually have to align with some sort of crypto economics, or just economics to make sure that there are no malicious party making enough inferences to sort of get what your model is doing right. So actually in this case, if privacy is not really the point of having machine learning on chain, then really the main point of I having machine learning on chain is verifiability. And if we are only concerned about verifiability, what we can do is actually take an optimistic approach, which is what we have introduced at Aura. So if you take game theory plus machine learning plus blockchain, then you get OPML. So essentially it is like how Op row up works, but with machine learning programs.
00:08:54.053 - 00:09:54.810, Speaker B: So you can have a execution client which executes the machine learning, and only when there are challenges, then you will run this into your for proof virtual machine and have an interactive proof challenge between the submitter and the challenger. So it's a slightly different trust assumption between like OPML and ZKML. So CKML is pretty much trustless in a way that if you're given a proof, you can always verify it. Well, with OPML is any trust assumption, which is as long as there's one honest party in the network, then your system is guaranteed to be correct. So the major difference between OPML and CKML is the size of the models that you can run. So I mentioned a little bit about practicality of CKML. There are actually bigger models that can be run on CKML already, but with sort of crazy time, which is not included here.
00:09:54.810 - 00:10:51.482, Speaker B: But majorly, if you want to do CKML in a reasonable timeframe, these are the largest models that you can use, which is decision forest, maybe nano GPT or GPT-2 something of that sort of size. But with OPML, because of this native execution, you can essentially run any model. So here's just two examples that we've already put on chain, which are stable diffusion and Lama. And these are like at least thousands, if not a million times bigger than the models that you can do on CKML. So going back to this, well, what if we can have the best of both world? What if I want the speed of OPML, but I also want some of the privacy guarantee of CKML. And that's when we introduce Opie. So I'm not going to explain this meme.
00:10:51.482 - 00:11:49.224, Speaker B: If you want to know the meaning of this meme, you can actually google this word and its meaning in Japanese. So Opai stands for optimistic, privacy preserving AI. So it's just a fancy term of saying, hey, let's put OpMl and CKMl together. So we have already established that just now that CK alone actually doesn't guarantee model privacy because of the nature of machine learning. So even if I don't know what the program is doing, I can still actually infer what it is doing with enough samples. So what we're trying to argue is that OPI can also provide a similar level of security with a lower cost. So if model privacy is important, let's say for just some part of the model, then we can just apply CK to the parts of the models that we want to hide.
00:11:49.224 - 00:12:16.938, Speaker B: And one immediate question might be, well, what are those parts? Um, so let's give an example. For example, stable diffusion. I think everyone probably know what stable diffusion does, right? It takes tags into and turn it into image. Um, and stable diffusion is a public model, right? Stability. AI has open source it. So it's not something that is a secret. And there's really no need for sort of model privacy in a CK setting.
00:12:16.938 - 00:12:55.074, Speaker B: Um, so, but however, there was a lot of people who fine tune stable diffusion, Lora or whatever methods that they do. So they fine tune stable diffusion to get images of certain styles that they want. And these fine tuned weights actually are modified from the open source weights. So a portion of those open source weights are modified into these fine tuned weights so that you can get certain style. And these potentially are the weights that you would want to ck. If I use ck as a verb. So you want to make it zero knowledge, but probably not all the other ones.
00:12:55.074 - 00:13:39.028, Speaker B: So actually, the bottom line of what OPA is trying to do is we will just apply ck to the selective part of the model that we want to hide the model weights. And for the rest of the model, we'll just do it optimistically. So you will imagine if you have a model, well, models are typically not linear, but basically optimistic, optimistic, and up to the point that you want to hide. Do some ck and not optimistic, optimistic. And then again, ck for the parts that you want to hide. So stable diffusion was one of the example that I gave. But actually, even for stable diffusion, even just for the fine tuned part, that's still too big for nowadays Zkml.
00:13:39.028 - 00:14:06.052, Speaker B: So that's how crazy it is. I thought that, well, if we take stable diffusion, get only the fine tuned parts, that should be good enough for CKMl. Actually, it's still crazy. You need like hundreds of terabytes to do so, of memory to do so. So, um, here I actually present a more tangible example, which is a, um, fine tuned LLM. So large language model. In particular, we are fine tuning a llamas, uh, llama two.
00:14:06.052 - 00:14:47.140, Speaker B: So llama two 7 billion, which is the large language model that is open source by meta. So, um, the way that they are fine tuning these models are that they, they have certain so called interventions. So in particular, to fine tune, like a llama seven b, they have 32 interventions into these models. So only in those interventions, you are sort of modifying the model weights. Right. So llama seven B means that there is 7 billion parameters. But when, when you're fine tuning, you're only fine tuning around 2 million parameters, and there are 32 of these which are all separate, right? So they don't need to fit into the same CK circuit.
00:14:47.140 - 00:15:23.198, Speaker B: They can be 32 separate ZK circuits. And then each of these are actually quite small. So what we're trying to do is, okay, let's apply OPI framework on this model, and here are some of the benchmark results. It's actually not in the paper yet, because this is. Or some latest benchmark results that we got, and we are benchmarking it against two other projects that have proven lama seven b using the prover. So the black bars are a project called Ligatron or. Yeah, Ligatron.
00:15:23.198 - 00:16:20.606, Speaker B: So they are running it on a very high memory machine and trying to prove the entire lama to 7 billion in their CK vM. And then the light gray bar is actually a latest paper called CK LLM. So what they did is they actually used GPU Nvidia a 100 and then tried to prove the entire lama two seven b. And so from the left to right, you have like the peak memory usage of each of the batch mark and then the prover time and also verify time. Um, so for the scion um, bar, which is actually our approach is done very naively by coding those intervention in circle, uh, and then using just snark js to uh, prove it with growth 16 prover. Right. So all very naive approach, not optimized yet.
00:16:20.606 - 00:16:57.532, Speaker B: There's actually a lot of optimizations that can be done. Uh, but here are some like preliminary results, and this is only proven on my laptop, like M three Macs, um, Mac MacBook Pro. So. And here are some of the results, which as you can see that there's actually a lot less memory consumption, right. Because we're not trying to fit the whole model into the CK circuit, we're just trying to do part of it. And then the proofer time, I actually calculated like the 32 circuits sequentially just to make sure that it doesn't violate the peak memory, right. Because if you do it parallel, then that's not the peak memory usage.
00:16:57.532 - 00:17:25.242, Speaker B: Need to multiply that by 32. But assuming we only have like, let's say three gigabyte of memory, you can do it sequentially and it only still takes like 1 minute. And then actually when you try to verify all the proof, it's actually very quick and also very cheap because it's, you know, in growth 16 and you can do it parallelly. And the circuit is actually very small. And the prover is also small. Sorry, the verifier, the proof size is also small. Right.
00:17:25.242 - 00:18:02.594, Speaker B: So here are some of the. So you can see that actually by applying like Opai onto a CK circuit, you can have a lot of gain and something that seems ridiculous, which is like taking 14 hours. So yeah, to put this into perspective, this is 14 hours and this is 14 minutes, I think, to something like 1 minute. And when we say proving, actually it only means one token. So one token in LLM means like a word. So to generate a whole sentence that means that you have to do this repetitively. So 14 hours or even 40 minutes is definitely not something that is practical.
00:18:02.594 - 00:18:45.282, Speaker B: But then the OPA approach can actually still be optimized by, let's say changing the proving scheme, changing the method that we actually currently code the circuit. I only use circum. You can totally use lookups or other things that's plonkage and improve that. Right. So um, that's sort of the results. And um, for the rest of the talk, I actually want to cover a little bit, which is now we have a tangible, practical, uh, verifiable machine learning approach, uh, that can be verified on blockchain. And what do, or what, what does aura actually do, um, like do with it, right.
00:18:45.282 - 00:19:42.734, Speaker B: So a lot of the times we get a lot of question, which is like, why do you want to put model on chain? What's the benefits of putting model on chain? And most of the time people are thinking about the top question, which is like, what can AI do for blockchain? What sort of model we want to put on blockchain? But at aura, we actually asked the flip side of the question, which is what blockchain can do for AI. And so there's actually, we're trying to solve two of the issues of AI in the industry. And just now we have only covered the first one, which is verifiability, right. Why do we want verifiability? It's ultimately because there is closed source models that is like dominating, right? And we have no idea what it's happening with the models. Sometimes the performance of chappie JPT actually drifts. Drift, meaning that it slowly degrades or changes its performance. So we actually need some verifiability to know that what service we are being provided for.
00:19:42.734 - 00:20:30.734, Speaker B: And also, the second problem is actually not so much about verifiability, but about monetization. So often AI companies that have open sourced their foundation model means that there's no revenue. I don't know if you have heard, but stability AI is also actually not doing that great on the book. How do you actually monetize? Or how does AI company actually move forward other than just close sourcing their models? They need tokenization. So what we have just covered actually only do the first one, which is verifiability. And we need the second piece of the puzzle, which is tokenization. So at aura, what we've introduced is a concept called initial model offering.
00:20:30.734 - 00:21:41.814, Speaker B: So it's actually tokenizing a model. Um, what you have is model ownership. So that's actually an ERC that we've written. Uh, essentially it specifies how a person would benefit from, let's say, model generating revenue. But not only that, we also need sort of verifiable inference, and the inference being sort of a piece of asset, right? So as an example, let's say we have an AI generated art, right? This piece of art turns into NFT. And first of all, we need to verifiable ML because we want to verify that this art actually comes from this model, so that we know that the revenue of these arts should flow back to that particular model and its owner. So it's kind of a closed loop where we can sort of monetize and design a scheme where people can own a model and then also have a revenue source flowing back to the model ownership, even if the whole thing is open source, because we can always have verifiable inference knowing that, hey, this model has been used for certain inference.
00:21:41.814 - 00:22:36.146, Speaker B: So that's sort of our approach at aura with AI cross crypto. So we introduced optimistic machine learning, and now we're seeing how ZK, as well as privacy preserving features can go into it. And then we have actually realized OPML in practice with our on chain AI Oracle. So our AI Oracle is sort of an interface where developers can call in a smart contract inference. And then at the back end, or the layer, or the infrastructure layer, we actually use OPML to guarantee the security of this AI Oracle. And then of course, the tokenization that I've just mentioned. Right, so that's the end of the talk here, is like, if you want to access the preprint on archive, here's my link tree.
00:22:36.146 - 00:22:41.814, Speaker B: And as well as also leads to like the ERC discussions, if anyone wants to take a look at the ERC.
00:22:42.754 - 00:22:50.354, Speaker A: Thank you. Any questions?
00:22:55.734 - 00:23:07.594, Speaker C: Could you expand a bit more on why you have to prove the entire net, all the parameters? Would it possible to prove a subset of the nodes?
00:23:09.174 - 00:23:13.062, Speaker B: Sorry, I'm not sure if I understand the question. You mean like the model or.
00:23:13.198 - 00:23:28.164, Speaker C: Yeah, so as I understand it, the issue with ZK for DNNs is that it's extremely compute heavy to traverse the entire roll net and prove every computation. Yeah, so that's similarly optimistic. Sorry.
00:23:28.284 - 00:23:40.244, Speaker B: Yeah, that's true. But then you also need to, you think it's the same problem with optimistic or what you. Sorry.
00:23:49.104 - 00:24:20.754, Speaker C: Hello. Okay, yeah, I'm saying that if the issue with ZK is that you can't prove the entire DNN, would you not be able to prove a subset of it, and then set an arbitrary confidence threshold where you say, hey, given I've proved n different paths, that is sufficient enough confidence that if all those proofs pass, I assume the validity of the rest of the net and the output.
00:24:21.974 - 00:24:57.474, Speaker B: Right. So that's exactly what OPI is trying to do, right, which is doing it partially, I think it's not considered safe for doing it ZKML, just for part of it, for multiple reasons. So first of all is a lot of time people are talking about input privacy, and the less of the network you prove, the less sort of obscure your input is, so the easier it is to inverse compute what the input was. So that's why at least for, if you want the input private cks, it's not safe to only do CK for part of it.
00:24:58.434 - 00:25:03.866, Speaker C: Yeah. So you couldn't prove like the final layer. Say it has 512, right?
00:25:03.930 - 00:25:04.178, Speaker B: Right.
00:25:04.226 - 00:25:21.654, Speaker C: You say, okay, I'll prove the path to one of these. And if as long as that proof holds, I'll assume that the 511 remaining are also valid. Right. Then you would hide the rest of the net. Technically, or am I misunderstanding it?
00:25:22.154 - 00:26:04.834, Speaker B: I'm not sure if you can do that because you also need to prove where the. Well, so the neural network is complicated enough that even if you take just one output, let's say the final layer, and you just want to prove one of those, I'm not, it's not necessarily one out of 512 size smaller, if that's what. Yeah, I think you still need most of the layers, especially like, if it's not like, because it's not just like fully connected network, right? Yes. If it's a fully connected network, then what you will say will hold true. Where I would just, if you just take one path, it's actually one out of n of the size.
