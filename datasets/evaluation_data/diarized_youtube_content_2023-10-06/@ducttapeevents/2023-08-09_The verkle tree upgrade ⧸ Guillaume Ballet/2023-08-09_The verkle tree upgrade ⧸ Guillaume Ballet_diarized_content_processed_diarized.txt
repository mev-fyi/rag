00:00:18.200 - 00:00:18.740, Speaker A: Nice.
00:00:20.094 - 00:01:07.744, Speaker B: All right, thank you. Before I start, I've been asked to make a small announcement. The Ethereum foundation and Uniswap are trying to fund local projects or are seeking to fund local projects, and so you can help them choose which projects to fund if you visit the quadratic funding booth. Yeah, very nice, ladies. They will show you how to do the voting so that you can help them choose between great projects and crazy projects. And speaking of crazy projects, I'd like to talk how Ethereum is going to transition from vertical trees to, sorry, from the current Merkle Patricia tree and start using Virko trees. So I'm going to give a very quick primer on Virko trees.
00:01:07.744 - 00:01:59.014, Speaker B: And the idea is that Ethereum currently stores all of its states in the leaves. How does the clicker work here? The leaves of a tree? And so that's a lot of data. And so you can't summarize all that data directly, like, add all this data directly into the block. So what you do is you use a small cryptographic summary of part of the data, and you build a hierarchical structure. And the top of the structure is what goes into the block. So you don't provide all the data, but you give some guarantee, some commitment that data is the correct one. And now the challenge is what happens if you want to prove that, for example, the red square belongs to the is somehow validated by this information.
00:01:59.014 - 00:02:47.676, Speaker B: So the way we currently do it is by using a commitment technology that's called a hash, and it served its purpose well. But the problem with the hash is that to verify a hash, you need all the data as input. So if I want to prove that this indeed contains this red square, I also need to pass the green square. And then once I've got, I verified this bit, I need the sibling here, the other green sibling, to verify that this was correct. And in the structure that is used by Ethereum, you don't have two children per node, but you have 16. So for each level, you need to pass 15 siblings. And that makes for a lot of data.
00:02:47.676 - 00:03:31.114, Speaker B: And we want to start making Ethereum more stateless. So we would like people to not keep that data, just grab it from the block itself. So if we want to put all this information in blocks, the blocks would be way too big. So what vertical trees do is that they replace hashes with what's called vector commitments. That's the v for Verkle. And what the technology of vector commitment allow you to do is to just pass the information along the path. It gives you a way to prove that this is a member is verified by this without passing the siblings.
00:03:31.114 - 00:04:21.022, Speaker B: So the proof would become much, much smaller. And this is why we're trying to switch to this technology. So, very quick summary of changes. Like I said, we replace hashes with type of commitment, that's called a polynomial commitment. We add the proofs to the block, we add all the data that you need to verify the block to the block. And then, because the data, because you don't no longer need to have so many, like the number of siblings in a tree no longer affects you, we increase the number of siblings to 255. And that makes for a more shallow, which means the path itself also becomes smaller, shorter, and as a result, the proof gets smaller.
00:04:21.022 - 00:05:06.930, Speaker B: And another big difference is that instead of currently, we're differentiating accounts from the contract data, which is called storage. We just instead, like the tree, becomes agnostic. All it sees is 32 bytes, values doesn't matter to it what they are. And the last significant change is the gas instead. Until now, the gas was a gear that limiting the amount of leaves that were at the bottom make it more expensive to add more leaves. Now we just try to make the block smaller. So every time you need to add something to the block, proof it will cost you more.
00:05:06.930 - 00:05:44.702, Speaker B: But storage is no longer really the target. So, just a quick update on the current state of the implementation. There is a testnet that exists, you can reach it at this address. I've been told the faucet is broken, of course, as should happen during any demo. But yeah, apart from that, it's working well. And yeah, we have basically two implementations that didn't really manage to talk to each other yet, but we understand the problem, so it will be fixed soon. So let's talk about the transition.
00:05:44.702 - 00:06:43.604, Speaker B: Now, the problem of the transition is that you have very large data structure using one proof format, if you will, and you need to convert it to a different format. And on top of that, this new format is a bit more cpu intensive. So to do the transition, it's going to require a lot of resources, and that is the biggest problem. Yeah, one thing I wanted to add is that it's not scheduled for like right after the merge. There's going to be an intermediate fork in between, because we need to reduce to remove self destruct. And the reason why we do this is because I was saying, we no longer make the difference between an account and storage. So you can't really know what storage an account has, so you can't really destroy accounts anymore.
00:06:43.604 - 00:07:25.434, Speaker B: So yeah, we're dependent on this feature. So, right. Explaining the issues. The first one is memory. You can more or less fit all of Ethereum data on a sizable disk, but no one has enough memory to store everything. So if you want to store this structure that is currently on disk here into ram, it just doesn't fit. So what you need to do is instead, because one, because for example, this structure here can be summarized by this one, by this node, you just replace it by its commitment and then it fits.
00:07:25.434 - 00:08:12.160, Speaker B: But the problem is, it's called thrashing. What if I write a new value here? This node is not currently present in memory, it's just a cryptographic summary. So what I need is to make some space by replacing some subtree by its cryptographic summary, if you will then load the previous state of the tree and finally do the write. And that's quite intensive. There's a lot of communication between the disk, the Ram, et cetera. So that's called thrashing. And you can see in one of the early tests, this represents the memory usage, and the memory usage goes really high.
00:08:12.160 - 00:08:41.534, Speaker B: And then I hit the limit of the memory. So I have to make a lot of space in ram. And you can see this is the growth of the database. Every time there's memory being freed, there's a bit of a spike in the database. So clearly there's also, the database is more efficient at storing data. There's a lot of optimizations, but the problem is really visible here. You need to empty memory really often.
00:08:41.534 - 00:09:34.034, Speaker B: So we have four proposals I'm going to go over. The first one is, I would say the simplest, also the most unacceptable. It's just saying, ok, the conversion is going to take a week to do. For example, for a week we just give empty blocks so that a node has the resources available to perform the transition. So that's why it's also known as rollup appreciation week, because for a week all you can do is use rollups and hope no one is going to try to do anything nasty. So it has a lot of advantages, let's be honest. If you omit the usability one, which is all the resources on the network are dedicated to this.
00:09:34.034 - 00:10:41.310, Speaker B: But yeah, usability wise it's not really recommended. So there's another possibility, which is to freeze the tree at some point, and then clients will only store the delta for a period of time to give only a limited amount of users with a lot of computing power the time to do the translation, and then the translation, once it's done, it's verified it's spread over the network, and then clients apply their divs to it. It's nice because not every client has to do it, not every node has to do it. The problem is that you need to make sure that the result of the conversion that you are given is valid, and that means you need to solve this problem. The verification happens at the social level, which is not as good as an algorithmic solution. There's another solution that is a bit similar. I call it stateless sync.
00:10:41.310 - 00:11:25.380, Speaker B: And the idea is that because the proof will end up in blocks eventually, might as well use this fact. Sorry. Because the data you're going to use will be present in block, you can reuse that idea for the conversion. So you download instead of storing the diffs, instead of each client storing the diff. What happens is that the people doing the conversion also produce the proof with each block and clients. Basically, clients have to trust even more, but they have to do even less work. But the issue is still there, that those who produce the conversion, they still have to track the network.
00:11:25.380 - 00:11:52.984, Speaker B: And. And that's a bit complicated. The third solution is the one I'm going to detail a bit more. It's called the overlay tree. And the idea is that you freeze the tree in place, the Merkel Patricia tree. So the current format, you freeze it in place, and then you start with a fresh tree, and then other people can do the conversion, and there can be submerging. And I'm going to explain that with pictures.
00:11:52.984 - 00:12:22.664, Speaker B: So you decide that at a given block, the merkle Patricia tree is frozen. So those triangles are supposed to represent an m for Merkel. And. Yeah, you said this block, this is the last tree that will ever be produced. And then the next block, you start with a fresh root. So it's an inverted triangle for verkle. And when you write to the tree, you write to that new root, so the old route here doesn't change.
00:12:22.664 - 00:13:35.280, Speaker B: So what you can do, thanks to that, is after a while, because when you're sure that there will not be any reorgs, you can delete the internal node, because you consider that all this structure here that is needed to verify the adequation between the data and the block. Yeah, it's correct. Otherwise you wouldn't have made it that far. So you free some space, and then the question happens, what if you want to access a location that used to be present in the old tree, but is no longer present? Well, that's the problem of this approach. First, you hit the vertical tree, and you hit the vertical tree, because its short, is presumably smaller. So that's why you go there first, and then if you don't find it, you go to a key value store or the moco Patricia tree, and then you can make some improvements. You can decide, well, because this data was present, I might as well, like in the old storage, I might as well bring it to the new storage if I asked for it next time.
00:13:35.280 - 00:14:24.604, Speaker B: And you can even delete it from the old store. Right. And so some versions of this, of these method, assume that the tree will be frozen forever. But what can happen is that while all the chain progresses, you can also perform the conversion offline. And then when the conversion is performed, you just merge everything, and you can merge everything, like maybe block by block, each block you add more, let's say 10,000 leaves from the, oops, sorry, from the old tree into the new one, and until finally you have no data left. Yeah. So that's one of the methods.
00:14:24.604 - 00:15:16.320, Speaker B: I would say the biggest drawback is that it takes roughly four months to complete. But during those four months, at every block, you can verify that everybody is synced, because you know exactly what the status of their vertical tree is and what the status of their miracle Patricia tree is. So that's the advantage. The problem is, of course, that you're in lock steps with the smallest machine on the network, because you only transfer, for example, ten leaves, 1000 leaves, whatever the smallest machine can support without coming out of sync. And that's pretty much it. I don't know if there's time for a question, four minutes for a question. But before that, what I invite you to do is check the testnet.
00:15:16.320 - 00:15:28.284, Speaker B: Very interesting. And if you have, I also make some threads about vertical tree questions from time to time, so it might be interesting to check that out as well.
00:15:28.944 - 00:15:32.484, Speaker A: Okay, anyone in the audience have questions for Guillaume.
00:15:36.384 - 00:15:37.344, Speaker B: Early lunch?
00:15:37.504 - 00:16:05.410, Speaker C: I know, I was reading an article from Vitalik, I think it's an old article where he said that because the VRKL tree uses polynomial commitments, it uses homomorphic encryption to make it more efficient, and that it could be susceptible to quantum magic. To quantum magic.
00:16:05.562 - 00:16:07.134, Speaker B: Quantum magic, yeah.
00:16:08.354 - 00:16:25.470, Speaker C: Then we will need to scale back to some stark merkel. I'm not sure, I didn't understand it. But the question is, should we approach the verkle route, or do we bet that it will happen and we should go another route?
00:16:25.662 - 00:16:55.814, Speaker B: Yeah, if this happens, if quantum magic happens, whether or not it's likely to happen is, of course, everybody's opinion. But yeah, if it happens, we will have to switch to a different technology. It's not quantum secure. That's true, but you can anyway. The vertical tree is always one step. We want to snark it all eventually, so there will be more upgrades. I'm not really worried about that.
00:16:57.994 - 00:17:11.894, Speaker A: Okay, I think we have time for one more question. Yeah. Okay, Peter, troll alert. Be kind or be really, really trolly.
00:17:13.434 - 00:17:24.342, Speaker D: So, question. During the transition, you have both merkle trees and verkle trees. And synchronization wise, then, would this synchronization be expected to sync both at the same time?
00:17:24.478 - 00:17:52.664, Speaker B: So depending on the method. That was surprisingly nice. Thank you. Depending on the method, it might look, for example, the overlay method. Synchronization is expected to happen on both sides. Yes. When it comes to offline, you need to download one, one big chunk of data once, but then you need to apply the delta.
00:17:52.664 - 00:18:05.204, Speaker B: So synchronization would force us to also download the data. So in that case, we would disallow synchronization for a period of time. I think that would be the best approach.
00:18:06.544 - 00:18:18.664, Speaker A: Does that answer your question, Peter? Beautiful. Ok, I think we're done for time. Thank you very, very much, Guillaume. It.
