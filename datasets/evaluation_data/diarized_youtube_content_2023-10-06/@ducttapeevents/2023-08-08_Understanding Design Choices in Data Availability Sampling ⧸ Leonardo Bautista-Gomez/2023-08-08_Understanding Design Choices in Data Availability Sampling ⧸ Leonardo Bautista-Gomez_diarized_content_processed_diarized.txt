00:00:01.160 - 00:00:27.074, Speaker A: Can you hear me? Yeah. Okay, thank you. All right, thank you very much. So, yeah, I'm Leo. I want to present some work that we have been doing about data availability, sampling. I want to say this is a research collaboration with the Ethereum foundation and the Codex team, which are my team members, team colleagues, Shaba, Dimitri, and myself. So before I start, I want to talk a little bit, introduce myself.
00:00:27.074 - 00:00:53.856, Speaker A: I come from Latin America. I was born in Nicaragua. I grew up in Colombia. I did a bachelor in computer science in Paris and master in distributed systems. Then I did a PhD on supercomputers at Tokyo Tech. And then I worked at the Argon National Laboratory in Chicago for three years, working on error detection for scientific computing. And then I moved to the Barcelona Supercomputing center, where I work also in scientific computing, but also on blockchain research.
00:00:53.856 - 00:01:24.596, Speaker A: I got several grants from the Ethereum foundation. Now I'm a researcher at the status and the founder of Megalabs. So this is the outline of my talk. First, I'm going to talk a little bit about the motivation, which is Ethereum scaling. Then I'm going to try to explain the data availability, sampling structure, some model input that we have, some data that we have gathered that we are going to input in our simulator. So I'm going to present the simulator that we are developing and then some conclusions now. So, Ethereum scalability.
00:01:24.596 - 00:02:08.254, Speaker A: So if we talk about Ethereum like the global computer, what we want to do is to transform that into a global supercomputer that is way, way more performant than we have today. So when we talk about performance for blockchains, we usually talk about transactions per second. Today we can do about 15 transactions per second. And we want to go into a world where we have the capability of executing ten of thousands transactions per second, three orders of magnitude increase. And this is extremely challenging, of course. Now the idea is to do this, to achieve this through rollups, but rollups need space to put the data that they require for them to work properly. And so for that, we need to kind of increase the block size.
00:02:08.254 - 00:02:56.722, Speaker A: Now, if we increase the block size, if we look at the block size that we have today, it's usually in the order of 100. We want to increase this to, say, something like 32 megabytes. Now, the problem of doing this is that we are kind of forcing all the nodes in the Ethereum network to become supercomputers themselves to be able to handle all this amount of data and to make it fast enough for this to work. So instead of doing this what we want to implement is sharding, more specifically data sharding. These are shards, by the way. So the idea is that instead of handling the whole block, nodes will only have part of the block. So you can, for example, divide the block in rows or columns, and then you send a certain number of blocks, sorry, number of rows and number of columns to different nodes.
00:02:56.722 - 00:03:28.736, Speaker A: So that's the idea. Now what is the problem behind this is that sometimes node goes offline and then we might lose the data that they have, right. And of course this is unacceptable. So we use something that is called erasure coding. It's a technology that is well known in computer science. And the idea is this, following one, for those that are not familiar, you take your dataset and you divide it in chunks, like for example, four chunks, as we see here. And then you apply some magic mathematical tool that is going to increase that with some other four chunks, for example.
00:03:28.736 - 00:04:04.634, Speaker A: And now that you increase, you have four extra chunks of data. Now notice that this is not replicated data. This is just some kind of encoded data that doesn't mean anything if you look into it, but will allow you to recreate any four blocks when they disappear, if they disappear. So it doesn't matter which four blocks you lose, you will be able to recreate the lost data. Okay, now this is all good. Well, not really. What happens if the blockbuilder is malicious actor and then he's going to send you the real data? And then the razor data is not really erasure data.
00:04:04.634 - 00:05:06.396, Speaker A: It has not been computed with erasure coding, but it's just garbage, right? So what happens is that you think you might be able to recover everything, but in fact you will not be able, because the data that you have for the encoded data is just garbage. So that is the problem. How do we fix this is using another mathematical tool that is called KCG commitments. And this allows you to, when you receive a chunk of data, have the guarantee or the proof that this chunk of data is actually has been calculated using resolution encoding and following the polynomial and et cetera, et cetera. Okay, so then once we have these mathematical tools, we can build some more complex structures, like for example, two dimensional encoding. So for example, you take your block of data, you divide it in two dimensional grid like this one, and then you encode the first row, you apply the KCG commitments to it, and then you do the same to the second and third and the fourth rows. Then you do the same with the columns and you end up with a structure like this.
00:05:06.396 - 00:05:42.394, Speaker A: And then you can actually do that, the same thing on the encoded data, and then you will end up with a structure like this. Okay, now, this is the current proposal for downsharding, and it looks more or less like this. I'm not going to go into the numbers here. They are online if you are interested. But basically, what you see here is the blue part is the original data, or the data that is going to be blob data, and there is meaningful data. And the red squares are just encoded data that will help us reconstruct the original data in case many nodes goes offline. All right.
00:05:42.394 - 00:06:15.074, Speaker A: Okay, so what about this allow us to do is that when you have, for example, in here, the blue squares are the data that is available. You can, as far as you have 50% of the data in a row or in a column, you can reconstruct the entire row or column. So here we can reconstruct all, all these rows. And then basically, now we can reconstruct all the columns and we have the whole data set. So the block, we can say is available. Now, if you end up in a situation like this, for example, you lose all these white spaces. But then the blue spaces will allow us to reconstruct these four rows.
00:06:15.074 - 00:06:56.424, Speaker A: And then once you do that, you can reconstruct all the columns, and then you have all the data available. Again, if you look at this configuration instead, here is more tricky. You have three rows and three columns where you have all the data, but all the other rows and columns have less than 50% of the data. So that means you cannot reconstruct anything there. And then basically this block cannot be reconstructed. So in this case, we want the validators to say, well, this block is not available, therefore cannot be part of the canonical chain. All right, so this is just the first part of building sharding, or data sharding.
00:06:56.424 - 00:07:30.638, Speaker A: But then we need more steps. First, we build this data structure. We then disseminate rows and columns to validators. Once we do that, then we distribute these samples into a DHT. And then we do what is called data availability sampling, which is nodes actually sample for specific chunks of data. And then if they receive all the chunks that they are supposed to receive, then we can say that the block is available. For example, if you sample 70 chunks of data and you receive all of them, that means that you have like one in a trillion chances of the block not being available.
00:07:30.638 - 00:08:15.544, Speaker A: So you have a fairly high confidence that the block is actually available or reconstructable. Right? So in this talk, I'm going to focus only on the disseminations of rows and columns, which is the part that we have been working on in our simulator. So what you have is you have many nodes in the network, beacon nodes that are connected through the network. And behind each beacon node you have multiple validators, or sometimes one validator, depending on whether you are a sole staker or you are an institutional staker. The blockbuilder has to disseminate all the rows and columns to the validators according to their assignment committees. Some of the numbers. Today we have around 600,000 validators.
00:08:15.544 - 00:08:52.121, Speaker A: We have to distribute about 250,000 samples, and we have about 10,000 beacon nodes in the network. Yeah, so all these data we have gathered using our crawler, this is a dashboard that we have developed at Micalabs. If you're interested, you can look at the QR code. I will not have the time to present all the details here, but just to go very briefly, we have a bunch of data in the dashboard. We have the number of nodes. We know that there is around 10,000 nodes in the network. We know that the distribution is about 40% to priest, 40% to lighthouse, and then 9% to Tecu and Nimbus, and then the others lostar, Grandeen, et cetera.
00:08:52.121 - 00:09:28.912, Speaker A: We know about the geographical distribution of the nodes. We also know that about 55% of the nodes are in clouds, about 40% of them are in residential homes. We also know how many nodes are subscribed to a certain number of attestation networks. And based on all these data, we can put all these parameters into a simulator to try to actually simulate the Ethereum network, but with data sharding and prototype sharding. So the simulator. So this is what we have been building over the last few months. This is work in progress.
00:09:28.912 - 00:10:03.240, Speaker A: If you want to access it, you can go and look at the QR code. The code is open source, of course, but as I said, this is work in progress, so don't be surprised if you find a few bugs here and there. And if you do, please write issues and we will be happy to fix those. Or just contact us. We'll be happy to work with you. So the simulator simulates a gossip sub network. For those that are not familiar with gossip sub is a graph mesh in which you can push messages and then you can send gossips and pull new content content when you need it.
00:10:03.240 - 00:10:47.804, Speaker A: Today we have about 64 channels for the different attestation networks. The idea is that when we implement sharding, data sharding and data availability sampling in Ethereum, we will have one channel or one gossip sub channel for every row and column. Now today the structure that we have is about 512 rows and columns. That's the current size that we are looking at. But this can change in the future. And so then we expect to have about 1000 gossipsoft channels in which validators will go to subscribe and say, hey, I want to subscribe to row 27 and column 32, and I want to receive those for every block and so on and so forth. Now, the simulator is highly configurable.
00:10:47.804 - 00:11:08.214, Speaker A: We have a huge simulation space that I'm going to present in a minute. And we have other parameters. For example, we can make simulations deterministic. That means that they will produce exactly the same result bit by bit every time you execute it with the same parameters. And this is extremely important for reproducibility. We don't want that. Every time you launch the simulation you get different results.
00:11:08.214 - 00:11:37.590, Speaker A: And there is actually of course, a bunch of randomness going on in a network, but we have ways to make it deterministic. There is a lot of other choices. You can decide to output all the data into XML files, to plot the data and so on and so forth. Now I want to talk about the simulation space, which is huge. So this is the configuration file. Of course, I will not have the time to go into all the details, but just to mention a few. You can decide how many numbers of nodes you want to have in the network.
00:11:37.590 - 00:12:20.134, Speaker A: You can also decide what are the type of nodes. For example, it's important to make the difference between solid stakers and institutional stakers, because it's different to have one validator than running 1000 validators in terms of bandwidth and load. And then of course you can decide what is the bandwidth for each one of those types of nodes. It's not the same running revalidator at home than in a huge cloud in a huge machine. You can decide also how many validators you want to have per node for those two different types of nodes. We can also decide the block size. This has not been decided yet, how many rows and columns every validator should receive, the latency of the network, the sample size, and many other things.
00:12:20.134 - 00:13:02.074, Speaker A: So this is just to give you an idea of all the parameters that we can configure in the simulator that we can play with. We can also play with the failure rate, for example, and so on and so forth. So what can we do with the simulator? So these are some results that we have gathered. This is a simulation running in 7000 nodes, around 530,000 validators. So it's not that far from the current numbers, which is, I think, 600,000 validators. What you see on the left side is a figure that shows how many validators are subscribed to each row and column. So as I mentioned, we have 512, so that's why the x axis goes to 512.
00:13:02.074 - 00:13:54.212, Speaker A: And then we can see that around 900 validators are subscribed to each rows and columns. So there is a huge replication already going on there, in addition to the ratio coding replication. And then on the right side, you see a figure that shows how the simul, I mean, how the distribution of the rows and columns progress. And the x axis is in milliseconds, okay? So you can see that the whole distribution, which is the red line, goes from 0% to 100% in about 500 to 600 milliseconds. So it's basically half a second the amount of time that it takes for this particular simulation with 7000 nodes, and so on and so on, to distribute rows and columns to all the validators. And then you can see the blue line is how many validators have received all the data that they need and how many nodes have received all the data that they need. Now, what else can we do with this? We can also look at the send and receive data.
00:13:54.212 - 00:14:45.624, Speaker A: So, in this particular example, we define that the staking pools had like five gigabits per second of bandwidth and solid stakers, 50 megabits per second of bandwidth. And this is exactly what you see here. And this is for the blockbuilder. And then the solid stakers, they only have to share a slow amount of data. But we can also play with the parameters that mentioned in the configuration files. For example, we can increase the failure rate to 80% and see what happens. That means that 80% of the samples get lost in the network, either because the network is reliable or because the nodes go offline, or because there are malicious nodes that receive the sample and they don't disseminate them, whatever reason, right? So, and then what we see what happens is that the dissemination of samples get stuck at 35%, so it's not 100%, and then it gets stuck and it cannot increase and it cannot move beyond there.
00:14:45.624 - 00:15:35.874, Speaker A: So that happens. That, that means that nobody in the network has any data to reconstruct any row or column, and then basically nobody gets the data that they are supposed to receive. So this is the kind of things that we can do with this simulator. We can also, for example, change the amount of data that we want to send. For example, instead of sending the whole row or column, we can say, well, I send you half of it, and then you actually calculate the other half using rich salmon encoding, because you can do it. And then I say 50% of network bandwidth. So what happens when I do that? And we did the simulation and we see that instead of taking 600 milliseconds, sorry, we take 400 milliseconds, that means we are actually gaining about 33% in time, which is a great gain.
00:15:35.874 - 00:16:19.212, Speaker A: Now, I want to mention a little bit about Codex, because we're also working on decentralized storage. Now, Codex is a different, it's focused more on data durability, which is different from data availability. In the case of Ethereum Das, we are focusing on availability and not on long term storage. So those are two different perspective. Also, the verification is quite different between codex and Ethereum dash, where we have a snar based verification. In the case of Ethereum dash, we have an interactive sampling, and then the datasets are also different because in decentralized storage we are working with files of different sizes. And in the case of Ethereum Das, it's a fixed block size that is regularly coming every 12 seconds.
00:16:19.212 - 00:17:08.126, Speaker A: So those are the differences that basically between the two teams. But I just wanted to mention that all the other tools that I have mentioned here, which is erasure coding, rich Solomon encoding, data sampling, sublinear sampling, and so on and so forth, are the exact same of tools that we have been working on developing codex. And this is why this collaboration between the Ethereum foundation and us made sense. So just to conclude, basically, we have been exploring multiple designs for data availability sampling in Ethereum. We are building an open source Das simulator that you can all play with if you want. We have been also measuring the Ethereum network with the crawler that we have at Migalabs, and for long term storage, we are building codecs. So with that said, I think that's it.
00:17:08.126 - 00:17:09.554, Speaker A: Thank you very much for your attention.
00:17:10.334 - 00:17:23.714, Speaker B: Wonderful. Thank you very much, Leonardo. Yeah, fantastic work. I think we've got time for like one or two questions, if anyone's got any. Anyone like to ask a question? Oh, we've got one.
00:17:26.854 - 00:17:28.502, Speaker C: Hey, fantastic presentation.
00:17:28.598 - 00:17:29.022, Speaker A: Thank you.
00:17:29.078 - 00:17:49.858, Speaker C: I'm just curious, like, what would be a call to action from you for the community? I think a lot of people feel that Ethereum is an escape as they want it to be, and they're sitting around trying to figure out, like, what can I do to make Ethereum more scalable in terms of, like, data availability sampling? How do you think that they could apply their interest in work to furthering along the research and development of that.
00:17:49.986 - 00:18:22.800, Speaker A: Oh, nice. Very interesting question, I think. I mean, of course, first of all, if anybody has experience in simulating peer to peer network spleen, please come and talk to us, because we are looking for engineers and people interested in working on these challenges. So that's the first thing I would say. And also for our simulator, we need a lot of data in order to make sure that the simulations that we do are realistic. And so in that sense, we need a lot of data. We are gathering data about the size of the network, the number of nodes and so on.
00:18:22.800 - 00:18:58.508, Speaker A: But there are other things that we don't know. For example, we have staking pools are very protective sometimes with the data. We don't know how many validators they run in a node, what type of nodes they use, how much bandwidth they allocate for this. And all this information could help us make the simulations more realistic, I think. Lastly, what would be interesting is people being interested on these things, reading about it, and also communicating to other people, writing blog posts, explaining in videos what is data availability sampling, so that the community can also be more involved in this research project. Wonderful.
00:18:58.556 - 00:19:00.444, Speaker B: Thank you very much. Round of applause again. Fleonardo.
