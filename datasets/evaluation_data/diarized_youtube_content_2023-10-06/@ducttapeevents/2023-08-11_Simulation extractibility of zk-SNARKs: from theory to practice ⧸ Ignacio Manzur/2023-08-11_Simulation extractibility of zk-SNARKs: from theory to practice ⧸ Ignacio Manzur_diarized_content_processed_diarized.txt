00:00:12.200 - 00:00:59.714, Speaker A: All right, can everyone hear me? Good. All right, so, I'm Ignacio. I'm with the Nethermind cryptography research team. And today we're going to talk about an important property of certain proof systems, in particular, zero knowledge proof systems, just so we make sure that we all understand what we're talking about. So, what are ZK snarks? So, ZK snarks are special proof systems for relations. And we recall very quickly that, very formally, a relation is a subset of strings, binary strings. And when you have a relation, you have an associated language, which is essentially the set of strings such that there exists another string, such that the pair is in the relation.
00:00:59.714 - 00:01:42.926, Speaker A: You should really think about these as being instances and witnesses for that instance. And we are concerned with proof systems, snarks that are for NP relations. So, these are the languages such that if I give you the instance and the witness, you can actually decide whether the pair is in the relation in polynomial time very efficiently. However, finding such a witness for a given instance might be extremely difficult. And the point of the prover is to show to a verifier that it has the knowledge of that witness. This is very important. So what does this acronym stand for? So, ZK is zero knowledge.
00:01:42.926 - 00:02:40.324, Speaker A: Essentially, what that says is that, very informally, when the prover gives its proof to the verifier that it knows this witness, the verifier learns nothing, except for the fact that the prover knows that witness. The s stands for succint, which very simply says that the proof is short. Formally, the proof should be sublinear in the size of the witness, non interactive. This is the nice. It means that the prover only sends one message to the verifier, and the verifier then decides to accept or not. This unique message that the prover sends usually is called the transcript. And AR stands for arguments, meaning that this proof system should be sound, which means that whenever you have an instance that is not in the language, the prover should absolutely not be able to convince the verifier that it is.
00:02:40.324 - 00:03:13.914, Speaker A: And what we ask here is that this property only holds against, again, computationally bounded provers, essentially. So, if you're very observant, you notice that we're missing the k. So what is the k for? And that stands for knowledge. Okay, so what does that mean? So, here we have an adversary. It has some public parameters for this proof system. It has some reference strings. So we're in the trusted setup case, and it has access to some hash function or an oracle, and it outputs an instance and a proof transcript.
00:03:13.914 - 00:03:59.696, Speaker A: Ok, so let's just assume that this proof transcript causes verifier to accept with, like, some non negligible probability. Then what we ask is that essentially there exists an efficient algorithm which is going to output a witness for that particular instance. In some sense, the adversary had to know the witness, because it could just run this efficient algorithm itself. So, this is a property that we usually call knowledge soundness. It's a very strong property. It usually implies soundness. We only require that there's this efficient algorithm extracting witnesses for when the adversary can convince the verifier with non negligible probability.
00:03:59.696 - 00:04:59.874, Speaker A: Okay, so what they do not tell you in this definition is what the adversary really knows. So somehow this definition assumes that the adversary is isolated, it has only access to the proof system's parameters. Is that the case in the real world? Absolutely not. In the real world, what the adversary has, like if your proof system is running, for example, on a public blockchain, the adversary not only has access to the proof system's public parameters, it also has access to a lot of instances of people proving their own statements and the actual proofs of these statements. So what if the adversary sees all of these proofs and then says, well, here is a way to kind of modify them or combine them to prove a statement for which I don't know the witness. This is absolutely not covered by the knowledge soundness definition. So we need, essentially, when we deploy proof systems publicly, some stronger security guarantees.
00:04:59.874 - 00:06:10.978, Speaker A: So why does that matter? For example? So let's just take this example right here, where this dog, account a, wants to create a coin and send it to account b. What it does is it sends it to a mixer, and then the account b should claim that coin by essentially showing that it knows a secret. And usually we do this with a proof system, with a ZK snark. But now here's this very sneaky cat, and it sees the proof instance by account b. If your ZK snark doesn't satisfy this stronger security guarantee, what can happen is that the diskat sees the proof, it modifies it into a proof for the statement that the coin should go to its account itself, and it doesn't have to know a witness necessarily, for this to happen. So this is quite a serious problem. So how do we prevent this? So, we need to show that the adversary cannot turn the proof it sees into proofs for statements without knowing a witness.
00:06:10.978 - 00:07:30.922, Speaker A: Even if it sees proofs, even if it has access to some proofs, it cannot turn them into proofs for its own statements without knowing a witness. So there are many ways to do this. You could design your proof systems to be like application specific design, where you add, for example, an encryption of the witness, or a one time signature, and this comes at some loss of efficiency. Or you could use actually a snark that is already simulation extractable. And what that means is that even if you give the adversary access to simulated proofs for any statement, true and false statements, it cannot come up with a proof for a false statement or for a statement without knowing a witness, because we can still extract it. So simulation extractability is this stronger extractability guarantee, in which even if we give the adversary access to a simulator for false and true statements, it still cannot come up with its own proof without knowing a witness. So how do we show this? So this is actually work by Ganesh, Koshachlak, Colweiss, Nicolas Coen Zions in 2021, and very recent work by Dao and Grubbs, which essentially reduces the simulation extractability property to proving three main properties.
00:07:30.922 - 00:08:37.686, Speaker A: The first one is k unique response, which is essentially a measure of how much freedom you have in answering the verifiers challenges. The second one is in the trusted setup, k rewind knowledge soundness, which, as you will see, is some sort of special soundness, if you're familiar with this term. For non interactive protocols, in the transparent setup, you only need knowledge soundness of the non interactive protocol. And the last property is something called programmable zero knowledge, which in the trusted setup case should also be trapped or less, and some mild conditions on when these properties occur, especially in your proof system. What these people show is that this implies simulation extractability and this stronger knowledge extractability guarantee. And this is somehow easy. You show essentially that if you have an adversary that breaks simulation extractability, then it can break essentially one of these three properties, like especially knowledge soundness and unique response.
00:08:37.686 - 00:09:49.044, Speaker A: So what I propose is that we go through what these properties mean exactly, and then we look at two simple examples to show how this is applied and how we can essentially prove simulation extractability of many, many deployed snarks, which is good news. So I have some positive results, which is kind of rare both in computer science and in mathematics. So let's just go with k unique response. So we consider two mu plus one move protocol. So the prover always sends the first and the last message, and then the verifier sends challenges for each of these messages except the last one. So here is an interaction between a prover and a verifier and here, let's just say the verifier accepts. So usually the zero knowledge snarks, they are constructed in a way that you take this interactive protocol and you make it non interactive by using something which is called the fiat chamier transformation or the fiat chamier heuristic, in which instead of the verifier sampling, for example, uniformly the challenges, what you do is that you hash the prover messages, and to get the second challenge, you hash the first three messages and so on and so forth.
00:09:49.044 - 00:10:43.718, Speaker A: So when we use the fiat Shamir transform, usually what we say is that the transcript consisting of the prover messages is accepting. Why don't we include the verifier messages? It's because we can always recompute them by hashing. Okay, so here's another prover. P prime could be the same, and it produces another transcript. The K first messages are exactly the same as the previous transcript, but the last ones can potentially be different. So what the K unique response tells you is that if the last messages are different, then this new transcript cannot also be accepting, it has to be rejected. And what you can, you can summarize this by saying that the probability of having two accepting transcripts which are different because some of the last messages are different, but the K first messages are the same, is negligible.
00:10:43.718 - 00:11:22.428, Speaker A: You cannot have that. And here you say, ok, but how can the prover messages be different if the challenges are obtained by hashing? So the challenges should be the same. But what I'm not telling you is that you can program the case challenge. The adversary has the possibility of programming the case challenge. Even in that case, having two transcripts that are different but agree on first K messages is impossible. All right, what about this rewind knowledge soundness that we need in transparent case? So here's a running instance of the protocol where the dots represent prover messages and the edges represent verifier messages. Particularly this one is accepting.
00:11:22.428 - 00:12:13.004, Speaker A: The verifier accepts this instance. So what I can do is I can rewind to the point where shown in the screen, and I can actually choose other challenges to see what the prover does, essentially. And here, when I chose this other challenge, c, one tilde, the transcript was actually accepted at the end. But when I chose c, two tilde, then the transcript was rejected. And in fact, I ignore those branches where the verifier rejects the proof. So in this way, I can build this tree of transcripts where I rewind to the point which is of interest to me, and I choose different challenges. So what we ask is that when we build this tree, there also exists an efficient algorithm which looks at this tree and says, here is a witness for that instance.
00:12:13.004 - 00:13:12.604, Speaker A: And in fact, here I've represented a tree where the rewind happens only at one point, but the property gives you freedom of choosing different rewind points and many, many different rewind instances. It's just that in the protocols, when you try to prove it, many times you end up with trees that have this form. Okay? And the programmable zero knowledge is actually just a flavor of zero knowledge, if you remember, like, zero knowledge property is essentially saying you have a simulator that can simulate proof transcripts, and they are identically distributed to the ones a non approver would generate when proving a statement. And the programmable zero knowledge property says exactly the same thing, except that the simulator is allowed to program the K challenge. Okay. In the trusted case, it also has to do so without a trapdoor. But let's just go over that.
00:13:12.604 - 00:14:04.812, Speaker A: So, here is an example in the trusted setup. So, this is the Planck proving system. So, essentially what happens in Planck is you have an arithmetic circuit, and you try to encode this circuit with polynomials where you interpolate they have fan in two and fan out one. So you interpolate the left and right wires of inputs with two polynomials, and you interpolate the output wires with another polynomial. And the prover is supposed to send that to the verifier. Then the verifier and the prover should essentially interact with some challenges. And in the end, the prover should send this quotient polynomial t, at which point the verifier sends an evaluation challenge, which is called z.
00:14:04.812 - 00:14:49.836, Speaker A: And the prover should respond with the evaluations of all the polynomials, together with opening proofs. So you should think about these commitments at KZG. Commitments. And these openings are the openings in the KZG commitment scheme. So why is that unique response? So the point is that once you've chosen the reference string, once you've chosen a polynomial and an evaluation point, the KZG opening scheme is completely deterministic. You cannot do anything else. So, once you've chosen all the messages up until the point where the evaluation point is chosen, you cannot have two different accepting transcripts.
00:14:49.836 - 00:15:43.146, Speaker A: They will be the same after that. So this is essentially given for free by the property of evaluation binding of the KZGG commitment scheme. So why is it rewind, knowledge soundness here? It's a simple little trick where essentially we're going to rewind to the evaluation point. And we're going to choose a different evaluation point, and the prover is going to respond with the evaluations of the polynomials at different points. And so, because these polynomials have essentially finite degree, when we get enough points, we are going to be able to interpolate and recover them. So, interpolation is essentially an algorithm that allows you, given enough evaluation points of a polynomial, essentially one more than its degree, it allows you to reconstruct the entire polynomial. Okay.
00:15:43.146 - 00:16:09.894, Speaker A: And finally, why is it programmable? Zero knowledge. So the point is here, the simulator is going to do something kind of counterproductive. It's going to choose the polynomials it should commit, at first, completely at random. So this is rubbish. It should give something that is, it should encode nothing about the arithmetic circuit. So it should. In particular, it shouldn't verify any constraint of the Planck arithmetic.
00:16:09.894 - 00:17:23.836, Speaker A: So what happens is, as the prover and the verifier interact, the quotient polynomial, which should be a low degree polynomial, is going to be some very high degree polynomial. It's not going to satisfy anything. So how do we kind of trick the verifier into still thinking that it's a low degree polynomial? Well, we choose the evaluation point in advance, because the simulator can program that point. And what we do, when we know what the point is going to be is instead of committing to the quotient polynomial, which is not a polynomial at all, or not a low degree polynomial, we commit to another low degree polynomial, which agrees on the evaluation of the particular point we've chosen. And when we do that, essentially the verifier is going to accept this proof just because the opening of the KZG opening is going to be valid. And we can do that precisely because we can choose the evaluation point before choosing what the commitment to t is going to be. So here, very high level, we've shown that Planck satisfies the three properties that it needs to be simulation extractable.
00:17:23.836 - 00:18:18.822, Speaker A: Which means that if you want to use Planck as a public proof system, you at least have the guarantee that people cannot take proofs they see on your public blockchain, for example, and turn them into proofs for their own statements without knowing a witness. All right, so let's look at the transparent example. So let's look at the ETH start protocol, for example. And here it's very similar. So the prover is supposed to send commitments to some polynomials, some witness polynomials, and the verifier, at some point, sends this q point, which is also supposed to be an evaluation point. And the decision process is such that supposedly the evaluations of the witness polynomials that the verifier sent are supposed to satisfy some constraint equation, which is given by this equation. Star.
00:18:18.822 - 00:19:04.914, Speaker A: It's not important what it encodes. It's supposed to satisfy this equation. And how do we check that the evaluations of these polynomials are correct with the commitments here? The commitments are not KCG. They're actually Merkel commitments. So you put the evaluations of the polynomials on the leaves of a miracle tree, and you hash that and you get a root, and that's your commitment to your polynomial. So the way that estarc enforces the evaluations are correct is actually applying the Fri low degree testing to this function g, which essentially is a low degree polynomial if and only if the witness polynomials are low degree. And furthermore, they evaluate to the alphas and the betas at the points at the denominator.
00:19:04.914 - 00:20:03.038, Speaker A: So why is this protocol k unique response? And here the point is that we're gonna wait until very, very late in the protocol when we open the points in fry. This is done by essentially showing a path to the miracle root of the Merkel tree you've committed your polynomial with. And this is if your hash function satisfies some obvious properties, like, let's say, collision resistance or pre image resistance. This is going to be deterministic in some sense. It's going to be extremely hard for a proverbial to kind of cheat you into thinking that it's opened correctly, because it has to find, essentially, collisions in the hash or, like preimages in the hash. And why is it programmable? Zero knowledge. So here, the technique is essentially similar to what we had for Planck.
00:20:03.038 - 00:20:31.060, Speaker A: It's that the simulator is going to choose the polynomials completely at random. So again, this is bad. Usually, it won't satisfy the equation star at all. So what you do is forget about the polynomials. Let's just choose alphas and beta such that the equation holds the constraint equation. So why can we do that? For example, we can choose everything at random except one beta. And then the equation becomes linear in this last variable.
00:20:31.060 - 00:21:14.614, Speaker A: And so we just solve for the last variable. So once we've done that, we claim that the polynomials we've committed to evaluate to the alphas and the betas. That's absolutely not true. So, in general, this function g that we apply the low degree testing to is not. It's neither a polynomial, it's not of low degree. But what we can do is that we can again program the points where the fry protocol is gonna is gonna kind of ask for to see evaluations of that function g. And we're going to interpolate g with a low degree polynomial over these points and engage in the low degree testing with that function instead.
00:21:14.614 - 00:22:18.384, Speaker A: So that's a complicated way of saying that we know which points are going to be queried because we can program them. So, with this additional knowledge, which we don't usually have, we can trick the verifier, again, into thinking that we have a correct function and that we satisfy all the necessary constraints, but we don't, essentially. So that's how the simulator is going to be able to produce transcripts for the proof. And so this shows that the etHArC protocol is also simulation extractable and satisfy all the nice extractability properties that we talked about. And now, if you're very observant, you notice that, for example, in the Planck case, the way the protocol was set up mattered for how these properties were found. So KzG gave us unique response. Then we had to interpolate by choosing different evaluation points and so on.
00:22:18.384 - 00:22:58.346, Speaker A: So the Planck protocol kind of. It mattered how it was set up for us to be able to prove these properties. But here in the east arc example. Sorry, you kind of see that these properties only were only dependent on the fact that we used the fry protocol to test the low degreeness of these functions. It didn't really matter what happened before that point or after that point. The only thing that mattered is that the low degree test or the evaluation test was the Fry protocol. So, essentially, what you can show, or try to figure out is that you can generalize this for all fry based snarks.
00:22:58.346 - 00:23:46.780, Speaker A: And when you have a fry based snark in which the decision process of the verifier is such that it accepts if and only if you have polynomials verifying a certain constraint equation, and the evaluations are checked using fry. Again, you can use the same exact tricks and show that, essentially, this will also satisfy simulation extractability. And this stronger simulation extractability guarantees. So what's been really positive, I think, is that essentially all your favorite snarks are simulation extractable. Like Planck, Marlin, sonic, bulletproof, spartan, ETH, stark, plonky two. All been shown to be simulation extractable. So even gross 16.
00:23:46.780 - 00:23:58.584, Speaker A: In some weaker sense, it was also shown to be simulation extractable. So, yeah, there it is. Thank you very much for your attention, and I hope you enjoyed this talk.
00:24:00.284 - 00:24:20.880, Speaker B: Fantastic. That's been very detailed, very in depth. I'm sure we have some questions from the audience. The gentleman right here in the center is going to wait for a microphone before he asks this question because there are thousands and millions of people watching online and they want to hear your question. Go ahead.
00:24:20.992 - 00:24:22.204, Speaker C: That's a lot of pressure.
00:24:22.584 - 00:24:23.264, Speaker B: No pressure.
00:24:23.304 - 00:24:23.884, Speaker A: Yeah.
00:24:24.224 - 00:25:05.212, Speaker C: Thank you for the presentation. Loved the way you explained it. It's quite a complex topic. I, as a developer myself, faced this problem quite recently and like seeing this breakdown like that is awesome. My question is, do you see that actually as a developer building on top of ZK Starks, should I actually care about that? Or should the protocols itself always integrate smolition, interactability or non malleability inside of the like when they designed the protocol so that I can just relax? Because right now we're building our own solution. We have to work around to potentially prevent simulation extra tabili like malleability problems by hashing and doing some extra work. So what do you feel like about that?
00:25:05.348 - 00:25:42.956, Speaker A: Well, I mean, it depends, I guess, which proof system you're using. I mean, if you were lucky and it was in the list, I said, of the ones that were simulation extractable, hopefully you don't need to care that much about it if it hasn't been shown. I think it is a potential risk that someone figures out a way to, as you said, modify this proof. Like if you don't satisfy non malleability, in which case I think it should be important to add, for example, a one time signature or an encryption of the witness, as you said, to make sure that your protocol is non malleable. Which proof system are you using in particular?
00:25:43.060 - 00:26:07.144, Speaker C: I mean, that's a problem for us. So we are generally quite flexible. So we plan to use noir and their underlying system. But the problem is we need to figure out if they actually are simulation extractable or not. And that's a problem. So I was wondering if it should be an industry standard where all the proof systems that are used should be by default simulation instructable, so that we can just have peace of mind.
00:26:09.044 - 00:26:33.774, Speaker A: I think that's true, especially if your proof system is a condition for you claiming, for example, your coins or some if the non malleability kind of. Yeah. The security of the funds in your protocol is dependent on the non malleability of the proof system you're using. And I think absolutely, yes.
00:26:34.074 - 00:26:42.894, Speaker C: And one more question, a quick one. Are all interactive oracle proofs non malleable or not? There was a result for that or you don't know?
00:26:43.834 - 00:26:51.044, Speaker A: I think the simulation extractability definition applies to protocols that are non interactive in some sense.
00:26:55.104 - 00:27:11.896, Speaker B: Super in depth. You got more than one question into the block there. Someone else have a question? Yes. No, maybe one question from my side. How long have you been working on the topic?
00:27:11.960 - 00:27:20.594, Speaker A: On this particular project? On this particular topic? It's been maybe three, three, four months.
00:27:22.854 - 00:27:25.790, Speaker B: And where is it going from here?
00:27:25.942 - 00:27:55.554, Speaker A: So essentially, when we started, we wanted to. So the simulation extractability of the fry based protocols wasn't very well understood and hadn't been. So what we're planning on doing now is essentially publishing the write up we have on this topic and hopefully also work on some other deployed snarks, which maybe haven't been analyzed yet.
00:27:56.094 - 00:28:00.502, Speaker B: And are you cooperating with some other projects teams on the topic?
00:28:00.598 - 00:28:12.906, Speaker A: I should mention that this work was supported in part by a grant from the Xerox ParC foundation. But at the moment, it's been only the Nethermine cryptography research team on this project.
00:28:13.050 - 00:28:24.162, Speaker B: Okay, super cool. If there's no other questions from the audience. Going once, going twice. Zara also. No. No questions. No.
00:28:24.162 - 00:28:26.546, Speaker B: Okay, very good. Thank you, inaccio.
00:28:26.690 - 00:28:27.330, Speaker A: Super.
00:28:27.482 - 00:28:28.074, Speaker B: Round of applause.
