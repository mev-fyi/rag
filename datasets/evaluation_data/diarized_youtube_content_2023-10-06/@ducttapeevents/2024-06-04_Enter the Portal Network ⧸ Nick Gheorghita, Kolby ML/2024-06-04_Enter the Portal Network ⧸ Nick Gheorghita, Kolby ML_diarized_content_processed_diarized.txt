00:00:05.360 - 00:00:35.264, Speaker A: Hi, my name is Colby, ML, and this is Nick. We both work at the Ethereum foundation researching and implementing execution and consensus layer like clients. So let's enter the portal network. So Ethereum is kind of broken. And I know that's kind of like silly, but it's more of a, it's more of a problem of how we access Ethereum. Starting out, Ethereum was very accessible. You could boot up a full node in maybe like ten minutes and it would sync the full chain.
00:00:35.264 - 00:01:13.892, Speaker A: But now it takes days to sync the chain. And what? Oh, I'll just continue. Oh, crazy. Okay. Anyways, as you see, a little blob, so it takes days to sync the chain. And because of that, most people access Ethereum through centralized providers such as inferior and alchemy. This is kind of like crazy because I guess centralized sources was supposed to be the thing we're moving away from.
00:01:13.892 - 00:01:59.604, Speaker A: Ethereum was supposed to be this accessible thing where you just ran this program on your computer and you were able to send money. But that's not really how people interact with Ethereum anymore. So I guess how do people interact with Ethereum decentralizedly right now? And that's through full nodes, as you see on this image on the screen right now. To run a full node, you need around like 1 storage, and that's really not accessible. And that's also not including the knowledge required to run a full node. It's like if we want a theorem to be used, you would want your grandma to use it. And that implies that you don't even need to know what a full node is in the first place.
00:01:59.604 - 00:03:23.934, Speaker A: So it's like, what are we doing? So earlier I said to participate in Ethereum's core protocol, you needed to store around a terabyte of data. What if we took that terabyte and divided it into 1000 pieces and put it on 1000 nodes, but you could still look it all up on our network. And so if everybody only needs to store 1gb or so of data to access all of Ethereum, this lowers the requirement. So you could maybe store it on a phone, maybe you could access it from a Raspberry PI. Maybe you could access it from your laptop. What if you could access Ethereum without running a full node with almost instant sync times and being able to access the entire JSON RPC, which is how people interface with Ethereum today? But that's not all. What if you also had full archival access to Ethereum? So I know this is kind of ridiculous, because if anybody has ever tried to run an archival node before with Geth, you would know that instead of a terabyte of storage, you would need 20 terabytes of storage, which is kind of bonkers.
00:03:23.934 - 00:04:23.562, Speaker A: But the idea is that, well, if we distribute all this data and you can look it all up, something like this should be possible. So how does this network of distributed data tie back into full notes? How do we improve full notes? Well, there's this Ethereum improvement proposal named four four. Currently it's proposed as being deployed in two stages. The first stage would be genesis to the merge, as this is more of a solved problem. And then after that is deployed, there would be, I guess, the second half, which would be the merge to around six months. How does this relate to dev p two p? The current way blocks are gossiped on Ethereum. Well, advertising blocks on Ethereum would be shut off.
00:04:23.562 - 00:05:31.934, Speaker A: So your only way to access historical blocks would be through other methods, such as the one we're building or something like, I guess, bulk archival formats, which, something like arrow one files, which is just like a file format which stores headers, transactions and receipts in around 8000 block chunks. How will this affect syncing? Instead of syncing from Genesis, you'll sync from checkpoints. So and how will this affect archival nodes? Will I still be able to run one? And the answer is yes. Instead of syncing from DevP to P, you'll just sync from a bulk block formats like r1, as I said before. So I guess you have this. I guess it's like, why is four four important though? It's because instead of storing, I guess, a terabyte of storage, you can reduce that by around 750GB. So you only need to store around 250 to 300gb of storage.
00:05:31.934 - 00:05:59.950, Speaker A: Chain data takes up a lot of your storage, and it's not really used for anything because everything what's useful, quote unquote, is already executed in Ethereum's global state. So currently old block history is mostly for researchers and people who want to sink the chain from Genesis. For now, at least. So I'm going to let Nick go over on how does this crazy thing work. Nick.
00:05:59.982 - 00:06:37.484, Speaker B: All right, so now I'll go into a little more detail on how the sausage is made. Colby explained basically the overview, and I'll go into the details. At its base, we use a distributed hash table, the same kind of protocol you'd find in Bittorrent. This is how we address our nodes and the content around the network, and how it's basically the way portal network or the foundation of the portal network. Portal network uses this exact same disk v five layer that Ethereum runs on. To find nodes on the network. We use the same talk rec messages.
00:06:37.484 - 00:07:19.172, Speaker B: But within these talk rec messages, we embed overlay protocols. And these are specific to the portal network. And the best way to think about this is we have three different overlay protocols. At the moment, we have history overlay which chain headers, bodies, receipts, and then state which is state data. And those are two separate overlay networks, but together they serve the data you'd expect to get from any execution layer client. And then we have the Beacon network data, which provides the data that we use to sync with the head of the chain. So this is the portawhier protocol in a nutshell.
00:07:19.172 - 00:08:23.958, Speaker B: This is the set of messages we use to find and check liveness of peers on the network. It's also how we discover new nodes around the network, then another set of messages to directly send the content that you want between peers. And then the final set of messages is used to gossip and broadcast the content being stored around the network so that it finds its eventual home or node that's responsible for storing it. So, briefly, or just like a general overview, if you think of each overlay network can be thought of as like a 256 bit key space. And each of these little circles represents a node participating with a certain overlay protocol. Now, these circles, all in theory have different sizes. You can't really see it, but we have a big circle down here with a big radius.
00:08:23.958 - 00:09:11.634, Speaker B: And so every node has a associated data radius. And this is determined by how much storage you're willing to allocate to the network. So if you're willing to allocate 100 gigs, you'll have a fairly large data radius and be storing a lot of data. If you're allocating 1 mb, you'll have a tiny radius and probably won't store much at all. So your node id addresses your node around the key space, and every piece of data that we're storing inside the storage network is basically a key value pair. And to generate the content id, we take the hash of the content key, which also evenly distributes the content around the network. So no single node is responsible for an unfair amount of storage.
00:09:11.634 - 00:10:26.758, Speaker B: And then basically, like as a single node, you're responsible for storing all of the data that falls within your data radius. And then when a peer wants a specific piece of data, they calculate the content id and they traverse the network until they find a node that's closed, close enough to the content, and then they simply exchange the content so what we've described so far is basically just a database. A large database and an empty database is not very useful. So you might be wondering where does the data come from? And the data comes from a special kind of node called a bridge node. And from the perspective of the network, bridge nodes operate like any other node. They participate in all of the wire protocol messages. The only difference is every bridge node on their back end is connected to an execution or consensus or both client and then the bridge node is responsible for pulling data from the Ethereum client, translating it into the portal format, and then using that offer message before to broadcast the data into the network.
00:10:26.758 - 00:11:20.592, Speaker B: And this is where the data comes from. And then once it's in the network, it's available and we have different nuances to our protocol to make sure that it's always available. All right, so we have data, but like how do we know this data is real? We do this by using the different overlay networks. And depending on the specific piece of data there will be nuances to exactly how it is validated. But the most generic example is let's say you start with the state network at the bottom and you fetch an account balance. The account balance will have a proof, the data you receive will have a proof attaching it to the state route. So then after you have the piece of state network data, you'll go to the history network, you'll go look up the associated header, validate the proof against the stateroot in the header.
00:11:20.592 - 00:12:14.684, Speaker B: But now you have a header and you're not sure if that's real or not. And so then every portal client will be running the beacon network and we use the beacon network to sync with the head of the chain. And in this way there's various nuances and accumulators and the proofs we use and the data types that are passed around the beacon network. But this is how we validate headers are canonical and this is how we establish a chain of validation trade offs. So what's the catch? We've described something very cool here, but it's not always so easy. Like with most things in life, there are trade offs. And in the tradeoff between storage requirements and speed, we're leaning heavily into prioritizing, minimizing storage requirements.
00:12:14.684 - 00:12:55.234, Speaker B: So a full node will always be faster than the portal network. But the portal network is comparable. If you're using infuria, you have network requests. So it's for the average ethereum use case, the latency of network requests is totally acceptable for users. If you're not sort of the power user of an ethereum node, then the portal network, especially with its minimal resource requirements, should be more than enough for your use cases. The current status of the portal network. So we have our four four's history network live.
00:12:55.234 - 00:13:49.078, Speaker B: It's live right now. Anyone here can spin up a node, participate in the protocol, and fetch any data from pre merge history network, and that's cool. But we also have a testnet which we just launched, yeah, a couple weeks ago. And on the testnet we're running the state protocol and the beacon protocol, and we're sort of going through the final phases of testing the beacon network. The state network is still in the earlier stages, but we have a live testnet for this, or we have live state data on the Testnet as well. Throughout development we have developed a suite of testing and monitoring tools. GLADOS is our main public facing network monitoring dashboard, and we'll show that in a second.
00:13:49.078 - 00:14:54.524, Speaker B: And portal Hive is our version of Ethereum Hive, and we use that for client interop, client implementations. We have Fluffy, which is written in NiM, we have trin written in rust, which is what Colby and I are working on. We have Shisui written in go, and ultralight is written in JavaScript. So if any of you guys are interested in getting, getting your hands dirty, all of these repos have issues, you can go check it out and start contributing right now even. All right, what's the future of the portal network? Well, our two priorities right now are getting beacon network pushed to production and state network pushed to production. That is sort of our mvp story that we're really targeting by Defcon to have live state data available and validated using those three overlay networks. We also are working on a vertical support first state data, so we'll be ready for the vertical transition.
00:14:54.524 - 00:16:05.224, Speaker B: And as always, there are plenty of optimizations to be done to reduce the existing network latency. And then after that, we're going to be working on flushing out the archive, really just filling the state network with all the archive data so that we will have this global archive node that anyone can use. And, you know, eventually we want to be in everyone's wallets. We want all the wallets to use this decentralized provider to access the Ethereum protocol and to try and wean ourselves off of infuria, etcetera. All right, we're going to do a little live demo and we'll see how it goes. All right, so sorry, I don't know if you guys can read this, but basically all I'm doing here is in this window, I'm just going to start a trend node locally. You'll see right now it's getting its external facing iPad to generate the ENR.
00:16:05.224 - 00:16:31.740, Speaker B: And then after that it'll connect to the boot nodes. And you can see we're bonding with boot nodes there. Some boot nodes are. Yeah, two's not bad. Ok, so now I'm just going to run a python script and. Ok, so the content that exists in the history network is addressed by block hash. So I'm using infuria.
00:16:31.740 - 00:17:23.952, Speaker B: I know I shouldn't, but I'm using Infuria to just look up the block hash for a random block from Genesis two merge. And then if you see on the second half, I'm just using web3 PI, just like a generic ethereum web3 client to connect to Trin. And then we're sending the standard ETH JSON RPC request with the block hash and we'll try and retrieve that from the network and hopefully we're able to retrieve it from the network. So there we got block 14 million something. And so, yeah, so here's the state route from the block uncle's route receipts, route transactions route. Cool. So it's not the flashiest demo, but you know, it's cool.
00:17:23.952 - 00:18:03.136, Speaker B: Anyone can spin up a client connect to the network and you have all of this data available to you based on like this distributed data data storage model we have. All right, so just a little bit more. This is Glados, as I mentioned earlier. Currently we're displaying the four four stats and yeah, I mean 99.8 over the past. We have maybe a couple, literally a couple blocks left to fill out in this network. But the vast majority of four four's data is in there, in the top, in the top right.
00:18:03.136 - 00:18:22.420, Speaker B: You can see the different clients that I mentioned earlier in their implementations, the ones that are active in the network. This is a graph. I'm not going to explain everything. It's cool. You can go look at it and play around with it. This is our audit dashboard. So we're auditing.
00:18:22.420 - 00:18:53.584, Speaker B: This is just a long running process that's auditing all the expected content keys and making sure that they're available on the network. I picked an audit earlier. There are more fun audits to look at, but I wasn't able to find one. But basically you can see this is a visual representation of how a node traverses the DHT to try and find a target piece of content. We have the originating node here. It sends out requests for the piece of content to peers. Two peers responded not with the.
00:18:53.584 - 00:19:44.540, Speaker B: One peer responded not with the content, but with a list of peers closer to the content that itself. This peer responded with the same thing and then eventually we ended up finding the content from a peer, I mean, two hops to find the content. We also have a, I mean, I'm just sort of showing things from GladOS. This is a network explorer. So this is a 24 hours snapshot of all the nodes that were seen on the network and their active status based on the ping pong message. As you can see, some nodes drop off, some nodes come back online, but we also have majority of nodes are just constantly available. And colby, if you want to describe state network, sure.
00:19:44.612 - 00:20:46.990, Speaker A: This is our, I guess, our testnet instance of GLaDOS running on our testnet called angel food, which is a kind of cake. Currently we're auditing, I guess, state routes. So our stateroot strategy basically, I guess, starts out a state route and then does a random walk of the tree to an account. If you make it to an account, that's a successful, I guess, audit, and if you don't make it, then there is a failure. So currently what we're working on is executing all the blocks so we can, I guess, get the intermediate like state diff so that we can gossip it. Because when you do basically in a Merkle Patricia tree, when you delete a node and it regenerates, if it's a branch node with only one child, it converts to an extension node and we need to, I guess properly track what is changed so we can gossip down the network and then execute all the blocks. And then I guess the state network's like done.
00:20:47.102 - 00:20:48.670, Speaker B: There you go. Just like that.
00:20:48.742 - 00:20:53.194, Speaker A: Yeah, just like that. So I just need to execute all those. Shouldn't be too hard.
00:20:54.974 - 00:21:22.190, Speaker B: Cool. So thanks for listening. We've been working on this project for a while and we're really excited that it's live and available. Please run a client if you're so inclined, especially if you have validator somewhere running. It's very trivial resource requirements to spin up a node alongside and you're really helping the network. We have our specs on GitHub, but really the best place to start is our website eth portal.net dot.
00:21:22.190 - 00:21:40.154, Speaker B: We also have the Glados website. You can find everything from the website eth portal.net dot the best place to contact us to get help or just ask questions about how it's working. What's going on would be our discord server, which you can also find on our website. So cool. Thanks a lot.
00:21:45.334 - 00:21:47.034, Speaker A: All right, are there any questions?
00:21:57.674 - 00:22:16.674, Speaker C: Thank you. Yeah. Okay. Thank you very much. Awesome, awesome. This network, you mentioned, the data, I mean the data radios, I think so each and every node in a network has a maybe different radius. And you mentioned, I think, 100 megabytes.
00:22:16.674 - 00:22:23.344, Speaker C: So this is all the client has in the end, or, or how much data does it have?
00:22:24.604 - 00:22:44.340, Speaker B: It's up to the client to define how much storage it wants to contribute towards the network. So if a client only has 100 megabytes available, the data radius is dynamically set based upon the config, basically, and that broadcasts to the peers on the network what data you can expect to find from that client.
00:22:44.452 - 00:23:05.944, Speaker C: Yeah. Okay, makes sense. The end vision then is that we have this portal network. And then you mentioned, of course in the beginning that there are light clients probably running on smartphones, and these actually have not that much storage. So just are they also contributing then to the portal network or just accessing it?
00:23:07.484 - 00:23:40.914, Speaker B: You don't have to contribute storage to the portal network to access the data, but our expectation is that defaults are more than sufficient. Of course, people could spin up many clients with zero storage, but the lazy thing is most people will just run the client with the defaults, and we also expect that there will be, I mean, yeah, we expect that there will be plenty of storage, because cumulatively, it's not a wild amount of storage required for each network.
00:23:42.464 - 00:23:43.684, Speaker C: Okay, thanks.
00:23:46.224 - 00:23:47.524, Speaker A: Any other questions?
00:23:50.544 - 00:24:04.004, Speaker D: Hey, thanks for the talk. I'm curious, could the clients be used for transaction tracing as well, given that they have access to everything, they would probably need to run the vm and stuff, but yeah, is that an option? Have you thought about that?
00:24:05.264 - 00:24:13.434, Speaker B: I believe it's possible. It might take a fair amount of network requests, but yeah, all the data is available on the network, so.
00:24:20.694 - 00:24:21.030, Speaker A: This.
00:24:21.062 - 00:24:29.206, Speaker E: Question is not going to be very smart. But is it possible to build a wallet that uses the portal network instead of an RPC provider?
00:24:29.310 - 00:24:38.998, Speaker B: That's a very smart question. Yeah, of course that's the goal. Maybe I didn't stress it hard enough, but that is the dream. Yeah.
00:24:39.086 - 00:24:46.954, Speaker E: So if it's a dream, what's the main blocker there? Or what do you have to figure out in order for it to be reality?
00:24:47.294 - 00:25:35.844, Speaker A: I guess I'll take this. So I guess the first thing is being able to execute all the blocks, which is what I'm currently trying to do once we have a network with all the archival history. Well, that is, looking all that up is pretty slow because you would have to, to walk the tree on the network. So after that is we're looking to I guess upgrade our history, not our state network with I guess faster lookups. And we'll call this like I guess head state. So instead of doing like login lookups, so instead of having to walk the tree to find your account, unlike the latest like I guess, instance of the state, you could do like you could do a lookup directly on the account you're interested in.
00:25:36.984 - 00:25:57.174, Speaker E: Just another follow up question. So RPC nodes are used for acquiring data, but they're also used for compute. So would the portal network do anything for the compute side of things? Like when you interact with a smart contract or anything, there's some compute that needs to be done. Is it also part of the scope? Or.
00:26:01.714 - 00:26:29.524, Speaker A: I guess the way to answer that is maybe you're referring to something like an ETh call. Yeah, yeah. So you could do an ETH call and you would just like run an instance of the Ethereum virtual machine on your local machine and then you could just like I guess walk the tree or if you're doing head state, hopefully we have like I guess lookups at that point and. Yeah, all right, thanks so much guys.
