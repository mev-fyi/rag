00:00:08.480 - 00:00:57.414, Speaker A: Hey, hello everyone, my name is Chi. Today I'm going to present ac storage, building a modular storage layer on top of Ethereum. So a co intel myself have been working here for six years, primarily interested in infrastructure, especially L2 and data availability. And so also closely working with Ethereum foundation received a couple of grants. So for ecstorage is also today's talk is actually sponsored by the Ethereum ecosystem program. So what is the storage? It's basically another type of L2. But rather than scale Ethereum in terms of more transactions or more computation power, it aims to scale the ethereum in terms of more space, more storage capacity, using the robot approach.
00:00:57.414 - 00:02:10.324, Speaker A: So compared to existing storage solutions, for example famous ones like Filecoin RV, we have a lot of unique features here. So first is minimum trust, which users just need to send a transaction to Ethereum and everything's done, rather than have to submit an individual transaction to maybe filecoin or ARI and then submit the corresponding data hashes on chain. So everything here is minimize the trust required by the users. Second is permission network, so everybody be able to run the corresponding data node and receive corresponding storage incentive from the network. And also it is modular so that we do not require any protocol upgrades of Ethereum. Thanks to sake snark, we are able to verify the off chain storage similar to for proof and proof that to verify the execution result of options, we have a storage proof that implement on top of Ethereum smart contract. And lastly, which is really important is scalable which we are able to achieve petabytes of capacity with one to the 1000 transaction storage costs versus current Ethereum has.
00:02:10.324 - 00:03:46.354, Speaker A: So first of all, how we are able to achieve minimum trust. So the key idea is that to use the, the latest EIP 4844 blobs that Ethereum just launched about two months ago, which dramatically reduced the core data cost to right now is almost negligible with much higher capacity, with especially a lot of advanced technologies that is going to developer like PIDS, which for the previous speech that they are able to upload about 80 terabytes of data per year. So for the user it just needs to send those EIP 484 transactions to the Ethereum where the transaction will call our storage contract and record corresponding blob hash so that we know what kind of data the user would like to store. And then the rest of thing will be automatically handled by the nodes in our storage network that we built dedicated for the Ethereum. So in this way we are able to eliminate the centralized bridge. Basically every node here will be incentivized to download the data from Ethereum and store a local version broadcast to the peers and then store this data and prove to the Ethereum saying that I really maintain a copy of data and receive corresponding reward from the Ethereum smart contract. So with this way we are able to minimize the trust and also review of the Ethereum security.
00:03:46.354 - 00:05:53.634, Speaker A: That is is the major principle of Ethereum L2. Second feature is that we are open network with storage incentive alignment. So first of all, it's a permissionless p two p network by reusing for example lib p two p all these libraries, and so that we are able to offer accepted data from the peers and everybody is able to join a network and become one of the nodes and be able to download the data and also send the data to others. And it has some key features to implement core storage incentive alignment the main problem here is like Ethereum storage model is that all the storage fee, for example paid by asset store or pco or create contract will be paid to basically block proposer one time. So the block proposer receive all the storage fees while all the full nodes running the asset validator, they will not be receive any fees even though they run the full node or full replica of the ethereum nodes. So it's storage because we have so much data that we need to have a better way to align the incentive between the users that pay the fee and then the corresponding data nodes, so that the user just pay one time storage fee, which is similar as the current Ethereum model. And at the same time, this storage fee, when received and deposited in the smart contract, will gradually distribute to those data nodes that can prove that they really make a copy, a replica of the data off chain and receive the corresponding fee over time, so that we can align the storage contribution, that is number of the, that's the size of data times the time, and then proportionally to the, to their corresponding reward and be able to verify their contribution.
00:05:53.634 - 00:07:54.644, Speaker A: We need to also develop the on chain storage verification so that we are able to distribute those fee proportionally to their contribution, that is the size and time over time. And to achieve that, we need every data node to prove their off chain storage by sampling their local data. It's kind of like a pure DS, but it's constantly sample this data over time and then using snark to compress those results. Those procedures in a very, very simple proof, succinct proof, and once there's such a successful proof is being generated, then the data node is able to submit this proof to network to the smart contract and tell the hey, I really store this copy of data over this time I would like to receive corresponding reward and this log will be verified on chain using eclipse pairing. And then we are able to check that the corresponding node is really maintained copy so that distribute corresponding work. And lastly it is scalable because we only need to have two assets store key value pairs that is on chain so that we can maintain who owns the data and what is the data hashes of the corresponding 128 kb blob. And so that we need 128 key value pairs that store on chain by be able to points to the 128 data so that we can achieve 101, 1000 x and assuming that if there are no capacities for one terabytes then we are able to achieve petabytes capacity and that's how we get this number.
00:07:54.644 - 00:09:14.508, Speaker A: Another common question ask us is that what is your differences with existing existing systems like Filecoin Rv? So here I list give a table list of differences with different angles. One two thing I would like to highlight is first, user storage is working on kind of like offering like a key value programmer database thanks to the smart contract. So that we allow the users to use smart contract determining who owns the data, who is able to modify data, who is able to append data. While Filecoin re they are more designed for static files. So for example in re there's no way to update a file, there's no way to delete the data, but in its storage we are able to, thanks to smart contract we are able to programmably to manage how the underlying key value database look like with the value corresponding to 128 blob. And so this able to enable a lot of applications. For example decentralized social network like we can have a blob, we can implement on top of is storage.
00:09:14.508 - 00:10:08.114, Speaker A: And if you're in smart contract we can have a lot of interesting applications that we give some examples later. And another major difference is that everything on top of Azure storage is built on top of Ethereum. It can use Ethereum wallet to send transaction and upload the data. It uses Ethereum accounts and also eser as a storage fee. And also its development testing too is all on top of Ethereum. So that we are not only be able to reduce Ethereum security, but also be able to reuse Ethereum development stack Ethereum ecosystem. And this is really really important that we are able to maximize the Ethernet value at the same time lower the friction of the new user to onboard using is storage.
00:10:08.114 - 00:11:30.114, Speaker A: And so here is the roadmap of is storage for last year we received the grants, also secured the first place of ad account shipper demo and this year. So we have a couple of major milestones. One is that we have public testnet that is now running on top of the Ethereum sepolia testnet and it has been running for two months. Everything is really quite stable. Definitely we welcome everybody to be able to join network and try to crash if you find some way to hopefully to crash this network in the early stage. And also we have some proposals to for example integrate with Ethereum proto network and with the op stack. So here is some dashboard and also the terminal of our public testnet is on Sepolia and we have about 100 replicas with which most of the nodes are run by our third party so that they are able to join the network, download the corresponding blobs, synchronize the data and then do the periodic sampling, compress, disprove and collect corresponding reward.
00:11:30.114 - 00:13:07.054, Speaker A: So feel free to go to our website, there's a link to tell you how to join the network and how to run this node in the network. And so with is storage, we are able to imagine build a lot of new applications on top of Ethereum. One interesting application is that it can serve great Ethereum aligned solutions to address data expiration issue of ethereum. So right now for Ethereum EIP 4844, it allows each block block to carry six blobs, that is six times 128 data. Unfortunately, because it generates so much data, the Ethereum protocol specified that this data is only saved in the Ethereum network for about two weeks. That means after about 18 days all these Ethereum blobs will be discarded in the network and its storage will be a natural way to help saving all this data using ether online approach. And this problem will become severe when the Ethereum is upgraded with for example pds which will eventually be able to accept about 80 terabytes of data per year on top of Ethereum network and where to store this data, especially how its storage can work.
00:13:07.054 - 00:15:21.974, Speaker A: So these are some specification, for example EIP 4844 that right now is about 18 days and EIP 444 is also going to remove some historical blocks that is older than one year. And so right now the major users of Ethereum blobs are L2 s, which they compress, batch their L2 transactions and then submit to the Ethernet network and using the blob as the way to carry all the data on top of Ethereum. So thanks to Opstat, we also secure grant to work using its storage so that we can able to secure all these L2 transactions in the blobs using its storage so that when those data can be pruned after pruning by the Ethereum protocol, they are also still available on top of is the storage. And right now we have a testnet that is testing all these features so that we are able to be able to basically rerun get the latest data of this L2 by running the L2 transactions from Genesis to the latest day with part of data that is stored in storage even though the Ethereum has prim data. And another interesting application is is that we are able to combine with the computation power of Ethereum with storage power so that we are able to enable a lot of fully on chain applications. That's why we have proposed this raspberry assets protocol which I will give another talk tomorrow about the details. But in general, the idea is to term the EVM as a decentralized HTTP server so that now I can implement all those frontline logic in a smart contract and where the points to corresponding data on top of the AC storage so that now we are able to enable new type of applications with dynamic content that is managed programmed by the smart contract.
00:15:21.974 - 00:17:33.284, Speaker A: It's very similar to HTTP, but instead of calling a centralized IP address or DNS, it's basically calling a smart contract and tells hey, I want to retrieve this data with this coredata and then this smart contract will return corresponding data. For example, we using EIP 444 together with web3 URL, we have be able to upload NFT images on top of Ethereum network or even more powerful applications. For example, one application is we upload the who Vitalik block that is about 104 megabytes of data and then send all this data using EIP 4044 transaction to carry all these articles and send to a smart contract that is mapped by vitality blog eth that is from ENS. And now we are able to upload the full Vitalis block that is now running on top of the ethereum rather than for example ipfs which can discard the block at any time because ipfs is not guaranteed the availability of the data. So this is really, really crazy ideas, but there are some more ideas that can build on top of AC storage plus the smart contracts. Another interesting idea is fully on chain AI so that we are able to for example upload a large amount of the AI models or AI training data and using for example smart contract to determine the quality of data or to determine what is the relationship of the promote, that's the input of model and the corresponding outputs. So for example, for the on chain inference, we are able to work with aura on top of the core optimistic machine learning, so that we are able to almost instantly to tell what is the corresponding inputs on an output of a given AI model.
00:17:33.284 - 00:17:51.244, Speaker A: So that now we are able to fully deploy this AI on top of blockchain without any third party, centralized parties. So this is all the talk today. So happy to answer any questions regarding AC storage. Thank you very much.
00:17:55.414 - 00:17:57.954, Speaker B: Thank you so much. Are there any questions?
00:18:00.094 - 00:18:25.774, Speaker C: Hi, thanks. Great talk. You mentioned the incentives and then there's a one time fee for storage. But how you kind of like make sure like, or how you kind of distribute that over time and how do you know frequency? Like what if I would like to store something for ten years? Let's say like not just like for a few weeks or a year, but like let's say ten years. Can I like top up? Or like how do you make sure that the incentives are still there for the notes?
00:18:26.434 - 00:19:35.970, Speaker A: Yeah, that's a really great question. So basically how we're able to cover a long time, maybe ten years, hopefully 100 years or maybe 200 years using one time payment, because this is a bit counterintuitive versus our current fear model. And second is how, what is the interval? Right. So for the distribution, we basically using a core discounted cash flow model, which means that when the user pay the first time of the fee, let's give an example, like 5% of the fee will be distributed in the first year and for the next year we will distribute the remaining of the fee and times another 5% and another 5%. The underlying assumption is that first the storage fee will decrease versus the fear and also the ESA price will be gradually like according to our current observation, is going high. As long as this ratio is greater than 5%. Then even basically the Easter will be basically paid to the store providers.
00:19:35.970 - 00:20:42.588, Speaker A: But the rest of fee will be able to still be constantly cover the rest of the storage nodes in the following, while maintaining the same level of replications or security, because the security is one over n where n is number of replicas. So this is the basic underlying assumption. Actually this also applies to bitcoin, because bitcoin has assumption of having the block reward every four years, which in implies that in order to secure the same security, the bitcoin price must be doubled every four years unless there's a significant amount increase of the transaction fee. So we basically employ the same model, but we implement all this model using smart contracts. So it automatically calculates this discounted flow and how much fee to be distributed in for example a second level calculation. But given the whole discount rate is 5% every year, so you can check out the smart contract how we achieve that. So this is answer for the first question.
00:20:42.588 - 00:21:46.278, Speaker A: For the second question is regarding the interval. So the interval is a probabilistic process. And so for example, right now our design is 3 hours for the interval. The main reasons for choosing such long interval is because we would like to amortize the fee over this 3 hours time and also all the replicas in the network so that the corresponding fee is able to cover the snug verification fee because that will be expensive on top of ethereum. So in the, I think, let me see if there's, if you go to our dashboard you can see that there's basic mining summary tells how many blocks been mined and what is the corresponding reward versus the correspondence storage fee at the verification fee that we would like to reach some level of one over ten so that the reward is about ten times of the verification fee. That's how we choose the number. Yeah.
00:21:46.278 - 00:21:49.954, Speaker A: And that is really, really like important questions.
00:21:50.554 - 00:21:52.254, Speaker B: Awesome. Any other question.
00:21:57.354 - 00:22:16.254, Speaker D: I have a question about, I wonder what is your base data structure of the scheme? The data structure is still merco, Patricia, tri or verku three or some other such dynamic trees and so on.
00:22:18.054 - 00:23:03.574, Speaker A: That's great question. So actually in the small contra the study, data structure is mainly backed by ethereum key value mapping. By itself we mainly maintain two mappings. One mapping is to give a list array of the order data hashes that the off chain should store like this. This is an array of data hashes. And second mapping is basically a way to tell who is the owner of these data hashes of this array, so that the owner is able to for example modify the data or maybe remove the data or maybe create new data. So there are two mapping that takes two key value pairs using asset store on top of ethereum.
00:23:03.574 - 00:23:21.264, Speaker A: So with these two mapping now we are able to for example allowed applications to using these key value interfaces to modify the data and at the same time be able to verify this list of array of data has been corresponding that has been replicated in an off chain network.
00:23:34.984 - 00:24:02.564, Speaker D: I mean not layer one smart contract storage, but the L2 storage. So which data structure are you organizing? The off chain data and the base data structure which you generate a snark to prove the correctness of the data, prove the validity of the data. I mean the L2 data structure.
00:24:03.834 - 00:25:06.834, Speaker A: So for the L2, we are not like for example execution L2 which maintains its individual ledger. For the L2, you can imagine it just maintain an array of the blobs that is mapped after this array of the hash list that is on top of blockchain. So it's really simple. It just maintain array of blobs which for example when there's new data hashes that is append to this smart contract, and then all these nodes will download the corresponding data from the Ethereum dA network and append to the local storage. In this way when they are performing the verification, they submit a proof they are verified. It really matches the on chain data hashes. So this is the current very simple design, but really powerful thanks to smart contract because we can dynamically change this data using key value interfaces on our smart contract.
00:25:06.834 - 00:25:10.874, Speaker A: So this is how the data structure looks like.
00:25:13.054 - 00:25:23.094, Speaker B: I believe we do have time for one final question, if there is one. If not, thank you so much for joining us and sharing your insights with us.
