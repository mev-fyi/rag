00:00:09.800 - 00:00:20.914, Speaker A: Okay, so I now want to welcome Mario Havel, who supports Ethereum core protocol development at Ethereum foundation and Ethereum founder and glorified blogger Vitalik Buterin.
00:00:22.174 - 00:00:44.514, Speaker B: Thank you. Okay, very well. Thank you so much for coming here this rainy morning, all of you. Vitalik, thank you for being here. And sadly, Mihai could not be here. This panel was supposed to be originally with actually another Ethereum co founder. And yeah, I am the, I am the cheap substitute.
00:00:44.514 - 00:01:03.540, Speaker B: So, yeah, I literally woke up this morning to a message that I'm supposed to run the interview. Vitalik, like an hour ago I woke up. So here I am. And I have few questions ready because I was told that L2s would be an interesting topic. So I would start me with that. And there is a lot of, of course there is a lot of things happening. There is.
00:01:03.540 - 00:02:02.976, Speaker B: On your very recently, very active blog, you also touched on a lot of L2 staff. And I would start with maybe a bit high level, a bit of a reflection about the roll up centric roadmap, which when I imagine the ETH research blog post, it's like two, three years, I guess. And since then, a lot of things happen, a lot of new roll ups popping out. And so my question is about a reflection, like whether at a time a few years ago you had this vision that it's going to look like this today, or maybe, you know, because honestly, personally, I'm a bit pessimistic about the way the robots are maturing. You know, we still have the training wheels. So the question is whether you think that it would take this long, whether there are some technical blockers that you see. I remember around DeFCOn in Bogota, you've been speaking about the decentralized fraud proofs, and that's something which I found really cool and necessary.
00:02:02.976 - 00:02:10.204, Speaker B: But I think we are also not really closer to that. But please correct me. So, yeah, how do you see the ropes and record map after a few years?
00:02:10.904 - 00:03:42.750, Speaker C: Yeah, I mean, I think the thing that went well is that, you know, we actually do have a pretty large number of roll ups, and they really are trying to do lot of different things, innovating technologically, innovating culturally, innovating economically in a whole bunch of ways. The biggest thing that's taken longer than expected is definitely getting to having actual working proof systems. And I think arbitrum definitely does deserve a big round of applause for just being far ahead of all of the other EVM L2s for just getting to stage one. So let's. But it is feeling like the level of maturity of all of these proof systems is going up and up, right? And you know, we're seeing ZK based systems, different kinds of optimistic systems. I know there's like opzk things that do like one round fraud proofs based on some, I mean, Tyco has this design that like combines like SGX and it's kind of optimistic because in that kind of one round way where if you disagree with the results, you can contest it with a cksnark. So it's like getting there.
00:03:42.750 - 00:04:56.720, Speaker C: I think the thing that I would say is that software tape does often take longer than you would expect. Or rather the last 20% always ends up taking 80% of the time. And I think that would have been true in pretty much any ethereum scaling strategy, right? Because if you imagine Ethereum had taken a full layer one charting approach instead, then like, as I wrote in my blog post, right, like sharding and L2 is basically are the exact same thing. The only difference is basically like who has development responsibility and authority for like building different parts of the stack, right? And so they would have had the exact same problems. There would have been, you know, years of development and like actually getting something as complicated as a proof system, whether integrated into Ethereum. And then if it was optimistic, then that would have been super complex. If it was Ek based, I mean it would have taken all the way up until now, probably, plus much longer, because we would not have had so many different teams out there in the community trying different approaches.
00:04:56.720 - 00:05:39.890, Speaker C: So I think it's important to just appreciate the extent to which like scaling that's not multisigs is just a fundamentally really hard problem. And it's a hard problem that like, basically, like any aki ecosystem eventually has to brush up against and does brush up against, right? So I'm generally impressed by the level of sophistication of the work so far. And I definitely feel optimistic that we're going to actually start to see the training wheels shrink and come off over the next couple of years.
00:05:40.082 - 00:06:03.614, Speaker B: Awesome. Yeah, this is great to hear. Being optimistic and speaking about this exactly the optimistic ZQ world, I often hear ZK everything. I see it in the roadmap. ZK everything is the endgame. Also, the optimistic people speaking about eventually transitioning to the ZK. And I have basically two connected questions about this.
00:06:03.614 - 00:06:30.864, Speaker B: Whether this evolved for you as well, whether you see the endgame in the ZK different than before and how this transition can happen. What I'm wondering, we have these t shirts here, whether this could include privacy, because many of these weather roll ups or generally the ZK technologies on Ethereum focus the Zk on scaling, but don't utilize its properties for privacy. So I wonder whether you think it can be combined or can be.
00:06:30.904 - 00:07:37.112, Speaker C: Yeah, I think so. Regarding ZK for scaling, I think my picture on the endgame definitely feels much clearer than it felt about three or four years ago. And basically the endgame is what I've called the proof aggregation singularity, right, which is basically that ultimately, yeah, I think an ethereum block should contain one proof and that proof is a proof of a bunch of proofs. Those proofs can also be proofs of a bunch of proofs. And you know, you have different levels of proving, proving all kinds of things happening, whether it's inside of l two s, whether it's at the consensus layer or whatever else. And then finally all of those proofs get aggregated into one, right? And if you do this, then you could have a world where every L2 securely commits into ethereum in every single slot, right? So today the status quo is like seven day delay if you're an optimistic roll up and then something like one to twelve hour delay if you're a CK roll up. And a big part of.
00:07:37.112 - 00:09:00.043, Speaker C: Well there's basically two reasons why, right, one of them is prover latency and that's just like a prover architecture problem. Once provers have more parallelizability, there's nothing fundamentally stopping in EVM block from being proven within three or 4 seconds. And the other big piece of this is the fixed cost, right? Meaning especially the fixed gas cost of having a proof that gets verified on chain. And if we have good ecosystem wide aggregation layers, then we'll be able to move from every L2 needing to submit a separate proof to all the L2s together, really only needing to submit one proof. And once you get into that world, then committing in every single slot actually starts to become much more viable. And the other nice thing about this world is getting back to the privacy topic again, that's also a world where privacy becomes much cheaper and much more viable as a byproduct, right? Because the status quo is basically, yeah, you know, I use railway and railway is great, but railway is expensive, right? And like the fees are something like, you know, tens of dollars, I forget, is it like 50 or 80 or like some pretty crazy number. And no, ZK snarks really are expensive.
00:09:00.043 - 00:09:42.964, Speaker C: But if we have proof aggregation, then that exact same proof aggregation can also be used to aggregate these synarcs that are being used by privacy protocols, and they can also be used to aggregate synarcs that are needed for key store roll ups. And so account abstracted cross L2 becomes much more efficient. And they also enable a different kind of privacy strategy, which is a general purpose stealth address. So I think if we can actually build this, the infrastructure to make this ZK proof singularity happen, then that actually makes a whole bunch of problems a lot easier.
00:09:43.744 - 00:10:13.212, Speaker B: That's exciting. And it actually leads me to two questions. First, a bit more technical, something coming from the recent interop where we saw a discussion about how the vertical trees, the coming vector commitment Merkel tree transition is going to, would affect, it is going to have on the ZKVM and ZK proofs, some people being worried that it might not be ideal and actually speaking in favor of binary trees or maybe some other solutions.
00:10:13.308 - 00:10:52.872, Speaker C: So yeah, yeah, this is actually a really fun rabbit hole. We ended up talking about this quite a bit at our developer retreat in Kenya a couple of weeks ago. So basically, situation is, right that we've for a long time wanted to move from hexory Patricia trees, which are just like horrible, ungodly infrastructure that I regret ever creating. And like, they're really horrible. Okay, so like, can you guess what is the theoretical worst case size of a merkle proof needed to prove all of the accesses in a block, in an ethereum block, right?
00:10:52.888 - 00:10:57.264, Speaker B: Now, dozens of megabytes, maybe hundreds.
00:10:57.684 - 00:11:57.390, Speaker C: Over 300 megabytes, I think about 400. Okay, now it's why, right? First of all, because in a hex three patricia tree, every layer of the tree is 500 bytes because it's 16 hashes. And the second reason is because code is not even merkle tree code is hashed, right? And so if you access 24,000 bytes of code, then your Berkel proof just like has to contain an extra 24,000 bytes, right? 24,000 bytes for the code, plus 4000 bytes for the branch. 28,000 multiplied by, I think it's like 14285 maximum accesses in a block. And you know, you get your 400 megs, right? So this is not like a necessary property of Ethereum. This is like the result of just extremely bad design choices that were made a long time ago. And they don't just hurt merkle proofs, they also hurt, like our ability to increase the maximum code size, for example, right? And so for this reason, we've wanted to switch to a vertical tree.
00:11:57.390 - 00:13:18.362, Speaker C: Vertical trees are based on elliptic curves. And the nice thing about vertical trees is that even though the arity of the tree. So, like, the number of children that each node in the tree has actually gets bigger, 256 instead of 16, the size of a proof for each level instead of being the entire level, it's like one object for each level, right? And so instead of having like a 16 object tree where the proof is 32 times 16, you have a 256 object node where the size of a proof is just 32, right. And then you. So then a path from a root to any single object is only like about 200 bytes, and then you have an extra single proof that verifies that all of those relationships between the parents and the child node are correct. And basically the size, the worst case size of a proof, if you include both the tree itself and you include some of the changes to the gas costs that accompanying it, like basically charging per chunk of code that gets read instead of charging once for the entire code, then the maximum size goes down to, I mean, I forget the number, but we can reverse engineer it, right? Because, you know, a branch costs 1930 mil divided or I guess, well, I guess 1900 if it's in the list. 21.
00:13:18.362 - 00:13:37.148, Speaker C: We'll stick with 2100. 14. 285 multiplied by 200. And, you know, it's only about like 300 kb right now, so it's really actually. Hold on, sorry, did I say. No, it's not three GQ, it's about two and a half megabytes. I meant to say.
00:13:37.148 - 00:13:52.468, Speaker C: Okay, I forgot a factor of ten, but it's still like two and a half megabytes is very tolerable because the theoretical max size of an ethereum block today with just call data is like, I think it's like 2.8 megabytes. Right?
00:13:52.516 - 00:13:53.556, Speaker B: Including blobs.
00:13:53.700 - 00:14:17.654, Speaker C: Okay, sorry. If you add blobs, then it becomes 3.4. But it's. Yeah, basically. Yeah, like, vertical trees are just like a really big and really amazing improvement in that way. But the challenge is that vertical trees depend on elliptic curves. And we designed vertical trees around elliptic curves because two or three years ago, well, in part because two or three years ago, all of the proving systems used elliptic curves.
00:14:17.654 - 00:14:55.236, Speaker C: And so vertical trees were kind of very native to being proven inside of ZK vms. But then, since then, the ZK proverbs universe kind of decided to mostly give up on elliptic curves, and everyone started talking about the small fields. You know, you have your goldilocks with the 64 bytes, and then a bunch of other people are like, you know, yo, dog, I heard you like small fields. Let's do two to the 31 minus one. And then other people are like, you know, yo, dog, let's do two to this. 27 plus one times 15. And then eventually these binius people are like, hmm, the numbers are decreasing.
00:14:55.236 - 00:16:11.842, Speaker C: How about we just go all the way down to two, right? And basically elliptic curves in this kind of world just no longer have an advantage, right? And actually, directly proving elliptic curve based vertical proofs with these new proving systems actually puts you at a disadvantage. Right? So that's one argument. And then the other argument is basically that, like, these new provers, they're getting so fast that we might as well just like, skip vertical and go straight to binary. And if we go straight to binary, then we have quantum resistance from day one, and theoretically we never have to touch the state tree again, right? Which is like a really amazing outcome, right? It's like if we can switch over the state to binary, and then, then we switch over everything else to binary with Ss Ed, we basically have like one hash scheme to rule them all. Like, ethereum becomes a super beautiful and super clean system on a pretty accelerated schedule, right? And so there is this big desire to try to, like, see if verkle can be skipped. The challenge, though, is that we're still not at the point where we can do that efficiently enough. Right? Basically, the theoretical max number of hashes in one of these binary proofs would be that, like, a prover would have to prove over is like one to 500,000.
00:16:11.842 - 00:16:43.446, Speaker C: And the best provers that we have, they prove like low thousands of hashes per second. And then actually proving over vertical proofs is like, crazy fast. There's someone who used a GKR, which is one of these fancy schemes, to prove over it. Sorry. GKR is like an elliptic curve based fancy scheme. So the elliptic curve people are trying to keep up, too, right? And it's like just crazy fast. Like, the numbers on vertical, short term are just super favorable.
00:16:43.446 - 00:17:36.201, Speaker C: And so one of the questions is like, you know, do we do vertical first, or do we just skip it? And then the other sub question is like, do we prioritize verkle? Or do we prioritize, like, say four. Four. Four and a couple of other things, right? So the current path, I think, is that we're, like really accelerating, 4444 really still going forward on verkle, but at the same time trying to be agile about those things. And I think the nice thing is that most of the work on vertical, and I would say maybe 75% of the work on vertical and like 100% of the work on vertical that's being done today is actually not vertical specific. It's like specific to proof formats, infrastructure, transition logic, you know, gas costs. So it's like a bunch of different stuff that is going to be exactly the same even if we throw vocal proofs away. Right.
00:17:36.201 - 00:18:22.874, Speaker C: So even if we decide to, like, flip that, that switch and switch to a binary system, like, all of people's hard work, mostly, yeah, I'm in like stairs, right? So I think either way, we're in a good place. But it's like, it's definitely one of those fascinating technical debates. And I think. I think it also shows the importance of maintaining agility and basically constantly rethinking pieces of the roadmap and basically saying, let's look at the actual situation that we have in 2024 and let's make sure is the thing that we wanted to do three years ago still the right thing? Sometimes it is and sometimes it isn't. And if it isn't, then we do need to be open to pivoting.
00:18:23.864 - 00:18:38.248, Speaker B: Thank you so much for getting us through the rabbit hole. Catching up on months and years of core R and D basically with this explanation now. Yeah, that was quite technical. I love that. Personally, I'm not sure if everyone's following you.
00:18:38.376 - 00:18:40.032, Speaker C: Who here likes rabbit holes?
00:18:40.208 - 00:19:21.246, Speaker B: Binary or vertical? Oh, yeah. Before we have a few minutes, I was told, bit less technical topic, maybe more about the. More about the culture that you've write about recently. So one thing. Okay, so yesterday I saw a tweet from guard saying something like, Vitalik is the only creator, co creator of a cryptocurrency, a part of Satoshi, which got ETF, and he didn't say anything about it, which is respectable. And I don't want to ask about that. Just like I still don't want you to say anything about it.
00:19:21.246 - 00:19:50.714, Speaker B: Just that with these ETF's with cbdcs coming to maybe Ethereum or similar chains, with so many, whether government or just many other actors coming to this technology, do you think it might be harder to maintain the Ethereum culture, cypherpunk culture, for example, more practically in enshrining privacy or sensory resistance features?
00:19:52.214 - 00:21:08.752, Speaker C: I think. I mean, bitcoin does have a strong cyberpunk culture to it, and I think that is one of its advantages that's really worth recognizing. And I do think that kind of Ethereum's position has always been that, you know, we take the kind of golden mean between being bitcoin like and being one of these sort of fast chains that doesn't really care about decentralization and just says, like, you know, go, go, go. We're going to djen and scale. And I think this does also mean that Ethereum has this kind of always this constant, you know, cultural choice of kind of, do you go in one direction or do you go in another, another direction? And, like, I actually, yeah, like, I feel like it's over the last year, the mood has been kind of going a little bit more bitcoiny, I think. I mean, I think in part because, I mean, we just realized, right, that, like, there's, you know, fast execution layers. Like, there's lots of people that are making them, and, you know, there's even amazing, you know, like, teams and, you know, even teams in this room that are making really amazing, like, fast chains as part of the ethereum ecosystem.
00:21:08.752 - 00:22:17.222, Speaker C: Right? And, you know, if for ethereum to, you know, there is benefit and kind of, like, specializing in, like, being that kind of, I'm not secure core, it's like, there definitely is going to be pressure to try to, like, ossify things, and there's going to be lots of pressure from a lot of different groups. On the question of privacy, I think, I mean, a big part there is like, you know, there's always been this kind of debate of, like, layer one versus L2 privacy. And Ethereum has generally taken the L2 privacy route. And I think generally not because we don't believe in privacy or like, we think privacy is optional, but because there's just like, so many different technologies for getting privacy, right? Like you have, you know, GPHR 13, you have groth 16, you have planck Plunkey two plonky three goldilocks. You know, you have halo, you have, you know, Culk. You have merkle tree proofs. You have joined split protocols.
00:22:17.222 - 00:23:00.872, Speaker C: You have aztec vms. You have then, like, at least two different implement, like, philosophies of how to do privacy pools. You have just like a whole bunch of these different things. And it just always felt too early to, like, really enshrine one of those. And, like, it felt against Ethereum's philosophy to try to enshrine one way of doing privacy when Ethereum was all about enabling multiple ways of doing everything else, but at this. So, but the thing that Ethereum did do early on, right, is we did quite early on add the element elliptic curve precompiles, right. And the elliptic curve pre compiles were added specifically with the goal of making privacy protocols possible.
00:23:00.872 - 00:23:23.700, Speaker C: Right. Actually using elliptic curve protocols for ZK rollups. That actually came after. Right. That came in 2019. The original reason to put Zk or elliptic curve or BN 128 precompiles into Ethereum was to support privacy features. But then it is possible that Ethereum's approach is going to change over the next couple of years.
00:23:23.700 - 00:23:48.578, Speaker C: The reason, basically, is that our ecosystem is moving from indefinite optimism to definite optimism to some extent. If you remember five years ago, it was like, let's get excited about everything. Do you like stable coins? Yeah. Do you like daos? Yeah. Do you like ens? Yeah. Do you want to turn everything into a daO? Yeah. Do you want to have prediction markets? Yeah.
00:23:48.578 - 00:25:02.714, Speaker C: Should the prediction market be a DAO? Yeah. You know, should Ethereum, you know, do zk? Yeah. Should we do sharding? Yeah. Should we do hypercube sharding with like, an arbitrary number of layers? Yeah. And then, you know, it's okay. Okay. You know, over the next ten years, things have kind of matured and, like, we have like, a much clearer understanding of, like, you know, like, what actually makes sense in terms of things like scaling, in terms of technology choices, I mean, even in terms of applications and the, like we talked about the proof singularity, for example, right? There is a lot of value in enshrining some kind of proof aggregation eventually as part of the Ethereum protocol, right? Because whenever you do something as a L2, especially if it's an aggregation protocol, aggregation protocols inherently have fixed costs because it's like no matter whether you have 50 people on the bus or one person on the bus, you still have to pay for the whole bus and you have to pay for the whole driver.
00:25:02.714 - 00:25:53.384, Speaker C: Right. And so, like, the economics of that are, like, very unfavorable. And that's actually a big, I mean, like, part of the reason why, like, public transit systems often end up subsidized. Right. And so having, like, protocol layer buses for proofs is something that I think is like, just from an economics perspective, is something that's worth eventually considering. And, you know, when we do that, that would mean that, like, we'd have to pick a particular proof system, right? And I think having, like, in protocol features that are organized around, like, even like, private privacy needs in a more explicit way is, like, something that could totally happen. But, like, you know, that's the choice for Ethereum's future and the community of the future.
00:25:53.384 - 00:26:00.284, Speaker C: So, you know, it's one of those discussions that I think it continues to be worth having.
00:26:00.724 - 00:26:04.452, Speaker B: Yeah. Awesome. Thank you so much. I think we have to wrap up.
00:26:04.548 - 00:26:05.304, Speaker C: Okay.
00:26:05.644 - 00:26:19.184, Speaker B: Are we out of time or we have. Okay, no more questions though. Thank you so much. Many rabbit holes that we looked into. I really appreciate that. And, yeah, I guess that's it. Thank you so much, everyone.
00:26:26.734 - 00:26:35.134, Speaker A: Thank you both very much. We're going to take just a couple of minutes. We'll be back as soon as we can with our next speaker.
