00:00:13.520 - 00:00:59.078, Speaker A: Fantastic. Hello everyone. Thanks for coming to my talk. Can I start? So this talk is about data availability, or is it really? In recent years, this term was used to refer to the problem of persistence and retrievability guarantees of side chain transaction data, in particular, and most relevantly, data transactions submitted to Zkrolabs, which state validators of rollups need to reconstruct the state. But my talk. Please go. But my talk wants to broaden this context.
00:00:59.078 - 00:02:18.994, Speaker A: Admittedly and maybe provocatively, hijacking this consensual narrow meaning somewhat when the founders of Ethereum re recognized the Turing complete blockchain as a layer of global secure computation, they started imagining what a complete computational environment would look like, an infrastructure underlying the newly decentralized Internet that they termed web3. Who remembers the word computer here? Components of this infrastructure were conceptualized using the analogy of computer hardware, the Ethereum blockchain being the cpu of the world computer. They envisioned the hard disk for data storage, codeswarm, and the messaging layer codewhisper, referring to this as the holy trinity of components. For various reasons, manifesting this greater agenda proved much harder not to crack and subsequently fell out of scope for the Ethereum foundation. Nonetheless, research and development in this direction never stopped. So their ambition is to realize the serverless Internet and the data layer serves its purpose. Go back, please back back.
00:02:18.994 - 00:03:59.554, Speaker A: In this context, a complete future proof base layer infrastructure a data layer serving the Ethereum ecosystem does not only have a wider scope of uses on its agenda than the term data availability refers to, but also must deal with the requirements going beyond what is assumed under this narrow sense of this availability. These aspects include representation, permissions, and persistence. The first concerns what form is the data accessible pertaining to units of data, units, internal structuring, linking people blobs to each other, and categories of content. In more familiar terms, it is concerned with chunking, addressing mutability and searchability of data. The second aspect is concerned with previous who can access the data and who knows who can access it? In more familiar terms, it is concerned with encryption, access control, censorship resistance, and proximity liability. The third aspect is concerned with how long can we access the data and how certain can we be that the data can be accessed. Now let's look at some of the major use cases of such an ecosystem by generic data layer and formulate requirements based on the needs of these use cases.
00:03:59.554 - 00:05:02.274, Speaker A: Starting with what the data availability is, in the narrow sense, is concerned with support for blockchain related data. Secondly, the problem of hosting application code and associated static assets, of course, don't get confused. This includes JavaScript, web assembly code that is driving dynamic pages of the application uis and thirdly personal and application data. However, we must not stop here being a generic data layer with the objective to power web3. We do not only include storage of data, but need to take care of distributing, transferring, querying and discovering data. If you go the whole way and ambition to implement the serverless Internet, we need to tackle the functionalities that in web two are carried out on the server, namely carried out by server hosted databases, server side processes, and server mediated communication and notification subsystems. This is no small feat.
00:05:02.274 - 00:06:40.714, Speaker A: Let us zoom into these use cases and formulate some requirements. The importance of the data layer first of all, let's look at the blockchain related stuff. We can regard the blockchain state snapshot, for example, as a baseline. Since it has no special requirements, it is public, it is only auxiliary data to help synchronization or quick bootstrapping, and therefore has no requirements on data availability. This contrasts with transaction data for roll up support, which is strictly needed for state validators and therefore have strong availability as well as low latency retriever among its necessary requirements. Mainstream go back mainstream state support is an interesting case because it raises the possibility that the native representation of Ethereum state, namely in the form of patricia, tries maps directly to the distributed storage representation. Namely, blobs of data corresponds to serializations of the of the patricia trinodes and the content address links are kept intact and therefore it opens the possibility of querying the Ethereum state by traversal in the distributed storage system, and therefore potentially providing decentralized alternatives to centralized providers of endpoints like inferior and alchemy, or support for live client requests, or even decentralized blockchain explorer.
00:06:40.714 - 00:07:41.154, Speaker A: Now, hosting dev code raises a whole different concern about usability. First of all, it requires, as we talk about browsing UI, it requires URL based addressing from the data layer. This can be described as a minimal reference to a collection and a path based reference to an asset within the collection. The former already requires mutability and can be solved by ENS resolution on the blockchain, and the latter requires representation of a collection and index lookup support. Importantly, application code raises privacy concerns. Ideally, accessing a web through UI should not leak the user's identity and therefore requires request for anonymity and also really in the spirit of true permissionlessness, making available of m three UI must not be leaking the identity of the originator. Just think tornado cache here for a second.
00:07:41.154 - 00:09:53.034, Speaker A: Thirdly, serverless data support should include searchable structured data necessitating sub ens mutability because of quickly changing datasets. Optimized updates optimize the structures structures optimized for updates of structured data indexes, potentially historical persistence and of course infinite scalability as well. But most importantly, if you also want to encompass communication and notification subsystems, these are hard and harder not to crack, and there's not often that the existing solutions offer a solution for these problems, namely communication and notifications. So let's go ahead about the commonalities between go back please. Let's talk about the commonalities between existing active projects that try to fill this gap of generic data in the crypto space, or something similar to that they will share the idea that the network that this emergent behavior of this data layer is ultimately implemented by a peer to peer network where the peers use some sort of underlay connection pattern, but also usually use an overlay connectivity and topology to make routing efficient. Referencing suburbs of data through content addresses is a common choice because it already serves as an integrity solution, but it's usually combined with the model where you can place the participating nodes in the same address space as the chunks themselves. And therefore you can use the overlay topology to allow efficient routing to nodes that store information about chunks that are close to their own address.
00:09:53.034 - 00:12:16.984, Speaker A: They differ, however, in their storage model, which are now simply called either stewardship based, as in the file sharing type type of paradigm where you open up your hard drive and register whatever content you have on there and let leeches become seeders, or the direct storage model in which I call address based storage. When the neighborhoods of a particular proximity hold the actual chunks that are close to them, they do not always have location information about those chunks which are close to them, as for example a bittorrent tracker, but actually store the data. Now, in terms of persistence guarantees, we can mention three types of solutions, namely redundancy, stewardship and incentives. So redundancy is concerned with it can be interpreted as replication resilient against the percentage, particular percentage of data loss remedied by local or cross neighborhood replication, and most efficiently, however, in a distributed way using error correction codes, that is, information theoretically the optimal way. Stewardship here refers to a few things. Yeah, it refers to stewardship also in the sense that, for example, the file sharing type of systems have the origination of content based on stewardship, since they make the entire semantic content like the entire content that's semantically relevant to users in full from their node. And also replication is also done by its consumers, and therefore it leaks information above the originator, and therefore it's not competitive with censorship resistance per se.
00:12:16.984 - 00:13:54.326, Speaker A: This is also true if insurance is based on a kind of marketplace, because there also it leaks if the insurance is provided, that is, by the insurer. If that's true, storage is provided by the insurer, it exposes them and make them susceptible to targeted attacks. Now, incentives, positive incentives means that not storing the content gets rewarded upon showing evidence of entitlement. So they prove the retention of data between recency and relevance, but they have no punitive measure associated or implied if any particular piece of content gets lost. This contrasts with negative incentives, which essentially is insurance of particular content. In case of negative incentives, insurers sign up on a collection and in case any chunk gets lost, what goes missing? The insurer can be challenged and stand to lose their deposit. They staked most distances where storage incentives are primarily in the form of a marketplace, providers most the collection and the originator and ensure oligd identity and therefore provide an attack service and raises privacy concerns.
00:13:54.326 - 00:15:48.504, Speaker A: We argue for a solution where the primary mode of storage incentives is just positive incentives. More about incentives, please, please go to Daniel Knight's talk tomorrow at 420, when he's talking about the incentive system in planting this one. So now I'm gonna arrive at the time. Time. So now I arrived at the most important part of the talk, which, which is the takeaway message here is that although decentralized storage solutions are quite similar in some ways, so they share a lot of commodities, which would lead you to think that any further specific details about architecture is just an implementation of detail. I'm going to argue advice, and I'm going to argue that low level architectural choices have major consequences on the requirements, and might be not only sufficient, but even necessary requirements to satisfy some of the requirements that I identified in the first half. So give me a second to breathe.
00:15:48.504 - 00:16:57.908, Speaker A: So, address B storage. Address B storage, as I said, means that chunks are directly stored at the place with nodes whose address is close to their content. This implies that there needs to be a protocol where content is uploaded and chunks need to be routed to the neighborhood that they belong. And this already shows that this solution provides a proper cloud service and not only presence like the file sharing solution, and this allows for users to basically upload and disappear. So use the storage solution as genuine cloud service. Most importantly, it contrasts with stewardship based approach. Where we're storing the file is this kind of.
00:16:57.908 - 00:19:05.864, Speaker A: If you're looking at your local storage, it leaks information about the originator and therefore it's not censorship resistance address B storage provides local redundancy right away from the start and most importantly incidentally. Actually not most importantly, but incidentally, it comes with a bonus that's a constant upload of chunks provides a cover traffic for address based messaging. What is address based messaging? Address based messaging is when you send a message to a node address, and this is typically useful scenarios where you have the FC establish a communication channel with someone as a recipient. You don't know. This is basically the solution for spamming versus sending anonymous messaging. So this primary insight here is that this messaging thinks that messaging is secondary to storage, although on first look it looks like it's just in most routing, but in fact if you make it parasitic on chunk uploads, you can hide the messages in so called trojan chunks which are first for any third party, undistinguishable from an encrypted chunk, and therefore can only be opened by the recipient themselves. On top of this, it inherits all the properties, so some storage of chunks, namely incentivization, asynchronous delivery and a couple of traffic to mitigate against traffic analysis attacks.
00:19:05.864 - 00:20:27.234, Speaker A: Now this is the most important slide probably so. I mentioned that peer to peer networks use under a transport protocol and the corresponding connection type and an overlay topology for routing. However, if one chooses to keep a live connection, it has an immediate benefit that you don't have to establish a new secure channel for every request and the peers that you select, you can be sure they are online. Most importantly, this enables request forwarding as a type of routing, which is in contrast with iterative routing. Iterative routing is when you iteratively ask your peers to give you closer and closer piece to the destination address, but you directly contact them. This leaks the identity of the requester have relaying or request forwarding leaves the requester ambiguous and therefore allows private browsing and censorship resistance uploads. Also, due to the fact that in case of retrieval, the chunk data travels back along the same path as it was forwarded.
00:20:27.234 - 00:22:03.234, Speaker A: As the request was forwarded, it gives the opportunity to forward us to opportunistically cache the chunk content, which is by the way incentivized bandwidth incentives. And this kind of miraculously takes care of automatically scaling popular content so you don't need cedars become leeches type of store. Same thing, but there is kind of a similar effect. So the most important innovation that I want to introduce is single owner chunks. So we talked about content that has chunks so far which owe their integrity to the irreversible nature of the hash function because their content address is based on hashing the content. Other single owner chunks implement their integrity by the attestation of their single owner. And this allows for constructs where you have a predetermined address where you can put arbitrary content into, and therefore someone can follow an address which is by convention so called rendezvous point for someone to leave a message.
00:22:03.234 - 00:24:20.030, Speaker A: This allows for genuine communication channels to be established, and if you implement sequences of common conventions about sequences of predefined addresses, it can implement feeds which are good for solving sub DNS mutability as well as version tracking and all the rest of it. Finally, here is your coding. If cleverly defined once more relatively small chunks, then any reasonably sized asset, if it's downloaded in full, can be razor coded, and it does not always provide the benefit that is normally associated with razor cooling, which is increased data availability and higher certainty of persistence. But it also allows for masking the network contention errors of a percentage of chunks, or even condemn the connectivity gaps in your routing table, and therefore it puts an upper bound on retrieval latency. Quite surprisingly, this is one, and I will conclude by saying that what I talked about the text message is that if you want to implement a generic data layer that is in alignment with the original ambitions of the Ethereum project, namely a generic data layer underpinning the decentralized serverless world by map, then we need to meet certain diverse requirements. I argue that some of these low level choices are not inconsequential. They actually have far reaching consequences in terms of the satisfiability of these requirements.
00:24:20.030 - 00:25:37.026, Speaker A: And in fact, I argue that swarm has the features that allows it to comply with these requirements. And in fact, in some of the features at least, my claim is that these features are not only sufficient, but in fact necessary. This strong claim, of course, needs a proof which was well beyond the scope of this talk anyway. So I leave you with the message that the idea of the word computer lives on, and if this generated data layer gets in production with poor features and secure operation and proper unpoofable incentives, then we can prepare for the next big thing where the original dream of implementing decentralized eBay, decentralized Uber all you can eat etcetera can be finally implemented. Thank you for your attention and be a few questions. We have time for some questions.
00:25:37.090 - 00:26:02.114, Speaker B: Are there any questions from your audience? All right, thank you very much, Victor. We have time for a few questions from the audience. So if you have any questions, my colleagues will come up with the microphone. Oh, there's a question up there.
00:26:08.994 - 00:26:09.498, Speaker A: Hi.
00:26:09.586 - 00:26:39.034, Speaker C: First of all, thank you so much for the talk. I had so much fun looking at those images that I could barely focus. They were so pretty. My second question is, if you imagined a world where data access and the interaction with a blockchain type of application could be in the hands of the many, in the hands of every one of us, what would that look like in practical terms? And I know it would be an oversimplified answer. Much appreciated. Thank you for your time.
00:26:46.854 - 00:28:36.826, Speaker A: Well, I have quite an ideological approach to answering this question, but I try to be as neutral as possible because it's part of our credo that there's a bayesian infrastructure. We should be content neutral. But in the world where this is possible, I think that what I call the four f words can be mitigated. The four f words are basically various aspects in which freedom and fairness has an obstacle. This affords a force fallacy, fraud and friction and with basically a secure, secure layer for enables free trade for data economy at least. Of course it can never claim to be relevant for your local baker directly, but things that can be search for ship resistant proper, which is ever more proportion of the real economy. If it implements ways in which the barrier of entry and the availability of productivity barrier reentry goals decreases and the availability of productivity tools and tools that you need for conducting business and finding business relationships is kind of trivially available and cheap, then you can expect all the benefits of free trade, which is the obviation of the need for state control and in general centralized control or any kind of stupid regulations.
00:28:36.826 - 00:28:50.074, Speaker A: And secondly, as a consequence of the resulting closely knit social network, on a planetary scale, it's a conduit for world peace, no less.
00:28:51.214 - 00:28:52.114, Speaker C: Thank you.
00:28:54.694 - 00:29:09.334, Speaker B: Are there any more questions? Right, I'll hand this over to the right MC. All right, thank you very much again, Victor. Put your hands together for Victor.
