00:00:01.280 - 00:00:29.714, Speaker A: Hello, we're starting. We are now applying the pyramid principle to ethereum development by Joseph Gattelmeyer of a key blockchain security. Thank you. Thank you everyone. So you must really dislike Vitalik or just for record, he's now on the main stage. So it's kind of challenging having a talk in parole. But thank you anyway for coming.
00:00:29.714 - 00:01:30.244, Speaker A: So my name is Joseph, I'm CEO of blockchain. We are a security company, we are performing security audits and I'm also professor at the Czech Technical University in Prague. Basically just what we do, besides the security auditing, we are also trying to give something back to the ecosystem. So we are big believers of open source and we are developing some open source tools and we are also trying to get back something to the community by providing good education. So we teach one subject at the university and we are also doing lessons for free for anyone. We have one school of Solana and then we have school of solidity. So basically we are operating on these two chains on Solana and on EVM.
00:01:30.244 - 00:02:11.024, Speaker A: Oops, no problem. So we are quite lucky that we could be working for some of the top tier players in the ecosystem. Namely for example our last audit for save. Or we are auditing every release of XLR. And we also audited quite intensively later. Zero. And our last audit was made for pro bono and because we sometimes even help projects that are building some interesting things and maybe, you know, Solede project.
00:02:11.024 - 00:03:02.856, Speaker A: And it's basically a guy who is rewriting open zeppelin solid libraries to assembly to save some guests. So we offered him a free audit. And this talk has a little bit abstract name, it's called security pyramid. And my motivation for the talk was that when a project approaches us, basically for the audit in the first day, we see with like 50 to 50 probability if there will be any criticals or not. How we do it is basically you can see which security or best practices is the development in following. So for some of you it may be like very obvious because it's the same in web two, web three. But I just thought that it's worth mentioning that.
00:03:02.856 - 00:03:50.914, Speaker A: And if we save one project from making obvious mistakes, then it was worth to mention this. So basically the torque will focus on anything you can do before the security audit. And it looks like this. So at the very bottom the things have the biggest impact and basically are the most fundamental in the development process. And you can see that the audit is like sitting at the top of the pyramid. So it's the top of the iceberg and of course it's like the final output of the security process, but the impact is that limited. So basically by having the audit, it doesn't mean that it catches all the previous layers and the biggest impact you can make on the lower layers, not on the top of the iceberg.
00:03:50.914 - 00:04:41.074, Speaker A: So let's get to the single layers probably, you know, rate if you are devs. But still it's a surprise that many approaches don't do that. So every code should be peer reviewed. It's good to follow the standard git flow where basically developers create branches, the future branches and the future branches that are merged with pull requests, which means at least second pair of eyes checks the code. And this also is kind of like use case. For example, a lot of projects we have are written by how I call it like full stack hero. So basically like one guy who develops the backend, who develops the contracts and is doing his own p requests, which maybe sounds good for the investors that we have a one strong dev that can do everything, but it doesn't work.
00:04:41.074 - 00:05:14.850, Speaker A: Here's an example. It was a solar project, but basically which things it can lead to when there's like one guy doing everything. Basically this is a discussion we had with the developer on Slack. And the developer basically was not handling some edge cases in the on chain program. And his argument was like, I don't need to handle that because my backend will never send it. But come on guy, we are in blockchain. So everyone can call your contract, everyone can do everything with that.
00:05:14.850 - 00:06:01.144, Speaker A: And this is how it works and this is how it should work. So the tests and everything should take place in the contract. If not, of course the auditors, what they do in the audit report. This led to free criticals, which means basically he has to pay the audit again, because with this kind of thing he cannot publish that because of course the community wouldn't accept it as a secure project. So that's some of the outcomes of having too much centralization in one person and developing everything. So the next step, unit tests, probably you have heard about them, but still very few projects have them written properly. So basically the development team should be writing the tests.
00:06:01.144 - 00:06:52.464, Speaker A: It's good to test coverage every function for the audit. We always say that d test coverage should be at least 90% better is 100 before that. Of course the auditors are very expensive developers of tests, so it's better to do this internally. And also a good practice is if the tests are written in some kind of like different tech, then is the tested one or with different person. So for example, when you have a solid contract, it's good to write a test in Jscript or python. So there's not possible to copy paste just logical mistakes from one language to another. And also it's important not to test only the happy path scenario, but to test also some edge cases that should not happen.
00:06:52.464 - 00:07:51.520, Speaker A: And of course the test should be part of the CI CD pipeline. And that's why the selection, for example, of the testing framework also matters, because if the tests take like two minutes, so probably then someone will just skip them in the pipeline. So it's necessary that the test execution is very fast. I will later on speak about some tools that I recommend. So this was kind of obvious, and it's the same for web two, web three, where we have some kind of specifics, is static analysis, which in web three is kind of helpful. So these are basically tools that you can run over your contracts and it might find some issues. The problem is that it has many false positives and there are different tools with different approaches, but there is none of the tools that basically catches everything and can replace no audit or whatsoever even.
00:07:51.520 - 00:08:30.524, Speaker A: It's like a very sexy story we have for the VC's. We have replaced the auditors by a tool that's running on AI whatsoever. Give me money. So a lot of projects do that, but in the end nothing works like this. So there's a tool that quite works well. But what we are always looking for is that the tools have a high precision, which means they are confident about the issues they report and the lower recall, which means less noise of the first positive issues. And if we move to the fourth stage, so that's something that just a few projects have.
00:08:30.524 - 00:09:37.454, Speaker A: And this shows some of like more advanced approach to our security. And when we, for example, open a repo and see that the project has sub fast tests, so we can say like at the first auditing day, okay, they probably will pass the audit, because just by having this, it means that they will be, they are some, at some level of the security. So fast tests again are written by the development team and they are trying random inputs on different functions, different contracts, calling them in different order. And with this, basically you can be quite sure, for example, in defi on mathematics, that it covers some kind of undiscovered bad use cases or bad scenarios. And it's also good to test the external libraries, which is kind of like specifics. And what we also do is we are doing a lot of cross chain fast testing, if you will be more interested in this topic. There was today a talk by my colleagues called state of python testing frameworks.
00:09:37.454 - 00:10:15.610, Speaker A: But just in general, fast testing cross chain is good. For example, when you are minting an NFT, so you can be sure that it's minted on two chains. So basically on chain a it's doing some inputs and it checks the outputs on chain B. And there's also quite interesting technique called differential fast testing. We use it, for example, when there is some kind of like external library that's not too battlefield tested as, I don't know, open the plain. But it's kind of like not audited. But the scope of the library of course exceeds the scope of the audit.
00:10:15.610 - 00:11:15.120, Speaker A: So we don't have time or budget to audit the external library, but the library is very important for the project. So how you can do it, if it's some kind of like math library, so you can take math library, implement it in different language that's doing the same, that's more established, and just try the same random inputs on library a and library B and check the outputs. And if the outputs are the same, it's fine. If there is some difference, then probably the library B is not behaving as it should, and we have done it in one of the last audits. So for example, there's a Defi project using the APDK library, which is a library for floating point computations. And we have just taken its equivalent written in Python that's called Bigfloat. And this bigfloat library is using the C library that's used basically everywhere.
00:11:15.120 - 00:11:57.616, Speaker A: So we just assume that this library works and we've compared it with the solid ABDK library, and we have found that on some specific input it provides different outputs. And we have identified that there's like precision problem. If you compare these two hashes, so it's like on the 6th position from the end. So it's kind of like problem that's not occurring very often, but still different output. It could mean a problem. And the test was quite simple to test this library with the differential fastest thing, and then is the audit. So the audit should, should be at the very end.
00:11:57.616 - 00:12:52.994, Speaker A: And for the audit to be successful, it's good to provide the auditors as much information as possible, because of course you don't want the auditors to be spending their time ineffectively. So what we always like prefer is that we need developer documentation, we need diagrams as much as possible, architecture description. Very useful thing is some kind of like video work through the code base, and the outcome is that of course, when the stages one, two, four are missing, the audit will not save you. So the most probable scenario is in the audit when we found one or two criticals, we just write an exploit for that. Writing the exploit takes some time. So for example, when we have an audit for five man days, we discover critical. Then we are writing too many days, the exploit for that to verify the critical.
00:12:52.994 - 00:13:44.744, Speaker A: And then basically we spend our time on the audit by writing the exploit. So in the executive summary, we just write, okay, we have discovered it's critical, then we run out of time, which means fix the critical and order the full audit again, which of course is not the thing that you want to do as a protocol here in the audit. So basically the audit will just tell like, hey, go and return again. So the process still have to do the four stages by their own. And as I touched a little bit audit topic. So there are audits and audits, and at least in our view, the audit is not a stamp. Some protocols prefer that, you know, buying audit from a high profile auditing brand that just serves good for the retail users as they know the brand.
00:13:44.744 - 00:14:32.508, Speaker A: I'm not naming anyone, but you probably know, but the outcome of the audit should be more like this, which means some kind of like very complicated process, which security is. And as a process, it's not like single delivery, which means it's not like one audit. It's always good to audit continuously, which means auditing every release. And the audit serves not just for the protocol, but also for the users. So it of course, should be public, it should be very easily readable. And by this non stamp approach, of course, the audit doesn't tell if the protocol is safe, it just tells that, okay, we have tried this, this, this and that. And based on our findings, we see this.
00:14:32.508 - 00:15:17.470, Speaker A: But the disclaimer is always like, we cannot take responsibility. But for example, important part of the audit is something that's called fastmodel, which describes even how, for example, the treasury of the protocol is handled if there's any multi signature whatsoever. And if we see that it's too centralized and for example, allows some kind of like founders rug pull. We always write it in the audit because the audit is meant for the public, not that the protocol is paying us to give them a stamp. So recently there was like, I know, free rug pulls in a row of audited projects. So with a good auditing company, this shouldn't happen, because it should be stated in the audit report that this is possible. And of course, the audit should hack the protocol.
00:15:17.470 - 00:16:03.874, Speaker A: It should not like find some low hanging fruit with automatic analysis, it should be about the manual code review. And. Yeah, so this is how it looks when the critical issue is discovered. Thank you, MitJorney. And with the auditing companies, different companies have different values. So it's of course good to look for some auditors that audit high profile projects. And like, my personal favorite metrics, like the number that I look for is how many times has the auditing company been hacked? What does it mean if the company audits a protocol and after the audit it gets hacked? It's a problem for the youthing company.
00:16:03.874 - 00:16:53.710, Speaker A: On the other side, this can happen, will happen and happens even to the best players. So what can save the company is the number of projects the company has hacked, which means the many criticals they found. So if, for example, in our case, we in the last two years discovered about 62 criticals and we have been zero times hacked, which is of course great. But even if the number would be one, it's still kind of okay. On the other side, when you discover zero criticals and you are getting hacked all the time, it means that the company probably is worth nothing and the output of the audit is some kind of holy book. Of course, that then the protocol can publish on the website. And I just touched some kind of like, best practices here.
00:16:53.710 - 00:17:29.024, Speaker A: So I will also include some specific tools that I can recommend. A lot of the tools are written by us, so product placement, but it's open source joke. We are not selling it. So for the unit tests, of course, there are like two approaches at least, so we can write them oversold contracts, even in JScript or in Python. We are more for the Python world. For the JScript testing framework, hard hat works pretty well. We have developed Python testing framework, which is called woke.
00:17:29.024 - 00:18:03.676, Speaker A: Just disclaimer for the name. We found the name first before any movement. So my answer is always, why woke? We got the Python package name first, so we just know care. And there's also testing framework in Python called Brownie, but it's now an abandoned project. And there's testing framework called Ape. And these testing frameworks run against local dev chains, so each of them supports anvil, ganache and Hardhat. As I said, important thing with the testing framework is the speed of execution.
00:18:03.676 - 00:18:43.400, Speaker A: So we just wanted to focus on this a bit. So we have taken the 271 unit tests of Uniswap VF project, which are written in hardhat. And basically we have rewritten these tests for brownie, ape and vogue. So it's quite a lot of work. And then we compare the speed of execution of this test suite over the three development chains. And of course the output that we were hoping for is that work is quite fast here. So we can see that this test suite was executed in three and a half seconds on the anvil chain, which is for example like ten times faster than Brownie.
00:18:43.400 - 00:19:41.004, Speaker A: And in our view, this is very important, because for example, when we as auditors help the project to increase the test site, then we can just put the test as part of their pipeline, which for example was the case of the Solidi project. So basically they accepted the test suite as a part of their pipeline. And of course if it will be running 1 minute, they will not accept that. So if it runs 3 seconds, it's kind of okay. And if you'll be more interested, we've wrote a comic paper about the evaluation with testing frameworks. It's now under review, but if you just send me a message on Telegram, I can send it to you, but we cannot publish that. And for the static analyzers, the most established static analyzer is slitter written by trellobits, which has a quite low, has a very good precision, so it's not reporting too much false positives.
00:19:41.004 - 00:20:39.494, Speaker A: And then we have a static analyzer in our vogue testing framework, which works together with versus code extension that we call tools for solidity. So basically, if you are using versus code, the easiest way how to install vogue is by just installing this extension, and it loads all the dependencies. And the output is that here is a screenshot of the versus code, if you can see that. And basically when a user or the developer, for example, creates here possible render and say so, it opens a dialog where it's a testing framework, notifies the user that there is a possible re entrance. So you don't have to run any CI. And you can see just like in your dev environment. And for the fastest thing, there's the first and still very good tool from trailer bits called Ehidna.
00:20:39.494 - 00:21:16.134, Speaker A: And we also have a fast testing in invoke, where basically on top of those specific things for fast testing, we can also support cross chain tests. And we use it with a lot of projects, for example, that we are writing on accelera. And yeah, as the audit is finished, everyone is happy and this is how it should look like. So thank you very much for your attention. And if there is any question, we still have about three minutes. I guess not. So thank you very much.
