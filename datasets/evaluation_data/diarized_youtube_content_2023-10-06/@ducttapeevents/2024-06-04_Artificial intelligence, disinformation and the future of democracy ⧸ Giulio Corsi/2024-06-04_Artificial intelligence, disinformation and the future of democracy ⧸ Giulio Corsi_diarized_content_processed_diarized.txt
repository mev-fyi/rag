00:00:12.640 - 00:00:41.434, Speaker A: Hello and welcome back to the flower stage in today's East Prague Saturday. We are continuing with our schedule with our next talk, so please come in, take your seats and get ready for this talk. The next talk will be from doctor Giulio Corsi and he will be talking about artificial intelligence, disinformation and the future of democracy. So, big round of applause for Giulio.
00:00:47.054 - 00:01:00.604, Speaker B: Oh, wait, Mike. That's right. Ok, let me just set up. Ok. Hello, everyone. My name is Julia. I'm a researcher at the University of Cambridge, where I work on AI, disinformation and democracy broadly.
00:01:00.604 - 00:01:58.324, Speaker B: So today I'll be giving sort of like broad introductory talk. It could get a little strange at times, but please bear with me. So I thought I'd start this with sort of like a broad definition of what we mean when we talk about disinformation. And what a better point to start that, a 1997 article from a bill basically claiming that climate change is not a thing and it's largely caused by. You can't really see that, it's a bit grainy, but it says that around 96% to 97%, 97% of global emissions are caused by natural phenomena as opposed to human activities. And I thought this would be a good starting point as a definition of what we mean by disinformation. So disinformation is false information that is published with a clear intent to deceive.
00:01:58.324 - 00:02:42.790, Speaker B: And this was the case here because at the time, in 1997, science was already very much settled on the topic. This often involves some financial, political or social gain from the public believing this information. In this case, it could be people using more oil or resources. So that's a key trait of this information. And another key point that you all might be aware of is that this information is considered to be on the rise. It has been talked about a lot more in recent years, particularly since 2016. We have a lot of research and evidence of influence of, for example, bought factories and AI.
00:02:42.790 - 00:03:49.642, Speaker B: Even already in the spread of this information. And especially now that we are approaching a pretty intense election season, this is still an even more important topic to discuss. But you might ask, is this information actually something new? And I thought I'd show this very strange image from an 1835 story called the great Moon hooks, where the sun, not the british sun, an american newspaper, started printing these stories about life having been found on the moon, and that life on the moon was characterized by bat like people that habited this pretty strange environment. And this is sort of to show that this information is not new. It's something that has been around basically since the start of human civilization. And you can find examples of this information going today. We could vote, called viral even in roman times, and it was often with a clear intention to deceive for some kind of gain.
00:03:49.642 - 00:04:51.730, Speaker B: This case, for example, the sun, gained a lot of readers, sold a lot of copies, until the story was finally debunked after a pretty long time. And there's a lot of stories like that. So it's not something necessarily new, but something is changing today. And in this case specifically, we're looking at how AI is changing the processes through which we consume information, really. And the first point that AI is really changing is content generation, because AI shapes the very existence of content, as AI models are increasingly used to generate synthetic media. And this may not look like such a monumental change, but when you think of how humans created content through history, it is quite an exceptional change, because in the past, generating new content was always, say, a function of human creativity, time and effort. You couldn't really create content at scale unless you got more people or just more manpower, really.
00:04:51.730 - 00:05:40.904, Speaker B: Today, this is changing because you can finally automate content generation in different types and forms of content at a very rapid pace. So this is a very, very significant change in the way content is generated. The second point here is diffusion. AI very heavily influences what we see in our daily lives, as a lot of the information that we see is mediated by AI based recommendation algorithms. I'll talk about this in a bit more depth later on. But basically, anything that we consume, from news to social media, goes through some sort of AI filtering. And the last step, which is a bit of a forgotten one, is content reception, mainly as a function of the first two.
00:05:40.904 - 00:06:32.372, Speaker B: AI really shapes the way we receive information. And this is important because this information is always the product of content that is generated, that is diffused by also how we accept it, and whether we accept it. And whether or not we accept it, is really about how resilient we are, how media literate we are, and other several psychological factors. And AI really shapes that, for example, by making us more partisan, more divided, or at times more skeptical. And I'll go through the presentation mainly focusing on these three blocks of how information circulates, and then I'll talk about some of the risks that we are facing today. The first block is content generation. So, as I said, AI models can now create realistic, synthetic content that is often indistinguishable from human generated content.
00:06:32.372 - 00:07:20.388, Speaker B: That's not always the case. We probably still at a moment of technological development where we can still, most of us, most people can tell whether something is human generated or not. And by the way, throughout the presentation, you will see some images that come from a paper that some colleagues and I have worked on in the past year that collected a lot of AI generated media circulating on social media. And this is the famous photo of Pope Francis on a puffer jacket, which is probably the first real viral AI generated image ever. Okay, so AI models can create content that is indistinguishable from human content. And this is very important. They can do that in multiple modalities, such as text, image, videos.
00:07:20.388 - 00:08:23.974, Speaker B: Audio is also a kind of unexplored one, since a lot of recent deepfakes have been in audio form. And this is very important because this can circulate in very different ways and can really create any type of content. And another important point here is that generative AI models are increasingly accessible. You've probably seen how AI is increasingly considered a product that you can buy, you can pay some money to access. There is a flourishing open source ecosystem at the moment, which is great, but also makes a lot of the more dangerous capabilities a lot more accessible. And also, AI is increasingly capable, and this is making the creation of misleading content simpler than ever before. And I wanted to stop for a second on the point about capabilities, because this is something that, it's quite noteworthy from someone that works and studies AI, how quickly all of this is happening.
00:08:23.974 - 00:09:26.884, Speaker B: And to show this, I wanted to show three images from a project that I'm co leading at the United nations, where we look at the role of generative AI to spread false information on nuclear accidents. And these are three images that we created as test cases, from three models that are basically one year apart. So Dali two, which is a 2022 model, this is all the same prompt across different models, created images that are really quite bad and couldn't really fool anyone. Adobe Firefly, which is just a few months later, it started getting slightly more credible. But if you really look into it, there's still really bad quality images. And one year apart, we have Dali three, which is a pretty massive leap forward, still not completely credible. AI still struggles a lot with photo realistic images of objects, but it's really improving at the moment.
00:09:26.884 - 00:10:05.924, Speaker B: This is something that we produced several months ago, and now you could even look at probably better cases, and better models, such as mid journey, could probably produce better images than this. And this is to show sort of like the pace of progress. And it's the same across other modalities. For example, now we have video models, which is something that we haven't really had until now. Text generation is improving very, very quickly. Probably a lot of people here have used models such as chat, CPT, or Lama, and these have been, been increasing really, really quickly in quality and in state of the art capabilities. So this is a little bit about the pace of progress.
00:10:05.924 - 00:11:02.274, Speaker B: But what are really the threat factors that we have identified about this generation of synthetic content? Bear with me, this is a bit more boring, but so some of the most important threat factors here are scale. AI has an ability to generate this information that really exceeds human capacities, making it a lot more scalable. This is really a question of quantity. We can now produce false information in much larger quantities for, say, the same amount of input that we could before speed. AI systems can very rapidly create content, which means that this information can now adapt to evolving narratives and changing circumstances. So, for example, something happens in a political setting, it is extremely quick to adaptively produce this information through AI. So basically, at this stage, this information can be reactive.
00:11:02.274 - 00:11:50.204, Speaker B: This is the case, for example, for emergencies, where you could disrupt very easily emergency communication by producing reactive disinformation or adaptive disinformation. Another point is cost, which not all disinformation is really about money, but it can be, and cost can be a very important factor. And there's a very interesting paper by my colleague Josh Goldman that looks at the cost of this information through AI. And basically they have a nice function that shows that the cost has decreased very, very dramatically through AI. So it's extremely cheap to use AI to produce this information. And the last point is hyper personalization. So AI can be used to tailor this information to specific individuals, groups very, very easily.
00:11:50.204 - 00:12:51.254, Speaker B: And this can be used obviously to exploit other vulnerabilities that have to do with hyper personalization. So these are, say, the main threat factors that can be identified at the moment and sort of answer the question, what really changed? Because, as I said, this information has always been around, but this is how AI, in terms of content generation can make the threat a little or a lot bigger. Now, we can go on to the second point, which is about content diffusion, and in this case, AI based recommendation algorithms. Most of the information we consume is curated by I. And this is something that we are getting more and more aware of that, as I mentioned before, for example, news and social media are some of the most widespread applications of AI. It is something that in the past we probably didn't even use to call AI because it's quite basic machine learning. But now they're more often than not.
00:12:51.254 - 00:14:18.804, Speaker B: Called AI, these algorithms tailor content to our preferences, usually optimizing for engagement, which is increasingly seen as a problem because what we find engaging or what gets us to engage with platforms is not always the nicest type of content. So in a lot of cases, it tends to actually optimize for content that is harmful and despite their scale of application, because arguably these are the most widely adopted AI system in the world. Because if you think of, I don't know, Facebook's recommender system is used by probably billions of people every day. There's very little transparency about recommender systems, or at least there are several tactical papers basically explaining in principle what the systems do. And this is true for a lot of social media companies, but then there is very minimal transparency about how they're actually deployed, what the central neural networks of these systems do. And this is sort of changing right now with the EU AI act, for example, but there's still not a lot of transparency. So what are the risks that we identify about AI based recommendation algorithms? So these systems allow for hyper personalized recommendations, which can be based on metrics like location, real time homeline behavior, several other types of explicit preferences.
00:14:18.804 - 00:16:07.014, Speaker B: And this is obviously, well can be seen as a reduction of digital freedom, invasion of privacy, but can also be used for more detrimental outcomes because this personalization can be manipulated for political purposes, for example, by reinforcing system beliefs. I think it happened to a lot of people that we go into any type of rabbit hole and then recommender systems keep pushing us the same type of content over and over again, reinforcing our beliefs, which ultimately reduces exposure to diverse viewpoints. There's a lot of research on this point, for example, on YouTube, and how the YouTube recommender system basically very easily gets stuck on specific types of content and keeps recommending them over and over. And recent studies have shown that most recommendation algorithms actually favor politically loaded false content. One of the studies I did, my study and published a few months ago using Twitter data, where I found that basically on Twitter, far right, and toxic content was amplified a lot more than any other type of content. So there's several risks that we can see at the moment with AI based recommendation algorithms now onto content reception, which is the third block, and how AI can erode epistemic resilience, which is our ability to resist false information and to. Yeah, and this is a very interesting image that we found in that paper, which is an AI generated image of the moon landing being enacted in a movie setting, which was spread probably not to the status of viral, but was circulated a lot to claim that the moon landing hadn't in fact, happened.
00:16:07.014 - 00:17:44.903, Speaker B: So what's happening with how we perceive and receive information? Because of AI, we are increasingly skeptical about the opaque way in which platforms recommend content. This has been the case in the past couple of years with a lot of platforms where communities, especially some fringe communities, have started feeling like their communities are being, for example, shadow banned, or, or given some sort of targeted treatment in social media, which has led to mass platform migrations, which means a lot of groups of people have moved to more fringe platforms, which again influences the way we acquire information. We also think that repeated exposure to synthetic content, for example, to these kind of images that we believe to be real, and then we find out that they're not, can slowly erode how the public trusts information. And as it becomes harder and harder to distinguish between real and fake content, users could become more cynical about the authenticity of all information. Because if enough times we're showing that what we actually saw, what we listened to, some videos we saw, and we believed was actually AI generated, we can in time become more cynical about information. And that's something that can be very easily weaponized, for example, within political systems, because we have several cases of, for example, audio of politicians being leaked and then actually emerging that was actually AI generated. And this kind of patterns do break trust in how people receive information.
00:17:44.903 - 00:18:51.364, Speaker B: So some of the risks of any false and epistemic resilience are that the formation of ecochambers, for example, can increase polarization, extremism. And all of these reduces how open we are to information that contradicts what we believe. So we just get more closed to being contradicted and to being changed our minds, the growing distrust in information. So when we get very skeptical about information that we see can provide fertile ground for political disinformation and can be easily weaponized. I think a very interesting case here is that last year there was an audio of London Mayor Siddiq Khan that was circulated and then proved to be a deepfake. And at the time, some government officials in the UK asked, what if we are not able to disprove this kind of deepfakes? What can we actually do? And at the moment, we don't have an answer to that, because we can't really disprove or prove that something is AI generated reliably. And this, in time, can very easily be weaponized, especially in politics.
00:18:51.364 - 00:19:39.680, Speaker B: And ultimately, this compounds and reduces the public's resilience to the risks of the two other channels that we discussed, such as information generation and diffusion. So all of this is to describe the first three blocks. Let me see how we're doing with time. Okay? Okay. So how does AI impact political systems? Again, this is a synthetic media that we found on Twitter of Alexandria Ocasio Cortez kissing Elon Musk, that started circulating, I think, after a Twitter exchange between the two. And the point here is that three AI malicious actors can very quickly saturate information ecosystems with content that is false or misleading once this information is circulated at scale. So once these kind of images start going viral, it is very difficult to correct.
00:19:39.680 - 00:20:22.172, Speaker B: So we have a lot of literature from psychology that basically shows that once we start believing something false, it is extremely hard to refute. It actually convince us of the opposite, which again, makes us resilient, less resilient to this information. And all of this impacts belief formation. So here, the obvious channel is that what we see, what information we look at, actually impacts what we believe. And that's a very obvious element of how information works. But that's something to keep in mind, because the content that we consume actually very heavily influences our beliefs, especially on news and social media. So I'll go through this very quickly, because this is a bit of a more boring part.
00:20:22.172 - 00:21:07.414, Speaker B: But there are several risks for political systems that can be identified at the moment. First one is election interference. So AI can be used to manipulate public opinion, for example, by macro targeting, delivering tailored disinformation to specific voter groups. So this is something that is already happening. It has happened a lot, and it's something to be particularly mindful of. Close to several elections happening this year, AI is compromising emergency identification and responses, because, as I mentioned before, AI can be used to produce reactive disinformation during emergencies. And we're starting to study this a lot, because this is a big worry for many countries that imagine we have like a nuclear emergency tomorrow.
00:21:07.414 - 00:22:15.794, Speaker B: What if we have a malicious actor actually trying to sway the public not to run away from the area? Or like all of these kind of situations that we actually risked many times, for example, in the conflict between Russia and Ukraine. This is something that has been considered a lot, and how AI can actually make these situations a lot more complicated. Then we have the case of polarization and creating artificial division, because AI algorithms amplify content that reinforces existing beliefs in most cases, which can create eco chambers, which is a concept that is still really debated in the literature. And finally, that what I mentioned before about a very broad erosion of trust, which means that people in time can get more and more skeptical about their trust in information. But is it all bad? And the answer is really no. A recent study that I did with colleagues at Cambridge showed that only a minority of AI generated content online is political. We found that not a very large dataset, but a pretty representative one.
00:22:15.794 - 00:23:12.464, Speaker B: We found that only 13% of AI generated media, we found was political. All the rest was actually satirical and fun. So the majority of content is still that kind of thing, that kind of like fluffy animals and hidden messages. And that's what people tend to do with most things on the Internet. But this may change in the future, and that's what we are really worried about, as we keep seeing more and more examples of dangerous AI generated disinformation going viral without any safeguards. This is often used for political purposes, when it is in an adversarial manner between hostile countries, but also for internal propaganda. And here I thought I'd show a very strange set of images that were circulated in Italy in the past few days, actually, like last week from Italy's Lega party, that all of a sudden started using generative AI to create this very strange graphics.
00:23:12.464 - 00:24:01.698, Speaker B: Basically, the first one is pregnant men, to complain about Europe, saying that that's what Europe leads to, and that we have a group of muslim people, all with the same face burning Dantes divina commedia, and then we have a person eating a bug, an insect, basically to show what the EU imposes on us. And this, we're like, funny to most, probably not that scary, but it's sort of showing what's happening. We don't have a lot of control in this. We don't have a lot of rules. These are not even marked as AI generated, even though they clearly are. So we're facing a situation where it is a very important time to get this right. And also, finally, in the next two minutes, AI can also help fight this information.
00:24:01.698 - 00:24:49.596, Speaker B: So it's not really all bad. We have a lot of cases of AI being used to detect false information, such as automated fact checking, which is very imperfect, but definitely getting better over time. AI can help content verification. For example, if you use any type of system online to check whether something is AI generated, that will be a machine learning model. And AI can also be used, for example, through positive recommendation algorithms, to prevent the spread of false content. There's a lot of research on this, for example, on bridging recommender systems, which are systems that prevent conflict and actually encourage positive interactions and to improve public awareness, for example, with public facing chatbots. I'll skip solutions since we don't have time, but, yeah.
00:24:49.596 - 00:25:28.042, Speaker B: So some final considerations. AI is a very powerful tool which is being used increasingly in political settings, which, again, support in some cases, but also undermine democracy if not done right. Awareness and proactive measures are essential to mitigate the risks of this information. And very importantly, this is a really crucial moment in time to make sure we get this right, because as shown by the strange vignettes before, we still don't have a lot of rules around it. But this is the moment where we are actually setting the rules. So it is very important that we manage to get this right. And that's it.
00:25:28.042 - 00:25:32.534, Speaker B: Thanks. And if we have a few more minutes, I'll be happy to take any questions.
00:25:37.034 - 00:25:46.064, Speaker A: Thank you, Giulio, for your talk. Unfortunately, we are out of time, so we won't have any questions. Yeah. Catch Julio around the cornerstone.
