00:00:07.400 - 00:00:33.146, Speaker A: Okay, and we're back. The next speaker does really not need an introduction, but for the sake of completeness, I will introduce him anyways. It's Gavin Wood. He is the co founder of Ethereum and the co founder of Polkadot, and today he's here to talk about the gem chain. Gem stands for join accumulate machine. Yes, thank you all. And that was recently proposed and published in the gray paper.
00:00:33.146 - 00:00:38.574, Speaker A: And I'm sure he can explain this all much better than I can. So please welcome him on stage.
00:00:44.034 - 00:01:24.402, Speaker B: Thank you. Some pretty cool visuals there. Always nice to see the Ethereum logo being altered in these various ways. Lots of imagination, right, jam? Yeah. Join accumulate machine. It's a protocol that I've been kind of working on for, I don't know, I guess, nine months or so. Now I put forward the, you might have heard of it.
00:01:24.402 - 00:02:43.804, Speaker B: I put forward a gray paper which basically details the protocol, formal definition, maybe not quite the final version, but final enough for putting a paper out. And I want to give you a bit of an overview of what it is and maybe draw some parallels to stuff that you are more familiar with in the ethereum ecosystem. So first, the driving factors. What was the point of the work? I mean, it's the same old stuff, really. That is ten years now since I've been working on this stuff. And I guess over the years you kind of think a bit more, you kind of ask yourself these questions, what is it that I'm working towards? What's the point of all of this? And, you know, every now and again I sort of put a paper out and I try and sort of get a bit further down in the weeds as a bit more sort of clear as to what it is that we're trying to deliver. And in this particular sort of my latest iteration of this are these five points.
00:02:43.804 - 00:03:27.590, Speaker B: Right. And I think the five points are kind of interesting because if we look at the progression of web3 protocols, we do see some of these things appearing and kind of in this order. I'll leave accessibility aside for now, but resilience, this, I think, is the key property that web3 systems need to have. If they're not resilient, they're not web3. It's kind of the point of this. And bitcoin was like a truly resilient system, and it was also coherent. Right.
00:03:27.590 - 00:04:09.810, Speaker B: Coherency is a crucial one. And I think with Ethereum we took that resilience and we added generality to it. You can sort of see that as being sort of a fairly obvious narrative that you can attribute performance sort of came later. And, you know, I'm kind of Mister Polkadot. So I'm going to say this. You know, performance is something that I believe Polkadot brought to the table beyond resilience and generality. But the problem, or one of the identified problems with Polkadot that I would say is coherency.
00:04:09.810 - 00:05:01.664, Speaker B: And this is going to be a sort of theme of the talk. By introducing performance, coherency suffered somewhat. And specifically, what I'm talking about is sharding, and sharding sharding through a persistent fragmentation of state. And having a persistent fragmentation of state only leads to very, very, very eventual consistency, coherency. I mean, that's not quite right. Consistency is fine, but coherency, which is basically the ability for one state somewhere in the system to have an effect on state somewhere else in the system. The reality is, if it's in the same shard, that effect could be synchronous.
00:05:01.664 - 00:06:26.844, Speaker B: It can be immediate, but if it's in a different shard, it's a lot more painful, a lot slower, and there's a lot more contention between how many things can, in terms of cross state interactions can happen. And this leads to a principle that I'm sure exists. And it's not a principle specific to blockchain. It's a principle that you can find all over the place throughout all various sort of disciplines, computer science, biology, physics, you name it. You can generally find this principle size, synchrony, antagonism, and basically it means as the system grows, it's harder for the interactions between the elements of its state to happen in a synchronous or coherent. Basically, things tend to split up into their own little segments. And it's the job of protocol designers and system designers to try to not let this, as systems grow, which we assume, we generally want to not let this result in a very asynchronous or incoherent eventual situation.
00:06:26.844 - 00:07:37.534, Speaker B: So jam is generally coherent with transient decoherence. I would argue that if you want something to be decentralized, resilient, unless you have some extremely clever new stuff to bring to the table, probably you're going to have some degree of coherence reduction. It's not going to be, you know, a classic, let's say, ethereum l one model where you've got like a play pen that everything just has synchronous access to everything else all the time. You're probably going to have costs dependent on which particular state element that you are and which one you want to interact with. However, what we, what we really want to do is reduce this decoherence to limit it. Jam does this by essentially splitting computation into two parts. In core computation, which you can imagine is like computation that happens in the roll up, so to speak.
00:07:37.534 - 00:08:22.384, Speaker B: This has access to a decentralized data lake, but it's otherwise fairly stateless. And then secondly, on chain computation, which is statefull, has access to the on chain database. And the basis of the coherency is this like refine, accumulate, split. Yeah. Okay, so we have, we're able to, by splitting up the state and. Well, no, by splitting up our computation, we're able to have these separate computation elements we call these cores. You can think of them as being sort of separate roll ups.
00:08:22.384 - 00:09:21.244, Speaker B: And we allow a decentralized data transfer, or direct data transfer between these cores. And we do that through placing the data into the decentralized data lake and then bringing it out again, we do this like part of the protocol. This isn't something that l two s that are building on jam need to be particularly concerned about. They can just use cryptographic commitments, basically references, and pass those around, rather than having to actually drag the data out of the lake, repackage it up, and put it back in. So you can think of jam as a roll up host and a roll up reactor. So it both hosts the roll ups, and it allows their, their outputs to be sort of reacted together into a single coherent state. And you can see in this regard, rollups as a sort of preprocessor, but preprocessor with a common access to this decentralized data lake.
00:09:21.244 - 00:10:09.224, Speaker B: The data lake isn't quite, isn't perfectly synchronized. So it's not like you can put data in the data lake at time t and then have it be available to everything at time t plus one. But there are ways of ensuring that if it goes into the data lake at time t, you can have access to it at time t plus one if you're careful, and will generally have access to it at time t plus two or t plus three, otherwise. And we're talking about blocks here. So that's all factored by 6 seconds. We have an algorithm called elves, which is basically our roll up algorithm. You can think of it as a bounded security optimistic roll up.
00:10:09.224 - 00:11:02.784, Speaker B: And we have consensus algorithms, kind of similar to Ethereum. It's a hybrid thing. We have one for the block production and one for finalization, if you want, like a picture. Here is my, a very roughly hand drawn picture of like, what the jam join, accumulate machine looks like. So we have this idea of this outer ring that has transient decoherence basically means every within a six second window, these individual computation elements don't know what is going on with the other computational elements. So there's no synchronous interaction between computational elements. However, that ends after that six second, everything gets folded into the reactor core, which is in the middle, that's fully coherent, has this database that is.
00:11:02.784 - 00:11:42.834, Speaker B: That has pretty decent access characteristics, and then the whole thing starts again in the next 6 seconds. And this transient decoherence has access to this very, very large data lake. 1.6 petabytes data lake. So the jam data topology, it's not a circle. It's not like this sort of singular sand pit. And it's not like Polkadot, either, where it's got lots of singular sandpits all sort of connected together.
00:11:42.834 - 00:12:22.114, Speaker B: In fact, it looks more like a flower. So it's got these kind of petals, and they each share a bit in the middle, where they can do synchronous consolidation, like fully coherent consolidation. But within the bigger part of the petal, which is where most of the data access and computation happens, they operate independently. This is how we scale out, right? You've got to scale out some way. We're not scaling up. We're not just, like, making the validators faster. We want to actually parallelize the workload across the network.
00:12:22.114 - 00:13:21.234, Speaker B: If you want to sort of zoom in a little bit, this is like, kind of what a core looks like, or one of the petals. Some of these numbers are a bit out of date, but it gives you a sort of rough idea. Basically, the cores, these, the big part of the petal, this is where all of the real heavy lifting happens, and it can pull in about two megabytes, a second of data and also push out about two megabytes, a second of data to the data lake. Security wise, they're equivalent. That's why we have our security system. And then in the middle, the smaller part of this, the blob, that's the thing that gets shared with all of the other petals, all the other small bits of the petal, and that's where this singular sort of database exists. You can think of that as more like the on chain part.
00:13:21.234 - 00:14:17.564, Speaker B: Oh, dear. This is the old PDF. I gave a second PDF. Do you think you can find that Obi I air dropped it to you? Yeah. All right, good. There are a few extra slides, and unfortunately, things are in a different order. I thought the order was a bit weird, but, yeah.
00:14:17.564 - 00:15:29.002, Speaker B: So the jam protocol, that's more or less an overview of what it is. I think a more important thing I wanted to sort of talk about is maybe not more important, but somewhat similar important thing I want to talk about was the approach that we're using. For me, the approach is a bit of a return to nostalgia ten years ago. One of the things that I think was really fun doing ten years ago with Vitalik and Jeff was this co development of implementations of the same protocol, something that I generally call a protocol first development strategy, basically where you put forward what something should do and don't worry too much about how it does it and let. Aha. No, no, this is not it. Second PDF, guys.
00:15:29.002 - 00:16:28.564, Speaker B: Second PDF. And you put forward a. Yeah, you put forward what it is that the thing should do, but you disregard how it is actually going to do it, and you let people figure that out. Now, it's crucial to be very disciplined when you're doing this. You need to not fall into the trap of saying, well, you know, this development team is very good, and we're just going to develop the thing, and then hopefully other people will also develop something and it will be kind of compatible. This is sort of what we did with to some degree. This is what happened with Polkadot, where the software came first, the implementation came first, the protocol was specified following that, and then implementations sort of came following that.
00:16:28.564 - 00:17:20.125, Speaker B: And in reality, implementations never caught up with jam. The approach is very different. That's partly borne out by the fact that gray paper exists before there was an implementation, and partly through a number of additional things that we are. Okay, alrighty. Additional things that we're doing to help encourage this. So let's have a look. All right, so we have this idea of like the gray paper protocol specification.
00:17:20.125 - 00:18:51.674, Speaker B: Secondly, the jam prize, which is basically a fund that we just hand out very clear rules as to how to get it based on the implementation of the grey paper and the jam toaster, which is basically a common shared piece of hardware on which each of the jam implementations can kind of interact and sort of test themselves and trial themselves and analyze their behavior. So, yeah, the gray paper, it was kind of fun to get back into writing a formal blockchain spec. I haven't really done it. I never did it for Polkadot. It was really an effort to make it readable for people who are very familiar with the yellow paper and its formalisms and its notation. And I view jam as being kind of equidistant between Polkadot and Ethereum in that regard. It's many of the elements, many of the opinionated elements of Polkadot have been removed and have really tried to pare back the protocol into that which is really just which is necessary to be at the base layer.
00:18:51.674 - 00:20:15.644, Speaker B: The prize is like a bunch of. A bunch of dot KSM, and it's like separated into milestones, basically. Again, just sort of easy on ramping. And there's one of the things that's like additional to the actual crypto incentive, you know, one of the things that Polkadot has really been pushing, I think that's kind of nice to see happening, is the fellowship. So for those of you who are not familiar, Polkadot basically has a dao that exists within its sort of system. And one of the key elements of its dao is kind of what we've called the fellowship, which is a little bit like a company, maybe in the old sense of the word, a company at arms. And this fellowship can provide on chain salaries in stablecoin, it's funded by the central treasury, and it's very much without any kind of help from centralized real meat space organizations.
00:20:15.644 - 00:21:16.104, Speaker B: It exists as a purely on chain concept, and it's actually like employing people, not in the official legal sense of the word, of course, but it's giving people regular monthly USDT salaries. And that's something that I'm hoping to leverage for, you know, for jam implementers. There's a few other bits in there, but a lot of this stuff is maybe not the professional audits, but the international seminars. This is sort of from the Ethereum world that like, you know, when eth two was just starting to be implemented, I remember we went to, I actually went to one in Taiwan. It's quite good fun. I think it's a good idea. And even this, like fellowship and salaries and taoification of the protocol is something that, you know, we were discussing on the basis of Ethereum foundation ultimately becoming at Dao back in 2014.
00:21:16.104 - 00:22:37.740, Speaker B: So none of this is especially new to the Ethereum community or mindset, but I think it's something, stuff that's important and stuff that we should be pushing forward. Lots of different languages, not going to go too deeply into this, but basically another thing that we want to do, like decentralization takes a lot of different forms, right? There's protocol decentralization, trying to make sure there's many different validators who are helping maintain the state of the system, pushing it forward block by block and so forth. But we also have to look at like human decentralization. If you've only got a single team behind something or only a single language, that's really ever used in a protocol, then it doesn't help its resilience. And so one of the things that actually really want to push is to have many different languages bringing in many different paradigms of implementation into the jam protocol. And yeah, like, you know, you got crazy code at the bottom. I don't think anyone's going to implement a block importer in whitespace, but I put it there.
00:22:37.740 - 00:23:40.614, Speaker B: Maybe there's some crazy people that think it'd be fun to do Jamtoaster, basically just a cluster that can host a full scale jam network, 1023 nodes. So yeah, this is something that we're building out and hoping to get some real testing to make sure that the numbers that we are aiming for are indeed realistic. So unfortunately, this bit of the talk I've already done, I was meant to be the other way around. But let me see if there's any bits in here that I can talk about a bit. Okay, yeah, so in jam, just to give some maybe clearer idea as to what it's actually providing. So you're familiar with smart contracts, which are basically objects in an object oriented programming language, object environment perspective. In jam, these things are called services.
00:23:40.614 - 00:24:19.188, Speaker B: I could have said smart contracts, but I think it would have given the wrong idea because services are a bit lopsided. Smart contracts are generally very coherent within their immediate environment. Services are not. A service like a smart contract is permissionless. You can create a service without any real, any permission. It's pretty cheap, but their code has three entry points, unlike a smart contract code, which has just one. And then separately you kind of use a decoder to try and simulate multiple entry points.
00:24:19.188 - 00:24:58.744, Speaker B: But in reality, a smart contract only actually has one. What it does when it gets a transaction or a message call services fundamentally have three refine, accumulate, and on transfer. So refine is basically the big part of the blob, and that does the roll up, and it's stateless other than access to the decentralized data lake. And there's actually a static lookup system as well. And then accumulate you can think of as the small bit of the blob. And that's actually to integrate the output into this shared super coherent state. And it actually has an extra bit which is on transfer, split apart the accumulate and on transfer.
00:24:58.744 - 00:25:50.034, Speaker B: For regular ethereum style smart contracts, accumulate and on transfer would actually be the same thing. Indeed, refine, accumulate, and on transfer would all be the same thing. It is permissionless. So the jam chain hosts all of the stuff, just like an EVM style chain would would host code data state. There are deposit requirements, so there are no direct limits other than economic limits. But you actually do have to, if you're uploading code and data, then you have to actually have funds in there basically to prevent indefinite staterooms of. It's transactionless.
00:25:50.034 - 00:26:37.084, Speaker B: So jam does not solve stuff like sequencing or anything like this. It's really just there in the very basic sense of being this kind of very, very parallelized quote unquote decentralized supercomputer. There are extrinsic information fed onto the chain, obviously, but they're not user transactions. They're actually only validator generated data. And it's just to allow the validators to have a central coordination point. I'm not going to go too deeply into the refine accumulate on transfer model, but there it is. The other thing I'm going to do, a quickly mention, is the PVM.
00:26:37.084 - 00:27:21.544, Speaker B: So this is a with Ethereum, obviously. There was the EVM with Polkadot. We used webassembly as our, as our sort of standard language for expressing what computation needed to be done in jam. We're moving over to the PVM. PVM is a RISC V derivative. It's very much based on RISC V on the 32 bit variant, but it's a few things have been moved around. They're all very easy things for you to take Risc V code and turn it into pvM code so we can reuse all of the Risc V tooling.
00:27:21.544 - 00:28:20.750, Speaker B: It's a very simple static translation. Doesn't take long, one to one, basically. But it's important to allow us to do what we want to do with it. If you're not familiar with the RISC V ISA, it's little endian 32 bit registers, 32 bit addressable memory, 4k pages, although you can actually alter this, basically it's pretty similar to x 86. And it turns out that if you use the regular version of the RiSC V, ISA has 32 registers, but there is an embedded version that uses that has only 16 registers. Now, unfortunately X 86 only has 13 usable registers, but it turns out that one of the 16 registers is the zero register, so it's always zero. So it doesn't really matter, we don't need to do that one.
00:28:20.750 - 00:29:13.580, Speaker B: We can do that in other ways. And then two of the registers, operating system reserved registers. So the compilers never actually use those two registers, which means they only actually use 13 registers, which are the same number of registers that we have on x 86. What this boils down to is that RISC five, or at least this specific version of RISC five, can be very easily translated into x 86 native code. And indeed it can be done so in a streaming compiler, that is order n in the length of the program. Now we've got a streaming, indeed a streaming recompiler, and we found that we can get this recompilation down into hundreds of nanoseconds. Hundreds of nanoseconds.
00:29:13.580 - 00:30:26.584, Speaker B: It's definitely in nanoseconds per instruction anyway. I think it ends up being a few tens of milliseconds, microseconds for sensibly sized programs. And a very interesting additional thing is that the metering can be done much closer to the time spent on the underlying that the underlying hardware actually needs to compute the program. And the way that we do metering, we sort of do all the regular stuff, chunking instructions together into sensible blocks, and we actually use the floating point unit to do all of the arithmetic for the metering. So we end up with a synchronous metering that costs somewhere between five and 10%, or two and a half to five percentage points, which is pretty decent. So we're ending up with native performance that is of the order of magnitude of about half, sorry, end up with performance that's about half of native. And it doesn't go down too much when metering is enabled, which allows us to do all sorts of really interesting stuff.
00:30:26.584 - 00:31:16.504, Speaker B: Basically, we can push an awful lot of computations through and still have consensus sensitive computation results. So the PVM is a pretty major innovation that jam sort of uses to really beef up the amount of computation it can do beyond this 341 x through core parallelism. The authorization model is a little bit, it doesn't use gas. I mean, it doesn't use a gas price, it doesn't use like per transaction purchases. There are no transactions, so it can't. But it also doesn't use this long term core rental stuff that Polkadot does. Currently that's being phased out, but that's what it launched with.
00:31:16.504 - 00:32:29.844, Speaker B: In fact, it uses what's called agile core time, which is somewhere between the two. It kind of supports both models. Essentially, it has 341 different cores. Each one of these have an index, and cores are sold on a month by month basis in an auction. But then there is a secondary market that allows this month long access to a core to be carved up into bits, to be interleaved, straddled. And then once you've got like a bit of the core that you want to sell, you can then trade them around like nfts and once you want to use a piece of this core time, you can assign it to an authorizer, which is basically just a function that sits on chain and returns either true or false, depending on what work you want the core to do. And you can write that function so that it will recognize the work that you want it to do, return true for that, and then the guarantor, basically the network knows that that's a piece of work that it can do on that core.
00:32:29.844 - 00:33:22.418, Speaker B: So basically you've got, for every core, you've got a function that tells it what works it's allowed to do at any particular time. Now this is a little bit of a simplification, because we have other ways of making sure that we don't need quite such a high degree of synchronicity, but this is the basic way. And then finally, rough limits. We do require some pretty beefy validator nodes, but they should be within regular validator operators. Reaches less than a gigabit up and down on the network. 16 core, decent spec machine, 64 gigs ram and some decent hard disk. And roughly speaking, what you get for that is some basically like a supercomputer.
00:33:22.418 - 00:34:02.268, Speaker B: It's not really comparable with modern supercomputers, but it's a lot more than a workstation anyway, about 150 x of native cpu speed. So if you want some sort of arithmetic operations per second, that's about 600 giga ops per second. A decentralized data lake of about 1.61.7 petabytes in total data. And that can be accessed at about 600 or 700 megabytes a second by the overall jam code that is running on JAM. And it has a synchronous database, sits in the middle. This is what you might call the state, the state access.
00:34:02.268 - 00:34:40.748, Speaker B: And this should be around 60 megabytes per second in terms of total access into that core. And about 1 million operations like read write accesses also per second for the whole of Jam. It's agnostic to any of this stuff. It's actually agnostic to transactions and transaction sequencing as well. Didn't put that in there. But yeah, staking is left as a higher order thing to be done on top in whatever Jam is actually computing economics. It's like, whatever.
00:34:40.748 - 00:34:50.124, Speaker B: That's an opinionated thing. Jam doesn't have any opinion. Governance same. Good. That's the talk. Thank you.
00:34:53.744 - 00:35:14.154, Speaker A: Thank you for this very comprehensive introduction to jam. Please stick around because Gavin will also be joining us on stage at 01:10 again for a panel with Vitalik. So if you have any questions. Sure you will find him around. And I think we are doing a break so we will be back shortly.
