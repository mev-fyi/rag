00:00:00.920 - 00:00:26.194, Speaker A: My name is Max. I have been in crypto for a while. I don't know, some people here maybe know me, some people don't. Working on decentralized marketplaces for trials specifically for other things. Hopefully soon. And I have a clicker here. And first of all, first and foremost, I would like to say that no AI tools whatsoever were used in the creation of this presentation.
00:00:26.194 - 00:00:54.954, Speaker A: Enjoy. 100% human made content. How cool is that? Another thing is that I don't want any of this content to be used or analyzed by any of the AI tools. Of course, you can take pictures and upload it, but I will sue. And today I'm going to talk about progress. You were taking picture. Go ahead.
00:00:54.954 - 00:01:52.024, Speaker A: But you remember what I said, right? Inevitably, about inevitability of progress. I always. This phrase comes to mind from a book by Kurt Vonnegut, one of the greatest, I guess, science fiction writers that I know of. So it's a little bit of irony, a little bit of mockery, but just a little joke for the beginning. Anyway, so today, what today talk is going to be about, it is about progress. It's about, and of course, all of you here, you have aspirations to change the world for the better, hopefully change the future, somehow help other people. Right? You know, at the end of the day, improve the human condition.
00:01:52.024 - 00:02:53.654, Speaker A: And that's what we call progress. But let's talk about progress. And today it's a little bit of a scatterbrain talk. It's, you know, I had so many ideas, and I had to put them all into this presentation. And so I'm just gonna give you some bullets with this presentation, some thoughts that you could use later on in your own work or life. And so let's start with this very jovial and hopeful idea that humans actually cause extinction. And if you read Harari's sapiens, he talks about it for a few pages, that indeed, when human, homo sapiens moved into a new territory where they weren't before, inevitably, inevitably, mega fauna, that is, very large animals, were extinct in that territory very, very quickly.
00:02:53.654 - 00:03:35.860, Speaker A: So just a nice thought for you, again, just a bunch of thoughts. Use them later, as you wish, right? And then in the same book, Yuval Hari says humanity is now a predominant religion on earth. And he uses the word religion very broadly, very not very concretely, not in the sense that we use the word religion, I guess, in everyday life. He says, what is religion? Well, you know, it's some sort of fiction. It's a story. And he calls, like, volkswagen is a religion, right? Because it's not real. We sort of believe in that thing, right? And all sorts of other things he calls religions and humanity.
00:03:35.860 - 00:04:20.354, Speaker A: That is the idea that we have to do everything that's in our power and forget about everything else, but to again, improve the human condition, that is to move the progress forward. And the clicker goes, click, or it doesn't. There you go. There's another interesting idea that Yuval mentions in the book as well. Sorry, it's not going to be about just one book. There's a bunch of other things but agriculture. And he quotes Gerard diamond here, the guy who wrote this book.
00:04:20.354 - 00:04:53.682, Speaker A: Some of you maybe read it, it's called guns, germs and steel. He says that agriculture was actually probably the biggest blunder of humanity. Why? Well, because let's say 10,000 years ago or so, human gatherers, we are living in the forest and they were just running around. That is a lot of physical exercise. They're always outside living a nice life. And then they, for some reason decided to domesticate a certain plant, and now they were stuck to a certain area. That is, if they leave that area, well, they're gonna die.
00:04:53.682 - 00:05:18.334, Speaker A: And that's why the war started. That's why all sorts of ailments of the human body started. Because actually our bodies are not designed to pick vegetables or whatever all the time like that. No, we're designed to run. So again, feel free to take pictures. Don't feed it into an AI, but research it a little bit further. So another idea.
00:05:18.334 - 00:06:34.546, Speaker A: So that's the idea of the evolutionary mismatch, which means that this concept describes the idea that our environment, we change our environment so fast that is actually detrimental to our well being, right? That is, we create a new technology and our bodies are not evolving as fast to adapt to that change of the environment that we ourselves cause. Right? So that's the idea. That's called evolutionary mismatch, which again, in humans it causes. Now we live sedentary lives, again, our bodies are designed to run, do all sorts of crazy things, but now we sit in front of a computer all day and it leads to, I don't know, obesity, diabetes, on the physical front and on the mental front, of course, to anxiety, all sorts of other psychological ailments. So that's that. So another idea, that's his idea. That's, I think, the most original idea that he had is that there is no metric for progress.
00:06:34.546 - 00:07:05.454, Speaker A: He's saying that we have no number that would say that today we live better lives than 10,000 years ago. There's simply, it doesn't mean that we don't. Maybe we do, but we don't know that for sure. And maybe, just maybe 10,000 years ago or maybe more, we lived better lives. Certainly, certainly there were things that were absolutely horrible. Child mortality and people spearing each other and all sorts of. Of conflicts and stuff like that.
00:07:05.454 - 00:08:23.422, Speaker A: But just maybe we lived lives that were at least as good on a different level, of course, and we will never know when. Anyway, another idea that everyone says that, that sort of our Faustian, as Oswald Spengler calls men, that's what we think, right? Hey, in 2050, surely, surely we're going to work at, you know, 2 hours a day max, and we're going to, I don't know, whatever, smoke weed and the rest of the time, or whatever they do things that are enjoyable to us. And I would argue we say that all the time and somehow that doesn't happen. You know, 100 years ago, 200 years ago, people and philosophers were saying, hey, due to technological progress, definitely, definitely we're going to work maximum a few hours a week, and then the rest of the day we're just going to have fun. Didn't happen. Progress, right? I love this one because it leads to some other ideas. Of course, some of you are familiar with the idea of paperclip Maximizer.
00:08:23.422 - 00:09:16.512, Speaker A: There is a swedish philosopher, his name is Nick Bostrom. He came up with this in 2003. And he says, this is a thought experiment in the field of AI, or general artificial intelligence, where he says, hey, if we program an artificial general intelligence incorrectly, if we give it the wrong program, for example, we take an AGI and say, hey, make paperclips. Because, hey, that's what our company does, right? So that AGI will convert the whole earth, all the resources, the solar system, the galaxy. That would be its goal, right? Or you can give it another task, let's say, hey, make me the richest person on earth. And guess what? Next day, all the other people are dead because it killed them. And now, hey, you're the richest person on earth.
00:09:16.512 - 00:10:43.712, Speaker A: So that's the problem with paperclip Maximizer, right? Just one bootloader theory. Elon Musk, he didn't call it that, actually. He said that humanity is a type of biological bootloader, right? Which means, what is bootloader? It's a tiny, tiny little program that sits in your computer. Is there all the time that your computer requires that program to be there in order to load the more complex, more complicated operating system, like Mac or Windows or Linux or whatever. And so humanity is a type of biological bootloader, he says, for the creation of AI, for actually right now, maybe that's the goal of the human race, to be that bootloader, to bring AI into being. Maybe, I don't know, just a theory. And so there is a cool story, how Elon and Grimes, his, I guess, wife or whatever they were discussing on Twitter, this idea of Roku's basilisk, which Roku is some anonymous user of some sort of forum somewhere, and he put forward this idea that, let's say, in year 2050, we create, finally, the general artificial intelligence.
00:10:43.712 - 00:12:13.074, Speaker A: And in retrospect, that AGI will look at everything that you are doing here, and me included, myself included, and punish those who are preventing the creation of that AGI and reward others, people who were actually working on the creation of AGI. So just an idea. Maybe you should start working on AI projects. So, combining some of those previous ideas. Some people say capitalism is a paperclip maximizer, is a type of paperclip maximizer. And so the idea here is we don't know how that artificial general intelligence guys who didn't like the talk artificial intelligence is going to look like, right? What if it's not just about a bunch of servers connected by wires? What if those algorithms, that being that we're creating a God, really, what if, could use you as a resource, right? What if it already is doing that? What if we programmed that AGI with. And again, some other people say corporations are AGI's as well, because they have certain programming, they can make certain decisions and stuff like that.
00:12:13.074 - 00:13:30.052, Speaker A: They have freedom of speech, if you didn't know. And so, yeah, a lot of people, Charlie Strauss dad junk, he looked them up, they wrote articles and they had speeches about this thing that capitalism is a type of. AGI is a type of, in fact, paperclip maximizer that we programmed to take everything on this planet, natural resources, human resources as well, and convert everything into not a paperclip, of course, but a number in a database that is money. So corporation is an AGI. Similar idea. And so reading about all of this stuff about AI, just educating myself a little bit, my free time, I saw this paradox that a lot of these philosophers, a lot of these people who work on AI, they talk about how, hey, we're going to create, surely in the next, I don't know, 1020 years, general artificial intelligence, and it's going to be super smart, it's going to be super powerful, it's going to have access to vast amount of resources. It's going to be able to educate and reprogram itself.
00:13:30.052 - 00:14:31.548, Speaker A: So basically we are creating a God. And guess what? We're going to program it to work for us. It just doesn't make any sense to me. It doesn't compute to me that you're going to create that and somehow that super powerful being is going to work on your stupid whatever you want there. To me, I call those concepts God in a bottle, right? You're creating a God and you put it into a bottle and make it work for you. So kind of like a genie, right? And then this, I call this second concept an ant hill, right? So you walk in a forest and there is an anthill, there is a bunch of ants, you know, running around and stuff like that. And so for us, or for that super powerful AI, humans are going to be like ants, maybe even lower, maybe like bacteria, right? And if I walk in a forest, I don't really care about these ants, I will step on them.
00:14:31.548 - 00:15:36.090, Speaker A: I will not go out of my way to sort of like, okay, I'm not going to harm any of these little beings, but if they are in my way, if I want to build a house in there, guess what? Right? And so that's how it's going to be. So I don't think we're going to be able, when the genie is out of the bottle, we'll not be able to control it. And again, maybe the genie is already out of the bottle and it's already consuming the planet and you included your resources. So anyway, and by the way, the programming that we put into this capitalism as a paperclip maximizer, it's called Friedman doctrine, right? So there was this guy, Milton Friedman, Nobel Prize winner for economics, who in fifties, sixties, around that time, he said that the only social responsibility of a business is to make money. Forget about everything else. It's not that money is number one. And then there's other things.
00:15:36.090 - 00:17:01.628, Speaker A: No, it's just money. Natural resources, local environment, forget about it. Forget about it. Consumers forget about everything money, that's the programming of the paperclip maximizers. And so there was this very interesting case in the United states, of course, where else, again, take a picture, look it up, where the Supreme Court has decided that indeed, corporations have pretty much all of the rights as physical persons, you know, and again, in sort of in our legal framework, western legal framework, we call them legal persons, right? But in fact they are super powerful and immortal persons that have all the rights like you do. And guess what? For some of the programming, I think of those corporations and certainly things that we use today, the OpenAI and chat GPT and blah, blah, blah, all the creation of, of fake images and stuff like that. And of course, all the TikToks and YouTube algorithms, they're all AI algorithms right now, but I think all of them are just distracting us from the reality.
00:17:01.628 - 00:18:11.902, Speaker A: Those are not even close to general artificial intelligence, but in fact, they probably use us to get as much data about the outside world as possible. Again, maybe when you film a TikTok video and the algorithm is trying to promote it for you, and then you get sucked into this black hole of content, maybe that whole thing exists simply because that paperclip maximizer wanted to take a picture, a slice of the real space, just to learn about it. Again, just an idea. Another crypto anarchists, they love this book by James C. Scott that's called seeing like a state, where he basically, again, I basically gave you a summary of a bunch of books here. So that's the summary of the book by James C. Scott, which he describes really large scale projects that were supposed to improve human condition.
00:18:11.902 - 00:18:37.020, Speaker A: He talks about the communism, the capital of Brazil that they built in the middle of nowhere. And again, it was supposed to be super cool, and it's going to work. It doesn't. A bunch of other projects really. And he says, hey, they never work. And actually sort of sub idea there. If you want to help a local community somewhere, leave them alone.
00:18:37.020 - 00:19:02.646, Speaker A: They will figure it out. Don't go there. Don't try to change them. And since we started with a quote by Kurt Vonage, I wanted to finish with one which is from the same book. And if you know the name of the book, you can raise your hand and get a point. I'm so disappointed. But this is it, guys.
00:19:02.646 - 00:19:37.574, Speaker A: 7 seconds left. But I have some time for some questions. Thank you, Alex. Thank you. Bring it on, bring it on. Come on. Hi, stranger.
00:19:37.574 - 00:19:57.968, Speaker A: Hello. So what is your conclusion? There is no conclusion. As I said, this is just a bunch of scatterbrain ideas. Bullets that you load into your gun and then, you know, hey, you're in a situation where you have to use one of those ideas. That's what you do. That's it. Thank you for the question.
00:19:57.968 - 00:20:12.884, Speaker A: One more, Alex, here. Okay, there you go. What is your question? Yes, finish your question.
00:20:14.384 - 00:21:02.324, Speaker B: It is, and the problem is that the direction that is given to progress is given by actors that care just about money, mainly. And so I'm curious about what's your point of view about that? I mean, the piece of progress and the technological development is accelerating and we already like faced huge changes in like the way, like psychological and anthropological changes in the way we work, love talk, do things together in the last hundred years and will be crazy what is going on in the next like 51 hundred years. So how can we work on wisdom? I mean, not just on progress and on building things?
00:21:02.864 - 00:21:04.304, Speaker A: I think I understand the question.
00:21:04.464 - 00:21:05.728, Speaker B: So what's your point of view?
00:21:05.856 - 00:21:49.450, Speaker A: Thank you for, see what he did right here? Put a bunch of those ideas. He's like, okay, I'm gonna combine. Combine, exactly right. So, so you can read between the lines here. What do we do? I think some of the motivation here for this scattered brain talk is that I see a lot of people and I spend a fair amount of time in Silicon Valley and I keep meeting those people around the world and I see them and I talk to them and they say, hey, technology is the answer to all of the questions. You have a problem, technology will help you. Hey, we have, I don't know, epidemic of obesity in the United States.
00:21:49.450 - 00:22:42.674, Speaker A: Technology will do it. And I don't think that's the answer anymore. For some time I definitely thought that I was one of those bright eyed people in Silicon Valley thinking, okay, yes, let's do it. I don't know, let's whip out the JavaScript and Python and whatever and fix the world problems. But over time, I guess I changed my position on that. Technology is not the only answer. And I think what we forgetting right now, or sort of not think, not taken to equation is maybe spirituality, maybe, you know, again, maybe if when you are a bees, it's not the technology that will suck out of your fat, I don't know, maybe it's a bad and not quite politically correct example, but maybe you should stop eating less, right? And maybe that's what we should do.
00:22:42.674 - 00:23:25.904, Speaker A: Because one of the things I see is why are we destroying natural habitat of so many species? Why are we driving them to extinction? Because hey, we want to increase our human condition, we want to improve our lifestyle, et cetera, et cetera. And that comes at a cost. We all consume more energy. And maybe, and again, a lot of people are like, hey, technology will help us, let's get more energy. Therefore, let's cut the Amazon forest, put all the solar panels, whatever, right? Hey, maybe just chill out. Maybe you don't need that, I don't know, whatever, flying Tesla in ten years. Like, no, maybe you just sit down and relax and by doing so help the world, you know what I mean? So that's my stance, please.
00:23:25.904 - 00:23:33.824, Speaker A: Yes. You know where to find me. Thank you. Thank you, guys.
