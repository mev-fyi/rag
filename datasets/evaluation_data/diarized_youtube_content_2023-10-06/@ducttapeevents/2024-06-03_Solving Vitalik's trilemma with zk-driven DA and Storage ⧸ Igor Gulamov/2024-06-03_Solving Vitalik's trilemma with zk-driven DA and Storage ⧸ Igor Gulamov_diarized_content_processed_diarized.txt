00:00:10.320 - 00:00:26.154, Speaker A: Thank you guys for coming for the next speech. And we will welcome Igor Gulamov. And he's going to give a talk about solving Vitalik's trilemma with ZK driven da and storage. Welcome.
00:00:33.784 - 00:01:33.084, Speaker B: Hi. So I will tell you about our solution of Vitalik's trilemma with ZKD driving data availability and storage. So what is Vitalik's trilemma? We have three properties. Oh, it looks like technical issues happens. Okay, now it's better. So, okay, what is happening? It's blinking. What does it mean with Electrima? It's not corresponding what I'm showing on the screen.
00:01:33.084 - 00:02:57.010, Speaker B: Okay, okay, so what does mean Vitalik's trilemma? We have three properties of decentralized ledger. It's scalability, security and decentralization. And the trilemma means that we can select only two of three. Why? The solution of Vitalik's trilemma is important because we need for mass adoption scalability, especially for migrating from web two to web3, you know, solutions like Forecaster, which need a lot of storage transactions and so on, and with very low costs. So that's why we need the solution of Vitalik's trademark to make possible another more heavy solutions, not only decentralized Twitter, but decentralized Instagram, Facebook and maybe YouTube. The historical review of this solution is the first. It was multi chain ecosystem proposed by Polkadot and Cosmos.
00:02:57.010 - 00:03:52.842, Speaker B: The idea is that there is core engine on which multiple blockchains could be implemented and these solutions can use this core engine for interchange communications. Next, it was plasma based on game theory. And if something is going wrong, anybody can start the game in which he can win it and get his assets back. Next, it was evolution of plasma. It is optimistic. Roll up the K roll ups and the K validiums. And the most fresh solution is data availability layer.
00:03:52.842 - 00:05:05.524, Speaker B: It's implemented in Ethereum, in prototype sharding and also so there are solutions like Celestia, agent layer and avail who provide data availability. Also the issue of that availability is that the data is stored for short time. It could be two weeks or one month. And these things are originated from plasma when it is enough to absorb the operator during two weeks. And then if you have some assets or something else, you can save the data you needed and then you can make transactions with this data. Why rollups are not enough for solving the Vitaliks trilemma? Because rollups don't scale the data. So here is how rollup is working.
00:05:05.524 - 00:06:28.654, Speaker B: We have ll two with state transition proofs, but also we have the data and we should publish the data with high soundness way because all users need this data to make transactions. Users cannot make transactions for rollup if they have no data. Also that's why we cannot deploy rollups into rollups and so on because the data is floating up. The data is floating from l two to l one. Before dunk sharding it was stored in cold data. Now it is using dunk sharding and it also could be used another data availability solutions. But anyway this is bottleneck and also the current data availability solutions are storing the data just small timeframe and the next the users should store the data by themselves or they should trust any off chain infrastructure third parties who do it.
00:06:28.654 - 00:07:59.102, Speaker B: We need recursive roll ups to solve Vitalik trilemma and we need to solve this issue of floating up the data. Our approach is data centric solutions. The current solutions are filecoin, rwave and estorage. They use data availability sampling for proof that the data is stored and available. And also miners can mine the rewards using the data stored by nodes. To solve the Vitalik's dilemma, we need some improvements. The first is using Ritz Lamon codes for more than ten times better storage utilization because current solutions are using replication and what does it mean? That means that if we need high soundness storage for the data, we need to make a lot of copies of this data and hope that at least one of this copy will be alive.
00:07:59.102 - 00:08:59.314, Speaker B: And then the data is stored and available with Ritzel among codes, we can mix all the data together and make blow up. And then if any source amount of new blown up data is available, then we can reconstruct the source data. It is much more efficient solution than just replication and we can reach more soundness on it. And the next one thing is wall chain probable zhi proof. It's possible. It's shown by Mina blockchain that we can do a lot of state transitions, consensus related things and so on. And we can compress this all into one single proof.
00:08:59.314 - 00:10:16.990, Speaker B: In our research we shown that all data specific things we needed for data centric blockchain functionality. It's possible to prove with ZK snarks in one single proof and it is working like ZK data availability and storage oracle and this is very useful thing if we need to solve Vitalik's dilemma and build recursive roll up system. Speaking about Ritz Lamon codes and its efficiency, here is how it's working. Let we have four numbers, we can encode it as a three degree polynomial like this, then we can make blow up. We find another points. Then, for example, if we forget some part of this data, now we have another points. But any four points are enough for fixed degree three polynomial.
00:10:16.990 - 00:11:14.548, Speaker B: And that's why we can recover the source data. And this is much more efficient than just to take these four points and make a lot of copies of it and hope that each point is present at least one time. Because if the scope of data is large, there will be high probability that at least one point will be lost. That's why read slammon codes are better. Because we can prove that all data is safe. Speaking about Ritz Lamon codes, the important problem is how to prove to all data storage nodes that their shards are correct. And we can do it with polynomial commitments.
00:11:14.548 - 00:11:58.836, Speaker B: So we have polynomial commitment to source data. And we can make this polynomial equation to prove that commitment of the shard for each node is correct. Read the lemon shard. That's why all nodes could be ensured that they receive the right chunk of data. And that's why anybody can collect some of shards, any subset of these shards. We use blow up factor eight. It is 64 to 512.
00:11:58.836 - 00:13:15.944, Speaker B: So if anybody collects 64 shards from 512 shards, they can reconstruct the source data. Here how data centric roll up is working we have data storage network l two, l one. So idea is that we can make zero knowledge proofs that the data is stored and available. We can do it because we can prove with narcs all process related to storage and availability of data, including state transitions and random data availability, samplings and mining of nodes and so on. So we can make zero knowledge proof that the data is stored as available. And we can join it with rollap execution proof. And by this way, we can construct the proof with state transition and data availability inside.
00:13:15.944 - 00:14:24.108, Speaker B: And when we publish this proof on chain, we don't need additional things that like floating up the blocks. Because with this proof, we already know that the data is stored and available and users can use it. So here are some parameters of such roll up. The capacity is more than one petabyte. The soundness is more than 110 bits. That means that if half of network is malicious, then the data could be lost with probability to in order -110 very low number. And the cost if the node is headsner node is fifteen cents per gigabyte per year.
00:14:24.108 - 00:15:31.870, Speaker B: It's including eight time blow up already in specialized rigs for storage. The cost will be lower and recursion is unlocked. Because this roll up provides data for itself. That mean that such roll up could be deployed inside rollup and we don't need to float up any blocks and we can build decentralized infrastructure of rollups. Next, could we reach more scalability? The answer is yes. Because we unlocked recursive roll ups, we can build a network of rollups providing infinite scalable system. Only one overhead here is that we need to manage mining nodes on top layer to prevent concentration of malicious nodes in one single segment of the network.
00:15:31.870 - 00:16:46.854, Speaker B: We have modeled all these situations and we have strict proof that this soundness estimation is valid for asymptotic infinite scalable system with such architecture. So how it could be reached when somebody left from the pool, we should find the replacement node and also we make two random mixings and this is an out to prevent concentration of malicious nodes in one single roll up. And that's why we can build infinite scalable system like this. So here are some links for our research. I think solution of Vitalik's dilemma is important for migration from web two to web3. Not only decentralized Twitter, but decentralized uber decentralized Instagram, decentralized Facebook and so on. And of course it's important for general purpose blockchain specific problems also.
00:16:46.854 - 00:16:48.734, Speaker B: Thank you for your attention.
00:16:51.954 - 00:17:11.464, Speaker A: Thank you. Do we have questions? Silence. No questions. Thank you very much. Thank you for your presentation and thank you for your attention. If you are attending any side events, please go. And tomorrow we start here, 1030.
00:17:11.464 - 00:17:13.704, Speaker A: See you back then. Thank you.
