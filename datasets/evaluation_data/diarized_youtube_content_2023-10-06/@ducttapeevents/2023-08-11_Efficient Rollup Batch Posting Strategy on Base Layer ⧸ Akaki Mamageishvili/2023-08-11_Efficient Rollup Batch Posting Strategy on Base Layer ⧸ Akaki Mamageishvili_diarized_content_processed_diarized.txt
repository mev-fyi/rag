00:00:08.840 - 00:00:51.174, Speaker A: So, hello, everyone. Welcome to my presentation. I want to thank organizers for organizing this excellent event. So this is a joint project together with Edward Felton, who is a co founder and chief scientist at Off Chain Labs. And I joined them last year as a research scientist. So if you are not familiar with off chain labs, this is a company that built arbitrum roll up chain and contributes to its development at the moment. So now what are rollups? This is probably the shortest introduction in the roll ups.
00:00:51.174 - 00:02:01.244, Speaker A: So the purpose of rollups is to scale the base layer. And in our case, it's Ethereum, but in principle, it can be some other layer, one blockchain, and it does by moving the execution phase of the chain. But they inherit security of the base layer by posting the transaction relevant data, which is called call data, to this base layer. And here, in case of optimistic roll ups, as arbitrum is, transaction relevant data is just transactions compressed. But you can imagine some other technologies that can pose some other specification of the data. And then second part, where the roll up technology comes in, is some roll up specific mechanism that checks that execution was done correctly. Okay, so now here we are solving a problem of posting call data at the right time.
00:02:01.244 - 00:03:22.074, Speaker A: And as you saw in the previous presentation, I think there was a slide where Ethereum base fee rises very quickly and very high, but it doesn't last forever. It lasts for few hours, maybe at most. And these periods, the roll ups or sequencer of the roll up, or whoever posts the transaction relevant data, call data, want to avoid posting because the price is high. But on the other hand, they also want to avoid delaying the posting too much. And one instance is that there was a time where Bayes fee raised 100 times and it lasted several hours, and the automatic mechanism of the sequencer didn't post the data. And then arbitrum developers had to manually post the data. Okay, so now how do we model this optimization problem? So, we assume that in round I, so round corresponds to every time, let's say new batch is created, we count how many batches do we have currently queued, and we also look at the current base fee price.
00:03:22.074 - 00:04:16.210, Speaker A: And then we need to make a decision based on these two numbers. Do we want to, so how many batches do we want to post? Okay, so this is our strategy s. That is a function of the price that we see now. And how many batches do we have at the moment? Okay, so intuitively, you can guess that s should be weakly increasing in the Qi. So more batches we have in the queue, more we want to post and it should be also weakly decreasing in price, higher the price, less we want to post. And this just helps the optimization in that it decreases the search space of optimal strategies. Okay, so next is how the cost is modeled.
00:04:16.210 - 00:04:52.156, Speaker A: So first part of the cost is exactly how much we pay if we post. Ni many batches. And batches have the same size approximately. That was the case at least with arbitrum that creates almost equal size batches. Then the first part of the cost is direct cost of posting. And second part of the cost is delay cost. And here we take quadratic function of how many are remaining for the future.
00:04:52.156 - 00:05:38.738, Speaker A: How many batches do we want to post in the future? But you can take some other function. We thought that quadratic is easier to analyze and it has some nice analytical properties and also because we wanted it to be faster than linear function. So there is this normalizing factor c should also capture some preferences. So here I want to make a comment. For example, if the roll up has a technology that delays the finality, then you should not withhold a batch more than this time for sure. Otherwise there is a security issue. And in case of arbitrum, that delay time is quite large.
00:05:38.738 - 00:06:19.504, Speaker A: It's one week. So it's fine to assume some quadratic function. And anyway, the coefficients that we are plugging in, it makes too expensive to delay. So we are never reaching one week, period. Okay, so further justification of delay costs, first is psychological. When the user see that batches are not posted, they think that maybe sequencer is broken and that creates maybe some kind of panic. Another is l, two users, or roll up users, they may have preferences about how fast they want their transactions to be posted on the base layer.
00:06:19.504 - 00:07:29.230, Speaker A: And third part, which is not very relevant at the moment because the sequencer acts in the interest of the roll up, is that pricing of the transaction and compensating the ethereum, compensating the sequencer for posting on the ethereum on the base layer. And further, there is a delay, harder the mechanism to compensate becomes. So that's further justification of the delay cost. Okay, so now what is the optimization problem? In the next round, we are subtracting how many batches we posted, but one new batch arrives. So that's how rounds are actually defined. And we assume that in the next round, the price of the gas fee is a random variable that depends on the current price. And at the moment we assume that this is independent of what rollup does or what is our strategy.
00:07:29.230 - 00:08:25.834, Speaker A: But I would say this is not completely true at the moment, if, for example, there is some huge roll up, it can also move this price up if it posts too many, or down if it doesn't post anything. And in the future, if roll ups become even bigger, then this may not hold. But at the moment, let's say it's a reasonable assumption. What is the objective value? We want to optimize discounted costs, so we sum up costs over the infinite horizon and we discount the future. So one unit of cost we incur today is delta in the next round and delta Square in the next to next. So we care slightly less about the future and more about the present. And this also makes the expression converge.
00:08:25.834 - 00:09:21.056, Speaker A: So initially we take that we don't have any batches, so we start from fresh, and there is some price, the base fee. But the results are independent of this initialization, actually. So now what do we think about the gas fee? So first we assumed that it's uniformly distributed on this interval of seven over eight times the price, and nine over eight the price. And these constants are coming from base fee mechanism, this EIP 1559. So if there is a maximum demand achieved, then the base fee is increasing by 12.5%. This is one over eight. If the demand was zero, then it decreases with the same amount.
00:09:21.056 - 00:10:16.044, Speaker A: And we were assuming, let's say, in the next round, demand will be uniform at random, and that's what happens with the price. And then if, for example, arbitrum posts, or yes, any other roll up posts, every five blocks, then we need to take convolution of five uniform random variables. And this is quite a good approximation for, for normal distribution. But then we looked at the data, and it doesn't look very uniformly at random, uniform at random. And there is a big outlier. We see that in about 16 17% of the cases, full blocks were created. And that is happening because of the priority fees.
00:10:16.044 - 00:11:06.684, Speaker A: And how EIP 1559 works. And there is some small spike, also some small outlier at the zero blocks. And this is also happening because of this EIP 1559. And there is another paper that we later found out, that shows that sometimes if there is a big miner, it prefers and thinks that this miner will propose to also not miner. But now proposer, proposer proposes two blocks in a row. It prefers to keep one block empty, so that base fee goes down and then make next block full, because then it can collect more priority fees. So whatever, this is what data shows, and we have to work with this.
00:11:06.684 - 00:11:51.464, Speaker A: But if you look at 1 minute relative change now, it looks kind of normal, very small spike at 1.8, which means that five times in a row there was a full block. But otherwise it looks quite normal with some, yeah, some bias on the left. But that is also explainable. And yeah, actually I skipped this part here, that the product of this two numbers, t one and t two, is less than one. So this is again coming from the EIP 1559 transaction fee mechanism. But anyway, as we see here, uniform.
00:11:51.464 - 00:12:39.264, Speaker A: So the normal distribution is a good approximation of what is happening to the price when roll up comes next time to post a batch. Okay, this I already talked about. So, related literature is actually inventory policy problem, where so there is some manufacturer produces something. In our case, manufacturer is roll up and produces blocks, and it wants to either sell depending on the price and demand. In our case, these are cost of posting. So that's the first difference. But in principle they are dual problems.
00:12:39.264 - 00:13:20.824, Speaker A: And second difference is that manufacturer has linear costs, or at least that's what we found is in the literature, and that is associated with maintenance costs. But in our case, we think that it's not linear because of the nature, it's not just maintenance costs. We don't keep it in the warehouse. But yeah, users have some requirements that their transactions are on the base layer, or we have some deadline that we need to post. So to solve this problem, we applied q learning. So it's dynamic programming. So we need to calculate two matrices.
00:13:20.824 - 00:14:39.564, Speaker A: So first matrix is the q from the q learning, I suppose parameters state st that tells what is the price and also how many in the queue. We have the batches, and a t is an action. So in each state we need to know what is the best action or what will be the value of the queue table if action at is taken. And then we need to calculate the optimal action, and that is denoted by o vector. And we have this value update iteration with one minus alpha, let's say probability, we keep the original value of the table, and with alpha we take cost that is incurred in this state plus delta times, because delta refers to the future expected cost. But taking into account that when we go in the future, we will take optimal actions. Okay, so this is where, this is what this mean a means.
00:14:39.564 - 00:15:18.738, Speaker A: So we are taking the action that minimizes the cost. So we assume that in the future we will optimally distribute or post batches. Okay, so this is very clean iterative formula. And yes, I already explained what is ct. That's the cost that is incurred by taking this action 80. And to find this action a, that is minimizing the cost. In the future, we take o vector element because we know that o is optimal.
00:15:18.738 - 00:16:14.426, Speaker A: And then we calculate o or reassign values of o. Once we iterate and update values of q, new matrix and initialization can be done in arbitrary way. But we just took one of the initializations, which means that at every moment we post all the batches. Okay, this initialization, but you can post zero batches and it doesn't really matter, it converges very quickly and with the same speed. Now the complexities and how much we could try how large values of price and the size of them, q. So we discretize the price space. Of course, we couldn't calculate on the continuum.
00:16:14.426 - 00:17:21.064, Speaker A: And yeah, so we assume that we cannot hold more than some upper bound on q batches. And then the space complexity is the size of this large matrix is p q squared, and this is not a bottleneck. Then we need to also assume something about the support of the random variable that defines the next round price. And if we assume that this is also linear in the price, and in case of, let's say in uniform case, it's not exactly linear, but in case of normal distribution, it's linear, then time complexity is p squared, q squared. This is already quite slow for large values of p and q, but there is also the convergence parameter I. So we want, we will not calculate the precise values of the matrix, but we need to converge to something. And this parameter depends on the learning factor, so higher the learning factor.
00:17:21.064 - 00:18:04.158, Speaker A: So it's not actually exactly monotonic, but what it really depends on is delta. So higher the delta, slower it is. So if we take delta closer to one, which means that we care about the future cost almost the same as today's cost, then it becomes very slow. Okay, so now what are the insights that we obtain? First is that there is a threshold price below which all batches are posted. So if the price is low enough, just post all the batches. And this is actually quite intuitive. But if the price is high enough, then it depends.
00:18:04.158 - 00:18:55.174, Speaker A: Now we want to keep some size. So there is another threshold that depends on the price above which we post everything and below which we don't post anything. Okay, so there are these two thresholds here. Then we compare this algorithm that we calculated the optimal algorithm to, the old algorithm of arbitrum, and compared their performance. So in the new algorithm, the Qlearn algorithm, we have two parameters. So first we need to define the larger threshold TP. And then depending on the price, we need to define, decide how fast, so how large is the other threshold that depends on the price.
00:18:55.174 - 00:19:51.834, Speaker A: In the original algorithm, there were three parameters, so there was an acceptable price under which it posts the batch, and each batch is associated with the these three parameters. Then there is some exponent, and third, there is update time. So what happens if the price is not below acceptable price? Then it waits for this update time, which in that case was 2 hours. And then it multiplies the acceptable price by exponent. And that exponent was two, so it was waiting for 2 hours. And if the price was still not good enough, then it was doubling the price. So there is also trivial algorithm that we looked at, and this is just post all batches immediately and has good property that there are no delay costs.
00:19:51.834 - 00:20:39.074, Speaker A: But of course, sometimes then you post very high cost and you don't wait for the price to drop. And also it has minimum load on the system when you do this l one cost calculation. Okay, so there is another algorithm that also completely ignores the delay cost and just waits for the price to drop below some threshold. But there we looked at the data and saw that if for example, you take threshold cost 100, then you need sometimes to wait 1394 blocks and this is unacceptable. And with 60 it was 14,000. Crazy. So that is completely unacceptable.
00:20:39.074 - 00:21:26.704, Speaker A: Okay, so now we compare these two algorithms that we have, and see that the old algorithm actually does better job when it comes to publishing costs. So the observable cost, but the new algorithm doesn't make it too bad. So the publishing cost slightly increases, but delay cost is dropped very much. And if you look at the maximum delay, it's much lower. So for example, for parameters 62, maximum delay is only 42 minutes. While in case of, let's say the original algorithm, it would be 193. That's like more than 3 hours.
00:21:26.704 - 00:22:16.888, Speaker A: So there is a lot of future research to be done in this direction. First, we don't have a theoretical proof of optimality of our insight, so why do we need to have these thresholds? And I think it's possible to do fairly easy, but I'm not an expert in the optimization field or that kind of optimization. Second is. Yeah, so I mentioned that it can be that price changes depending on the strategy, and now we assume that it's independent. So that is also interesting problem to look at. And then the same approach doesn't apply. So there was also mentioning of the CIP 4844 by Mario, I think.
00:22:16.888 - 00:22:59.648, Speaker A: So this is new initiative for the batch posting or the roll up data posting called data posting, which creates a new market. And 1 may think that all this, what we did, now, becomes irrelevant because this data will become almost free. But this is not the case. And I think insights that we obtain in this research will be useful in the future when this proposal will be deployed. And the last question is assume so far. As far as I know, only arbitrum uses this strategy and other chains are doing. So.
00:22:59.648 - 00:23:19.754, Speaker A: I don't know exactly what they are doing, but most of them are not doing this strategy. And it would be interesting to observe if this strategy is an equilibrium strategy. So if everyone else does this, is this equilibrium, or do the separate chains prefer to deviate from this? Thank you for your attention.
00:23:27.294 - 00:23:56.864, Speaker B: Thank you very much. I think, yes, the microphone is working. We have about five minutes for any questions that has been super technical and super interesting. I don't have any questions because I don't follow everything, but maybe Wolfgang in the middle of the room has a question, or someone else. Yes, sir, right in the middle. Do we have a microphone for that gentleman right here in the middle? Or you can shout, absolutely.
00:24:04.624 - 00:24:10.524, Speaker C: Hello. And there is a threshold below which you take everything, is that right?
00:24:10.864 - 00:24:21.320, Speaker A: So there is threshold below which you post all batches, and above which there is another threshold, depending on the price, you want to post, more or less.
00:24:21.472 - 00:24:24.044, Speaker C: And what do you do in between the two thresholds?
00:24:24.904 - 00:24:52.330, Speaker A: So there is no in between. So let's say we post threshold 60. So under 60, we post everything about 60. Let's say if it's very close to 60, we post almost everything. But let's say keep one. So from 60 to 64. So there is some functional form, we keep one batch, but if it's higher and higher, we keep more and more.
00:24:52.330 - 00:24:56.906, Speaker A: So for, let's say, 200, we would keep ten batches, but the rest we will post.
00:24:57.010 - 00:25:01.010, Speaker C: Oh, so it's not that you keep everything in the queue, it's that keep.
00:25:01.042 - 00:25:07.734, Speaker A: Some amount, depending how large it is, and larger it is, more we keep.
00:25:08.994 - 00:25:22.454, Speaker C: Okay. And do you know, or have you thought about what happens if you include some randomization in the keeping process? So instead of keeping it linearly or quadratically as it increases, maybe there is some randomization.
00:25:23.494 - 00:25:56.862, Speaker A: So, I found one result that shows that deterministic strategy of this type is optimal. So it doesn't specify what is the strategy, but it says that some strategy like this, which specifies for each price. So this is from inventory policy literature. So that specifies for the price or the demand in that case. And quantity posting or selling fixed amount deterministically is optimal. So you don't want to randomize or do some weird things and.
00:25:56.918 - 00:26:00.514, Speaker C: Okay, that's great. And is this already.
00:26:02.374 - 00:26:03.054, Speaker A: Productive?
00:26:03.094 - 00:26:05.014, Speaker C: Is this how arbitrum sequencer works?
00:26:05.054 - 00:26:23.554, Speaker A: Yes. So we deployed it last year in November, and it already went through the spikes a few times. But there is not enough data to make some conclusions yet. But at some point we will track the data and observe what it is, does what it is actually doing in the production.
00:26:23.854 - 00:26:25.474, Speaker C: Awesome. Thank you very much.
00:26:25.814 - 00:26:49.302, Speaker B: Thank you for the question. We have 1 minute and 10 seconds until the next session. Time for one more question. Anybody? Anybody? Wolfgang, you're taking notes? No. If not, then we thank you very much for the presentation. Super. And you can just join us on the side here.
00:26:49.302 - 00:26:54.174, Speaker B: We're going to move on to the next section, but before that. Thank you very much.
