00:00:10.640 - 00:00:51.854, Speaker A: OK, we have the slides, so it's great to be here giving a talk in Prague. I'm actually from Prague and this is the first time I'm giving a talk in Prague, so it's really nice. And in fact, when I say talk, we plan this as a workshop. We have quite a lot of time, so we are assuming people are going to take it as a workshop. So I'm hoping you have your laptops ready and you're wanting to hack along. If you don't want to hack along, that's also okay. But there will be some gaps because we are hoping to help people set up and actually try get things going and run some of the experiments.
00:00:51.854 - 00:02:06.702, Speaker A: So what's the plan? I will quickly cover where we're from, which is Vega protocol. I will then try to convince you that in defi you really want to start thinking about agent based simulations to understand the protocol and how it interacts with the wider ecosystem. And then Tom will take over and he will introduce the Vega null chain and Vega market simulator. Then there will be a bit more time to actually get you guys set up with the market simulator. And then Tom will talk you through scenarios, agents, environments, sort of the basic building blocks that you can use to create interesting things happening. And then bit of motivation why we want all this in terms of optimizing parameters on protocols and then bit about how reinforcement learning might be interesting in this context. And then the rest of time, which hopefully there should be a fair bit, will be the practical session and will sort of help you build a basic agent.
00:02:06.702 - 00:02:45.464, Speaker A: So let's get going. It's a very small audience, so if you have questions, it's probably better if you just sort of interrupt and ask straight away rather than like waiting for hour and a half till the end because you know, then you might lose interest and not want to ask your question or whatever. So quickly. What is Vega protocol? Vega is a layer one blockchain built on proof of stake, what used to be called tendermint. Now it's the comet BFT fork. That's what used for consensus. It's optimized for trading margin products.
00:02:45.464 - 00:03:30.434, Speaker A: So for margin products you can think of derivatives or promises or whatever you want to call it, price discoveries through limit order books and auctions. Vega chain itself, even though it's layer one, has got no native assets and all assets are bridged from Ethereum. So you can think of Vega as a side chain to Ethereum. Any community member can create markets by interacting with the network. And basically if there is an oracle there can be a market. And of course, markets are not very interesting if there is no liquidity on them. So there is a liquidity incentive mechanism for limit order books that is fairly bespoke to Vega protocol.
00:03:30.434 - 00:04:44.734, Speaker A: So quickly, why not stay within Ethereum ecosystem? Or why not run all this on some other general purpose blockchain? And there are a number of reasons, mainly around sort of design and economics. So the first one, and almost the main reason is that you want to, for trading purposes, you want to design the chain economics quite differently to a general purpose blockchain. So on a general purpose blockchain, you pay gas for transactions. The more compute or storage you use, broadly speaking, the higher the gas. So in particular for trading, it doesn't matter whether the economic value of your trade is big or small, you still have to pay the same fixed amount of gas, because the, the storage cost and the compute cost of a small trade, economically speaking, is the same as for a big trade. And that's suboptimal. And you see that no one in traditional finance is running things this way, and there is probably a good reason.
00:04:44.734 - 00:05:33.594, Speaker A: The other important thing is that if you're running limit to order book or you're running an auction, people placing limit orders is valuable. It's information and it's liquidity. And you really don't want to penalize them for doing so by asking them to pay gas. Then if you're trading derivatives, there is risk management. And if you want to avoid the liquidation paradox, you need a disinterested third party which will execute the closeouts, which has to be the chain itself. Again, on a general purpose blockchain, someone has to pay the gas for closeouts. And then you end up with these adversarial closeouts that lead to liquidation paradox.
00:05:33.594 - 00:06:17.806, Speaker A: So you avoid that staying with derivatives. If you want to have a safe and capital efficient derivatives market, then you have to run some relatively heavy risk related computations. And the fact that you can run it on bare metal, if you're building your own chain, is an advantage. And you know if it's your own chain, you can optimize for latency, which you care about. Whereas on a general purpose blockchain, you don't necessarily always want to minimize latency. You also think about throughput, and there is this push and pull. There are some applications that will more throughput, some applications than one lower latency, et cetera.
00:06:17.806 - 00:06:55.484, Speaker A: If you have a purpose built blockchain, you can really optimize. So all of these we already have running on Vega and the other thing that's kind of big is you want to avoid minor extractable value and front running. And we are building a fairness pre protocol on top of Vega to deal with that. The author of that fairness pre protocol is Klaus, who may or may not be in the audience, but he's certainly here for the event. So if you're interested in wendy, you can speak with Klaus. Okay. So that's why run your own l one.
00:06:55.484 - 00:08:00.744, Speaker A: And now is kind of the first little break, which is very, very short. And the idea is that now would be the time for the people who want to hack along. Just take your laptop, go to this URL, or get the URL from the barcode and start getting yourself set up. You will need a bit of python, bit of python package management, and if you want to run the front end, then some packages for the front end, and then if you want to run the RL, you also want to install Pytorch. It's all in the readme. So now that everyone hopefully has the URL of those who are actually keen to code, which disappointingly, doesn't seem to be terribly many of you, I thought it's like all hackers. Come on, guys.
00:08:00.744 - 00:09:04.320, Speaker A: Okay, well, you know, it's here. If you change your mind, you can find a URL later. So I will now continue making a case for the agent based simulation. So for doing, for doing what we are doing. There was a fun article about ten years ago by Don Farmer in the Nature Scientific journal saying that, you know, economists should be doing a lot more agent based simulations to understand what really is happening and to improve their predictive capabilities, which have been, well, we know how the economic predictions are. They're hit and miss. It's a great idea, but in fact, they have a problem in the sense that they would have to build these digital twins for everything because things are happening in the real world.
00:09:04.320 - 00:09:53.844, Speaker A: Half of the economy still runs on people pushing paper, etcetera. You would have to build a model for all of this, and it will just generally be a pain in the neck and a lot of work, even though people are trying. In DeFi, we have the massive advantage of everything actually being a code. If I want to simulate uniswap, Aave, whatever, I can take something like ganache, hard head, get the contracts in and start pumping simulations through. This being only limited by the speed of execution. Maybe I need to do some optimizations, maybe I need to do something clever, but at least I have the code. So in DeFi, we have this huge advantage and we should be making use of it to understand what is really happening.
00:09:53.844 - 00:11:01.844, Speaker A: And what's happening is that DeFi protocols are becoming more complex. I mean, you've seen how the simple rules, for example, for uniswap, gave rise to the minor extractable value where people started really pushing things into the mempool to get advantage. And so these simple rules can lead to quite complex behaviors. And once you have these complex behaviors, it's just hard to understand things on the level of proving theorems, saying definite statements. If I see this, this is going to happen. And the other thing is, of course, that a lot of the protocols have got a bunch of parameters which are to be set by governance. You have uniswap fees, you have Aave liquidation threshold, you have sort of the magical parameters in AavE which set the interest rates for various assets.
00:11:01.844 - 00:12:20.664, Speaker A: And all of these have to be set right in some sense, so that the protocols do what they do. And then, of course, when we were building Vega, we ended up sort of relegating a lot of the decisions to kind of network parameter settings and risk parameter settings and various market parameter settings. And just understanding how everything will behave is, you know, just next to impossible if you try to do mathematics, or you can try to do mathematics, you can set things up as games, you can set things up as principal agent problems, you can go to your maths friends and they'll have fun approving stuff for the next ten years. But if you want answers bit sooner than on a ten year time horizon, then you have to do something else. And something else are these agent based simulations. And of course, with defi, you have interoperability. So just because you understand, let's say, how the simple rules in uniswap, what can happen and what won't happen, or in Aave, you then have, through interoperability, you have people doing flash loans and all sorts of tricks, which can just increase the complexity of what can happen exponentially.
00:12:20.664 - 00:13:23.220, Speaker A: What is an agent based simulation? It's basically, you have a computational model, which for us is easy. We don't need a model. We have the protocols themselves. We can work on the real thing for simulating the actions and interactions of autonomous agents. And the autonomous agents will be some code. And I'll come back to this one. And how the agents behave now depends on whoever is setting up the experiment, that there are broadly three classes of agents, and all of them kind of follow this loop where there is an environment, you're in some state of the environment, and you're getting some rewards based on this you know, the agent makes some decision, takes an action which influences the environment and so on.
00:13:23.220 - 00:14:20.914, Speaker A: And this is repeated. And broadly speaking, the three classes are, you know, you can have zero intelligence agents in the sense that actions are hard coded. You know, I don't know if some price is below something, buy, if a price is below something, sell, etcetera. You can then go one level higher and sort of say, okay, the agents will be doing some kind of optimization, maybe solving control problems or trying to find some equilibria. And based on those, they will be taking actions which are in some sense optimal, depending on what their objectives are. And this is nice, but it sort of assumes full knowledge of the environment. And of course, you have to solve the control problems, which is, or find a Nash equilibria or some other kind of equilibria, which is not always straightforward.
00:14:20.914 - 00:15:21.044, Speaker A: And then you have the reinforcement learning agents, which basically look at this loop, collect the rewards, and then just take initially random actions and try to maximize these rewards. And as they learn, they sort of hopefully learn to take the, take the optimal actions. Right? And then I promised I will come back to this. And, you know, we were pitching, or we are pitching this for understanding how the protocols work and for optimizing the parameters and having your protocol set up in the right way, whatever write means in the context of your protocol. But these simulations are actually quite good, quite good additional tool to whatever Qa you're doing, and they can help you find bugs. And the reason is sort of.
00:15:25.784 - 00:15:26.072, Speaker B: If.
00:15:26.088 - 00:16:32.780, Speaker A: You just do your traditional fuzzing, then the space of possible things is huge. And quite often in d five, what happens next is so state dependent that you're very unlikely to get to an interesting state that will then trigger a bug by pure randomness. Of course you will eventually. It's the whole monkeys on typewriters writing a Shakespeare play eventually, but it will take a long time. And what the agent based simulations allow you to do is you sort of explore or fuss around the states that are relevant for your protocol, because, you know, the simulation is not reality. But you can hope that the agents will behave broadly rationally, so that if you have an environment that's being run by these agents broadly rationally, and then you do a bit of fuzzing around it, you can hope to avoid bugs like this. Now, binance has probably bigger things to worry about than codebugs at the moment, but we leave them to that.
00:16:32.780 - 00:16:54.298, Speaker A: They're not defi anyway, so. Right. And I think at this point, I hand over to Tom, who will actually talk you through what has been built at Vega, which mainly means that Tom built it. Cool.
00:16:54.466 - 00:17:42.618, Speaker B: Yeah. So now that we've got justification out of the way, I'm going to run through a bit about what we've built, how we think it sort of interacts with these problems, helps us solve and get a better idea of some of them. We'll then take a bit of a break for anyone who is doing some coding to get set up. If you're not, you can go for a coffee or something and then I'll mention what we're talking about next when we get to it. So the Vega market simulator is a few sets of layers. If we start with our base layer, we built out what we call a null chain, which is ultimately a full Vega stack with the consensus layer stripped away. The previously mentioned consensus layer is tendiment based layer.
00:17:42.618 - 00:18:40.374, Speaker B: We remove that and we add a layer that just accepts whatever transactions it receives and crucially, gives us total control over time flow and that kind of thing. If you think in your Ethereum world, it's very similar to something like hardhat or foundry or that kind of thing. Once we've got that base layer, we built out a nice, hopefully nice API layer of various functions in Python, which let you interact with the chain itself. But in a market sort of trading sense, what this lets you do is really talk about your agent's actions in terms of market primitives and fundamentals, rather than worrying too much about being on a blockchain. So you can see here just a few examples of functions. A market order, which is an order to buy a given size. We've got a few arguments here, but they're pretty much, they're market related.
00:18:40.374 - 00:19:31.764, Speaker B: You don't really care that you're on a blockchain. You could be interacting with a centralized exchange, but you can talk about trading as a trading thing, make sure that it all works correctly. Once we've got that API layer, we build out a bit of logic, a bit of what we call scenarios. We'll go through a bit about how those are constructed, but they're basically a way for us to simulate a given real world scenario, maybe less real world scenario, and answer a question. Ultimately they're built of this kind of construction. So what we have is an environment, well, a scenario class which contains an environment that sets up the chain itself broadly and things like the number of steps we're going to run. The components themselves.
00:19:31.764 - 00:20:42.156, Speaker B: Vega consists of, well, the chain we run consists of core process itself, which is the chain, and a data node that is ultimately sort of a SQL database for your historic queries and things. So it sets these things up, sets up the logging and block size, that kind of blockchain thing. And then we just have a set of agents, each of whom get a go. They get an initialized step where they'll maybe force themselves some assets if you're running in an old chain world. And then they would just loop through a load of steps however many times you want and just observe what happens really. So at this point I'm going to have a quick break and for anyone who wants to code, they can come around and help set up this QR code takes you to the slides where you can get this link. If you don't want to want to code, go grab a coffee or something or just have a break and we can come back in 15 minutes or so.
00:20:42.156 - 00:21:41.364, Speaker B: And then I'm going to go through a bit about what we've done with the market sim and the null chain and how we found that it really helps answer a few questions. Some of the things we've discovered and how it's helped as Vega itself has moved into a sort of live trading environment and how we've been able to make decisions that hopefully improve the markets for participants on that, any question, and if you are doing any coding, can you give me a wave so I know who to check on? We've got a few at least. Okay, cool, thanks. See you in a bit. We've also got a few commands if you want to. If you load this up and do manage to get it working, give running this command a go. Give some interesting plots and outputs and things.
00:21:41.364 - 00:22:17.154, Speaker B: But that's all. You can all find that in this or we'll bring it up again later. So once we've got the sim running, there's a few different routes that we can really sort of dig in and inspect what we're doing with that. And what's cool is that they're the same ways you can interact with the real world market when it's running on a node elsewhere. So there's an API layer that's built out for Vega protocol itself. So that gives you access to trade information, information about what's happening on the market. Party balances, that kind of thing.
00:22:17.154 - 00:23:15.934, Speaker B: Graphql layer, which gives similar stuff with a graphql front end and we can sort of see what we're logging out to the console as well. One really helpful thing is that we've got the front end that's built out for live trading is also a really good debugging UI because it's designed to give you all the information you need. So instead of reading out from pure text, you've got a nice front end you can really dig into. There's also functionality to be able to view the front end because we're on a blockchain which is totally open, you can view the front end as any given party, so you can watch the trading as any of your agents and see what it's up to. So with this framework, we've built out a few different agents. If you're looking in the repo, there's a couple of files called agents that you can dig into. These we've got market makers who will algorithmically do market making of some level of intelligence.
00:23:15.934 - 00:24:00.404, Speaker B: They generally have. There's a few mathematical ideal functions that determine, based on some assumptions of the market, how they should trade. And then there's curve market makers who post those. They post a nice shape of order books on the market so that you can sort of get a nice market of any given depth or function that you want. We then have various traders designed to be more or less intelligent, but simulate the market a bit. We've got people who blindly take liquidity. We've got informed traders who only trade if their sort of estimate of the market is going to give them a profit or not, and traders who trade based on momentum and technical analysis, that kind of thing.
00:24:00.404 - 00:25:11.280, Speaker B: So what kind of thing do we use this for? One of the initial key drivers that we found was useful for this kind of framework was parameter setting. So David mentioned earlier that there's, there's a big number of parameters on Vega protocol itself, which are ultimately market and community controlled. But these have a big impact on how the network itself runs. And a lot of these are really hard to sort of reason out a priori what exact number should be or how this will impact the trading on the market. And you see this across defi. But when we move into sort of an agent based world, we can set up our scenario, we can set up how we think actors will act, trading will happen, and we can observe and run this output and get what might be a sort of a range of scenarios, or have a more scientific approach to setting these parameters and hopefully making a more robust network in the process. But we still have limitations.
00:25:11.280 - 00:26:18.810, Speaker B: We've got to set these initial conditions. We've got to decide what a real world market might look like. And we want agents with a bit of randomness to them too. So one example of where we found this really useful was we were running what we called mainnet simulations prior to the network itself going live, and noticed that data size on the data nodes was growing really fast. So we had this parameter in the network called markprice update frequency, which basically determines how often traders are remargined, remarked their market, where sort of margin maintenance will happen, people will have money moved from or to their accounts, they'll be closed out if they're in default, that kind of thing. So obviously when you do this, it creates a whole bunch of data which fills up the data node. So we don't want it too fast, but also if you do it too slowly, you're going to find out that when you remarging, a whole load of people need closing out.
00:26:18.810 - 00:27:13.216, Speaker B: Maybe the network itself is people can't pay their debts, that kind of thing. So we want to set up an experiment here where we run for certain length of time, we log various things and we just set a range of values and really just see what happens. So this is the output that we observed. We have, you can see on the left here, just that the update function is sort of working. When we move out to these longer 32 and 64 2nd slowdowns, you can see that the price moving here, each time they update you got a big jump and you might get a load of people closed out there that you don't really want. So when you then move over to looking at the growth rate of the database, you can see that we've actually had a noticeable impact on the growth rate here. After only I think I might be standing in the way.
00:27:13.216 - 00:27:58.064, Speaker B: But 20 minutes or so of trading, there's about 100 meg difference between the fast and the slow ones. And if we re plot that we can see that we can actually make quite a good decision here because once we're moving out to maybe five or 6 seconds of update frequency, we've got most of the growth rate slowdown that we want. This is in decreased percent. We've got most of that slowdown, but without going too slow. So we're still pretty close on here to staying close to the market. Five, 4 seconds, 8 seconds are still pretty close. So doing that we can be more scientific about making these decisions than otherwise, just sort of estimating, guessing what might be a reasonable number.
00:27:58.064 - 00:28:48.322, Speaker B: Beyond that we can do things like fuzz testing a market in a wide range of scenarios. So this is designed to kind of avoid that issue that we saw earlier, binance where they said, oh, we had one trailing stop order in a weird scenario in this market state that blew out our network for two, 2 hours. In a decentralized world, bringing your network back up is even harder. You've got to talk to validators, you've got to get code out, they've got to get it up. So we want to avoid these early as much as possible. So this is what we run with our fuzz testing, where we can run a whole range of scenarios with mainly more sensible traders, but then some doing totally nonsensical things. Broadly, they'll look at the input, they will look at the whole range of things that they can send, and they'll send garbage and make sure it doesn't explode.
00:28:48.322 - 00:29:25.414, Speaker B: We can also have degen traders who over leverage themselves. They constantly get closed out. You can see we have an insurance pool. When someone's closed out, we confiscate their money to pay back other people if they can't pay it in the future. You can see that's building up. We've got various closeouts, but the market itself is still trading along nicely. So we run these sort of overnight every day, and it gives some confidence about being able to keep the network up in a range of scenarios.
00:29:25.414 - 00:30:13.890, Speaker B: We can also monitor that. We're really covering every edge case and path of different arguments which could be used. Now this is all great for setting up your code and making sure that that's all working. But there's further examples where it's really useful to be able to simulate a market and make good decisions. So as I mentioned, Vega has recently started live training on it. As part of that, the community made proposals for how to configure a market. And that's got, again, a whole range of parameters, especially things like rest parameters that are hard to a priori reason through what it should be, but that are really important.
00:30:13.890 - 00:31:17.068, Speaker B: You don't want massive blowouts, you want stable markets, but you want to allow people to trade. So you need to have this balance. And giving the community tools to be able to analyze that and really dig into what the effects of these changes are and meaningfully discuss these is really important to being able to properly decentralize and have a community governed and driven protocol. So one example of this was risk monitoring boundaries. So we have a configuration where large price moves, instead of allowing a very large price move to happen, we drop into an auction for a few seconds where, or a configurable amount of time where we'll auction, we'll determine price, and then it drops back into real trading if price is sort of within the bounds again. So this is a very useful tool to prevent against these large price spikes. But as you can see, the configuration is descriptive, but not entirely easy to reason about.
00:31:17.068 - 00:32:08.890, Speaker B: So at a high level horizon is looking back at a certain number of seconds for a price move. So 60 seconds, 600 seconds, a probability of a price move happening over a certain time. And then if the price that is going to happen is larger than that, how long we drop into an auction for. So this was as proposed in the initial proposal, and it's kind of hard to tell are those reasonable numbers or not. So what we can do is you run a scenario with a load of people trading and see what happens. Going back to November last year, when there were a few things happening in the market that were a bit disruptive, we saw that there was actually a lot of auctions happening with this. So you can't entirely see on here.
00:32:08.890 - 00:33:08.778, Speaker B: There's a few lines there you can see, but broadly, you can see on this top analysis that each of these red lines is dropping into an auction, a price monitor auction for 60 seconds a minute. And so that's happening when this market is moving quite a lot. These are sort of valid moves. We saw them happening on centralized exchanges and all this. And dropping into an auction all the time on this would be pretty disruptive, not great for trading. So people can, can look at that, they can analyze that themselves and say, I don't think we would want this, but what about if we chop out these two and only have these slightly wider ones? What happens then on the same, we can run the same scenario, the same traders, and analyze it again. And you can see that instead of all these many auctions, we still hit a few.
00:33:08.778 - 00:34:12.124, Speaker B: It's still a big, it's a volatile day, we're still moving around a lot, but they're pretty short, and we only hit them a couple of times jumping around. So the committee can then say, is this reasonable? Do we want this? Do we want even less monitoring? But ultimately that means we can make more reasoned and informed decisions about complex market proposals without just sort of finger in the air guessing it. So given all that, why do we also want RL agents? So a lot of our other agents, they will broadly follow randomized but somewhat known parameter paths and actions. But the problem with that is that you don't necessarily explore your whole state space. And if you're going in a fuzzing world, you've got a vast state space. So what we want are agents who look at a scenario, take a somewhat reasonable action, but maybe do something new when they can. And that's exactly what RL agents are designed to do.
00:34:12.124 - 00:35:33.896, Speaker B: So by leveraging various sort of pre existing learning frameworks and things, we can throw an RL agent in, a couple of RL agents and see just over long runs, if they blow up the market, if things, you, weird things happen. And we noticed when we first kicked this off with our first version of RL, within sort of a few days, we managed to hit a few crashing bugs that we hadn't found before. So it really validated that we were sort of exploring new ways of doing things and new weird edge cases of the market. Since then we have moved to a v two RL agent where I don't know if you know, OpenAI gymnasium or stable baselines, which gives us a whole suite of fairly cutting edge sort of neural networks and learning frameworks that we can just plug in. If you get the market sim set up, you can plug in an action and a reward that are fairly customizable and it will learn an agent for you trading on the market. And then you can point it to, I don't know, broad or, or other markets and see how it goes. You can see here we configured this agent with just an action and a reward and learning against some very stupid trading agents.
00:35:33.896 - 00:36:11.224, Speaker B: It managed to make a profit. So we were sort of exploring the market, we got an agent trading around and yeah, that allowed us to sort of explore new things. So that's broadly wrapping up our talking part of this. If you've got any questions, I'd be happy to take them. Otherwise, if anyone is interested in either coding now or setting it up at home, in future, please drop us a line. I'll go back to the. There's no forwards, go back to the one with the QR code on it.
00:36:11.224 - 00:36:39.104, Speaker B: But yeah. Thanks very much for your time. If any questions, we also have hats and some flags if you'd like anything. Thank you. No questions. Cool, thank you. It.
