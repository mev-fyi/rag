00:00:11.320 - 00:00:13.594, Speaker A: I don't know. I. Okay, thanks.
00:00:23.974 - 00:00:49.050, Speaker B: Okay, check. Check. Okay, everybody. Hi, I'm Daniel. I will be moderating. I'm going to start with some context and framing. I would say that surveillance and freedom, whether it's digital or analog historically, has always been a bit of a cat and mouse game.
00:00:49.050 - 00:01:44.724, Speaker B: So there's always been. While surveillance technologies and systems have always been being developed, there's always been a counterpoint to this, usually ahead of it. That's just the nature of where that edge lies, right. I would say where we're at in the world today is that web3 is a massive revolution around privacy. Obviously, it's really the mouse getting out ahead and offering an alternative to our system, but to provide some context here today, I think we're mostly talking about the cat. There's been other conversations around the mouse, but we'll be really talking about the cat. The system outside of web3, where things sit, an emphasis on China and Asia, the SDS examples learned from there.
00:01:44.724 - 00:02:20.904, Speaker B: And then we'll also open it up to questions, because I want to see if there so be thinking about this. This relationship between the cat and the mouse. Most of you all are in the mouse world 100%. So I think if we can try to have good questions that bridge between the two, that can open up some richer discussions around implementation pitfalls, et cetera. So, just prompting you all now be thinking about relevant, important questions from mouse to cat. Okay, hopefully that's helpful. And then I think we just open it up for some quick introductions.
00:02:20.904 - 00:02:23.498, Speaker B: A couple sentences for each of you.
00:02:23.626 - 00:02:24.002, Speaker C: Yeah.
00:02:24.058 - 00:02:24.654, Speaker B: Good.
00:02:25.194 - 00:03:02.624, Speaker C: I would start. Martin, thank you very much for having me here. I'm an academic in Germany, say, in the field in between of cultural science and computer science, and did that presentation at 11:00 on the social credit system development in China. So, following up Pierre's presentation yesterday, and since we in some points agree, and in some disagree, we would have to talk something about. About mice and cats, maybe. Thank you.
00:03:03.844 - 00:03:28.194, Speaker D: Hi, my name is Yihonji Lim, known as a jew. So I'm also an academic principal academic in sociology. So my special list area is north korean human rights, especially women's rights. So I've just published a book on north korean women's human rights and their activism. So, yeah, I'm glad to be here. Thank you.
00:03:29.414 - 00:03:45.394, Speaker A: Well, hi, I'm Pierre. Thank you for being all here. It's a pleasure. I'm also an academic, although earlier in my career, and I work mostly on China and digitalization of the chinese state and specifically the social credit system.
00:03:47.144 - 00:04:02.764, Speaker E: Hello, everyone. My name is Giulio. I'm a researcher at the University of Cambridge, where I mainly study the use of AI as a disinformation system, and it's used in influencing the way we see information and receive information. Great to be here.
00:04:04.544 - 00:04:29.944, Speaker B: Okay, so I think I want to drop right into. Maybe what we start right into is on the cat and mouse. So. And let's orient our audience a little bit. You all have told me that, respectively, web3, crypto, blockchain, etcetera, is not your area of expertise.
00:04:30.724 - 00:04:31.900, Speaker C: No, not expertise.
00:04:31.972 - 00:04:44.064, Speaker B: If I had to ask you, what are the most interesting technologies happening in web3 right now at the. For front of this issue, you would say, I'm not sure which ones I would say are exciting or how would you answer that?
00:04:47.004 - 00:05:15.774, Speaker E: Yeah, yeah. I would probably say I'm not sure. There's definitely, I think a lot of prospects, like, I'm not an expert in the field. I do think that there's a lot of prospects for the use of blockchain technologies in key characteristics. I would say, for example, the importance of decentralization. I think these are all features that will come to be more and more important, especially for information systems. But I'm not familiar with specific technologies at the forefront.
00:05:19.274 - 00:06:36.584, Speaker D: Yeah. I mean, for me, beyond the sort of technological sort of details inside, obviously I cannot mention anything about that. But when I look at what's happening in North Korea and how North Korea has been sort of exploiting this sort of decentralized technological system in order to evade their sanctions, but also I think it's not just about the exploitation of the system, but also how it is used. It is not used for the general public. While people, if you look at actually why people are struggling with the starvation and poverty, they are using it to feed elite group, just a very small minority of leaders for their luxurious lifestyle, whilst also continuing to use that money to develop mass weapons development, as you know. So I think for me, it depends on, you know, which hands actually get hold of this kind of technology and how it is used. I mean, there is definitely potentiality, but in practice, I think we have to be very careful about the implications of this kind of technology.
00:06:38.564 - 00:06:51.196, Speaker B: The potentials are pretty dire. Martin, I mean, you said to me that you think democracy itself is at stake. Can you unpack that just a little bit more?
00:06:51.300 - 00:07:50.264, Speaker C: Yeah. So if we observe the state of affairs, then we can see that, say, for instance, in neoliberal countries like the us, private companies grab everything. So data wise in the very east, and for instance, in the People's Republic of China. It's the party who gets everything. So this choice is not easy to do, is it? Should it. Should it be the state or should it be private companies? Or should it be like in the EU, where there are lots and lots of regulations? So we are at a sort of tipping point. I have the impression about how we could integrate communication technology into whatever we want our lives to be like politically, whether it should be parliamentary democracy or an autocracy or whatever.
00:07:50.264 - 00:08:31.772, Speaker C: But this is. This is going wild at the moment. And there are attempts which are, say, not very convincing, say, from the EU to confine this. But then there is still the stuff we give for free or for very little service. If we just want to know whether there are discounts around the places where I go, I would. I would open up my GPS data just for, say, discount, almost for nothing. So there is an everyday aspect to it, and I could think that you meant this also.
00:08:31.772 - 00:09:31.044, Speaker C: This everyday aspect that is not, say, a direct consequence of high technology, but of everyday life, say, of culture. I think we should discuss how we want our societies to look like, who should own the data, and then the high technology like this. What we listen to during that conference about privacy and partial revealing of information is very important that we could implement it. But in everyday life, we give our stuff away for almost nothing. So this is the main phenomenon I see. It's not a technological one, but it's a cultural and economical and sociological phenomenon that we face at the moment.
00:09:31.704 - 00:10:38.886, Speaker B: So, one way out of the current situation is policy frameworks, a social discussion that results in material changes on a regulatory basis, and probably some kind of algorithmic accountability or something around all this stuff. But we have democratized information warfare at this point, and the power of various actors to interrupt that dialogue. Right. To degrade the commons around conversations, understanding the basic principles of democracy from good faith communication, and ability to even have communication, maybe. Julio, this is a good point for you to drop in around AI, to give your perspectives on AI, where that goes, the problems, but also maybe ways out as it intersects both with policy and in general.
00:10:39.070 - 00:11:14.694, Speaker E: Yeah, absolutely. Thank you. First of all, I want to agree with Martin's point on democracy. I think we are really facing a dilemma where I think when we think of data, in our data, we do need transparency, we need accountability, we need democratization in a way. And I think that all the actors involved broadly can hardly be trusted in these tasks. We don't want governments to regulate how information circulates. We also don't want companies to drive how information circulates.
00:11:14.694 - 00:12:28.912, Speaker E: And we do need some level of transparency and decentralization there and maybe look into new patterns or new ways of handling data and thinking about data that doesn't involve control, whether from states or corporations. And I think with AI, definitely there is a lot there to think about in terms. I mean, like, if we start from the point of data, there is a lot of discussion at the moment about how our data is used to train AI. And that is a very hot topic at the moment because a lot of people are saying we never gave consent for data to be just thrown into large language models, for example. And that is something that is, it is a very significant privacy issue, even though it usually doesn't lead to any type of recognizability. Like, you can never know whether it's your data or my data, but it's still an invasion of privacy when your data is unknowingly used, maybe from some, something you wrote on Reddit or on Twitter is thrown into a model for training. And then I think the other very important point about privacy is the very high level of personalization that we have through AI at the moment.
00:12:28.912 - 00:13:25.994, Speaker E: For example, through recommender systems, recommendation algorithms, which are tools that knowingly or knowingly for us, use our data to predict and rank what we are likely to enjoy more as content, which doesn't sound as daunting, I would say, because ultimately, we like to see things that we like in a way. So it's not necessarily an evil task, but they come with very significant privacy risks. For example, if the systems are manipulated, or, for example, if the owner of an algorithm, which could be a social media company, decides to push some content more than other types of content in a non natural way, these are all very significant risks for our information systems. And, yeah, and I could go on to talk about that, but, yeah, I want to hear from everyone else as well, if you have any comments and points on that.
00:13:26.494 - 00:13:26.830, Speaker C: Yeah.
00:13:26.862 - 00:13:45.314, Speaker B: Any other thoughts on AI relative to your work? Also, I'd be interested, Martin, to hear about just information or perception of what is true, what is real, what is culturally valuable, and the breakdown of that potentially from your side.
00:13:47.134 - 00:14:33.476, Speaker C: Yeah. So this is a very big problem because those AI systems that are now fashionable are built in a way that they have to hallucinate. It is a built in feature. You can't get rid of it. If you build it in that way, you do. So I'm very skeptic what good applications of this technologies could be, because if you really want to know whether it's in an old fashioned way, true or false, you should not ask them, because they are built to dream in between facts and to extrapolate. And I'm really concerned to see that, that this is thought to be a good thing to use in education.
00:14:33.476 - 00:15:27.164, Speaker C: I'm not sure about this. I'm not sure it should be part of education in skepticism very, very fiercely, so to speak. So in education, I would use that as something that could save time from time to time, but which you would have to look at very, very skeptically on the outputs. And when I do a glance to the US, I'm really shocked about the way how facts are ignored, obvious facts. And I have more the suspicion that these systems fire it more and more. And this is really, really very nasty, because they are built to not know what is true and what is not.
00:15:29.184 - 00:16:38.024, Speaker E: If I can sort of object a little bit to that point, I think I do see it as a bit more complicated in the way that I don't necessarily distrust AI or like LLMs or chatbots in particular. I think they do a lot of things well. I would be favorable in using them in classrooms for productivity tools and for many other tasks. I do think that there are two main caveats that we should be aware about, and one, as Martin mentioned, is hallucinations, like these models do hallucinate and make up things that don't exist at times. And whoever uses these models should be aware of that characteristic. And they're not meant to be used as a Wikipedia, and that should be kept in mind. And the other thing is, I don't think, I think one of the risks for knowledge is that they do collapse knowledge, even when they say the right things, like the knowledge that we acquire is sort of like collapsed towards the middle of the distribution, because if you train any model or a large amount of data, they will spit out the most likely outcome that they have learned from the data.
00:16:38.024 - 00:17:17.454, Speaker E: So if there is a variety of opinions in a society, and LLM would probably converge that knowledge towards the middle, because that's just what's more likely for a machine learning algorithm to output. And I think that is a risk in many ways, because I think divergence of opinion is a very important part of how societies work and our knowledge grows and progresses. And I think that is a very real risk of LLMs in education. Like, we don't want to only learn the center of the distribution, we also need to learn about the sides. And AI sometimes doesn't allow that. But I am very positive about the use of AI given these two caveats, telling people to be careful about these things.
00:17:18.434 - 00:17:36.378, Speaker B: Best case scenarios for AI in addressing this, Julio, best case scenarios of positive AI applications, in the issue of disinformation of the commons and in privacy and freedom.
00:17:36.506 - 00:19:01.994, Speaker E: I think best case scenario is that we get it right in terms of regulation. I think in the EU, there's a lot being done with the EU AI act, where the law, even though it's not going to be very easily applicable, there's a lot of really important principles, for example, that the data of an LLM should be fair, should be transparent, should be representative. So these are all principles that would help make AI safer. And I think the best case scenario is that we develop very hard guardrails on how AI can be used, and we regulate, regulate that in a way that allows progress, allow us to use these systems, which are very powerful and very helpful in many ways, in the best possible way, without creating unnecessary risks. I think regulation will be very important, and I think for information, we do need technological progress, and that's where blockchain or web3 could, in a way, enter into this, because we do need better ways of validating what's human and what isn't, which is something that we are very lagging behind at the moment. And there has been a lot of things like proof of humanness in the past. But I think, in general, the fact that at the moment, we're not reliably able to tell whether something is AI generated is a pretty scary prospect.
00:19:01.994 - 00:19:49.150, Speaker E: And humans are very good still detecting when something is not human generated. It sounds unlikely, but when you put enough people in a room to detect whether a text or an image or an audio is not created by humans, we are pretty good at that. So I think, in an optimistic way, I do think we still have the capacity of making AI safe, but we do need to find technologies and systems that allow us to do that. And then we need policy that will make this go as safe as possible. But I think in a best case scenario, AI will be a great tool for productivity to learn. You could learn a lot of things. You could learn to code.
00:19:49.150 - 00:19:53.394, Speaker E: There's so many things that it's good at if you are aware of what the limits are.
00:19:53.894 - 00:21:23.202, Speaker B: Yeah, I would say in a world where trust is fundamentally breaking down, whether or not on a social or political, cultural level, we're royally screwed unless there are trustless frameworks, right? I mean, or I think, to Martin's point, like, where are the havens? Where maybe trust still exists in some semblance, politically or culturally, I can tell you, in the United States, that is quite unfortunately going down the drain right now. So well. And I think. I think this can allow us to transition right now to, you know, if we think about the world right now as going through a massive industrial revolution. And that revolution is essentially the production of tokens of pieces of information that can be utilized agnostically by anyone, and obtained these mass production of tokens of information in the world at large. And China is the most relevant example right now of them trying to have a repository at all levels of governance and jurisdictions of a centralized information system that could be drawn upon at any levels. And so I think it potentially be interesting to go into that territory, as an example, with both Martin and Pierre for a moment.
00:21:23.202 - 00:21:48.034, Speaker B: And I'm particularly interested to start with the question around the social credit system. Again, back in this cat and mouse of where it's not working, what are the breakdowns of this? Maybe we start with the intentions initially, and then we talk about the breakdowns and where it really falls apart in practice. Either one of you can start with the intentions.
00:21:48.074 - 00:22:27.084, Speaker C: So I think we would agree on that. The intentions are very, very, very broad. It is a generalization of financial credit security to social credit security and building up of trustworthiness. So the scope is very, very broad, explicitly. And there are lots of documents that we both found and just did a publication on this where we could read the original material. So the scope and the intent is very broad. If you look at what came out of it, it is in some ways obscure.
00:22:27.084 - 00:23:19.514, Speaker C: It's hard to know what came out of it. It is a very, very big task. You said in your presentation yesterday, put up data about what's going on, where. So it's still a project at least. But what I also found was that there is actual propaganda and advertising in the public that say your social credit score is your second identity card, as the sonologist photographed in Changsha airport. So it's not a way, but it was intercepted by Covid. And there were much more effective technologies to hinder people to take on their rights with the health app, which is then by chance red and not green, if you want to go to that place to protest against loss of your fortune.
00:23:19.514 - 00:23:33.034, Speaker C: So there are much more efficient means. But there is still something in the background, and that will not go away and evaporate without chase, I think. I'm very sure about that.
00:23:35.634 - 00:24:12.240, Speaker A: We are in broad agreements. First of all, it's not going to go away. And I would probably just add on what I found personally amusing in different ways that the authorities themselves don't really know what to do with it. That's how I would argue it in some field work I did. If you talk to people in the financial system, they would tell you, what interests me in social credit system is credit rating, it's financial integrity. But other people in the state council or different part of the administration, the bureaucracy, would say, no, we would have. We could use it for broader regulatory purposes.
00:24:12.240 - 00:24:49.388, Speaker A: So depending to who you talk to, and after 25 years, nobody really know what to do with it still. So that is a reason why in the presentation I was really making the point that what really matters the most for now is this infrastructure that exists. And as you said, and I second this a lot on what you said, it is a more efficient means. I lived in China for most of the COVID period, so I had this health kit that we had to go around. And this was interesting because it was both a cut and mouse thing. So it was working broadly. But then, interestingly, each province and each city had different health kit.
00:24:49.388 - 00:25:19.970, Speaker A: So when I would go from Beijing to Changsha or whatever other city, my Beijing code would not be recognized there. So I would have to apply for different codes. And in the end, it's not sure how much the information would actually be shared between different systems. Although, and this is one of the points that you also made before on the role of big corporations. Most of these health kits will be by Alibaba. Alibaba was providing the same service to many different cities around the country. But then, for some reason, it was not clear how much the data was actually circulating.
00:25:19.970 - 00:26:09.870, Speaker A: And so when we're breaking down a bit, this monolithic idea of China, we see that different administrations have different views about certain systems we are facing in China. The same issue that we have in the west, with predatory companies that would be harvesting as much as they can information on people. Again, on social credit, for example, to continue, there is this famous sesame score that made the headlines years ago, that was made by Alipay. And the central bank wanted their hand on this data because it was like mass financial transaction data on the whole country, on the population. And Alipay has been fighting a tug of war with the governments for at least five years, refusing to end the data, and been telling the central bank, you don't even have enough people to deal with this data. You don't have the skills. We know how to do this because we've been doing this for ten years.
00:26:09.870 - 00:26:20.564, Speaker A: You guys don't know to do it. Why would I give you my data. So breaking the idea of the monolithic China is important to kind of get like the infighting that could be a different part of the state.
00:26:21.504 - 00:26:51.036, Speaker C: There's a small anecdote from Shanghai. There were those traffic signs where people were shown up if they crossed the road on red light. This is put down. Now. I had a look and now I know why. Because it is a typical computer problem. There was advertising on buses with a big face on it, say a businesswoman, and she got all those bad points because she was on the advert and always crossed the road and red lights for the pedestrians.
00:26:51.036 - 00:27:03.144, Speaker C: So this shows that it is far from perfect and working. And these are say, use problems of an overly complex thing.
00:27:03.704 - 00:27:51.914, Speaker B: Well, I mean, this is where AI comes in, right? There's the infrastructure, there's the data, it's overly complex in current system, human system breakdowns, but accelerating technologies will sooner than later solve some of these breakdowns. I think it's safe to assume, right, AI is going to enable different interest groups, whether it's the bank or other groups, to obtain what they need to obtain quite easily and maybe even implement or take actions on that. So I'm curious where it also breaks down more from. You mentioned an example of like a local gangster who had the worst credit thing. Maybe you just say that because I think that's an important breakdown. That's non digital, right? Non technical.
00:27:52.454 - 00:28:39.718, Speaker A: Absolutely. This is also a question I would ask everyone around this table is like, well, maybe we're more familiar in the west, but there is also, I mean, in China, there's for sure industrial lobby of surveillance that makes millions of hundreds of millions, if not billions, of just selling AI, whatever enabled softwares for facial recognitions. And here is where my expertise, my personal expertise stops. But when you read a little bit, it says that it's not clear how much facial recognition and I enable works that well and well. In a way, some companies have become extremely good at selling new production to local governments who are going to pay hundreds of millions. And again, this is where I would hear everybody else opinion, because I'm. Well, there is no argument that surveillance in China works very well.
00:28:39.718 - 00:29:06.404, Speaker A: And to a scary level, this is not a discussion but how much the new technologies that are being branded actually add up to the system that already exists or if it's also kind of inflated bubble, because we need to have AI product at the moment. So here I would be curious to hear others and what they say if this new technology really are an enabler or sometimes they both are enabling more efficient surveillance or also just some kind of cosmetic new product that we can sell.
00:29:07.424 - 00:29:48.524, Speaker E: Yeah, I mean, I think I broadly agree with what you're saying. Like, the technologies for civilians were already there for a long time and maybe social media was all along the biggest sort of surveillance technology in terms of like enabling surveillance. I think things like face recognitions have improved a lot in the past years, thanks to, say, the improvements in AI. Like, I don't know if you all saw, like, for example, how an AI can now solve captchas very easily. So these are things that are relatively new. So some improvements have come for surveillance, for example, just improvements in facial recognition. But I do agree that the technology has been there for a long time.
00:29:48.524 - 00:30:43.594, Speaker E: And if a government wanted to surveil a population like, you don't need AI, you don't need anything that is there at the moment. It was probably easy before social media as well, but with social media, messaging apps is just extremely easy at the moment. And it's all very shady as well. Even within Europe and within the EU, we don't have that much information about how data is being used. Even after GDPR and DSA and the AI act now, there's still a lot that is not known and something that I'm very passionate about. We still have very little access to social media for research, which is something that we used to have for many years. And now, even though the DSA actually mandates the platforms that are designated as online, large platforms have to give researchers access.
00:30:43.594 - 00:31:29.332, Speaker E: That's still basically not done and most platforms will still refuse access if you ask. So there's still a lot that we don't know and it's not improving. But I do agree that AI is at best like a small fraction for now. But then this could change. AI adds a lot to our predictive capabilities, which again, is something that's existed for a long time, but we can predict a lot better now, for example, your preferences that we could, before just using your data. So these things are improving and they could get better at personalizing, could get better at scaling. AI can make surveillance at scale less resource intensive.
00:31:29.332 - 00:31:40.224, Speaker E: So that's something that could change, could make it cheaper. So there's a lot there that could change. But yeah, the technology was there already and probably the intent is the intention to do it was there too.
00:31:40.384 - 00:31:41.168, Speaker B: Thank you.
00:31:41.296 - 00:31:42.104, Speaker D: Can I add this up?
00:31:42.144 - 00:31:42.576, Speaker B: Yeah.
00:31:42.680 - 00:33:27.778, Speaker D: I think in terms of sort of the role of technology, I mean, you mentioned about the sort of surveillance already done well before the technology, which is, if I use the examples, countries like North Korea, which is very true because they have the system where through which, you know, all the human beings managed to actually survey people through the planting the fear into people's head. So it's, you know, the whole idea of a panopticon. I don't know how much you're familiar with the idea of a panopticon where, you know, this Bentham's ideas about this prison cell where the people are kind of believed to be watched all the time when you don't actually know where the God is watching you or not. So this kind of a psychological kind of warfare is already there. And, you know, people, people cannot trust anybody. However, with the editors sort of technology, it can have a huge implication for people's human rights and their ability, because at the moment, for countries like North Korea, people can, with quite a lot of struggle and risking their lives, they can escape their countries, but with the facial recognitions and things like that, you know, I believe the technology at the moment, obviously, we've just started having the whole range of discussions around ethical implications of AI and all of this. But the speed of development has been really fast, and I expect it to be fast, and the technology will improve hugely significantly.
00:33:27.778 - 00:34:04.994, Speaker D: And with that, people's chances of being able to exercise that freedom, freedom of movement, freedom of expression, and freedom of liberty will be affected quite substantially. And that's a real worry, because the government's ability to be able to actually monitor and survey people will be better and better, and it's going to be really difficult. So I think we have to be very cautious and very worried about the huge implications for this kind of technology on individuals, really.
00:34:05.334 - 00:34:21.813, Speaker B: And today, like whether in Korea or the Uyghur population in China, are there examples of how any of surveillance is being used maliciously that you could point to?
00:34:21.973 - 00:35:20.094, Speaker A: Absolutely. I mean, this is a thing that should be always said, clearly, as much as I spent a lot of time deconstructing some ideas about China, looking at what happened in Xinjiang and the mass concentration of Uyghurs population that happened mostly between 2018 and 2020, 2021 was absolutely an example of how a state can become repressive. And interestingly, it was both done by an incredibly large expansion of digital surveillance with density of camera and gateways and mandatory phone scanning at different checkpoints around in the cities. But also, as you mentioned, and I think that North Korea would be an interesting example also human surveillance and the eyes of the others and denunciations or whatever. And when you combine both, this is where you see how terrifying it can be.
00:35:20.954 - 00:36:25.966, Speaker C: I would like to bring back the characteristics of large language models and this kind of AI to this topic. If we imagine that these kinds of AI's would be involved in mass surveillance, then their outputs would be very much convincing. It would actually really look that I broke in a jeweler's shop because that guy looked like me, actually, and it would fit to my habits. And it would be so convincing because these hallucinating things, the storytelling without any grounds, factual grounds, is built into those systems. This is the absence of reason for nunft that there is. It is so convincing, but it could be wrong. And if you think that there is mass surveillance with lots and lots of extremely convincing evidence, that is plainly wrong, because it's by statistical means, say 20% is extremely convincing, but also wrong.
00:36:25.966 - 00:36:44.986, Speaker C: What could we do with that, really? I'm really scared for this because there is no reason for nunft to check it, and it will be. And this is the very thing these machines do, extremely convincing, especially because they.
00:36:45.010 - 00:37:04.752, Speaker B: Offered you a bargain deal at the Kabob shop right next to the jewelry store. And you went there and your gps data, you know, points to Martin as he got a kabob and some jewelry. And so he's gone for life. Now, Jude, you want to talk about some other human rights intersections and your concerns or views?
00:37:04.928 - 00:38:54.542, Speaker D: Yeah, I mean, I mean, beyond, to be honest, the examples of North Korea, I mean, you know, the list of human rights violations are, you know, quite endless. So I don't really have to go into details, but in terms of the implications of AI and the technology, one of the things in relation to what Martin just mentioned is actually it will come to the crisis of identity at an individual level, but also humanity level, because we are starting to have questions around what it means to be human and that we are raising those questions. And with the development of AI and with the sort of speed of development, that will be where, what is the sort of the line between differentiates between humans and the machines, but also when it comes to, you know, AI has been used on a much more regular basis, and I'm confused with my replica of my version, that really raises questions around who am I and who are we? So I think, you know, there are whole range of ethical but also humanist questions. We really have to think very carefully about, you know, how we are going to use this technology and what are the kind of consequences, quite often something unforeseen. Because I think, you know, whatever kind of a witness is, people seem to once get hold of something. There are always this minority people who are very much focused on the power, and then they are enjoying that exercise of power, and that can have a really negative kind of dangerous outcomes. So that's.
00:38:54.542 - 00:38:56.314, Speaker D: Something worries me.
00:38:57.094 - 00:39:28.294, Speaker B: Okay. We're gonna open it up for questions shortly. I just wanted to maybe lay out another framework, which is if door one is the centralized acceleration of power by companies and governments that leads to a sort of authoritarian dystopia. Right. Door one. Awesome. Door two is sort of a libertarian quote, unquote, democratization of all information.
00:39:28.294 - 00:40:02.650, Speaker B: Right. And all of the security goes out the door. Right. So whether that's bioengineering, we know weapons of mass destruction, that's now available with CRISPR, available to anyone with a garage and the basic chemistry set or any number of terrible things that people can do, that you have to try to protect your population as a government against. Right. You have to control for these really radical, extreme measures that are much more than just guns. Right.
00:40:02.650 - 00:40:13.970, Speaker B: That can happen harms, that can happen public. That's door two. Right. You don't control for that. Right. That's sort of a catastrophe too. Right? That's not dystopian, it's catastrophe is door two.
00:40:13.970 - 00:40:45.612, Speaker B: So we're really looking for. Okay, door three. What's door three here, right. Where is what are, or four or five. And I don't think we have the answers to what door three or four or five is, but. But it's quite serious. I continue to try to elevate the level of the seriousness of the conversation for the developers and the builders and the people in this community, because in some ways, I don't think they're hearing the contextual, the alarm bells that how critical some of this technology may actually be.
00:40:45.612 - 00:41:29.834, Speaker B: I would add just one more tension as a summary before we go to question answer, which is the accelerating pace of technological development versus democratic institutions, speed at which they can regulate and keep up with information is also a very difficult bridge. And the other tension being information disinformation, common, the commons of democracies being able to make sense to steer through this accelerating, confusing, complex world that we're in. So not to end it on a bummer note, but those are the tensions, I guess I'm seeing. Any other major tensions that you all feel like you're wrestling with or see when you look at this landscape? Anyone? Yeah.
00:41:31.254 - 00:42:39.322, Speaker A: No. The way you summarized it is, I think, pretty compelling. Again, I have only a limited expertise, mostly on one area, but I think it's also interesting what I always find amusing, and it's probably the same here is the, how would such question play out in a country that is by definition authoritarian when the counterpart doesn't exist? And so it's only internal checks and balances between different parts of the administration. If you look at the whole AI regulation discussion in China, which I'm only very farly familiar with, there is interesting debates between the more academic or maybe scientific communities that would want to engage and will engage politically with the rest of the world and more, maybe more part of the bureaucracy that are viewing pretty much everything from national security lens. Another example like this that is very brief. Well, last year or two years ago, China implemented basically control check, or like a capital, like a data check that couldn't be exported freely. For companies, it was a huge issue.
00:42:39.322 - 00:43:35.386, Speaker A: Let's say you have a company that have R and D in China and you were in theory, not supposed to transfer much information that you do in your R and D to maybe your headquarters in Europe and the US. Of course, this created a gigantic backlash because foreign companies were like, what? Or even chinese companies were like, what is going on? Interestingly, what came out after a year is that they completely relaxed these controls, realizing under the pressure of the business environment, but also realizing that the administration in China charge of this simply didn't have the number of people. This is also maybe, I don't know how much, again, I don't know how much AI will automate on not certain tasks, but when it comes to the regulator, even if you're the national security oriented regulator and you want to check everything, maybe you're just not going to have enough people to do with this, because in the end, you still need bureaucrats, you still need civil servants or engineers, and there will be only a certain amount of them. So maybe that's going to be just a little signal.
00:43:35.410 - 00:44:26.274, Speaker E: Yeah, yeah, I think I agree. And I think it connects to the point on how quickly regulation moves compared to technology. And I think that will be a very relevant problem in the future because technology is just increasing in pace and regulation just can't keep up. There is a monetary problem where the people you would need to get to work on these technologies from a government perspective are often too expensive for a government. Like, imagine from a large language model perspective, like if you're an EU regulator, you're given access to, I don't know, the insides of GPT four. Like, what can you do about it? They wouldn't have the expertise to start looking into it. So it's just like regulation moves slowly and often doesn't have the means to actually look into these things and I think that will be an important trend for the future.
00:44:28.274 - 00:44:29.654, Speaker B: We can open it up.
00:44:30.674 - 00:45:17.246, Speaker C: I would have one more suggestion. These systems look intelligent. These AI's look intelligent because intelligent people interact with them. If you let them interact one with another, it very quickly is shown that there is a lot of nonsense coming out of it. So there is also this two parts, these two parts in conversation. And it would be really necessary to flag everything that comes out of those systems which are really important, really handy, and a big surprise for many people. Also for me, if you would flag everything as something that came out of those types of machines, because then we could learn how to deal with them.
00:45:17.246 - 00:45:48.264, Speaker C: So we learned how to deal with other digital stuff also, which mimics reality. But this is a new kind of stuff. And if I knew that something came out of those things without reason, without then I could think of it and deal with it in the proper way, then it could be handy and I could be skeptic. So this would be a wish of regulation. Flag everything as being coming out of that explicitly.
00:45:52.444 - 00:45:54.544, Speaker B: Okay, any, any questions?
00:46:07.564 - 00:46:50.064, Speaker F: So yeah, you've talked a little bit about regulation, and I want to kind of move that around because we're developers. And the problem with regulation that you pointed out is that it's low, but that's probably good because it's also incompetent. You know, if you look at the pharma regulation, it hasn't protected us from fake drugs, it hasn't protected us from side effects. It hasn't protected, like, just hasn't worked. So I'm glad that regulators are slow. But also we're confused between regulation and legislation. Right? Like it used to be that doors would never fall off of airplanes because there was regulation.
00:46:50.064 - 00:47:20.734, Speaker F: Like suing people after the fact is not regulation, that's enforcement. And so what would you suggest to us as developers as we're developing our technologies in this age of mass surveillance, in this age of technology, which has more unintended consequences than intended consequences. If we actually wanted to self regulate as an industry or as developers, because we know we can't count on those authorities, what would you suggest that we be doing?
00:47:22.154 - 00:48:19.964, Speaker E: Yeah, I think, I mean, first of all, I do get the point. And I think the regulation is always a very tricky balance between, like, stifling innovation and actually making societies safe. And regulators should be competent. There's no excuse not to be. And we hope that regulation should be competent in what they do. I'm mainly talking about how quick it is for high risk scenarios, which there's a lot of technologies, particularly dual use technologies, that can have very large scale, high magnitude risks, and these are the ones that should be regulated quickly and with expertise. I think my advice to developers, which is something that I hear a lot from my AI ethics colleagues, is just develop technologies thinking about what some of the downstream implications could be, which is something that surprisingly does not happen a lot, especially in AI.
00:48:19.964 - 00:48:55.284, Speaker E: A lot of computer scientists don't think about the implications of some of the products that they have developed, particularly in AI, particularly larger labs. And I think so my only advice could be to think about what could go wrong and have that kind of mentality, where I do really see a divide between people that think about building and people that think what could go wrong about that technology. And it's a fair division. But I do think that it would be helpful if developers had some thoughts about potential impacts of releasing any technology into the world as well.
00:48:58.864 - 00:49:03.604, Speaker B: Anyone else wants to answer the question? Any other question?
00:49:08.704 - 00:49:19.944, Speaker A: Hello, and thank you for the talk. My question will be, what could be your definition of digital freedom and what it will be in front of the evolutionary of AI in the next years?
00:49:25.364 - 00:49:32.464, Speaker C: Digital freedom, I would insist that it's not different from the other, from the other freedom we have.
00:49:37.684 - 00:50:25.872, Speaker E: I can take it. I think my definition of digital freedom is that we know what our data is used for and we are in charge and control of our data. I think today that doesn't happen a lot. There's a lot of implications that we're not aware of. And we see, for example, with data used in AI for very responsible purposes, which could be detecting military targets through AI predictions. There is a lot that is being done, it shouldn't be done, and that's not the purpose that the data had originally. And I think digital freedom for me is really about knowing what data is used for and being in charge of the data that I produce and knowing that one day will not be used to know where I live for like war purposes or for targeting purposes.
00:50:25.872 - 00:50:29.084, Speaker E: I think that would be a good definition of, for me, that freedom is.
00:50:31.424 - 00:50:38.954, Speaker B: Awesome. I think we are at time, unfortunately, lots of interesting things to think about. Thank you everyone for joining. And thank you.
00:50:46.614 - 00:50:47.670, Speaker D: Can you go?
00:50:47.862 - 00:50:51.014, Speaker C: Did you know beforehand that we would walk on this carpet?
