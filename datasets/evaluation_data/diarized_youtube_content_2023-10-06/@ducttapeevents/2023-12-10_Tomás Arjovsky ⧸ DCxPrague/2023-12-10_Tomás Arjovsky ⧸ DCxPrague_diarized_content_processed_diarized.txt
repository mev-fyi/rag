00:00:00.600 - 00:00:30.354, Speaker A: Thank you. Hello everyone. So my name is Tomas, the same that he said. I'm going to be talking about a new consensus node today, which we've been working for the last four months. So what are we going to be talking about today first? Well, this is our agenda. We will be talking about why do we want to node, why we'll be doing elite analytics here, doing some very quick technique. Well, not quick, it's actually half of the talk, a technical overview and then our journey and what we learned.
00:00:30.354 - 00:01:11.310, Speaker A: We're not going to focus too much on the internals because I expect that this environment, probably you will find that a little bit boring. Probably you already know about that. So I will mostly focus on what LDxir brings to these kind of challenges. So first, let's start with why. These are our two main reasons. The first one is client diversity, which is the most straightforward one at this moment. I think that the biggest client in consensus that's been used is prism, with about 50 or between 50 and 60% of the network.
00:01:11.310 - 00:01:43.232, Speaker A: And we want to keep improving that, building new consensus clients. We also talk about language diversity, both talking about elixir, which is a new programming language, and also new languages in human languages. We want to try. I'm from Argentina. Most of the core team is from Argentina, which is, that's Buenos Aires marked in a map. And the main idea is that we also want to make this accessible for people that are outside of the english speaking world. Many people talk English in Argentina, but not all of them.
00:01:43.232 - 00:02:13.884, Speaker A: So we want like to welcome Latin America to the core development. Latin America usually consumes ethereum in a lot of forms and ways. For instance, using stable coins to counter inflation and that kind of stuff, especially in Argentina, that's something that we know very well. But at the same time, there are not so many core developers in there. So we want to have some teams that are not based in Europe, in Australia, in the US, et cetera. So this is who we are, this is a core team. That's me, that's Martin, that's Thomas, and that's Paul Henry.
00:02:13.884 - 00:02:41.054, Speaker A: The three first are from Lambda class, which is the company I work for. Paul, we met him in the ECC in Paris, and he just decided to join us. The four of us are Ethereum protocol fellows. We got accepted for the fellowship and we've been working on this project for the Ethereum protocol fellowship. So like big shout out to the Ethereum foundation for supporting this project. These are our contributors. We are very thankful to get so many collaborators.
00:02:41.054 - 00:03:06.618, Speaker A: So let's actually start with the talk. Why elixir? Elixir has many features. Elixir is the language that we chose. It has many features that are exceptionally good for this kind of development. Who here knows about Elixir? Can I see some hands? Okay, three hands. That's more than I expected, one at most. So basically, Elixir is like the new version of Erlang.
00:03:06.618 - 00:04:19.394, Speaker A: It's slightly different, but they run on the same virtual machine. It doesn't eliminate its predecessor. Erlang is still a language that's being used today in very high load applications like Discord, WhatsApp, high frequency trading and high frequency beating and some stuff like that. That requires a lot of requests per second, because the main point of these languages is that they want to have predictability over different processes. So the idea is not that it will be the most efficient at each of the requests, but it will give you some predictability that there is not going to be one request hoarding all of the cpu, that's the main advantage of these ones. And when we basically have, I don't know, hundreds of requests per second from non trusted peers, then we want to be sure that none of those requests horse all of our cpu. It also has fault tolerant processes, which kind of makes this a little bit better, because if, for instance, some of you send me a new block over the gossip protocol and that block is malformed, I don't want my whole application to crash down.
00:04:19.394 - 00:04:46.761, Speaker A: And it's functional, which is very nice for state transitions and pure functions. We'll see a lot more of these later. So let's give a quick technical overview of everything. So this is kind of how everything works, right? Execution client, consensus client. These are the peers which are horrifying. I didn't find a better emoji for this or whatever. These are the gossip protocols, and basically, well, this is like the most known part.
00:04:46.761 - 00:05:23.820, Speaker A: We have the EVM state, which is kind of our database. This is what we represent. And we have the mempool, which is where transactions are temporary stored before processing them into the AVM state. There's communication over the engine API. And then we know that the goal of the consensus client is kind of to maintain the beacon state, which holds the validator set. Like who can actually send a transaction and enable validators to, well, send new transactions, enable just full node users to receive blocks from the blockchain and process them locally. But what happens here? What is this? We don't have a mempool in consensus.
00:05:23.820 - 00:05:59.904, Speaker A: So what do we have here? Well, here we dragons and basically this is like the, I won't go into too much details, but this is the basic flow. Like we get a new block, we do all of these things. Like we decompress it, we deserialize it, we validate it, we save it in a local database. Then we calculate the state transition, because basically a block represents that, a state transition between two different blockchain states, and we save the new state. So this is all tasks that can be done individually for each block. Like if I get block one, I can do all of these. And if I get block two, I can do all of these.
00:05:59.904 - 00:06:31.724, Speaker A: And those two tasks don't need to know each other. Those are completely independent, those are per block. But eventually I want to not only save these, like save the block and the new state locally, but I have a fork trace tree here. And not only the tree, but I have the latest messages, I have the tree weights, I have to perform fork choice. And these values are global. So basically in this world we're all happy because we basically are dealing with, we are dealing with immutable states, and we are dealing with stuff that's per block. And here we're dealing with global states.
00:06:31.724 - 00:07:02.174, Speaker A: So this is where everything starts getting started getting busy. But also there's not even that happy path because in here, validations are not exactly the best thing because basically we sometimes need some state for the validations. So how do we do this? Some classical languages use tools like mutexes. Elixir has something that we consider better, which is gen servers. I want to see how much time do we have? I'm getting. Okay. Oh, that's all right.
00:07:02.174 - 00:07:56.828, Speaker A: So basically, how do this work? Basically, in like the most basic units outside of course, like variables and stuff. The most basic execution unit in Erlang and elixir is called a process. So let's imagine each of these circles is a process, and they basically communicate between each other with calls and responses and that kind of stuff. But you could be wondering, okay, let's imagine that this process is my fork choice store, right? But what if I receive multiple calls? What happens? Do we need a mutex? Well, fortunately we have something called genservers. And aside from genservers, Erlang and Elixir have these internal queues. Like each process in Elixir and Erlang has their own queue called mailbox, where they receive the messages. And then the only thing that I need to define in the gen server is how I react to each possible call that I can receive.
00:07:56.828 - 00:08:22.574, Speaker A: So this is a functional call. Basically I say, hey, if I get a message like this and I have this state, then I will issue this response. And this is my new state. And that's all I define. Like I don't take care of any kind of concurrency, I just define this as a state machine. I define the state transition and that's it. So basically this allows me to build pure testable and not worry about concurrency state transitions.
00:08:22.574 - 00:08:56.472, Speaker A: So I don't know if I sold you the functional programming yet, but this is one of my goals today. And basically this is how we use it in our node we have for instance a new block processor. This means that for each new block I'll basically be running a new task and each task will run in parallel. And I will be doing so because running processing elixir is very very cheap. It's not like running an OS process that takes many megabytes and it's very heavy. This is just basically free. This basically calls an adblock.
00:08:56.472 - 00:09:54.638, Speaker A: Adblock seems like a normal function, but it's actually sending a message to the mailbox of the fork choice store. And basically what this does is it just calls a function that says hey, I'm calling the function like the remote function, add block with this block. And internally the forktraystore knows that it will start with this state. In this case we won't be returning anything and we just will transition to a new state, which is awesome because now I can perform validations and mutate the fork choice store atomically. Like I don't need to take care of, hey, what happens if I receive some state to perform a validation and then I send a new state to the forktree store? Then I might be like the state that I used to validate. This might already be changed by the time that I send the second message where this doesn't happen here, because this is just one function. And the great thing about genservers is that we basically perform this, get one message from the mailbox, react to it and then get the next message.
00:09:54.638 - 00:10:15.094, Speaker A: So this is all sequentialized. All right. And there's also a second use that we use for genservers. I didn't actually say what a genserver is. A genserver is just a process that implements this handle call handle cast interface. It's called genserver because it's literally a generic server. That's something, that's a tool that the language provides just from the get go.
00:10:15.094 - 00:11:08.098, Speaker A: It comes with servers, you don't need to do anything over it. Yeah, so basically we can hack this into building cron jobs because, for instance, a process can send a message to its own mailbox, and after a while it can get its own message. So basically we kind of build a cron job or a daemon or whatever you like to call it. Basically we use a primitive called send, after which we use to send a message after a certain amount of milliseconds. And how do we use it here? Well, one of the problems we used, we use this to solve is the pending blocks. For instance, someone sends me a block, but I don't have the parent, and if I don't have the parent, I cannot add it to the forktrace tree. So what we do is we have a single gen server that periodically it does two things.
00:11:08.098 - 00:11:44.258, Speaker A: It maintains a set of the blocks that do not have parents yet, and periodically pulls peers for their parents. Why don't we just do this on demand? Like why don't we do this each time a block arrives? Well, two reasons. One of them is spam. We can receive an enormous amount of blocks, and we don't want to instantly start requesting all of their parents because we may never do it. And at the same time it's just a bandwidth issue. Like we don't want that to hold all of our bandwidth. We can have this in a more controlled manner and just like wait, every 1 second or 2 seconds I can just send like 100 blog requests and then try again a couple of seconds from now.
00:11:44.258 - 00:12:12.234, Speaker A: And that's it. So what, that has been like a quick technical overview of why elixir is so great to build this kind of stuff. It's a very fun language. We've been enjoying it very much, and now we're going to a more like human part of the talk, which is what has been our experience building a consensus node. So basically this is what we have, and this is what we don't have at this point in time. We started four months ago, so bear that in mind. And we started with four people and then some collaborators.
00:12:12.234 - 00:12:52.302, Speaker A: We have 80% of the switch transition. We have checkpoint sync, so we can get the current, the latest beacon state and start getting blocks from there until the current slot. We already have here, our networking, decompressing, digitalization, et cetera. We already have implemented the fork Tree store, the LMD Ghost algorithm, Casper algorithm, which is basically fork choice and finality. We have implemented these handlers which are on tick, on block, on attestation, and we have some level of observability with Prometheus and Grafana. So basically we have like most of our, of our structure done but we're still missing some key stuff. We don't yet have validators integrated.
00:12:52.302 - 00:13:35.560, Speaker A: Like you cannot just plug in a validator yet. At the same time we haven't like we have tested with execution clients, but we haven't tested like for long periods of time. We haven't set up a single client like a full node running for days to see how it performs. We don't have our own lib two p implementation yet, which is kind of one of the saddest things yet because I'm actually going to talk about this in the next slide. And then we want to have a local testnet for automatic integration testing, like having maybe ten nodes in a local computer that don't need to sync with the full blockchain in order to just test an integration suite. Yeah. So what about lib p two p? So basically this is our problem right now.
00:13:35.560 - 00:14:48.568, Speaker A: We are currently using Goleap B two p, which is great, but as it's in a different language and it has many blocking calls, et cetera, we need to run it in a separate process. It's controlled by the beam. It has like, I'm not going to go into details unless there's extra time and someone wants to know about it, but basically we have something called ports in which we can have a controlled external process that if this dies, it doesn't kill all of this machine, all of this process processes, but this one actually can react to this one dying. So the problem that we currently have is that go has very good concurrency model, which is go routines, and elixir has a very good concurrency model which is processes. The problem is that to communicate between the two OS processes, we have just a single file descriptor which is, it's just a bummer because it's just like a basic pipe and that's it. So here we have a beautiful world of one go routine per topic, and here we have a beautiful, even more beautiful world of one process per message that we get. But in the middle we just have one single route for all of that, and that's like a huge bottleneck.
00:14:48.568 - 00:15:33.754, Speaker A: So we want to re implement all of these parts ourselves in elixir just so that we don't run into this and it will be way more performant and we actually take advantage of most of elixirs features. And you would maybe ask why did we not do it? Well, because we only had four months until now. And basically the issue with networking is that if you want to implement all of the networking stuff by yourself, and if you have that you can not even start with development. Like you can start doing some basic, say, transition tests, running the spec test reading, but basically you can't do any real integration. You cannot communicate with a network. So basically we chose, okay, we have to go some ground in the middle. We'll just use the go implementation for now, and eventually we'll do our own.
00:15:33.754 - 00:16:08.346, Speaker A: So what did we learn? This is our current state. What did we learn in these last four months? So this was originally our plan. Our plan was to be always production ready since day one, which is kind of a philosophy that we usually share in the company. Being production ready always basically means that you start with us walking skeleton. You have all of the integrations ready and then you just start adding stuff on top of it. But at each stage, at each sprint or whatever you want to call it, you have a functioning thing, you have something that you can make run, and then it just works. It does something.
00:16:08.346 - 00:16:31.060, Speaker A: It may not be a node yet, but it does something and it works. Well, what actually happened is that we started reading the specs, which was great. Then we implemented, we actually did this. We implemented an end to end working skeleton. We integrated networking, decompression, deserialization, et cetera. So things were going fine. Then we read more spec tests, sorry, more specs, because the specs are great, but they're also huge.
00:16:31.060 - 00:17:04.228, Speaker A: And actually we found the spec tests, so we were extremely happy. Like we didn't only have specs to follow, we even have tests that went with that. And this is where actually the problem started, because we went into full state transition implementation frenzy, we just became crazy. And we started implementing all of the state transition and we forgot about being production ready, because it's just unit tests, it works. You can implement functions and everyone is happy instead of the node. Like the node is not happy, you're not integrating anything, you're just building unit tests. So ideally this works, but it doesn't.
00:17:04.228 - 00:17:46.098, Speaker A: Of course, we didn't do production readiness. And there's also a lot of other problems that are not covered in spec tests, because basically spec tests are about state transitions and operations and about mutating state. But you also have a lot of different issues that are not even in the specs, let alone the spec tests. For instance, peer scoring, like peers stop sending you messages after a while if you don't have certain request response implemented. Request response pattern in itself was something that was not so straightforward and not so mentioned in the specs. It is there, but it's not something that's prioritized so much because it's not where you get blocks or you get transactions and then, yeah, blocks with no parents. That was a bummer as well.
00:17:46.098 - 00:18:18.262, Speaker A: So what happened? So to talk about a bit more about the specs, this was one of our biggest learning. This is a huge resource. It's great to have this, because if we didn't have this, it would be very hard to build it. The specs were very clear and they have code examples in Python, so they are like, you basically have to translate code, which is great, and it's self documented and it's also testable. Like, the spec tests are a great thing. The real cons, like, I'm not even going to talk about what will happen to us, which is going to spec test frenzy. That's something that happened to us.
00:18:18.262 - 00:18:54.014, Speaker A: It doesn't need to happen to everybody. But outside of that, the disadvantages of the specs are that they are in no particular order and they are diff based. They say, hey, for phase zero you have these specs, for Bellatrix, you have these specs, for capella, you have these specs. You don't have a single consolidated current state of affairs. So if you want to build a node, this is what you have to do. You have to go through all of the blockchains, forks that happened as if you were a node yourself. And the specs are not supposed to be read by the node, it's supposed to be bred by humans.
00:18:54.014 - 00:19:24.934, Speaker A: And there's also no human friendly introduction which is related to this being in no particular order. There's not something that tells you, hey, this is a consensus client and these are the components. And this is the typical workflow of block. I'm not saying that this is anybody's fault. This is some things that would help a lot if we had them. And what helped us a lot was the east to book probably a lot of you know it. It's by Ben Edginton.
00:19:24.934 - 00:20:04.380, Speaker A: I hope I'm pronouncing that right, which basically is the missing link. It's basically like a human readable version of the specs, and it also actually includes a consolidated version of the specs that is not div based, which is just amazing. So we ended up using this a lot for our implementation. So highly recommended if anyone's going to go down the rich rabbit hole of building consensus clients. So what are our next steps? Well, finish the MVP, of course, that's the obvious one. Then we have a documentation goal, which is having a tutorial of make your own node. The idea is that we suffer through this.
00:20:04.380 - 00:20:34.480, Speaker A: We don't want anyone else to suffer through the same thing. So the idea is to build our documentation in a way that not only explains how our node was built, but also explains how you could do the same if you wanted to build a node. That's kind of the goal, because this is even a study method. If you can explain it to someone else, then you actually understand what's going on. So this is also for ourselves, not just for the world. We're not that altruistic. And also this is one of the main things for after we finish all of this, which is fear ad customizations.
00:20:34.480 - 00:21:28.472, Speaker A: Like what's the selling point of this? For now, our main selling point is that this is supposed to be very robust and very stable, and elixir helps a lot in that. But what's our selling point? Do we want to go full Lido and support dVts? Do we want to do set gate customizations? So that's where you come in. So basically, yeah, this is definition of definition, place of the talk. But if any of you have any ideas, any customizations that you or your company needs, if you have anything that you need and there's no consensus client out there for you doing the features that you need, well, hit us up, we may figure out something. So this is our contacts, this is the consensus telegram group, this is the repo if anyone wants to check the code. And this is just my telegram if anyone wants to send me a message. So yeah, that was it.
00:21:28.472 - 00:21:35.804, Speaker A: Do we have any questions? Yeah.
00:21:36.784 - 00:21:58.132, Speaker B: What would be actually like the idea of the advantage of this client over others? And for example, if I starting my new validator, what would be the reasons? And in which case I would like to choose your client over something existing.
00:21:58.308 - 00:22:04.508, Speaker A: As of today, don't choose our client because this is not implemented yet, but in the future, ideally, of course the.
00:22:04.676 - 00:22:16.682, Speaker B: Target audience, like those who run it on single board PCs or those who run it on big servers or like what's the idea? What's the target group of validators?
00:22:16.778 - 00:23:13.988, Speaker A: No, of course for the basic, this is not a big flashy selling point, but we want to build this as robust as possible and elixir and the way we built it kind of guarantees us that we will be able to maintain very good attestation duties and validator duties and that kind of stuff. So our main selling point for now, because we still lack some of those, is like predictability, robustness and observability. We are building an observability stacking Grafana and Prometheus you can already see a lot of stuff going on, like what are the most common peer rejection causes and all of that kind of stuff, which is nice. And we will be continuing to expand that. The idea is that you can just open the Grafana server and check all kinds of metrics about your server and kind of be able to diagnostic if anything is happening. That's more for the node operators and that kind of stuff.
00:23:14.076 - 00:23:28.628, Speaker B: And theoretically in terms of resources, it will be more efficient, like in terms, I don't know, like of memory consumption, storage or cpu consumption. Or will it be more efficient somewhere than what exists today?
00:23:28.716 - 00:24:04.344, Speaker A: It should be more efficient in bandwidth and basically it should be more, it should be more reliable. So we are not focusing on making it storage light or anything like that. We're focusing on making this fail. The less possible individual processes can fail, but your application will hopefully never fail. And you will be always able to. Always is a big word, but you will almost always be able to missing the english word, but to fulfill your attestation duties and your proposal duties, etcetera. So that's a basic idea.
00:24:04.344 - 00:24:06.896, Speaker A: It's not a big selling point yet, but it's what we have.
00:24:06.960 - 00:24:29.404, Speaker B: Okay, last one, since you said that about robustness, do you think about some kind of built in failover functionality? So I basically, I have like actually two nodes. One is active, one is space passive, which becomes active in case the active goes down. So and so I will not be slashed for using the same key.
00:24:30.024 - 00:25:03.482, Speaker A: That's a very interesting .1 of the beautiful things about elixir that I didn't go into because we didn't have that much time is supervisors. Elixir has a built in failover mechanism, which is you have, supervisors are basically processes that supervise other processes. And if those go down, they basically restart them depending on the policy that you give them. And you can build relationships between processes saying, hey, if this fails, then all of it, siblings should fail as well, but the parents shouldn't. And that kind of stuff. And that's one of the main fault tolerance mechanism that elixir has.
00:25:03.482 - 00:25:05.506, Speaker A: And that's why we think that it will be very.
00:25:05.570 - 00:25:06.706, Speaker B: You plan to use it?
00:25:06.850 - 00:25:07.930, Speaker A: We are actually using it now.
00:25:07.962 - 00:25:08.210, Speaker B: Okay.
00:25:08.242 - 00:25:24.998, Speaker A: Okay. We are going to fine tune it so that you don't like, you never have issues like the one you mentioned, like getting slashed because you are using your validator keys in multiple places. But yeah, we are already using that. So we expect that to be in place for when this is finished. Yeah.
00:25:25.086 - 00:25:26.198, Speaker B: Okay, thank you.
00:25:26.326 - 00:25:41.214, Speaker A: Thank you. I don't know if you have more time for any more questions, because it's also. Yeah, it's 56, and there's Antok next. So I think that we should wrap it up right. Okay. Thank you very much.
