00:00:09.000 - 00:00:53.000, Speaker A: Hello everyone, I'm Dablion Cordet Pat Ethereum. I'm building the lighthouse, the raz clion in Sigma prime. Thank you for coming. Even though it's raining and we just had lunch and yeah, just to make it more fun, I'm a lion today with a tail. So today I'll be talking about scaling, scaling Ethereum and this really exciting feature that we're bringing very soon that's called peer das. So just to make sure that we are all on the same page, I'll try to bring everyone that are really experienced with sharding people that don't understand yet what we're doing. So first, just to define what is scaling, scaling means increasing the capacity of the chain to make transactions cheaper.
00:00:53.000 - 00:01:39.364, Speaker A: Right now, as you know, Ethereum has very expensive fees and that restricts the type of activity to this hyper financialized, just basically training. If we manage to scale ethereum to a very high degree, we will allow other types of activity to be possible in the chain. Things that are more social, more like social networks, gaming, etcetera. The path that ethereum is taking, the big roadmap goal is dun scaling. I love the name, it's such a meme. But we'll just run through it and justify why we're doing this. So if you have been following ethereum, there has been a big shift in the roadmap in the last two, three years that's called the roll up centric roadmap.
00:01:39.364 - 00:02:20.303, Speaker A: So we have figured out that roll ups are the way to scale ethereum. We would offload computation to an l two and then let l one just scale data with sharding. So let's justify a bit why this is the case. Consider ethereum, bitcoin networks that value decentralization, and they achieve that by limiting the total capacity of a node. That allows any participant to run nodes at home with very constrained environments, but that limits the total throughput of the chain to these very constrained nodes. Whoops. Test, test.
00:02:20.303 - 00:04:48.444, Speaker A: I can get a mic otherwise. Whoopsie. All the audio straight. The electricity is gone. Yeah, sure. Because that lion. So my zodiac sign is lion, and from there it just grew.
00:04:48.444 - 00:06:45.476, Speaker A: I was doing music before and my artist's name was also lion. Yeah, no. Okay, I hope you enjoyed the pause. It was a schedule, unexpected. So rewind back. We were talking scalability, why we're doing roll up centric roadmap and just going through why that's the right option. So ethereum, we have this self imposed limitation that we try to make nodes really easy to run by limiting the total amount of work they have to do.
00:06:45.476 - 00:07:45.888, Speaker A: But that in turn limits the capacity of the chain and leads to these very expensive fees. If you want to scale, we could do some strategy by some chain that I'm not going to name, but we could just make what that node has to do, just more resources and that scales the network that works. That's a valid strategy, but it sacrifices on decentralization, less and less people can run nodes, it becomes more expensive, more bandwidth. So you tend to centralize in data centers and some very few professionalized participants, they can start to, then if they want to, they can capitalize and do bad things like 51% attacks and like minting funds, but bad things, if that happens, then the, the social layer has to come and say, hey, that's not okay, we don't like that, and hold the chain and do what's called a social recovery. That's kind of like the dao hard fork. That's really, really annoying. It's very expensive in terms of capital and reputation.
00:07:45.888 - 00:08:16.544, Speaker A: So ideally we don't want to do that. So the key to scalability is to automate that. And that's what rollups do. Rollups allow computation to be proved on chain so that invalid state transitions are impossible. Optimistic roll ups do it with economic games and validity roll ups do it with crazy cryptography. So that solves the scalability in the competition chain. But we still have data.
00:08:16.544 - 00:08:56.594, Speaker A: If data is not available, everything breaks. And here, when I say data, I mean transactions. Like any input that goes into the roll up, that could be state divs or deposit transactions. So why is that a big problem for optimistic roll ups? If you don't know these transactions, you cannot do this dispute bisecting game that we saw in the previous talk. Anyone could submit any set transition and it will just be valid because no one can dispute it. For validity roll ups, that's not possible because the transition is guaranteed to be correct. But you can put the roll up on a state that no one else but this malicious actor can progress.
00:08:56.594 - 00:09:38.076, Speaker A: It doesn't sound that bad, but it's really bad. It means that if you manage to pull off this attack, you can go to, hey you, I will only move your money out if you give me 50%, which is terrible. So now the role of l one becomes to guarantee data availability so that these attacks are not possible. And then you would ask, okay, who has to do these checks? And unfortunately, everyone. That's why this is such a hard problem. And what I'm going to explain now this is what I want to take you from this presentation. Is this slide really understand that? Because that's the key of everything, why we're doing sharding, why the role of centric roadmap makes sense.
00:09:38.076 - 00:10:08.990, Speaker A: So data availability is very problematic because it's a non uniquely attributable fault. So let's run through this. We have on the left a scenario where the proposer is malicious. It holds data, and then someone in the network say a slasher comes and points, hey, you are malicious, I'm going to slash you. But then immediately later, the proposer publishes the data. That's case one, and then case two. The proposal is non malicious, it publishes everything.
00:10:08.990 - 00:10:55.974, Speaker A: But the slasher incorrectly points that the data is unavailable. Then you, as an observer at t three, you have no way to tell these two scenarios apart. So who is at fault? You don't know, and that's why it's called a non uniquely attributable fault, and you cannot trust anyone else on data availability except for yourself. So that's why everyone has to do these checks. So the only way to scale data is we have to be able to convince yourself that data is available without checking all the data. And that's where sampling comes in. So the naive approach and what we have been doing so far is everyone downloads everything very easy scales.
00:10:55.974 - 00:11:13.832, Speaker A: It's safe. Well, it doesn't scale. That's the whole point. So point number two, we will not download everything. We will just download some bits of the data. And if those bits are available, then you say, well, some of the data is available, fine, this is terrible. This is not okay.
00:11:13.832 - 00:11:53.136, Speaker A: And it's because you don't have to convince yourself that some of the data is available. You have to convince yourself that exactly 100% is available. Even if a single byte becomes unavailable, all these nasty, nasty attacks are possible. You can break optimistic roll ups, you can break validated rollups just by holding this tiny, tiny, tiny bit of data. So this not safe. So what we're doing is idea number three, probabilistic sampling over extended data. So what we do is we take the data, we compute a polynomial, and we compute more points.
00:11:53.136 - 00:12:22.450, Speaker A: We extend it with this. Now only having half the data, you can reconstruct everything. So sampling becomes from a game on, number two of everything is available. Two half is available. And now this is possible. That's something that we can do, and it's safe and very, very, very scalable. So this new data, the extended data, we will not put it in blocks.
00:12:22.450 - 00:13:04.462, Speaker A: We put it in a blob is this something called a sidecar that has different properties and we expire them sooner than blocks, so we can price them differently. They have an independent fee market. And this construction, this is already live in Ethereum and it has enabled a bunch of scalability thanks to this independent fee market and faster expiry. So what we want to get eventually is to this dunk sharding roadmap where we have blobs with this cryptography, advanced sampling and very cheap. But figuring out the sampling part is very, very difficult. But rollups, rollups, one scalability. Now things are very expensive.
00:13:04.462 - 00:13:41.974, Speaker A: So what we have done and is live in the network since I think March is the then kun hard fork where we bring prototype sharding, it's just introducing the cryptography, the blob space. And thanks to this independent fee market and expiry, they have better scalability. I think as of today, blob space is already still free. The fees are so low because we haven't reached capacity yet. Cool. So far we understand why we're doing it, but now it's the time to actually do sharding. Prototype sharding.
00:13:41.974 - 00:14:20.984, Speaker A: It says sharding, but it has no sharding. Everyone has to download everything. What we're going to do now is start to shard. So we're just going to drop the requirement of everyone having to download everything. And what I want to show you with this picture is that the higher the sharding requirement, so the more that we dilute data among all participants, the harder things become. It's more difficult to find peers, it's more difficult to defend against attacks. When you go into these really hard regimes, things are really hard to do in a way that's like really safe, lifeless proof.
00:14:20.984 - 00:14:54.176, Speaker A: So initial steps, we're going to go face by phase, the initial phase with like a moderate sharding requirement. This is what we called peer Das and subnet Das. So just to understand you what PRDAs means, this is what we have today. We have blobs and everyone downloads everything with PRDAs. We would take these blobs and extend it. This is the part we were talking before with the KCG cryptography. And people, when they want to sample, they would choose random columns in this grid.
00:14:54.176 - 00:15:37.754, Speaker A: The ones selected in red don't load them, verify them. And then if enough columns are available, they can convince themselves that everything is available. Just to give you a taste, when we want to go to these very high sharding regimes, we go into this 2d matrix. It sounds really scary. We're not going to do all the details, but the important point you should do is here, red area, big. Here red area, small. That's better because it means that if you want to sample, you only have to download these tiny, tiny, tiny bits of data, and then you can make the block crazy big without breaking that rule that nodes have a constant load.
00:15:37.754 - 00:16:29.604, Speaker A: So to recap, sharding is hard because we have to do a lot of things that are network based, have failure rates, and we need to do them in a very short period of time, and that's hard. If we fail to accomplish the sharding requirements within these really tight windows, the network breaks down. We have safety issues, we can have liveness issues. It's more like real prone. So coming up with a way to do this crazy sampling and everything very quickly is hard, and we haven't figured out that yet. So, Piers, the key innovation of this design is accepting the complexity of sharding and doing something that's not as ambitious and reuses existing network components. So just to go over the flow of how it's going to work, we'll have the proposer.
00:16:29.604 - 00:17:17.530, Speaker A: The proposer will choose some transactions in the block, including blob transactions. It will compute this extension that we were talking about and then chop the data into columns and distribute to the custodians. We'll do that over gossips app, which is a really well understood and battle tested protocol that we use today in production. And then the new part would be the sampling, but we will do sampling, reusing again existing components that we know and love and are secure. So you would find your custodians with this b five. We will overload the node id with the initial bytes having some meaning of who has what. And then once you find your custodian, you query the data with the regress protocol, which again, really well tested.
00:17:17.530 - 00:18:01.386, Speaker A: We understand it. We have been using for a while. So far so good. The issue is that this initial design wanted you to complete proposal distribution to core students to samples in 4 seconds. That's really, really, really tight. And what's going to happen is that we will have some people sliding outside of this window, and the people that would slide here, they will be probably the solo stakers and those participants that don't have access to a high bandwidth, and they will be penalized for voting the wrong thing. So this is not great because we would be economically incentivizing people to move to data centers, and that kind of defeats the whole purpose.
00:18:01.386 - 00:19:10.254, Speaker A: So a massive simplification of the design thanks to Francesco was accept that this is not okay. And then we move the attestation deadline to the custodian section. Now from this line to here, that's what we call subnet sampling. You subscribe to columns, you get them from the proposer, and if you get enough of them, you consider the block to be valid. This is really weak compared to full time sharding, but it's sufficient if the cost of the requirement is high enough. So at this point you can be convinced that the block at the network level is available, but not at the local level, because your queries can be linked and the proposer can send you the columns that you are expecting, because you reveal that information and trick you into believing that the block that should be available is actually not. So that's why step number two, you still sample, but you only use that sample signal for the critical role of voting, for justification, transaction confirmation and proposing.
00:19:10.254 - 00:20:02.542, Speaker A: And because now you have a longer window, we can do that safely, even though you are a solo staker and you don't have a very fast connection. So so far, Pierdas progress has been really good. We like a bunch of core reps, almost everyone we met in Africa some weeks ago, and we hacked really hard on, on Pyrdas, and we went from really crude implementations to a working definite. So that was really exciting. We also decided all of these things, so we finalized the spec and I think now is in a very solid state that we just have to implement. Then these next months we're going to be refining the implementations, doing more tests, hopefully cleaning everything up. We have to do a lot of parameter choosing.
00:20:02.542 - 00:20:35.566, Speaker A: There are many trade offs that we have to do to ensure that sampling is safe and we have a good compromise between liveness, bandwidth consumption and safety. And then hopefully by the end of the year we'll have that in Mainnet. It's very exciting because piers has been considered for inclusion into the hard fork as of this Thursday. So that's hot news. And yeah, it's the giraffe fork, which is the tonneota. Cool. So yeah, I'm really, really bullish on pierdas.
00:20:35.566 - 00:21:00.614, Speaker A: I think scalability is by far the most impactful thing that we can do in Ethereum today. It's a democratizing force to ensure that all participants, rich and poor, can make use of this chain, that we allow more economic activity. So we should definitely invest as much resources as we have into making that a reality. And we will do that slowly, progressively, but still increasing decentralization step by step. Thank you so much.
00:21:06.474 - 00:21:13.494, Speaker B: Thank you and thank you so much for pushing through the technical difficulties. Appreciate that. I wonder, are there any questions?
00:21:17.114 - 00:21:26.984, Speaker C: Since you have to extend the block, how does that play into storage requirements or other types of problems?
00:21:28.164 - 00:21:54.434, Speaker A: So if you extend the blob, you're actually doubling the data. But because we are sharding, we are dividing the amount of data. So say that we have a cost of the requirement, that's a factor of eight of the block, then the actual sharding increasing would be a factor of four. So if we turn the parameters right, you as a staker should see your disk space requirement videos.
00:21:56.494 - 00:22:01.034, Speaker B: Wonderful. I think we do have time for another question, if there is one.
00:22:07.134 - 00:22:23.394, Speaker C: Thank you for the presentation. And does this also introduce new roles in terms of your presented proposer, custodian and so on? I mean new roles in terms of new. New client? No, this is baked into validator or full node software for.
00:22:23.734 - 00:22:45.438, Speaker A: Yeah, everything would be baked on the current software. You would still have to run the same software. They will just be doing a few extra things. So you will really notice if we tune it right. You even should not, as he was asking, your disregardment hopefully should go down. And your bandwidth depends on what we tune. But yeah, same software, same responsibilities.
00:22:45.438 - 00:22:47.994, Speaker A: You just have to do some extra work on the network.
00:22:48.374 - 00:22:49.314, Speaker C: Thank you.
00:22:50.094 - 00:22:55.686, Speaker B: Wonderful. So a round of applause for deployant. Thank you so much for joining us on stage.
00:22:55.830 - 00:22:57.070, Speaker A: Thank you for having me.
00:22:57.222 - 00:23:00.214, Speaker B: We're gonna be waiting for our next talk in a bit.
