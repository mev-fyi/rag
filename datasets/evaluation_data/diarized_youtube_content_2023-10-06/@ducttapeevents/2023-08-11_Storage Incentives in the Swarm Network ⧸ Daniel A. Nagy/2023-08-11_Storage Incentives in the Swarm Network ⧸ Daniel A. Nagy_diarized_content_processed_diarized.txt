00:00:13.960 - 00:01:08.274, Speaker A: So I'm going to talk about the storage incentives in the swarm network. That's not the only incentive system. There's also a bandwidth sharing incentive system, but I'm not going to talk about that one. So now I'm only going to discuss how the swarm network incentivizes nodes to store what they're supposed to store, and how it incentivizes users to only upload what they really want to be stored. So what is swarm? So there are three things that are called swarm, which is a decentralized storage solution which kind of encompasses everything. It's a network, a peer to peer network, and it's also a content address namespace. So in that way it's similar to ipfs, but it's dissimilar in that it also is a storage network.
00:01:08.274 - 00:02:02.584, Speaker A: So it actually stores data, not only indexes where everything is stored. So here's a quick overview of how swarm works. Very, very superficial introduction. So all data is broken up into small four kilobyte chunks. They are redundantly distributed among the nodes. It's all stored in a DHT, meaning that node addresses and chunk addresses come from the same namespace, and nodes actually store what is closest to them. So the content is divided up based upon the first few bits of their content address, and those bits determine which set of nodes is going to store it redundantly.
00:02:02.584 - 00:03:06.574, Speaker A: And these groups of nodes that are supposed to store the same content, we call them neighborhoods. So when I'm talking about neighborhoods of nodes, this is what I mean. Why do we need incentives? Well, first and foremost is to protect against spam. So before the times of the blockchain, there were several attempts at creating distributed content, addressed storage networks, and basically all of them were swamped with frivolous content that nobody really cared about, but they were just overwhelmed by uploads. So, and they did not reward the participation in any meaningful way. So basically what happened was that the amount of data that needed to be stored in them just grew and participation became more and more expensive. And then basically participants gave up on them.
00:03:06.574 - 00:04:11.914, Speaker A: So we need an incentive system in order to motivate participation of stores, and also we need an incentive system in order to make the allocation of storage resources efficient. So basically to store as much relevant content as possible. And here the emphasis is on relevant because we actually want those who upload to prioritize what they actually really want to upload and for how long, they want everybody to store it. And we also want to provide adequate redundancy, meaning that there's, we expect some churn in the network. So nodes come and go, and we want to make sure that content is not lost because some nodes go offline. So how do we implement it? It's implemented as a set of smart contracts on an EVM compatible blockchain. Currently we are using gnosis.
00:04:11.914 - 00:05:23.504, Speaker A: Originally we wanted to do it on the Ethereum main chain, but unfortunately it has become too expensive and we're not married to gnosis. So basically any L2 solution that fits the bill is a good candidate for running this incentive system. And we have an ERC 20 token that is issued and redeemed on the mainnet, on Ethereum mainnet. And we chose gnosis because it has good ERC 20 bridges that are sufficiently trustless and reliable. And the way the whole thing is implemented is a schelling game. So this is how you make these smart contracts aware of what is happening off chain, namely in the storage itself. So shelling games are a standard way of making smart contracts aware of some state of the world which is outside of the blockchain.
00:05:23.504 - 00:06:39.514, Speaker A: So basically in two sentences, what happens is that uploaders pay and stores get paid. So there's a way of redistributing these payments to the stores. And this presentation is basically about how uploaders pay and then about how stores get to receive this payment. So store upload their pay by affixing so called postage stamps to each of these four kilobyte chunks. Postage stamps are purchased in batches by on chain transactions, by registering a public key and the batch size with a smart contract. And once that public key is registered, you can attach a little bit of metadata to each of these chunks and seal it with a digital signature that can be verified with this public key. And this way each uploaded chunk is marked in a reliable way that it has been paid for.
00:06:39.514 - 00:07:49.780, Speaker A: So once the network receives a chunk, it knows whether this chunk has been paid for or not. And it only needs to look into the, into the state of this contract to see whether the payment of this chunk is sufficient, because all these postage stamp batches have a balance associated with them which gets regularly debited. So there's a rent that uploaders need to pay in order to keep data on the blockchain, on the sorry in swarm. And so this rent is paid in this token, bzz per chunk, per day, let's say. So basically it's proportional to the amount of data that you store and the duration of storage. The rent is going to be continually adjusted according to supply and demand. If there's time I'm going to talk about it.
00:07:49.780 - 00:09:13.678, Speaker A: If not, then please ask me and I will answer questions about it. But the short story is that the more saturated the network is, the higher. And if the network gets, if there's less data to store, then the rent goes down and batches with zero balance are of course invalidated and the corresponding content is garbage collected. So now we have a mechanism of collecting payment from uploaders and marking data that needs to be stored in an unambiguous fashion, and we need to redistribute these funds among the store nodes. So how do we do it? So, it's a randomized game where participation is tied to storing stamped content. So if a node is storing what they're supposed to store, then they are going to be rewarded from time to time. And because we have these neighborhoods, of course they're not going to be storing exactly the same data, because there are some latencies involved in how data is propagated.
00:09:13.678 - 00:10:34.224, Speaker A: So there's no exact consensus on what needs to be stored, but there's a rough consensus. So all the postage stamp chunks whose content address begins with the same bits with which the address, the overlay address of the store node begins, must be stored by all the store nodes that have this property. So essentially, all the nodes in a neighborhood agree on what needs to be stored, although it's not an exact match. So we cannot verify exact match, exact equivalence of what they're storing, but they still can verify each other whether they're storing the same thing. And in order to be civil resistance. So in order to prevent, like fake nodes that use the same storage, but just pretend to be more nodes than they are, nodes need to put down a stake in order to participate. And then we have this redistribution game that happens regularly in rounds.
00:10:34.224 - 00:11:58.924, Speaker A: So first we have an on chain oracle that picks a winning neighborhood. This is done with uniform probability independently of how much stake is in each neighborhood. So basically, each neighborhood has an equal chance of being selected. Then nodes in that neighborhood commit to and then reveal the merkel hash of a deterministically chosen random sample. So the reason we're doing this random sampling instead of building a Merkel hash of everything is precisely in order to have a little bit of tolerance to not storing exactly the same. So if the random sample has a large chance of being the same, even if there's a few chunks of difference between what they're storing, because the chunks haven't arrived to some of the nodes in the neighborhoods yet, but they are expected to agree on that random sample, and then the correct sample is chosen with a probability that is proportional to stakes. So an alternative could have been that the majority stake, the majority stake vote is taken as the source of truth.
00:11:58.924 - 00:13:24.130, Speaker A: But we decided against this because it would have meant that with a large enough stake, you can reliably take over a neighborhood and without any risk, basically censor. And with this choice that the correct, the source of truth is chosen in proportion to the, with a probability proportional to stakes, we have the property that usually the majority, most of the time the majority stake will get its way. But even the majority stake is not safe from having a different opinion from what the truth is if they play dishonestly. So this is an additional incentive to, to play honestly, because otherwise, even if you're in a majority, you risk to have your stake frozen. So currently, if you're not revealing the same Merkel hash that the source of truth revealed, then you get your stakes frozen. Maybe in the future we're going to introduce slashing. Currently we're only freezing because we don't want to punish too harshly the participants, because, you know, our software can have bugs in it.
00:13:24.130 - 00:14:45.968, Speaker A: So there can be reasons for losing the game, even if there's no bad intent on your part. So currently we're just freezing stakes, but that might change in the future. And then from those who did reveal the correct Merkle route, again with a probability which is proportional to the stake, the winner is chosen and the winner gets the reward. The smart contract is structured in such a way that the winner is the one who executes most of the housekeeping and therefore pays the gas price for it. So these three kinds of transactions that are submitted by participants, the claim payout transaction is by far the most expensive, and the commit and reveal transactions are actually cheap. And there's also some technicalities like when can you top up your stakes? We're also thinking about stakes to be withdrawable after some grace period. So, but these are really technicalities.
00:14:45.968 - 00:15:55.224, Speaker A: So this is the basic mechanics of redistribution. And just to reiterate, the only safe strategy is the shelling point, which is to play honestly. And since every reward is proportional to stakes, it's never worth operating sock puppets because you can achieve the same by just pooling the stakes and operating one node honestly. So basically, this is the incentive system. And I would also like to discuss what so how swarm, given this incentive system, can possibly be used for roll ups. Data availability, because superficially it seems like a perfect fit that here's a network where you can cheaply store data for a potentially short period of time. So you don't have to store it indefinitely, you don't have to pay for it to be stored forever.
00:15:55.224 - 00:17:11.044, Speaker A: You can actually store data for a given period of time. There are some nodes that are incentivized to store it, so it looks like a good match. The problem is that we cannot provide, swarm cannot provide data availability guarantees of the kind that, for example, dank sharding can. That given certain assumptions and this regular sampling, you can give some numbers, some number really close to one, which is the probability that you're going to get your data back. And also there are these more philosophical questions that data availability can be subjective, so something that is available for me may not be available for you. So what we came up with is that we can assume that layer one data, so what's on the blockchain is available to all, but it's very expensive. So basically you want to use that only as a last resort.
00:17:11.044 - 00:19:00.844, Speaker A: And in order to provide some kind of guarantee of availability, instead of the statistical availability, you can statistical guarantee, you can provide an economic guarantee that there's some state guarantor which can be the sequencer of the roll up themselves. So they have a stake and they have an on chain commitment to the Merkel root of the data, of the transaction data in this case, and the duration for which they are making it available. And what they do is they upload the data to swarm and they monitor the smart contract because the rent might go up and down, and if it goes up, then they might need to top the batch price up. So basically they need to keep the data available and pay for it. There's this smart contract to which they committed, and one particular transaction type with which you can communicate with the smart contract is an unavailability challenge that you can claim as a user that, hey, this specific transaction, which is the number whatever in this Merkle tree, is not available, I claim that this is not available. I could not download it from swarm. And at this point, if challenged, the guarantor has basically two options.
00:19:00.844 - 00:20:36.694, Speaker A: Either they respond to this challenge and they upload the whole transaction to layer one. And in this case, it was very expensive for both of them. So both the challenger and the guarantor had made a layer one transaction and they paid the gas cost of it, but the data ended up on layer one, where we assume it to be available to all. Or if it's really not available, then the challenger is reimbursed and the guarantor loses their stake. So it's an economic guarantee that if the data becomes unavailable, then the guarantor will lose this amount of money. And maybe this model is actually workable for rollups to store transaction data in swarm, because most of the time these challenge responses will not happen, because if the data is really available, you're not going to do the challenge because it costs you and you gain nothing. So it's basically just a griefing attack, but it's costly, so it's not free and it's relatively cheap for the responders.
00:20:36.694 - 00:21:44.900, Speaker A: So it's a griefing attack, but not a devastating one. So if the data is really available, there's no reason to do that. So it's not going to happen. So it's only there as a deterrent. And the storage incentives that I have introduced in the first half of my presentation actually incentivize the participants of the network to store them. And there's a separate set of incentives built into swarm, which are the bandwidth incentives, which incentivize the nodes to actually make data that they do store also available. So there's an incentivized network that is incentivized to store and make the data available on one hand, and on the other hand, we have this state guarantor that is going to lose funds if he doesn't keep this data on this network.
00:21:44.900 - 00:22:11.902, Speaker A: But if something happens, like something extra ordinary happens, then of course we still have the fallback to resolve the dispute on layer one. And in the worst case, the sequencer will have to upload part of the transaction data to layer one. So thank you. That was my presentation. I see that I still have more than five minutes for questions, so please go ahead and ask me.
00:22:11.918 - 00:22:24.994, Speaker B: Thank you very much, Daniel. Well done, very informative. And yes, we would have time for one or two questions from the audience, if there are some.
00:22:27.054 - 00:22:28.086, Speaker A: Thank you, Daniel.
00:22:28.230 - 00:22:47.058, Speaker C: So my question is that the 4 kb looks like a small choice to me because just a digital signature has some signs. I'm sure there is a lot of metadata, potentially for every chunk. So how was that, how was that chosen? That the chunk size is 4 kb.
00:22:47.166 - 00:23:18.124, Speaker A: So the four kb seems to be kind of a popular choice in chunking up data. So it used to be the basic unit in which SSD's operate. That's not the case anymore. But when this choice was made, it was still the case. And also there are other systems. So one example is a SQL database called sQlite. Very popular.
00:23:18.124 - 00:24:35.294, Speaker A: All of you have an SQLite database in your pocket because probably that's how your mobile phone stores the address book and many other things. And one of the things that you can do with SQLite is to. So SQlite transforms SQL queries into file operations on one single file, which you can also transform into query into range queries, HTTP range queries, which are compatible with the Swarm API. So what you can do is you can store a huge SQLite database on swarm and have a client side SQlite. So there's a sqlite implementation in JavaScript on the client side, and then you have the full power of SQL queries and you can very efficiently access swarm. So this 4 kb comes from the fact that many other systems have this kind of chunk size, but it's not dictated by any, anything in particular.
00:24:36.154 - 00:24:39.534, Speaker C: Okay, cool. I have one more question, but maybe.
00:24:39.954 - 00:24:41.186, Speaker B: Go ahead while you're there.
00:24:41.290 - 00:25:37.790, Speaker C: Okay, cool. So my other question is, so this is basically kind of similar to at around staking. I just also need a hard disk, right? So in some sense your competition is Ethera who main net staking, right. If they are paying, let's say 5% or, I don't know, 3%, what's the current percentage then to use my money and make my money work on swarm, I also have to buy a hard disk and do a little bit more than atherosclerosis, simple mainline terror staking. So my question is that how much free space do you have in the calculation that how much the interest on the main net can increase before it's totally not possible anymore to use swarm because everybody will just go to stake or may not.
00:25:37.982 - 00:26:34.900, Speaker A: So as I said, we have a mechanism that adjusts the rent. And if nodes leave the swarm network because it's not worth staking. By the way, I disagree that operating swarm involves more work than validating ethereum. Validating ethereum is also hard work. It requires different resources. But anyway, if nodes are leaving the swarm network, it creates a shortage of supply when we still have a lot of demand, meaning that this price adjustment mechanism will increase the rent and will keep increasing the rent until this situation persists, which will increase the kind of interest that you earn on your stake in swarm. So there's going to be a market mechanism that adjusts the rent if nodes are leaving the network.
00:26:35.012 - 00:26:44.904, Speaker C: Yeah, but only it's reasonable, right? So when AWS is cheaper, so how much leeway do you have compared to AWS or something like that?
00:26:46.484 - 00:27:38.134, Speaker A: So I would say that AWS is not a direct competitor. So unlike some other decentralized solutions, and here for example, storage being a prime example. So they want to compete head on with AWS. I think swarm is not a competition to AWS, in that it, like there are many use cases when AWS is the preferable choice. Swarm is a decentralized, censorship resistant network, which means that it serves the needs of those who need this kind of decentralization and censorship resistance, which AWS obviously doesn't provide. And it might have a very different price from AWS.
00:27:39.994 - 00:27:44.774, Speaker B: Okay, we have 1 minute and 20 seconds. Was there one more question? The lady here on the side?
00:27:45.754 - 00:28:10.654, Speaker D: Yes. Can you elaborate a bit more about the rent is adjusted to the available storage? Do you mean also the available storage in a certain neighborhood? And if this rent is adjusted, how does it work when you have a disrupted application that is taking a lot of space? And what happened with the previous stored data?
00:28:10.954 - 00:29:01.292, Speaker A: Right. So the incentives are laid out in such a way that we can kind of count on nodes being uniformly distributed across neighborhoods. So all the neighborhoods have roughly the same redundancy. I'm saying roughly because it's never exact. And this rate adjustment mechanism works as part of this iterated redistribution game. And indeed, each round only looks at one neighborhood. So if in one neighborhood we have a shortage, then the rent is nudged up a little bit, and if we have an abundance, then the rent is nudged down a little bit, and it only looks at one neighborhood.
00:29:01.292 - 00:29:48.164, Speaker A: But in each round we're looking at a different neighborhood. Meaning that if this is just a statistical fluke, that due to random chance, there's one neighborhood that is underserved, it will not have a dramatic effect on the rent. And everything that happens in the rest of the neighborhoods will kind of equalize it out. But if there's a systemic shortage, like there's a lot of data uploaded to swarm, it means that because of the uniform distribution of the hash function and the uniform distribution of nodes, we can expect that we're going to have a shortage in the majority of the neighborhoods. Meaning that in each round of the redistribution, the signal will be the same to, hey, increase the rent. Increase the rent. Increase the rent.
00:29:50.264 - 00:29:58.848, Speaker B: Ok, thank you very much. We've come to the conclusion of this presentation. Daniel. Thank you very much. Very interesting. I assume you'll be available afterwards?
00:29:58.936 - 00:29:59.384, Speaker A: Yeah, sure.
00:29:59.424 - 00:30:02.216, Speaker B: For some more questions. Thank you. A round of applause for Daniel.
00:30:02.320 - 00:30:02.664, Speaker A: Thank you.
