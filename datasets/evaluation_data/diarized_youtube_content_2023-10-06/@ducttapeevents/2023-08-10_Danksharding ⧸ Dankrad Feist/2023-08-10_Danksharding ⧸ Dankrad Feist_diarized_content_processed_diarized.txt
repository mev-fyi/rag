00:00:19.200 - 00:00:50.954, Speaker A: Thank you. Yeah, yeah. So thanks for the introduction. I will give a quick overview over what's now called Dank sharding, which is a construction that I introduced at the end of last year. And yeah, I hope at the end of the presentation you have some more idea on how it works. So I'll quickly. Oh yeah, I'll go come over here.
00:00:50.954 - 00:02:08.602, Speaker A: I will quickly talk about data shards or data sharding. I will explain how data availability sampling using KCG commitment works, what the 2d KCG scheme is. We'll go into proposal builder separation, which is one of the building blocks that we need. And then I will go to the come to the construction. So data shards are a way to provide data availability for rollups as well as other data scaling solutions. So what you might have seen that originally, like several years ago when we first started talking about charting, we thought that what we're going to construct, or what we need to construct, is execution sharding, which is we distribute the Ethereum execution across many different charts, and in this way we can scale the Ethereum blockchain to execute much, much more. But then over time, it has emerged that there are other ways to scale blockchains, and one of them are so called roll ups.
00:02:08.602 - 00:03:29.566, Speaker A: So roll ups are this construction where instead of executing all your transactions on the base ethereum execution layer, you only post all the data, all the transaction data, and you have another means that ensures that the execution is correct, so that the new state route is correct. And that can be done by different means, either by providing a complete proof, as a so called zero knowledge proof that the execution was correct, or you can use optimistic roll ups, which do that via fraud proofs. And so at some point, it basically became clear that rollups are actually a much more efficient means of providing execution, because they do not burden the consensus notes with all this load that comes to execution. They only need to, for example, resolve the conflicts in optimistic rollups, and so they are much more efficient. And at some point it emerged that probably even if we did all these execution charts, ultimately people would just use them to post roll up data on them. And so that's why in roundabout late 2019, our roadmap pivoted and we said, well, our main goal now is to provide a large amount of data availability. And yeah, I've got some numbers here.
00:03:29.566 - 00:04:16.004, Speaker A: So the goal is to provide about 1.3 megabytes of data availability per second on the Ethereum blockchain, which is about ten x the current maximum capacity. So if you have blocks completely full with data, but actually about 200 x. The typical capacity that we have on Ethereum now, the basic building block of creating data shards is data availability sampling. So fundamentally, we want to scale and scaling means we need to be able to, to create more. So data availability in this case, than a single node can process. Like somehow we need to break this bottleneck that every single node has to process all the blockchain.
00:04:16.004 - 00:05:01.072, Speaker A: And instead we have to know that, say, o of n data is available using just a constant amount of work. And so the basic idea is that we distribute this data into n chunks, and each node just downloads k randomly selected chunks. So that's shown in this picture. And the problem if you do that naively is, well, one of the chunks that you aren't sampling is missing. Basically, then this client just doesn't detect it. So there's this one single chunk of data missing, and the probability that it is among the ones that you sample is just very small. And so this doesn't work for blockchains.
00:05:01.072 - 00:05:50.634, Speaker A: Like for some other constructions, this might actually be okay. But because of the way that we work with blockchains for this, for example, financial applications, even like a single bit of data missing, could mean that someone is printing a billion ether in that chunk and you can't detect it because it's missing. So that doesn't work. So what we need is we need a way to amplify this. We need a way to encode the data so that data availability sampling actually works. And so the way we do that is we take our data and we encode it with erasure codes. And what we use are so called Reed Solomon codes, which is a fancy name for polynomial interpolation.
00:05:50.634 - 00:06:26.724, Speaker A: And so the way it works is you have some original data. In this case, it's four. I drew four chunks of data, and you extend it. So you put a polynomial of degree three through that. There's always a polynomial of degree three going through exactly four points. And you evaluate that polymer at four more points. And because of this fundamental property that any four points define a polynomial of degree three, it now means that any four of these eight points allow me to reconstruct the full polynomial.
00:06:26.724 - 00:07:06.550, Speaker A: And so we say the coding rate in this case would be 0.5, because you need this 50% to reconstruct the data. And this means that the data availability sampling becomes efficient, because now a small number of queries can ensure that a large amount of data is fully available. So let's say, for example, someone wants to construct a block and they want to hide the data behind it, they have to hide at least 50% of the data. Why? If they hide less than 50%. So if they make four of them available, then someone can just find those four and reconstruct all the data. So they haven't hidden anything.
00:07:06.550 - 00:07:57.190, Speaker A: They haven't succeeded. But if they make more than 50% available, then it's very unlikely if you make enough queries, that all of your queries are going to be available. So that's basically the trick. So, for example, if you make 30 queries, then it's two to the -30 which is one in a billion, which is a very small probability. And the one tricky thing here is that what we still need to do is somehow ensure that this coding is correct. So the one thing that we basically skipped over here is we have this, these eight blocks. But what if someone just puts random data in there? Like, they just aren't actually on this polynomial of degree three, it's just random data, and then everyone could download 50% of it.
00:07:57.190 - 00:08:30.974, Speaker A: But if they tried to reconstruct it, they would all arrive at different data. So that doesn't work. And this is where KCG commitments come in. KCG commitments are a type of polynomial commitment scheme. And what is cool about it is basically because it's a polynomial commitment scheme, it always commits to a polynomial. So you probably are familiar with Merkle routes. So they are a way of committing to any amount of data using a Merkle tree.
00:08:30.974 - 00:09:18.556, Speaker A: But the problem with that is it can be any data, so you have no assurances of what's actually in the data. In particular, you can't be sure that it encodes a low degree polynomial as we want. But if you use KZG commitments, then that's automatically guaranteed by the commitment. So basically, you can think of this KZG route as something similar to a Merkle route, but it always commits to a polynomial. In other words, it always guarantees that our encoding is correct. And the way we use it is we introduce. In addition, what we do is we introduce a two dimensional KZG scheme.
00:09:18.556 - 00:09:53.634, Speaker A: And so the way it works is we will have many rows of data. Each commitment commits to one row. But further, we also extend it in the columns. And the property of the scheme is that each row and each column will, on themselves be a code. So they will each be a polynomial. In other words, they each have this read Solomon code property. If you know 50% of it, you can reconstruct the whole row or the whole column.
00:09:53.634 - 00:10:28.494, Speaker A: And the reason we do this is why couldn't we just encode everything into one big KCG commitment. Well, the problem is then that you need a super node if you want to reconstruct. Let's say we detect, oh yeah, we have enough samples, there's 60% available, but some of them are missing. We need to get all of them. Ultimately, we want to make sure that all of them are available. So now someone needs to get all of them. And that could be a large amount of work.
00:10:28.494 - 00:11:42.976, Speaker A: But we want to avoid this assumption that there's someone like super powerful who can download all this data to reconstruct it. And so that's why we have this 2d scheme, because what we can do then is that we just assign different validators to different rows and columns, and they can basically reconstruct the single rows and columns. And where they intersect, they can distribute from a row to a column or from a column to a row. And what this means is that if the data is incomplete, then we can, in a completely distributed way, reconstruct this data. And the data availability sampling basically works the same as for a single KCG commitment with a small change that if you look at it, basically you need 75% of the square in order to be sure that you reconstruct construct. So basically, yeah, so the data, the number of checks you need to do to make is a little bit higher due to this property. It's not 50% but 75%.
00:11:42.976 - 00:12:19.504, Speaker A: And it means that you need 75 checks. Well, I mean, the 75 is just a random accident that it becomes 75. Don't think that's because it's 75%. And so this means that what's really cool about this, like the total bandwidth to ensure that all this data is available. So that's 32 megabytes per block, or 1.3 megabytes per second, is only about 2.5 kb/second so in the future, basically full nodes are able to ensure that all this data are available using only this really small amount of bandwidth.
00:12:19.504 - 00:13:28.382, Speaker A: The final building block that I want to mention in order to construct this dank sharding construction is proposal builder separation. So proposal builder separation is an idea that comes from the MEV world, and it's invented to fight the centralization tendencies that come from MEV. The problem with MEV is that we expect that sophisticated actors can extract more MEV than, say, at home validators who only have access to open source algorithms, for example. So that would create an advantage for big mining or validating pools. And so the idea of PBS proposal builder separation is that we contain the centralization tendency inside a special role. So we um, we, we let the validators be proposers as they are now. Um, but we add a new role which are the builders and basically validators role for new blocks.
00:13:28.382 - 00:14:14.004, Speaker A: Uh, becomes to just select a builder according to an auction system. Um, and uh, this means that for the proposals, uh, we asked before, we always need an honest majority assumption. And uh, so we have high decentralization requirements. But for builders we can live with a lower decentralization requirement because we only need an honest minority assumption. An honest minority assumption. In this case it means that what we want is that there's one honest and so non censoring builder, which doesn't mean that we only need one builder, we need several builders to ensure that. But the requirement is much lower than, than for the validating system.
00:14:14.004 - 00:15:11.284, Speaker A: Right. And basically the idea of dank sharding is this, is to use this proposal builder separation so quickly to come back to the 2d scheme. One thing we haven't yet talked about is that we found this way of reconstructing it very efficiently by doing that in rows and columns. But someone still needs to compute this full encoding. So we still need someone to do that initially, but basically we only want to rely on them for liveness and not for safety. And so thanks to this proposal builder separation, we can say like, oh, we say we externalize this role into the builder. So they are responsible for creating this encoding and distributing all these samples, and proposers are just responsible for selecting a builder through an auction.
00:15:11.284 - 00:16:26.956, Speaker A: And so that's basically how the construction works. So what we have is like there's a proposal block which is sent by a validator and they select a builder and the builder will build this full encoding with all the sharded data and the beacon block that will also after the merge, include the execution block. And then the committee votes as before, they vote for this aggregate of this construction, and then the next block is again this dual block. So in summary, we're using the 2d KZG scheme, which allows us to construct a 2d construct, a fraud proofy data availability scheme with high throughput and all the safety properties. So sampling of correct encoding and reconstruction, which ultimately is the necessity for convergence, so that ultimately all consensus nodes come to the same conclusion. They are fully guaranteed without any super nodes. So we don't have any requirements for a powerful actor in the peer to peer network.
00:16:26.956 - 00:17:20.736, Speaker A: In order to ensure these. And only the encoding and distribution of samples requires this builder. And thanks to PBS, however, we can ensure that ordinary validators are not required to fulfill these requirements. And so for liveness we have a new honest minority assumption that's dependent on this builder. And one of the implications is that what we are still currently doing active research on is to improve the censorship resistance properties of this. So there's great research done by Francesco and the research team on building these CR list techniques that allow ordinary validators to impose certain restrictions on builders so that they have to include honest transactions. Yeah.
00:17:20.736 - 00:17:38.202, Speaker A: And that is the construction. I conclude my presentation here and I'm open for any questions. Okay.
00:17:38.258 - 00:17:43.694, Speaker B: There's a microphone in the audience if anyone put their hand up. We'll get you the microphone. Ah, right at the very back.
00:17:51.974 - 00:18:05.874, Speaker C: Just a quick question on the proposer. In your scheme, can the proposer decline to take the auctioned block from the builder and just still make his own block?
00:18:06.214 - 00:18:19.476, Speaker A: Yeah, so yes, that's possible. Yeah. What's likely is that the proposal. So the proposal, you can also just submit your own bit. So this is always a possibility. There's no restriction on that. So yeah, for sure you can always construct your own blocks.
00:18:19.476 - 00:19:11.392, Speaker A: It's just likely that you will extract less mev. And it's also if you don't have the ability to construct say large data blocks, then you can construct a smaller block and just use that for example. So that possibility exists. Yeah. So why do we need proto dank sharding? So proto dank sharding is a simplification of this? Well basically proto dank sharding is not a scalable construction. It's a way to introduce some amount of extra data availability on top of the currently existing system to relieve the pressure a bit. That basically uses the same building block so that the upgrade later will be smoother.
00:19:11.392 - 00:19:18.894, Speaker A: And we need it because we're, we'll need more time to fully develop the full sharding construction. Until then.
00:19:22.794 - 00:19:25.854, Speaker B: Okay, another question from here. Yep.
00:19:27.194 - 00:19:27.974, Speaker D: Yeah.
00:19:29.314 - 00:19:30.174, Speaker B: Okay.
00:19:32.994 - 00:19:59.418, Speaker D: So about the amount of data basically we store, okay, we spread it across validators, across different nodes, across different charts, but, but then we also increase the amount of data dramatically, like it's like two times for polynomial and then four more times. So it's like eight more times. Basically eight more times the amount of data like the original data. So won't it be kind of four times?
00:19:59.466 - 00:20:05.254, Speaker A: Yes, so yeah, the encoding has an overhead of a factor of four. Yeah, that's correct. Yes.
00:20:11.414 - 00:20:20.594, Speaker D: We really increased the amount of data like we're going to store. We'll really benefit from it, like taking this in account.
00:20:21.774 - 00:20:44.254, Speaker A: Yeah, I mean we benefit from it in that not every node needs to store the full amount of data. So that is the trade off. Yes. So if you require every node to just store all the data, then you don't have this overhead, but then instead you have the problem that now you require very large nodes. So it's a trade off. Yes.
00:20:45.394 - 00:20:50.974, Speaker B: I think there was a question over here somewhere. I saw a hand over here. Okay.
00:20:57.914 - 00:21:14.294, Speaker E: Thank you for your talk. Just to start spreading some Fud. Can you tell? So there is a trusted setup needed for KZG commitments, and there is even project working on the ceremony client implementation here in the hackathon. So if you could say more about that.
00:21:15.234 - 00:21:53.578, Speaker A: Yes. So the KCG commitment scheme is based on elliptic curves and it requires a trusted setup. So that's similar to how many zero knowledge proof systems work. Like you've probably seen, like Zcash has done one in the past. But also many ethereum projects have done things like that. And the way it works is that it will require many people to participate. And it gives this guarantee that if at least one of these participants was honest, and honest means that they contributed and afterwards they destroyed the randomness that they used.
00:21:53.578 - 00:22:16.964, Speaker A: They didn't keep it or publish it or anything, then the whole thing will be completely safe. So if at least one honest participant was in the ceremony, which I think if you manage to get to sample or somehow get like thousands of people from the Ethereum community, I think it's a very safe assumption that I'm very comfortable with. But yes, it is an additional assumption, and we are currently working on making this as safe as possible.
00:22:20.824 - 00:22:31.574, Speaker B: Okay, the opposite corner of the room. Maybe if we kept this mic active as well, I can run to other people.
00:22:32.194 - 00:22:42.842, Speaker A: So the question is this sampling technique. It checks chunks availability at the moment of block production. But what about chunks storage?
00:22:42.898 - 00:22:44.134, Speaker B: Long term storage?
00:22:45.394 - 00:23:47.374, Speaker A: So what's the plan here? Okay, well, long term can mean many different things. So there will be a plan to have some amount of storage in the peer to peer network. We will require storing them for some number of weeks. What we do not want is to require nodes to store them forever. So basically after two or three months or something like that, nodes will be able to forget all samples. And the way you should think about the Ethereum network in the future is think of it a bit like a, a public notice board, so you can publish something and it guarantees like this has been published, and everyone who wanted to see it was able to see it, but it's not an archive system. And ultimately the only way you guarantee that data remains available is if it's interesting.
00:23:47.374 - 00:24:28.574, Speaker A: And I think basically we need to design our system so that it's not a crazy amount of data, so that people who find it interesting will just not be able to store it. But I think the way we design it is we keep that somewhat in check. We still make it, put it into the realm of individual people to store all of it if they really want to. You will have to buy a couple of hard disks every year probably, but that's definitely possible. I'm not worried about that data getting lost. I don't think we have to build incentives into it, but definitely we have to make sure that those actors can get the data if they want to. Oh, thank you.
00:24:28.574 - 00:25:11.854, Speaker A: Yeah. So very simple pointing question. How does this affect if in any way S stores and s loads and the relative gas costs? Oh, it doesn't have anything to do with that because the S store and s store load are about state and this data is about data availability. So this is a different kind of data that we're talking about. So on the base layer on the l one, this will not affect storage costs or anything like that. What is the difference? It will enable layer tools that can have their own pricing and have very different pricings for these operations. Cool, thanks.
00:25:11.854 - 00:26:01.740, Speaker A: Thanks. And cool that it's named after you. I just wanted to ask if in the process of building this, if you learned anything from these other data availability projects like Celestia or maybe even filecoin, I don't know. Okay, so Filecoin is not a data availability project, but yes, I mean Celestia is a very cool project and we are talking to them regularly as well. Mustafa also developed the basic idea with Vitalik several years ago. Yeah, I mean, I think like they are clearly doing something very similar. There are some differences in our exact approaches.
00:26:01.740 - 00:26:32.304, Speaker A: So one of the major differences, at least at the moment, is that they do not enable this distributed reconstruction that we are very keen on having. So they will require also for safety to have this powerful actor assumption. I think it's an interesting trade off and it's great to see them rolling out something hopefully a bit sooner than we will be able to. And so we will get first experiences about the whole data availability sampling.
00:26:37.884 - 00:26:53.512, Speaker C: Does this mean that the centralized staking services will earn most of the MeV until sharding is live? Or is there potential for the proposer builder separation to happen before sharding?
00:26:53.708 - 00:27:26.224, Speaker A: So yes, so there's currently a project which you might heard of, which is called Mavboost by Flashbost. So they're developing a software where they implement a form of PBS that is not built into the protocol, but with somewhat stronger assumptions on the honesty of the builders. You can still build something, and I think it will hopefully be ready for the merge. Yes. So even individual validators can profit from MeV at that point.
00:27:27.684 - 00:27:32.124, Speaker B: Okay. And I'm afraid time is up. Thank you very much, Dank Fred, for coming to talk to us about this.
