00:00:06.960 - 00:01:01.054, Speaker A: Hey everybody, I'm Jan and today I'm going to be presenting the topic of building Makerdao's economic risk engine. I work as a data scientist at Block Analytica. It's a DeFi economics risk intelligence earning business. And we up until recently, we were the team behind the makers risk unit, and we recently transitioned into an ecosystem actor role responsible for managing makers decentralized collateral. There's many ways to interpret the concept of economic risk engine. I will go basically in presenting a product that we launched recently, which is a decentralized product which aims to assess the the borrowing, the repayment of borrowing capacity for individual borrowers in DeFi. I'm going to go through a couple of topics.
00:01:01.054 - 00:02:00.644, Speaker A: First, I would contextualize credit scoring in treadfi and also map it to DeFi. Then I will continue with defining both why building an economic risk engine matters and how it can be applied to DeFi. And as well I will continue with the project leave on. That means getting more practical, presenting the methodology and also how we approach this problem. And I will continue even more practical with the product itself going through different sections and presenting how anybody can basically use it themselves. And I will finalize which feature work both from the perspective of how this product can be extended across other protocols, because currently it's only applied to the makerdao system. And additionally, also I will present how it can be integrated into the protocols themselves.
00:02:00.644 - 00:02:55.624, Speaker A: To begin with, credit scoring in the past has been all about assessing the capacity of borrowers to repay their debt. And if there is enough creditworthiness assessed in the borrowing capacity, then it's about the lending organization determining the interest rates. That's obviously in tradfi, but that can also be translated to DeFi. And in DeFi's case, because we don't have legally enforceable rules, we need different systems in place. That's why we've seen this proliferation of highly over collateralized protocols such as makerdao, compound AAVE and through liquidations by selling the collateral in case of rapid changes in non correlated assets. Because otherwise there can be bad debt. We have these systems in place that prevent system insolvency.
00:02:55.624 - 00:03:47.746, Speaker A: But it also means that we need to be able to assess this risk because there is still risk of bad debt. And for that reason we need to be able to assess the protocol level risk. But ideally, we go into the behavioral level, into the analysis of behavior of individual, let's say wallets. In Defi's case, that means that we go from the user level, we analyze the behavior, and then we bring it up with a bottom up approach to assessing the portfolio risk. There's various metrics that have been used in the past. Two of them are valid risk, which is very, very well known in shredfi space, and it's also used in DeFi and Capulet risk, which is something that we developed ourselves at maker. And we can see on the slide that we basically measure this constantly.
00:03:47.746 - 00:04:43.178, Speaker A: We run these risk models in the background, and we're able to detect whether, let's say, certain asset liquidity has worsened. And then that's basically shown in this capillary risk metric. And that can then inform also more protocol level decisions. When it comes to proposing risk parameters, when it comes to just the economic risk engine in Defi, the beautiful thing about blockchains is that all the transactions are transparent, so we can have access to that. There's many ways to split the computational techniques to analyze those. The one simple approach is just simulation modeling and machine learning. And while in much of our methodology, we're using simulation modeling, one of which, let's say this example, is agent based models, in this case, we're using machine learning, machine learning techniques.
00:04:43.178 - 00:05:47.816, Speaker A: And this is applied to the problem of decentralized credit scoring. That means that what we're trying to do is that we try to provide value to the ecosystem in two different ways. The first one is our capacity to assess risk on the protocol level, and through that, we can both understand risks better and also mitigate them, and thus increase ecosystem robustness. And on the other hand, by the assess risk being, let's say, low at a certain protocol, we can also then decrease the necessary over optimization on either the protocol level or the individual level, and with that increase capital efficiency. Additionally, there is also this space of identity and reputation that is becoming more and more that is growing a lot in DeFi and in crypto in general. And decentralized credit score can be a very powerful addition as a primitive in terms of financial reputation. Then we have Project Livon.
00:05:47.816 - 00:06:41.666, Speaker A: This is a product that we launched into public beta two weeks ago, and it's, as I mentioned, an ML based framework. It uses liquidations as a proxy for default. And the idea is that we then have a machine learning model that determines whether the predicted probability of default, and in this case, just estimating the probability of default, given worse historical price shocks. And the idea is also that by assessing individual level of risk of different wallets, we can also bring it up to the portfolio level, as I mentioned before. And we can see in the chart on the slide that we have a proportional contribution of different credit grades. Let's say different debt is assigned a certain credit grade of the wallets, and then we can see over time how that changes. And let's say if in this case we have credit grade of one means very high risk, credit grade of ten means very low risk.
00:06:41.666 - 00:08:03.608, Speaker A: And when this changes, we can also detect a difference in the, in the riskiness of the portfolio risk profile. I won't go too much into detail about the whole system design, but this is basically a simple diagram representing the level risk engine, which underlines the computed credit rates. And we begin by getting the on chain data of the maker system, both the current state and the historical states. And then we build these signals or features that are fed into vault liquidation ML model, and through a transformation and aggregation, we bring it up to the wallet level credit score, because obviously one wallet can have many wallets. And then on top of that ML model, we also add certain heuristics, such as the credit mix, which is the riskiness of underlying collateral or externally held capital, which I will go a bit deeper into later. Together we calibrate and rescale this credit scores, and in the end we get this credit grade, which is the final output of the product itself. One interesting component here is also of the methodology, is the challenge of not having that many volts that were liquidated, especially in the more recent maker history.
00:08:03.608 - 00:09:00.334, Speaker A: And that makes the machine learning problem quite challenging when it comes to class imbalance. And that's why we need certain techniques to basically mitigate that. Then we have the product itself. The idea is that this is as simple to use as possible, and as the user enters the product, and it's already available now for everyone, they can just have a short description of the product and they can get the risk assessment report on the wallet level through different pathways. Number one is that they enter the wallet that currently has an open wallet at maker, or on the other hand, they can also have a nice overview. We can see on the right, different wallets that have a certain assigned credit grade, and then they can click on a certain wallet that has a credit grade of nine. And through that they can basically get the risk assessment report.
00:09:00.334 - 00:09:54.938, Speaker A: This is one component of risk assessment report. We can see that for this wallet, its score is nine, which is very low risk. And then we can also see the portfolio comparison to other wallets, and that's basically a wallet count on the y axis, and it's also compared especially to other wallets. But at the same time, we have this portfolio view of, of how risky is the portfolio at the moment. And we can see here that most of the wallets are graded with 8910, which is also reflected, which is basically reflecting the current state of maker, because it's highly over collateralized. And if there is huge overclocation of vaults, obviously the credit grades should also reflect that. And finally, we have some basic stats on the wallet level, such as number of vaults, collateral and depth.
00:09:54.938 - 00:10:59.330, Speaker A: And that's something that also, we're adding components over time. This is a very important aspect of the methodology, which is the explanation itself. We have the ML model, and then we have heuristics on top of it, and the ML model creates baseline score, and we have the adjustments on top of that. And one of the key metrics, obviously, is the quantization buffer, which is the collateralization ratio over liquidation ratio. And then there's also external capital and credit mix that I already mentioned. And something that we also take into account, of course, is also like repayment activity and these kind of events, and also things such as whether the wallet uses third party services, such as automation services for managing the position, such as Defisaver or Oasis. We also created this analysis section, which is quite broad, and I won't spend too much time on it, but the basic idea is that we go first on the grade analysis.
00:10:59.330 - 00:12:10.194, Speaker A: That means the final output of the model, and we split it both on the current state and also the historical states, right? We can see on the bottom the debt by credit grade. That means the proportional contribution of different wallets, basically the wallet debt to the total risk adapted maker, and also the wallet count. Right? In a similar way, then we have the fact analysis, and this is not the typical fact analysis that is used in statistics, but this is factors here are signals or features in machine learning lingo. And there are various, let's say, intuitive explanations how this can be useful, right? Number one is, let's say, the quantization buffer, which I already mentioned. Then we have the tenure, because intuitively, the longer somebody has been using, let's say, maker, the less likely they are to be liquidated during a market shock. Then we have around the clock interactions, which measures how well distributed the events of the walls are across different time zones, which can also be an interesting proxy for automated management. Not necessarily, but that's up to the machine learning model to determine how predictive this is.
00:12:10.194 - 00:13:06.824, Speaker A: And then we have also, let's say, recent activity. How recent was the interaction of a specific wallet with its vaults? Then we come to the topic of external capital analysis. We split this into two aspects, right? And important to mention is why that matters is of course, like if somebody has a low crucialization ratio, but they have a large amount of assets in their wallet that can be used to repay their debt or increase their collateral, that can obviously be useful, right? Of course, the next level is also to bundle different wallets together, but that's basically outside of this specific presentation. But we can go deeper into that later. So the main idea is that we split it into two different parts. Number one is the passive external capital. That means external capital that is held idly in the wallet in no other protocol than, let's say, maker.
00:13:06.824 - 00:14:07.524, Speaker A: And then we have the active external capital, that's capital that is used in other protocols. And we then do certain adjustments, such as slippage on that asset amount, how much of a swap amount the user actually has that can be useful to repay their debt or protect their position in general, and also different haircuts, to just have a more realistic image of how much capital the wallet actually has to protect their position. And that's also brought into relative perspective when it comes to the outselling debt, because what we want in the end is to know how much external capital does the wallet have relative, or the user have relative to their outstanding debt. And that's basically the. And that's one of the key aspects that we analyze when it comes to the power of the capital via the, via the capital that is in the wallet. We also did segmentation analysis. I mean, I won't go too much into this.
00:14:07.524 - 00:14:49.630, Speaker A: It's just a technique to basically segment different wallets based on their behavior patterns that I mentioned before. And we have, let's say the early adopter, we have inactive protected high sear buffer whale and so on liquidated. So we have different categories or segments of wallets. And then we can also bring it into perspective when it comes to the credit grade itself. Right? So it means with the best credit grade, what are the segments of wallets there? Right? So, and those are certain interesting insights that can be derived through that as well. Finally, we came to the future work. Our idea is to continue improving eleven grade.
00:14:49.630 - 00:15:21.174, Speaker A: That means especially extending it across other protocols, especially cross collider protocols. We've already done some work on AAve, and we want to continue expanding this. And the question is also how to integrate into various protocols. Right. There are just two examples here. One is whitelisting. That means that if a wallet has liquid credit worthiness, then it can be provided or an access to a certain service.
00:15:21.174 - 00:16:08.514, Speaker A: Or let's say preferential treatment, meaning that again, good quality credit worthiness, they can have lower liquidation ratio or lower stability fees, interest rates or basically so auction time. So those are basically three simple examples. But this can be expanded obviously. The idea is also to extend this by making it more autonomous. So it means providing this as an autonomous service, as a risk service, and there's various frameworks popping up, one of which is autonomous. And then we can have this nice bridge between on chain data and then also on chain decisions and at the same time the off chain compute. And this can be done in a more and more transparent way.
00:16:08.514 - 00:16:34.254, Speaker A: And yeah, definitely also privacy preserving. That's all from my side. This is a QR code of the product. You're welcome just to scan it. And we're happy to hear some feedback, some comments about how this can either be used by you, how it can be improved. Yeah, happy to hear more from you. Thank you.
00:16:34.254 - 00:16:50.074, Speaker A: Any questions? Thank you. Thank you.
