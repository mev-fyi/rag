00:00:11.000 - 00:00:34.586, Speaker A: All right, thank you everyone. Today I'm going to talk about mutation testing. So just a brief introduction. My name is Eric Marty Haynes. I'm a blockchain, blockchain engineer at Nethermind. My background is in smart contracts. Sorry, my background is in computer science and software engineering.
00:00:34.586 - 00:01:27.644, Speaker A: And since then I've been focusing on smart contracts and defi, basically exploring the different areas in the space. For the past year, I focused on tokenization. I've basically developed the smart contracts for Libra Capital, which is a hedge fund tokenization platform. And I've also written a few articles about tokenization, about Nethermind, a bit about Nethermind. We solve the simplest solutions to the hardest problems in blockchain. We're a blockchain research and engineering company, and we focus a lot on the Ethereum ecosystem, with it, the startnet ecosystem, and we're doing all sorts of stuff there from just like the core clients, cryptography research, application development, auditing, all sorts of stuff. So let's go to the beginning, in this case, the beginning, of software testing.
00:01:27.644 - 00:02:19.584, Speaker A: Software testing has one simple goal, which is identifying and preventing defects in a software product. And software testing has been necessary since software has existed. So basically since the fifties, when software first appeared, there's been a need to test it and to basically make sure it was working as expected. Obviously, since then, even though the goal of testing has remained the same, there's been advancements on the techniques used and kind of shifts, like in mentality. So, for example, when short after testing was first introduced, we came up with unit testing, which is basically trying to test the smallest possible complete unit of code that can run by itself. And this way we can isolate different functionalities and we can test them separately and make sure they're working. This is very popular, still used nowadays.
00:02:19.584 - 00:03:24.164, Speaker A: A bit later on, like in the late seventies, there's a separation between testing and debugging, where debugging is defined as trying to find the cause of a bug or a defect that is already existing and fixing it. And testing is defined as trying to prevent these bugs and defects from a software product, even though they haven't been found yet. Later, like in the nineties, there's another shift towards automation. Manual testing can be very useful, but it's very time consuming and inefficient for programmers to do. And so with automated testing, it can be much simpler to just run tests all the time and make sure we detect defects as soon as they are introduced. Nowadays, there's also a bigger focus on prevention, so we're just trying to make sure that our code has as little defects as possible or what's acceptable given our business use case. And, yeah, we're trying to do that, basically prevent these defects.
00:03:24.164 - 00:03:47.246, Speaker A: So let's talk about smart contracts in this case, because testing in smart contracts is very important. And as you can see here, just in quarter one of 2024, we've had losses of over $300 million. And, like, defi exploits, hacks, etcetera. And this is just quarter one. Right? Like, we're later in the year, there's been more. And this isn't even the year where we've had the most of so far. Right.
00:03:47.246 - 00:04:37.866, Speaker A: And the main reason this happens is we need to prevent every flaw, right, but attackers only need to find one and exploit it. Right? So we have security as an asymmetric game where, like, the rules aren't the same for both players, basically. And why this is possible is because most of the time, smart contracts are open source. We need this to be able to trust and verify that they're doing what we think they do and feel comfortable depositing our funds in them. But at the same time, this makes it possible for an attacker to dig through the code and find any possible exploits. At the same time, they're permissionless. So this means if an attacker does find an exploit, it's possible for them to just go on chain and exploit it without having to, like, create an account linked to their identity or somehow reveal who they are or ask for specific access.
00:04:37.866 - 00:05:01.274, Speaker A: Right. We also have immutable code in the sense that sometimes if a smart contract gets exploited, it's game over. It's immutable. There's nothing you can do. You can't fix it. And even if you can, with a lot of smart contracts nowadays being upgradable, this upgradeability is not perfect. Sometimes there's a delay component into it, and often, even if it can be upgraded and fixed, it's too late and the funds are gone.
00:05:01.274 - 00:05:58.914, Speaker A: And the reason why all this is happening and smart contracts are targeted is because of the locked value they have in them. We have this huge amount of money in cryptocurrency basically just sitting in smart contracts, and this is just very tempting for attackers to go after. So we have a few different techniques to test smart contracts. A lot of these can also be applied to other software testing. We have unit testing, like I explained, basically verifying the functionality of individual components of code. Then we have integration testing, which is basically putting these tests together and testing that all these features that we've tested independently are also working as expected when combined. We have invariant testing, which is very popular with smart contract test, which is basically verifying that some invariants that we set and our code that should never be violated are basically holding up when we call functions in a random way.
00:05:58.914 - 00:06:25.356, Speaker A: We also have fuzzing, which is similar. It's basically we have our functions and we test them with randomized inputs. And this helps us detect some possible edge cases that we might find in our code. Here we have some different tools that are available for each of these tests. A lot of them you might see repeated. I'm not going to read each specific one, but yeah, these are just some popular ones. We also have some other tools.
00:06:25.356 - 00:07:06.400, Speaker A: We have static analysis, which is basically a technique for analyzing the contract code without executing it. We have symbolic execution. We're analyzing the execution of the code, the different paths that the code has depending on the inputs we use. We have formal verification, which is a very interesting technique where we prove the correctness of a given program or contract given a formal specification that we have predefined. And then we also have mutation testing, but this is grayed out because we're going to get back to it later. And again, here you have some tools that are pretty popular to do some of these kinds of testing. All right, we're going to take a look now at bank Sol.
00:07:06.400 - 00:07:30.418, Speaker A: This is a simple bank contract. Bear with me. I tried to make the code like, pretty simple. We basically have a mapping with the balances, a deposit function that adds eth basically to our balance well before the withdrawal. We have a getbalance function. It's just returning the balance of a particular address. And then we have the withdraw function, which is basically checking.
00:07:30.418 - 00:08:01.792, Speaker A: You have enough balance to be able to withdraw. It's subtracting that amount from the balance mapping. And then finally it's sending that ETH to the address, withdrawing and checking that the transfer was successful. And of course, we're going to write our tests for our code. And these are solidity tests, basically the kind of tests you would see like using foundry or something like that. Again, these tests are pretty simple. We have the setup here, then we have test deposit, which is just basically checking that our deposit is working.
00:08:01.792 - 00:08:18.598, Speaker A: We're testing that. First we have a zero balance, and then after depositing our balance, 50 ether. Our balances of 50 ether over here, we're testing withdraw. We're basically the same. We start with a balance of 75 ether. We deposit it, we check. We have that amount.
00:08:18.598 - 00:08:45.316, Speaker A: Then we withdraw 50 ether. And we're checking that the balance is different. And finally, we have test withdraw fail, which is similar, but we're trying to withdraw 15 ether when we only deposited ten. And so obviously, we would expect that to revert. And the test is making sure of that. So we can run these tests, basically command forged tests. If you're using Forztest, if you're using foundry, and we see all of them pass, we can see the gas they use, the time it took, that kind of thing.
00:08:45.316 - 00:09:20.304, Speaker A: Okay, so our tests passed, but how do we know if these are actually good tests? Well, we could use code coverage. And code coverage is a measure of the percentage of source code that was executed during testing. It's often used to evaluate the quality and thoroughness of tests. And usually high coverage suggests that there's a smaller chance of undetected bugs. Right. If we covered most of the code in our tests and we ran through it, we shouldn't expect to find, like, a huge amount of bugs there, at least in theory. So there's different types of coverage.
00:09:20.304 - 00:09:59.900, Speaker A: There's line coverage, which is the simplest one. It's just a percentage of lines of code that our tests have covered. We have statement coverage, which is similar, but basically with the number of statements executed, we have branch coverage, where we're basically checking the different branches of code that our program has and which of them have been executed during the test, and function coverage, which is just a percentage of functions that we've called in our test. So we can get the coverage for our tests with the command forge coverage in this case, and we'll get this. It looks pretty good. We have 100% line coverage, 100% statement coverage. We have 75% branch coverage.
00:09:59.900 - 00:10:37.066, Speaker A: We have four branches. There's one we missed, and we have 100% function coverage. So this would suggest, okay, our tests are really good, right? Like, our code is perfect. It's well tested, no issues. Well, code coverage has some shortcomings, and it's basically because a piece of code being covered, be it a line, a brand, etcetera, doesn't necessarily mean that it is correct or that it is checking the expected state change, that kind of thing. So it's not necessarily checking how correct it is that we executed it. And this, this is problematic for two main reasons.
00:10:37.066 - 00:11:09.322, Speaker A: The first one is it can lead to prioritizing coverage over actual quality of tests. Sometimes we try to get a good coverage because it's, you know, it's a metric people use. Audits sometimes emphasize the coverage of the tests that are in a certain test suite. So we try to go for 95 100% coverage. But we're just thinking about improving coverage, not about actually making our code secure and properly tested. And it also gives us a false sense of security because we think our coverage is so high. Our tests are perfect, but it might not be the case.
00:11:09.322 - 00:11:47.126, Speaker A: And here's a small spoiler in this withdraw function. In the test withdraw function that we have for testing it, we actually have an issue where we're not properly testing it. It's fine if you don't see it now, we're going to get back to it later. So there's another way, which is mutation testing. The easiest way to explain it is basically testing tests. And mutation testing was first proposed in 1971 by Richard Lipton, and it is based on two main hypotheses. The first one is the competent programmer hypothesis, which is basically that competent programmers write programs that are close to being correct.
00:11:47.126 - 00:12:29.270, Speaker A: They might not be 100% correct, but they're pretty close. And if they have issues, they're usually small deviations from the correct code. And we also have the coupling effect hypothesis, which is basically that simple faults in the code can couple together to form other faults that can be maybe bigger faults or different kinds of faults. So here's where we'll talk about the mutation testing technique to explain it. Very simply, it's based on mutating components in our contract or in our code to evaluate the existing tests and how good they are. So it works by modifying the source program using mutation operators. We'll get into those to create a mutant piece of code or contract, and then running our existing tests against it.
00:12:29.270 - 00:13:01.094, Speaker A: If a test fails, it means this mutation is killed. And for this to be possible, though, we need three conditions to be met. The first one is that our test must reach the mutated statement, right. If our test doesn't reach it, we're not able to execute it. The test's input data should create a different state in the mutant when compared to the original code. It doesn't matter if we reach it. If our test is just doing the default data on null zero, for example, and maybe the mutation doesn't cause any change in that case.
00:13:01.094 - 00:13:52.144, Speaker A: Third, the state must be propagated by the mutant and then checked by the test. If our test is not checking the result, then. Then it might not make sense. So these three conditions also mean that we have a set of qualities that a test suite must have in order for mutation testing to be the most effective. Just to be clear, it can work if some of these aren't met. But the better you meet them, the more effective mutation testing is going to be when applied, so we should have a high coverage, basically, so the mutations can be detected because we want to reach those, all those lines of code and branches, et cetera, functions in our tests, the input data should be varied and realistic to trigger different state changes in our tests. And also our tests should check the results of these state changes to ensure the correctness of the program.
00:13:52.144 - 00:14:27.278, Speaker A: So we talked about mutations that were killed, basically the mutations that our tests detected. What about the ones that our tests don't detect? So we have two kinds. We have equivalent mutants. These are basically false positives that contain mutations that change our code but don't really change its behavior. In the case of smart contracts, this even means sometimes we might have a mutation with the exact same bytecode because the solidity compiler compiled it into the same thing, even though the solidity code might be like slightly different. These we don't really care about because they're doing the same thing as our code. They just look slightly different.
00:14:27.278 - 00:14:58.602, Speaker A: The surviving mutants are the important ones because is they basically contain mutations that alter the logic of our code, but our tests did not detect it. And this is concerning, right? So it basically means that our tests are not checking all the conditions that we thought, and we should probably look back to our tests and improve them. So let's take a look at possible mutation operators. This is focused on like solidity smart contracts. There could be more mutations or different kinds as well. This is just an example. We have arithmetic operators.
00:14:58.602 - 00:15:35.604, Speaker A: So just changing a plus sign to a minus sign modulus to exponent that kind of thing. We have logical operators. We can change logical and to logical or, or the other way around. We can change bitwise operators, so we can change bitwise or to bitwise and sure, as well. And we can also change bitshift right to bitshift left. This is a very relevant one. We can change equality and inequality comparisons like equals, not equals greater than, less than that kind of thing.
00:15:35.604 - 00:16:15.924, Speaker A: We can change hard coded values. So if we have constants in our code, we can change a negative number to a positive one, a number to zero, true to false, a string to an empty string, an address to the zero address. We also have deleting lines or function modifiers. In the first case, here, we just deleted a token transfer in our mutation. In the second example, we deleted the only owner modifier from our function, so it would make the function accessible to everyone instead of just the owner and the mutation. And we can also swap the order of lines. So here we're doing a transfer first, and then we're registering when the last transfer happened.
00:16:15.924 - 00:16:49.830, Speaker A: But in the mutated version, we're first storing when the mutation happened and then doing the transfer. This could mean nothing. Or if our transfer has some custom logic that's using the last transfer for something, it could be problematic. Right. So we also have automated mutations, because, basically doing this by hand, again, is very time consuming. So, like with most forms of testing, doing it in an automated manner is usually better. Yeah, it can also lead to human bias, because we're the ones testing our code.
00:16:49.830 - 00:17:23.856, Speaker A: We might think that some sort of mutations are better, but we might miss some edge cases. So we have automated mutation testing. It's basically doing this sort of technique using a default set of operators randomly on our code. So these are some tools you can use for running mutation testing on smart contracts. I'm going to show you an example using vertigo rs. But here you have a few other ones. Foundry has a question mark because it's had an open issue for a year, including mutation testing inside of foundry.
00:17:23.856 - 00:17:59.602, Speaker A: It's still an open issue, but there's some discussion going on there, so hopefully you will see it at some point. So let's mutate bank sol and see the result. So if we run vertigo rs on our foundry repo, we will get this output. So it's basically saying we have three mutations, and two out of those three mutations were killed. We have the first one that was in line eleven, where basically it mutated the greater or equal sign to a less than equal. Sorry, less sign. And, yeah, this was basically checking if the user had enough balance.
00:17:59.602 - 00:18:19.816, Speaker A: Our test detected it, so it was killed. Here we have the other two. The first. The second one was also killed. It was basically changing a plus equals assignation to a minus or equals one basically in the balances when we're depositing. Obviously, this breaks our tests. So it was killed.
00:18:19.816 - 00:18:45.582, Speaker A: But this is the most interesting one we have, one that lived, one that survived in the withdraw function, where we went from subtracting a balance when withdrawing to adding to our balance. And somehow our tests did not detect this, and this mutation survived. So this is concerning. We should take a look into this. So, yeah, if we go to our contract, we can see there. That's the line that was mutated. We turned it into a minus equals.
00:18:45.582 - 00:19:19.450, Speaker A: And we can see if we do it manually and run the tests, they still all pass. Right? So something's up there. So here we can see the actual line of code where we're testing, and we can see that something doesn't make sense. We're checking that the balance is different than the starting balance, but we're not checking that it's the original balance minus the amount we withdrew. So this means since our mutation changes the amount of balance the user holds, it just increases it instead of decreasing it. Our test goes, oh, it changed. That means it works great.
00:19:19.450 - 00:20:00.784, Speaker A: But that's not how our test should work, so we can fix it. Now, we're actually testing that the resulting balance is the starting balance -50 ether, which is the amount we withdrew. And if we change that and run the tests again, we can see that the assertion is failed this time, and our test is failing with those amounts over there. So now if we run vertigo rs again on our fixed tests, we can see the same number of mutations, but this time, the three of them were killed. We have the first one again, the second, and the third one. And here, what's relevant, you can see it was killed. So we just use mutation testing to improve our code.
00:20:00.784 - 00:20:31.270, Speaker A: And as a conclusion to this talk, I just wanna like, if you keep something from this talk, I want you to remember these things. I think they're relevant. Testing is critical to reduce the risk of exploits and smart contracts. It's extremely important. Mutation testing is a very powerful technique and it works really well together with other techniques. And it's great for prioritizing the quality of tests over, like, the quantity or the coverage of your tests. And also, mutation testing tools are not as widely used as other security tools.
00:20:31.270 - 00:21:06.238, Speaker A: They're not as popular. The tools I showed, some of them are not the most well maintained. So it would be very interesting to see people using these tools more, supporting them and that kind of thing. So I hope at least you can take home that this is a technique you might not have known about and that it might be useful to you at some point. So that's all for me. If you want to find out more about nethermind, you can check our website, you can follow me on GitHub or Twitter if you just want to see what I'm up to or ask any questions. And here you have the example code, if you just want to take a look at bank Sol and.
00:21:06.238 - 00:21:20.124, Speaker A: Yeah, that's it. Thank you very much. So if there's any questions, you can feel free to ask them now.
00:21:22.664 - 00:22:32.462, Speaker B: Thank you for the talk. I actually have two or three questions, but you mentioned at some point that the game was kind of asymmetric. But if everything is public, I mean, the attackers and the co writers, they have same chances. It's basically like saying that life should be fair, which is not now. So I was, maybe you mentioned the fact that they need to find one bug to exploit, but I'm more thinking from an information theoretic perspective, so. And my, my second question about the tools, maybe because they aren't, I don't know if there is any, you know, if they discovered any real vulnerabilities in the, in the ecosystem, so maybe that they can have more, you know, more visibility. I don't know because it kind of feels like a weaker version of symbolic execution at some point, but maybe more effective and less demanding on resources.
00:22:32.462 - 00:22:35.214, Speaker B: I don't know what you're thinking about that.
00:22:35.294 - 00:23:05.794, Speaker A: So on your first question, I'll just basically say, I meant asymmetric. Like, if there's 50 critical bugs, and as a developer, you fix 49, it just takes that leftover one. So you fix most of them, but you still lost kind of thing. But, yeah, on an information point of view, there's equal information on both sides. Probably the developer knows more about the code base as well on the tools. What you said is interesting. The thing to keep in mind is mutation testing isn't really testing your code as much as it's testing your tests.
00:23:05.794 - 00:23:41.496, Speaker A: Right. So it's hard to point at a specific line of code in your smart contract and say, oh, this was prevented because of mutation testing. In the example we did, there wasn't a flaw in the contract. There was a flaw in the test. But still, flaws and tests are important because our code goes through cycles where it's maintained, it's changed. New people come on to develop it. So even though our contract at that point was fine, who knows if in the future someone accidentally rearranges line of code or does some refactor, they don't realize they've introduced a vulnerability and our tests don't detect it.
00:23:41.496 - 00:24:20.284, Speaker A: Right. So that's where mutation testing can shine. But it's very hard to point at an example where it was successful because you can say, oh, like, thanks to it, I fixed my test, and this test three months later detected a critical issue. It's hard to keep track of those things. But I do think you raise an interesting point. If people were more aware of some examples where this had worked to prevent some, like, exploit or something, I think people would be much more willing to give it a try or explore it. Any other questions? All right, well, thank you very much.
