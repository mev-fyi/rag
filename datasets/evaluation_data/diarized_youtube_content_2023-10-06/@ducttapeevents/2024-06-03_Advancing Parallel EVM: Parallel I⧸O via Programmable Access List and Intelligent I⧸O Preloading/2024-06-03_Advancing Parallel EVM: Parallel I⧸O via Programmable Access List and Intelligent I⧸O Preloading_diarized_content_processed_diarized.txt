00:00:12.040 - 00:00:35.226, Speaker A: Intro too early. But yeah, we're talking about paralyzation. Anyone knows about paralyzation? Only one. Oh God. Okay, so there's going to be a quiz for you at the end of the talk about paralyzation. If you win the quiz, we will. I don't know who's the conference host? Can we give them something if they win the quiz? A voucher, a stick? No, no.
00:00:35.226 - 00:00:39.574, Speaker A: Maybe food or like, I don't know, beer or something. Anyway, go ahead.
00:00:40.754 - 00:01:20.006, Speaker B: Okay. Hello everyone. So it's my great pleasure here to explore some our latest ideas on top of the parallelization eVM. So, oops, let's have to move upon here. Okay, great. A quick intro of myself has been working in this area, oops. For about six years previously working primarily in some big companies in parallelization high performance systems, got a PhD.
00:01:20.006 - 00:02:13.814, Speaker B: And so that's why I'm particularly interested in for example, Ethereum data availability L2, all those scaling solutions and got a couple of supports from Ethereum foundation. So what's going on? Oh, I guess I pressed, I used the wrong direction, that's why. Oh, I guess this is the wrong link. Okay, okay, great. Yeah. So why we want you particularly interested in parallel EVM? So basically we like to answer some ultimate question. What is the ultimate performance of EVM if you don't have any overhead of the fixed sample consensus, which happens like for example right now for L2, although sequencers can just produce the block almost instantly.
00:02:13.814 - 00:03:18.134, Speaker B: So there will be a lot of questions about in these scenarios how EVM can achieve what maximum pps in this setup. And also another question is that are we able to build an EVM that is even faster than like for example Sonana? And there's a lot of this question flying around and people say hey, why not use some web two experience to accelerate the execution? Especially right now we are heavily relying on for example, parallelization in order to speed up those transactions. So in the parallel EVM, there's generally two categories. One slides in called parallel execution, or sometimes called optimistic execution, which basically to execute a sequence transactions, future transactions. For example, we have ABC. Then the normal way is execute one by one. That's called sequential execution, that's ABC, but it's slow.
00:03:18.134 - 00:04:38.710, Speaker B: Then the parallel execution says hey, why not? We are able to running a. At the same time we also execute b and C using the state that the a actually is executing. And once after executing a and B, then it just double check if there's a conflict of the read and write set. And if not, then it can just easily merge the execution result of B with the ACE result so that we can get the corresponding results and that is able to better to utilize the current parallelization concurrency feature of the node. Another direction is called parallel IO, which aims to reduce the latency and boost the performance using some technologies. So why we want to study also accelerate the I O of the evolution of the ethereum. So here is a really nice chart from MEV that is flashbots that shows what are the major normalized runtime that would be used basically in the EVM execution.
00:04:38.710 - 00:06:03.230, Speaker B: And this is using the Mainnet data so you can see the corresponding block numbers and so they list a couple of the major codes. And the last one here is called Ops load, which is basically reading 32 bytes of the data from Ethereum by given a key. For example, I read a user's balance account or read the ownership of the NFT images and all this access code actually accounts for approximately about 70% of the EVM execution. Which means that even though we may be able to accelerate the other executions, like for example by using a better way to interpret the opcodes or other ways, we still have a large amount of overhead in this SLO operation. Why? The reason is that the evN I O patterns is very unfriendly for the iOS. First of all, a lot of these I O are random access patterns, which is hard to predict. For example, when I read a balance of uniswap balance of some, for example the pool balance, then the solidity actually will generate a random position using kaka and the corresponding key to randomly located position of the data in the database.
00:06:03.230 - 00:08:05.744, Speaker B: And such kind of a random access is really really unfriendly for the underlying database storage. And second, the partition megal tree that Ethereum now uses further worsen the case of random access because the random tree structure is also highly randomized. And so in order to solve this problem, there are a couple of the pioneer ideas that aims to say hey, whether we are able to figure out a way to reduce the latency that random positions, one thing is to hey, rather than just have these random positions or locations during the EVM execution, why not? We are able to use token incentive to let the sender to specify called accesslist which can tell the EVM that hey, there are some data that I'm going to access ahead of the execution. And now the EVM can parallel loading this data from this access list so that when reading this data in the later is already in the cache. So reading the data will be generally in the cache will be generally it takes zero latency and hence we are able to significantly reduce this I O speed. And so in our test it can enhance the EVM execution. Suppose we have full access list, we can increase the performance about twice with all those very simple strategy and this is really nice idea and this is also proposed by Vitalik about I think two or three years ago, but unfortunately there's recently there's a paper that is written by ETH Zurich, it's a university, it's not an Ethereum suborganization.
00:08:05.744 - 00:10:08.444, Speaker B: So there's a really good paper saying that they do a lot of starting on the mainnet optional access list and they found even though there's a great feature to accelerate, basically the transaction execution is highly underutilized on the mainnet with only about 1.45% of the transaction utilize this feature. And there are a lot of reasons, one reason is because the current incentive of incorporating this access list is very very minimum because it increased the core data price, because have more data, then the Ethereum needs to broadcast all these access patterns or access list to network and this has an extra cost and so a lot of users found oh, if the corresponding cost of broadcasting this access list is great, greater than the cost that gas savings that this EIP brings, then definitely nobody would like to use this feature. And also there's a couple of things because it's a lot of client execution, client return, some wrong optional access list and then it may incur even worse result. So if you are interested in this topic, especially like parallel execution, I think this is a really excellent paper to take a look and a couple of ideas which actually highly aligns with what we are going, I'm going to talk about. So in order to address these problems, especially underutilization of this sslist feature, there are a couple of things that we are working on. One is to propose an EIP that is 7650, which is a really good name, easy to remember.
00:10:08.444 - 00:11:53.962, Speaker B: The idea is basically say hey, rather than have the user to encode the access list in the transaction itself, why not allow the program, the contract to tell that hey, I'm going to read these random locations of data at the very beginning and so that the contract, because the contract has the full knowledge what the access future access pattern of the execution will be, then we are able to harvest the parallel loading and reduce the latency and further the sustained benefits because every time it executes its contract it will load using this parallel access list, programmable access list to load the corresponding data. So the benefit will be sustained. Another thing is inspired by reason AI and we also found that we are able to using some way to find a common access pattern so that we can be able to even without program this data. But we are still able to learn what are the common random locations when execution like smart contract call will read. And then we are able to learn these things and then load this in the future execution. So I will explain all these two in details. So first of all, before we dive into details of how we are able to implement this parallel iOS, one thing I would like to highlight is about what are the underlying storage, how many iPad iOS the underlying storage can offer.
00:11:53.962 - 00:13:13.294, Speaker B: So here I just give a commodity 4gb mme ssd which is about $300 I believe every node can offer. And so I run some parallel IO program which is the famous Fio. And so this will tell us hey, how many iOS that I parallel issue to the underlying storage and what is corresponding latency for each random access. So that we can tell that how much parallelization we are able to achieve in terms of IO using a commodity like disk. And we can clearly see that if even we just increase the number of random I OS to, for example twelve, more than ten, the number of the latency increase is just very very minimum. And for some numbers like four is almost basically the same. Which means that if I can issue, suppose I sequentially read this random data, or I ran this data parallel in four iOS, they actually use the same latency when reading all this data, which shows the current capability of the mmads that allows to hide high number of the parallel I O and random access.
00:13:13.294 - 00:14:14.124, Speaker B: And so you can also use the famous FIO tools to reproduce these numbers. But this tells us what is the ultimate optimization goal that we can achieve using a simple commodity PC. And definitely we can use even higher advance like using raid zero or using more advanced SSD's. But this is some number that we can easily get. So the program for access list is allowed a smart contract to basically specify which part of the data you'll be accessed ahead of the actual execution. So let's take Uniswap swap as an example. The uniswap swap, each time it will read five, that is the storage slots in including the address pairs of the tokens and reserve, which is basically a compact version of their balances.
00:14:14.124 - 00:15:28.676, Speaker B: That is the token that has deposited into the contract and price zero, price one accumulate last. That is using to calculate some statistics and some numbers so that they are, I think they are used to distribute some fee all those things to the basic LP and also yeah, the LP I believe. So these five storage slots will be constantly re when calling a uniswap swap. So suppose now we have a parallel I o loading in a smart contract and also the corresponding Obico that is supported. So now we are able to start. When we start this call of method we can say hey I'm going to read all this data, let's just prefetch all this data and ask eem to say hey I want to read this data in the future, please read this data ahead my execution and this can also refresh these two contracts to tell us hey, I'm going to call these two contracts. For example, I want to check the balance of the call token zero and balance of token one of these uniswap contract.
00:15:28.676 - 00:16:12.760, Speaker B: Then please load this contract ahead before I call this. So we can list as much as we could. It can be combined with some parameter that is input so that we can also load. So this is, you can see that this is fixed locations every time it's fixed. But definitely we can combine with some input parameters so that we can know that what is the future access by the smart contract. And every time when they call a smart contract, we automatically load all the data in parallel. So how we are able to with this now we basically reduce five random access into one random access.
00:16:12.760 - 00:17:13.180, Speaker B: Suppose we are using some devices with this high performance with this latency because we can see that there's no latency increase with about four or five iOS and which means that we are able to reduce these iOS. For example, suppose it's 70% of the execution. Then we can reduce it maybe half or even less because we are using the underlying parallel I O capabilities of disk. And also we assume the gas is the cost of the basic preloading is the same parallel preloading as seen as a one sequential loading. Then we can enjoy a constantly gas reduction. For example right now, one random access of the storage store, it takes about 2100 gas price gas, then we are able to save to single 2100. Suppose we assume this gas price is the same, but definitely there are rooms to tune.
00:17:13.180 - 00:18:23.944, Speaker B: But suppose we have this safe assumption, then we can easily save thousands or tens of thousands gas per transaction. So this is the basic idea of parallel IO. You can imagine it's kind of like traditional programming with parallel threading all these ideas in the program by itself. And at the same time we also observe that all these random locations that are constant for each uniswap call, and even without, for example, I tell them whether this location they do not depend on any input parameters. So the idea is that would be possible that we can have the EVM to learn those common read patterns associated with a method. In this case is the swap method which can be easily figured out by using the method id according to the ABI encoding. So that we tell that for this method this is common access pattern, for this method, maybe for the mint, this is another common access pattern.
00:18:23.944 - 00:20:12.754, Speaker B: So that once we are able to learn this and once this has become stable, then we can easily use this idea to for example, every time a call a method, find the access list, call another method, call a method, next time it will basically filter out this, find out the common list, and then for the future they can use this list to basically call all the smart contract automatically without having any for example introduce new eips. Just purely evm optimization. And that's basically idea what we call intelligent IO preloading as we observe a lot of patterns in all of smart contract which has this constant common access patterns in the smart contract core. So here's my concluding remarks. In this talk we explore two innovative strategies which aims to reduce the I O latencies by enhanced access list which proposed by Victoric. The first idea is that we are able to allow the programmer to specify what is the future data locations that I would like to read and please read this data ahead in parallel so that we can greatly reduce the latency when we really read this data because this data are already in cache. And the second idea is that given the method ID and also the contract, then we are able to learn to identify what are the common access pattern of these contracts and then just can automatically got the benefits of these parallel iOS.
00:20:12.754 - 00:20:48.174, Speaker B: We hope that all these technologies once we become mature in the future EVM, we are able to significantly accelerate the EVM execution. That definitely together with other features, for example that is like just in time compiler that mega is developing so that you can reduce the interpreter time to interpret I O codes and then SQL. So there are a lot of interesting ideas that's flying around and that's all of my talk today. Thank you very much. I'm happy to.
00:20:52.954 - 00:20:54.578, Speaker A: Anyone has questions?
00:20:54.746 - 00:20:56.734, Speaker B: Yeah, questions please.
00:20:57.034 - 00:21:31.414, Speaker C: Yeah, I want to ask. So paralyzed evms has recently become more of a narrative in the context of other chains. I was curious given that you're kind of closely familiar with this, if you have seen some chain or implementation of this that you think could be long term viable in terms of connecting for example Solana and Ethereum, because there has been neon eclipse and different kind of attempts at achieving this. So what do you deem the best effort at this right now?
00:21:32.714 - 00:22:36.674, Speaker B: So the question is about what is the best effort to make parallel EVM infrastructure in production or success. So first of all, I have like one is, according to my experience, previous experience, trying to fix a lot of parallelization bugs in general takes a lot of time to make concurrency works correctly. So first of all, it's really hard to debug and also it's really hard to make sure that the code runs correctly and also, for example, writing the test. So I think right now we are at a little bit early stage. One example I can tell you is that so Polygon has implemented their version of the parallel EVM in terms of based on top of gas. So we took a look at the code and study how it works. It's basically our idea of block STM software transaction memory using the optimistic execution.
00:22:36.674 - 00:23:25.212, Speaker B: But two things we observed, we found a couple of bugs and fixed them and saying oh, looks like there's some easy bugs. And I say why not? It's happening in the production and later file actually is still in the experimental stage. It hasn't been used in their fixed MMA net. So it helps to answer some questions. What is the ultimate performance gain using all these technologies, but still in order to run in production then it will. I think it would take some time, especially for some critical applications. I won't imagine if theorem layer one will use it because right now the bottleneck is consensus.
00:23:25.212 - 00:23:54.552, Speaker B: It's not about the iOS, but as more and more like L2s are coming and definitely they are able to harvest benefits. And also there's the idea. Diversify client. Suppose I have a client that runs parallel EVN and no other client can catch up. Then I have no idea. When it enters something that's wrong, I cannot tell because nobody can tell me. Oh sorry.
00:23:54.552 - 00:24:44.424, Speaker B: The transfer balance is supposed to transfer 100 e's, but because some parallel EVM is scheduling it should just transfer ten es. But with the diversity climbing. So we have two implementation of client, but each of them can catch up with each other and then it will be much easier also to figure out the problem. This is the same issue applies to ZK proving, because ZK proving is also using the diversity diversified Zika provers, so that if something is wrong then we can use multiple proverbs to make votes rather than maybe people just using a multi signature. So there are a couple of things to address this problem, but as an infrastructure I think it's in a very early stage. We need a lot of excellent engineers, excellent researchers to help to solve this problem.
00:24:44.544 - 00:24:45.364, Speaker C: Thank you.
00:24:46.704 - 00:24:47.844, Speaker A: All right, great.
00:24:50.104 - 00:25:04.064, Speaker D: From smart contract development point of view, do you foresee any downsides of using this prefetch? Like are there any reasons why every new smart contract will not just like prefetch everything?
00:25:04.924 - 00:26:31.570, Speaker B: That's a great question. So I have developed a lot of smart contract. One concern as a smart contract developer is the guess, because I really want to have other applications users when coarse, the gas can be safe in a really, really optimized way. That's why for example Opensea, when they try to optimize their DaX on chain, they use it assembly code rather than solidity, because they have money to do this extreme optimization. And for us, I think if there's a proper way to reduce the gas using this explicit gas metering to save this gas, and especially those parallelization execution takes a lot of execution, runtime, war time, and then, so that from the EVM perspective, and also developers perspective, there's interest alignment, so that we want everybody to implement this feature so that smart contract developer can save gas. At the same time EVM can process more transaction. So I think this is the ultimate goal, that win win goal that both developer and EVM protocol developers can be harvest from this approach.
00:26:31.570 - 00:27:00.568, Speaker B: But how to determine the gas, for example, how many parallel loading that we are able to accept, whether it's full four or five or maybe ten, there still leave a lot of room to discuss. But I don't think that should be impossible because we see that like 300 me disk is already be able to support a large amount of paleo IO. Yeah.
00:27:00.696 - 00:27:15.584, Speaker D: Another one. From a validator perspective, do you think it will increase the load on validator hardware? Will validators have to upgrade somehow or like what, what is running now? It will suffice.
00:27:16.244 - 00:27:23.024, Speaker B: So the question is like whether was the validator maybe had created concern that increased their nodes, right?
00:27:23.324 - 00:27:34.664, Speaker D: Yeah, I mean the load on their hardware, if they will need like more, I don't know, more storage or more memory or like, or. Yeah, basically, yeah.
00:27:35.604 - 00:28:49.908, Speaker B: So that is a really good question. For example, recently arbitram, they are having an issue that because they process too many transactions, so a lot of other nodes are not able to catch up with their latest transaction even they're just running all those transactions 100% because the sequencer or arbitrage sequencer has much higher requirement devices compared to others. So that is really good questions. Actually we run a lot of Ethereum nodes and to do a lot of experiments, I think I found there are some cloud providers that offering some Ethereum friendly devices with pretty much, I would say really high performance, like 128 gigabyte memory cores, about seven terabytes. Raid zero SSD can offer 7gb read and write. That takes about 70 euro per month. And I think that should be acceptable or something that we are able to design for for this spec from our protocol designer perspective.
00:28:49.908 - 00:28:51.664, Speaker B: That's my answer, yeah.
00:28:55.184 - 00:29:16.104, Speaker D: Okay. I was asking more about, from perspective of home validators who run like on, who have hardware from the, on the lower side, like of performance. So like rock pipes and things like this or arms. Yes.
00:29:16.184 - 00:29:18.968, Speaker B: So you mean the, whether it increased the cost of home validators?
00:29:19.016 - 00:29:31.164, Speaker D: Yeah, exactly. Will it increase the cost of running, of running home validator? Yeah, basically. Who don't have like big servers or who don't pay like to cloud services.
00:29:32.704 - 00:30:12.664, Speaker B: Depends on what kind of home validators like. For me, I kind of like geek for a computer. I assembly those things by myself. I don't think it is hard to buy some cheap like commodity devices. Like for example, I have set up like four SSD Samsung 990 which can easily offer 10gb read and write and with maybe less than 10,000 1000, so. But definitely actually it's really great questions whether I suppose I would like to run it in the cell phone or some.
00:30:13.204 - 00:30:16.076, Speaker D: Rock pie is very common. Sorry, rock pie?
00:30:16.260 - 00:30:17.424, Speaker B: Yeah, rockspot.
00:30:17.764 - 00:30:18.548, Speaker D: Yeah.
00:30:18.716 - 00:30:45.934, Speaker B: I do feel right now is you've seen using the modularity layer solutions so that for L2 they can run as fast as possible. But for layer one, like for example, we can have a vertical tree status list. Like bandwidth is also working on that so that we are able to allow, for example, maybe iwatch or some devices to vote at that time. The parallel evn actually is not the major issue. Like execution is not a major issue. Superwoman.
00:30:46.714 - 00:30:48.322, Speaker A: All right, thank you so much, Chi.
00:30:48.378 - 00:30:48.802, Speaker B: Thank you.
00:30:48.858 - 00:30:57.914, Speaker A: Did we learn about parallelization after that session? Okay, great, thanks. I learned something too. All right, next speaker is Tobias.
