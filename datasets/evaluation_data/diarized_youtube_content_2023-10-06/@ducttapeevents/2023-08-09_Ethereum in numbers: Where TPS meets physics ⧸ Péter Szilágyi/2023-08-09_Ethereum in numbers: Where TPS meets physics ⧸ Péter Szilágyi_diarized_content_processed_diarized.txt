00:00:22.000 - 00:00:59.154, Speaker A: Hey, all. So Ethereum and transactions per second, usually that's a combination you don't really want to read on the Internet. It's usually those sentences sound bad. And what I wanted to kind of talk about today is that usually when you read about transactions per second and Ethereum is how Ethereum sucks and your favorite Ethereum killer just rules all. And I wanted to talk about it a bit, how much truth there is to it. There's always the truth is always somewhere in between. But let's try to dig a bit deeper into it now before we do a little bit of a detour.
00:00:59.154 - 00:01:59.854, Speaker A: I don't know if you've probably most of you heard about the boring company. It's one of Elon Musk's company. The goal of the company is to dig tunnels underneath large us cities like LA and somehow try to speed up transportation system, somehow try to avoid the traffic. And, well, a lot of people would like to do the things that he wants to do, but the challenge is that tunnels are prohibitively expensive. So everybody says it cannot be done. And, well, Musk had this really nice idea, or really nice questions, rather that, okay, when we say that the tunnel is prohibitively expensive, is it because physically it's impossible to construct it faster or cheaper? Or is it because we socially accepted that it's prohibitively expensive? And the answer generally is that while a tunnel has a certain diameter because it needs to contain a ventilation system, emergency exit, etcetera, etcetera. But if you take a look at it a bit closer, the only thing it truly needs to fit is one car.
00:01:59.854 - 00:02:31.488, Speaker A: The ventilation system is necessary because most cars are combustion engine coils and they need oxygen. So without oxygen, the cars just stop. But if you just switch everything over to electric vehicles, boom, no more ventilation system, or at least a much smaller one. Emergency exits are obviously needed. Unless you take the control out of people's hands and you just automate everything, you just let the AI drive. In theory, no accidents happen again, the tunnel can be reduced. And plus there are a few more things that you can explore.
00:02:31.488 - 00:03:11.456, Speaker A: The interesting thing is that the observation here is that physics kind of limits you to certain aspects. And then we construct a lot of extras on top. But the truth value in this certain case, particular case, the true cost of ethanol is kind of limited by physics. The rest we just pile on top. Now to go back to Ethereum, what does this mean for us? Everybody on the Internet always says that Ethereum does not scale, and that's a very, very hard thing to unpack. It's very ambiguous. But what people kind of say, or want to say if Ethereum does not scale, is that Ethereum is too expensive for them.
00:03:11.456 - 00:03:44.164, Speaker A: This is again a bit ambiguous. What does it mean that Ethereum is too expensive? Is a Tesla car too expensive? Well, I mean, it depends on your salary and depends on what you want to use it for. So saying that Ethereum is too expensive, that's too subjective. But we can agree that Ethereum is expensive. Now, expense is kind of driven by supply and demand. So if you would like Ethereum to be cheaper, we can never make it cheap, because cheap, I mean, unless it's free, it's always too expensive for somebody. But if we'd like to make it cheaper, then the two things we can do is either raise capacity or lower demand.
00:03:44.164 - 00:04:28.460, Speaker A: And the EF is actually doing both of them, lowering demand. We're trying to drive people towards Il two s and raising capacity. Well, actually, can we do that? And well, capacity is kind of, people kind of define capacity as transactions per second, which is again super ambiguous because I mean, is a plain transaction, an ETH send, a Dex swap and NFT auction. These are all transactions, and these are absolutely not comparable. So what ethereum actually does under the hood is that it uses gas to abstract out the disproportionality of transactions. Now, ethereum currently runs at 1.1 mega gas per second on average, and we kind of feel that this is a pleasant number.
00:04:28.460 - 00:05:26.488, Speaker A: However, there are other networks like Avalanche and the binance smart chain who are really pushing the limit. And then the question is that while they are essentially internally geth, at least the EVM, they have the same state, so how can they push it faster? Is there something they not telling us? And there's a really nice hint there that kind of points to what they're not telling us. Binance March 1 year ago was running at 25 million gas per second, and currently it's running at seven. So we can say that the binance marching guys are super smart and they optimized gas, but how did they unoptimize it one year later? So what happened there? And the answer is that gas is a fairly bad approximation of how much it costs to run a transaction. Because usually we have four aspects to a system load. You have cpu load, memory, disk and network. Now running a transaction last year and this year takes the same amount of cpu, same amount of memory, same amount of network traffic.
00:05:26.488 - 00:06:08.736, Speaker A: However, disk is a bit interesting. Specifically, I won't go into the detail, but this is how ethereum stores its state it's kind of, all the accounts are laid out flat, and we have a cryptographic tree structure on top to just try to prove that everything is correct. And, well, since this is a logarithmic structure we have, every node has 16 children. Every mathematician will say that this is super efficient. We don't talk about it anymore. Yeah, the problem there is that as time passes and the state becomes bigger, this logarithm starts to appear and starts to cause troubles. Specifically, currently Ethereum has about 170 something million accounts which translate into a state trie of depth.
00:06:08.736 - 00:06:31.234, Speaker A: Eight. That's nice. That's a perfectly acceptable tri depth. So when we try to do a transfer, let's say Alice tries to transfer one ETH to Bob, then we need to create 15 new trinodes. We just need to create eight for the new account of Alice and eight for the new accountant, Bob. We have a shared root. Ok, 15 per transaction, still not bad.
00:06:31.234 - 00:07:04.212, Speaker A: Yeah. The problem is that we store the state in LeveldB, which is another try, which means more levels. And levelDb specifically stores data in seven levels, which amplifies my 15 writes into 105 writes. And in order to actually calculate the new Merkle path, I need the old one too. So that doubles my input output operations. So we're up at 200 plus input output operations per transaction. And of course, mining in Ethereum, it's not run, Ethereum isn't run by a single node.
00:07:04.212 - 00:07:50.084, Speaker A: So we have a miner, and then the block is mined and then propagated, mined, propagated. So every transaction is kind of ran twice. So if we do that, essentially what we arrive at is that running a single transaction will require 420 input output operations on the network. That doesn't sound bad, except when you start laying out the exact numbers of different technologies. Hdds are kind of kept by their seek times. You have the various SSD's with various transports, Sirulata, NVMe, PCI Express four. And I mean, I won't go into the numbers, but the idea is that every hard drive technology has its limits, and if we just take those limits and divide them by 410, we get some super hard numbers on what is possible.
00:07:50.084 - 00:08:42.774, Speaker A: And Ethereum currently tries to aim for PCI Express three NVMe hard drive. So it means that technically 857 give or take transactions is the absolute maximum that the ethereum network can do. The truth is, we are actually doing more than that now. And the way we are doing more is these limits are only relevant if every single transaction pushes everything to disk. So kind of like an archive node in our case, what we try to do is to try to keep as much data as possible in memory, because then all of a sudden, we don't have to screw around too much with disk. So what does it mean to keep stuff in memory? There are actually two two ways in which we can keep data in memory. One is actually done by the operating system for free for us.
00:08:42.774 - 00:09:34.324, Speaker A: If you have a 64 gig machine, the operating system will actually use the whole state, I'm sorry, the whole ram, everything to cache disk. So whenever you try to access all the data, most of the time it will access it from memory. The other thing that geth does is that we try to do state pruning. And state pruning kind of means that if Alice sends one ether to Bob, then one ether to Claire, and then one ether to Dave, then all these intermediate states that Alice's balance was decreased by one, two, three ether, you don't really need to save that. It's fine to just save the last state. And these are nice optimizations that allow us to reduce the number of input output operations, and thus to actually raise the transaction per second capacity of the ethereum network. However, this omits a very interesting detail, and that is that system memory is limited.
00:09:34.324 - 00:10:18.588, Speaker A: So if the operating system is really nice at caching stuff in RAm, if I have enough ram to fit the entire state into it, or we can use a very nice in memory pruning in Geth, if we have enough ram to fit it into it. Now, I'm not entirely sure what the average geth Ethereum node user has. How much ram? I don't know, 32, 64 gigs. That's probably on the high end. But Ethereum state is currently over that way over. And then what happens is that the more you overflow it, the more often the operating system produces a cache miss or page miss page fault, and that results in actually reaching down to disk. And the same happens with pruning.
00:10:18.588 - 00:11:14.184, Speaker A: I can prune, I can stuff a certain amount of data into our pruning algorithm, but as the blocks go, as the state grows, I either need to keep raising that limit, or I keep leaking more and more and more info data down to disk. And this is actually a self referencing cycle, because the more the state grows, the less ram I have to operate in it. So the faster the state grows, the less ram I have up to operate in it. And what happens is that I am eventually reverting back to my original original transaction per second cap produced by the hard drive. And the question is essentially based on this slide we kind of see that big state is bad, but how big is. I mean, it's very hard to put a number as to how big is bad. What we can try to see is, okay, how fast does it grow? Because the faster it grows, the worse it gets or the faster it gets worse.
00:11:14.184 - 00:11:34.044, Speaker A: And last week on a Sunday. So Sunday is considered a pretty idle time for ethereum. Pretty much nothing happens on the network. The Ethereum state grew by approximately, I don't know, 0.64 account per second, eight storage slots. That doesn't say absolutely anything to anyone, not even to me. It's just a random number.
00:11:34.044 - 00:12:15.438, Speaker A: If we look at how much actually it weighs, bite size, it's approximately, I know, 600 and something bytes per second, which is 55 megabytes per day, 20 gigs per year. Okay, those numbers start to make a bit more sense. But this is actually just the raw data, just the useful data that, okay, Alice has one ether or Bob has two ether. It doesn't contain any of these Markov proofs. If we actually just dump all of those numbers in there, I won't read through the numbers. We get kind of an extra double the size. So all of a sudden, the merkle tree weighs, I don't know, 40 gigs per year.
00:12:15.438 - 00:12:32.514, Speaker A: That's the addition. But this was interpolated from a Sunday. During the week, it might be two, three times more. So it kind of fluctuates. It's kind of hard to just say that, okay, this is the growth. But let's say, I would say maybe 50 gigs, 100 gigs per year. That is the growth of the Ethereum state.
00:12:32.514 - 00:13:35.802, Speaker A: I'm not talking about blocks, I'm not talking about receipts, I'm not talking about anything, just the raw state that's needed for contract execution. And the question is, is this a lot, or is this acceptable? And I don't know. I mean, if I have 64 gigs of ram, then piling 100 gig every year on top of it kind of exhausts it pretty fast. And what does that kind of mean? What that means to be a bit apocalyptic is that Ethereum and all its forks is kind of like on a potential death trajectory currently. Meaning that if you take any of the Ethereum networks, by Ethereum networks, I mean Mainnet or the avalanche contract chain, or the binance smart chain, or pretty much anything that is is a fork of Ethereum. And if you just stick a constant transaction throughput on it, the state will grow. Maybe it's slower, maybe it's faster.
00:13:35.802 - 00:14:06.092, Speaker A: In Ethereum, it grows at 100 gigs per second, but binance margin at some point was, sorry, 100 gigs per year. Binance margin was at some point running at 25 times our capacity. So that would be 2.5 terabytes per year. That's a nice number. And the problem is that the more that number grows, the higher ram requirements we have. And the higher ram requirements we have, the more and more data we flush the disk, the more I ops we generate.
00:14:06.092 - 00:14:35.804, Speaker A: And essentially, we're hitting that brick wall. That thingy I mentioned, those numbers, they are approximations. We can debate whether it's two x larger, 0.5 x smaller. It's the order of magnitude that's count. But those are the numbers that the hard drives impose on us. So the moment we revert to running on hard drives, the moment that those are, sorry, the hard limits.
00:14:35.804 - 00:15:16.220, Speaker A: So, but that still begs the question. Our mainnet is relatively slow compared to everybody else's mainnet. So does that mean that we just suck at it? Are others good at it, or is the truth somewhere in between? And my answer is that our main net can actually do significantly more transactions per second than we. Sorry. It can do significantly more transaction per second than we allow. However, I appreciate it. Very nice.
00:15:16.220 - 00:16:14.914, Speaker A: So Armenia can do significantly more transactions per second than we allow it. However, the more we allow it to, I mean, the faster we allow it to grow, the faster we bring that brick wall closer. And it's kind of a race against time. Can we keep that brick wall far away so that it doesn't actually impact us in a very, very detrimental way? Or is that brick wall something that just arrives and Ethereum shuts down and game over? And usually there are some very nice proposals. We have the EIP 4444 mat, I think, which essentially aims to get rid of historical chain segments. Unfortunately, that doesn't really help the state growth. We have shard blob transaction proposals, which try to move as much data as possible to l two s plus delete them afterwards at l one.
00:16:14.914 - 00:17:14.514, Speaker A: Okay, I'm sorry I haven't talked in a while. So it, yeah, so the sharp blob transactions try to move as much data as possible on top of l two s. And that little data that actually ends up on l one s, it's defined in a really elegant way to be able to be pruned off. But unfortunately, neither of them really touch on raising the capacity or moving this brick wall forward. And it's okay. So essentially, we are kind of running out of time, and the only interesting solutions we have is state run, which was shut down. We have exponential costs, which is currently the current situation, or we have probably, possibly stateless clients, and that's kind of the hopeful direction that we are going.
00:17:14.514 - 00:18:30.774, Speaker A: However, this does mean that there is no solution existing currently, and Ethereum has time. However, if you look at other chains which are pushing limits super hard, the danger there is that they will actually beat out Ethereum on chain growth and they will actually beat us to the brick wall. And that's not a problem for us because we can see what happens when people reach the brick wall and you can actually see it on a balanced smart chain. It's really interesting to dig through their issue tracker, but it's a dangerous place to be at to arrive at the brick wall without a solution. So currently, my takeaway is that Ethereum's transaction per second is deliberately low. We could push it significantly harder, but that would actually make it significantly harder for us or give us significantly shorter time span to fix things. It's definitely the smoke, but I guess the good news is that we are trying, and we have pioneers who are pushing it harder than us so we can learn from them.
00:18:30.774 - 00:18:47.438, Speaker A: And thank you. Aw, you made it.
00:18:47.566 - 00:18:48.966, Speaker B: You made it. Wow.
00:18:49.070 - 00:18:49.846, Speaker A: Barely.
00:18:50.030 - 00:19:10.724, Speaker B: Yeah, but you still made it. You didn't hit any brick wall. Do we have anyone in the audience who has any questions for Peter about who's first? Okay, great. Oh yeah, wait, we've got a, sorry, one behind you with a microphone. Yeah, thanks. Just quick question. So none of the things you talked about were like latency or Internet related.
00:19:10.724 - 00:19:14.964, Speaker B: So what if computers just get better fast enough?
00:19:18.064 - 00:20:15.854, Speaker A: That would be nice, but they don't. So the problem is that currently, so computers get better at certain areas, but current limitations are kind of disk latency. And currently the best technology is PCI Express and Vime hard drives, which have approximately 1 million IO operators per second, which translates to 2500 800 transactions per second. And there's nothing really coming that would make it significantly faster. So you could say that, okay, there are always some super fancy technologies that you could deploy in a data center or in a controlled environment, or if you are, are willing to spend some crazy amount of money. And I guess that's what kind of binance goes towards, that they have a limited number of validators, and then you can just push them super hard and have super high hardware requirements. That's an option.
00:20:15.854 - 00:20:51.284, Speaker A: Ethereum kind of tends to take the more public approach where anyone can run a node. I mean it does have the costs, but if you want anyone who's willing to run a node, be able to run a node then you kind of need to keep the hardware requirements within reasonable levels. And I don't think there's anything coming for average users that can actually make it ten x better. So, long story short, l two s are the way to go, and l one needs to be kept in a reasonable shape.
00:20:53.604 - 00:20:56.624, Speaker B: Okay, there was a second question here at the front.
00:21:00.484 - 00:21:17.904, Speaker C: Hopefully not a bad question. I run some nodes for pocket network, and I run Aragon as the archive node, mainly because it's so much smaller disk footprint than running geth. Are they doing something totally different? Is that applicable here, or is this some new technology? Thanks.
00:21:19.984 - 00:21:57.974, Speaker A: I'm not entirely sure I caught the question, but let me make an assumption, and then you can correct me. So I guess the question was somewhat around the lines that if Aragon is kind of smaller, then wouldn't Aragon be able to run more transactions per second compared to Lacithe guest. And that's somewhat true. The problem is that that's why I highlighted how much raw data we add to the network. It doesn't matter whether it's geth or Aragon. The raw data is kind of, that's the amount of data that gets added. And the Merka Patricia three, those operations per second, input output operations per second.
00:21:57.974 - 00:22:29.198, Speaker A: Those are the raw operations required to calculate the proofs. Now, in the case of Aragon, they do all those operations in memory, but the catch is that it has to fit in memory. And once it doesn't fit, Aragon also tries, or also starts to leak stuff out to disk. So again, it's kind of like this trade off your. In order to do the mercotition calculations, you must do those operations. How much you can store in disk depends on your algorithms. But if disk.
00:22:29.198 - 00:22:40.924, Speaker A: Sorry, how much you can store in memory. But if memory runs out, then you're falling back to disk and you're falling back to the brick wall. But yes, you might have a different Runway with a different data layout.
00:22:41.464 - 00:22:49.844, Speaker B: Can we move some more questions? Would that be all right? I'm sorry. Thank you. Thanks.
00:22:51.104 - 00:23:43.114, Speaker A: What was the reception of binance chains? Reduction of the capacity of the TPS. And would that be potentially an option to vary the TPS? So the reason binance machine, as far as I know, reduced the gas limit, they are pushing it super hard. The state was growing super fast. And the thing they hit, they had very, very strong machines, so they didn't hit the limitation that they couldn't process transactions. What they actually hit is that they couldn't synchronize. So if a new node was joining the network, it just could never catch up and they realized that they just have to stop, have to slow down, because they don't have the algorithms necessary to handle that throughput. Now again, the question is binance smart chain is using a bit different hardware.
00:23:43.114 - 00:24:03.734, Speaker A: They are really pushing the hardware, I don't know. So they reduce it from 25 to seven. Whether this seven is because that's the maximum they feel comfortable with, or I don't know why exactly seven, but it definitely was that they were, people had so many problems with running their nose that they just had to do something.
00:24:07.584 - 00:24:09.644, Speaker B: Okay, we have another question over here.
00:24:14.184 - 00:24:34.294, Speaker D: So I have a couple of questions. One of them is you said that LevelDB, so we all know that LevelDB is a suboptimal data structure, basically to store Ethereum state. So have you looked into other solutions or are you actively looking into other solutions than interim optimization?
00:24:36.314 - 00:25:20.418, Speaker A: A lot of people try to create an alternative database, but it's not really easy to create a new database from scratch. And the problem is that LevelDB also makes some trade offs. LevelDB actually is a very, very good system because it is very compact and it's highly performant. You could make something faster, but that could entail just the disk usage exploding. And the question is, okay, are you willing to be 1.5 times faster, but use ten x more disk space? Now certain people are, certain projects are experimenting with creating their own database. I mean I think parity at some point wanted to create their own database.
00:25:20.418 - 00:25:47.838, Speaker A: Avalabs wanted to create their own database. I haven't seen anything working yet. It's definitely interesting thing, but it is a very very hard thing to do. And other databases that are out in the market, essentially they all rely on the same fact, the same tri structure. LevelDB stores the keys and values together. RoxdB stores them separately. Now we have pebble, which is kind of somewhere in between.
00:25:47.838 - 00:25:55.714, Speaker A: There are variations, but as long as you have a tree structure with levels, you kind of somewhere introduce this amplification back.
00:25:56.154 - 00:26:10.362, Speaker D: Okay, thank you. And the second question is, I heard at the time that there was somebody within EF researching decentralized storage solution. Like a bespoke solution to do archiving.
00:26:10.538 - 00:26:52.334, Speaker A: Is there anything for decentralized state storage? I guess if you want to store a historical state, that's very nice. I mean you can definitely store it, but there you don't have the requirement to have instant access to it. Because I mean, if I want to dig up what my account balance was last year, I can wait half a second or 1 second or 2 seconds. However, if I'm a full node that needs to run this block now, or I'm a miner and I have 100 milliseconds to create a new block, then I cannot rely on some remote nodes to give me the necessary data. So just the network latency will be too high for me to gather all the state data to be able to run that block.
00:26:55.594 - 00:27:01.394, Speaker B: Okay, I think we're up for time. You survived it. Thank you very much, Peter.
