00:00:02.800 - 00:00:32.442, Speaker A: Hello everyone, my name is Togrul. I work at Scroll. I do mostly research and shitpost on Twitter. And today we're going to be talking about sequencer improver separation in zero knowledge rollups. So let's start with the roles that zero knowledge roll ups have in terms of protocol participants. So we have a sequencers and we have provers. So what is a sequencer? A sequencer is an entity in the network that is responsible for ordering transactions.
00:00:32.442 - 00:01:37.874, Speaker A: So you can imagine if you're a user, you generate a transaction, you send it through your wallet, the sequencer picks up those transactions, constructs, blocks, or in this case batches out of them and submits them to the base layer. So in our case it will be ethereum and you have provers. The provers are responsible for constructing the computing, the validity proofs for those batches that were committed by the sequencers. And basically what those validity proofs allow us to do is they allow us to prove that the transactions that were added were correctly executed and the state that was derived from them was correct. And this allows us to have the communication with the underlying base layer. And the way it works is, as you can see, you have a sequencer, a prover and ethereum, which is the base layer with which the sequencers and provers communicate. And first the sequencer publishes a batch, and then after the batch is published, it also propagates a batch to the proofer or the contents of the batch, the box.
00:01:37.874 - 00:02:19.665, Speaker A: And the prover computes the proof and publishes the validity proof on chain. And that's it, the finalized batch. Another question is, it's all good, but how do we decentralize it? Because unfortunately, all the current roll ups and all the construction that are going to come on in the next few months are going to be completely centralized. And while it might seem just fairly trivial to decentralized things, it's actually quite complicated. And there are things that you need to consider. Actually, paradoxically, it's more difficult to decentralize an l two than an l one. And let's start with the sequencers.
00:02:19.665 - 00:02:59.176, Speaker A: So how do you decentralize a sequencer? Well, an obvious answer would be slap some classic consensus mechanisms. So what you would call a BFD consensus. So tendermint, hot stuff, etcetera, and that's it. Call it quits, or maybe base roll ups. And I'll explain what base roll ups are later. So advantages of a classic consensus mechanism is you have fast confirmation times, which means, let's say, if you're submitting a bash to the base layer. If you don't have an external consensus or some kind of economic confirmation, you have to wait for Ethereum to finalize, which is usually quite slow.
00:02:59.176 - 00:03:46.064, Speaker A: And using a classic consensus mechanism allows you to have economic off chain economic guarantees that your transactions will be added at some point in a given order. And another big advantage of using a classic consensus mechanism is that you retain the control of the MEV profits. So let's say you order the transactions, you extract some values, some value from them by defining a certain order. And basically then the protocol can decide how to decide distribute that meV, whether to burn it, whether to distribute it to provers, et cetera, et cetera. And I'll come back to that later. And unfortunately, there are cons to this approach. And the main con is that it's non trivial to implement.
00:03:46.064 - 00:04:32.458, Speaker A: So the problem is that for an l one, if you add a classic consensus, it has certain properties that it guarantees. But for an l two, we don't really need those guarantees, because the ll one that we connect to already provides those guarantees. So it's more of a resilient mechanism that adds resilience and stronger real time censorship resistance guarantees. And therefore it's more complex to add, and it also adds a lot of overhead. And plus you need a fallback. So because a roll up needs a way for users to either continue with transacting or allow the users to exit, even if the sequencers fail, you need to have a fallback mechanism. So let's say you use something like tendermind.
00:04:32.458 - 00:05:25.750, Speaker A: If there's a liveness failure on Thundermind and there's no fallback mechanism, you're basically stuck, your transactions are stuck, and you either have to go through governance or some way, but it's not trivial. And the fallback mechanism would work in a fashion where you have, let's say, I don't know, ten minutes for the consensus to produce a batch, and if no batch is produced, then anyone can submit a batch and continue. And that adds even more complexity to the bridge and now base roll ups. So let me first explain what a base roll up is. A base roll up construction was proposed by Justin Drake a few months ago, I think it was in the winter. And basically it uses the infrastructure of l one searchers that would specifically listen to l two transactions, construct batches and submit them to the l one. And there are a few pros to it.
00:05:25.750 - 00:06:18.834, Speaker A: So one, it's quite trivial to design, it doesn't really affect the protocol that much. All the complexity is passed on to the searchers and two, there is quite a tight integration with Ethereum, and therefore you have quite strong real time censorship resistance guarantees and quite strong liveness guarantees. And you don't need to add any fallbacks or anything like that to your bridge. But there are cons unfortunately as well. And the first con is the confirmation times are very slow, or not very slow, but slow and equivalent to Ethereum. So let's say, because the searcher cannot provide any economic guarantees that the ordering will be exactly the same when the transactions are finalized, you essentially have to wait for Ethereum to finalize that ordering before you can transact any further. And that takes around twelve minutes on Ethereum currently, even more sometimes.
00:06:18.834 - 00:07:29.614, Speaker A: And the second downside, from the perspective of a roll up, is all the Mav profits are redirected to Ethereum, or more specifically to the searchers. And basically the problem here is that the provers are the main cost for zero knowledge roll ups. And how do you ensure that the provers are incentivized enough to retain them and have them participate in great quantities? And this doesn't really help because it basically redirects the large chunk of the protocol profits to the l one. Now let's switch to prover decentralization. So an obvious solution would be use an akomoto style competition mechanism. So let's say in bitcoin, you have miners, they compute blocks, they solve the cryptographic puzzle, and the one that solves it gets to basically propose the block. You could do the same here, where the first to compute the zero knowledge proof is the one to get the profit and to finalize the block.
00:07:29.614 - 00:08:01.828, Speaker A: And the second mechanism is a per slot assignment. I'll go into what it is later. A commodity consensus style mechanism doesn't really work. And the reason why it doesn't work is because it doesn't have randomness. So zero knowledge proofs are not really random, so they're deterministic. So if you and I compete, the same statement, the proof of the same statement, it's going to be the same proof, and therefore the most efficient prover is always going to win. There's no randomness.
00:08:01.828 - 00:09:08.790, Speaker A: So for example, in bitcoin, if you control 1% of the hashrate, your probability of winning the puzzle for a specific slot is roughly 1%. In here, if you control 1% but you have the most efficient prover, your probability of winning is almost 100%. So you can see how that has a long term centralizing effect, where because all the other provers are not, are just burning energy without actually being rewarded in any way, shape or form, they will just fall off and you'll end up having two or one mega provers that compute the proofs for every batch. And while it's okay from the safety perspective, because it doesn't really affect your safety of your funds, et cetera, but it's not great from protocol resilience because let's say that prover goes offline or they're attacked by a government or something else, some form of a regulatory attack or something like that, you're basically stuck. Your roll up is stuck. And the second approach is per slot assignment. So what I mean by per slot assignment is something similar to how Gaspar works.
00:09:08.790 - 00:10:02.174, Speaker A: So in Ethereum's Gaspar, you have the time is divided in slots, and every slot you have a new proposer that is selected as a leader to propose a block. And so essentially you have some pseudo random function that applies the validators, the existing validator set, and selects one of the validators as the proposers for that block. And the only difference here is that we need a timer with a permissionless fallback. Again, the same with the sequencer. And the reason for that is because in Gaspar, if you miss the slot, nothing really changed. The only thing that is changed is the fact that your chain grows slowly. Whereas for us, if you miss your slot and don't submit a proof, then essentially no other person can submit proofs for batches that are built on top of it.
00:10:02.174 - 00:10:43.194, Speaker A: Because essentially what happens is you would not be able to verify those batches without this batch being finalized. So you need some form of a fallback that allows you to basically have a permissionless system where anyone can submit a proof in case the selected leader does not submit a proof in time. And let's assume that we. Oh, sorry. Let's assume that we have a design. We picked a design, we use a classic consensus mechanism, and we use a per slot assignment for provers. So now the question is, yeah, you have two decentralized roles within one protocol.
00:10:43.194 - 00:11:33.924, Speaker A: But the question is, how do you make sure that the incentives are aligned? Because in this design, the problem is that because the sequencer gets the ordering rights, they get to keep all the MEV, and because they get to keep all the MEV, there is a possibility that MEV is a big percentage of your profits of the protocol profits, and therefore there's no incentive for someone to become a prover. And essentially people will become sequencers and not provers, when in reality we need both provers and sequencers to actively participate. So the solution is fairly simple. You might have heard proposal builder separation before, or PBS. It's a system in Ethereum where you have two roles. It can be in protocol or out of protocol. Currently, Ethereum uses out of protocol PBS, but at some point the plan is to add PBS to the protocol.
00:11:33.924 - 00:12:30.304, Speaker A: I'll abstract from the complexities of the design differences, but let's look at what a proposal and build their roles are. So a proposer is a slot leader. So what I described before, it's responsible for proposing a block and propagating it to the rest of validators for them or the community for them to vote. And a builder is an entity that actually constructs the blocks. And the builders compete with one another in an auction for their block to be selected as the winning block by the proposer. And what that allows you to do is to outsource the complex job of constructing blocks in an optimal way, where you can extract as much value to certain specialized builders and have all the rest of the nodes, validator nodes, relatively lightweight. So you can essentially run a validator node on Ethereum, on a raspberry PI.
00:12:30.304 - 00:13:16.934, Speaker A: But with a builder, it's a bit more complex, and the proposer have an option to just ignore all the builder blocks and submit the block themselves. But there's not much incentive in doing that, because the proposer is unlikely to extract as much value as builders. And so there's much more incentive to actually build one of the build on top of one of the builder blocks. And now we have the design for zero knowledge roll ups. We essentially follow the same pattern, but instead of having proposers and builders, we have sequencers and provers. So it's a sequencer improver separation design. So a sequencer would emulate the role of the builders in PBS in the initial stage of the slot.
00:13:16.934 - 00:14:14.906, Speaker A: So they would pick up some transactions, construct blocks in a way where they extract optimal value, compete in an auction where the prover selects the block with the highest pay, etcetera, etcetera. So the same logic basically, and provers emulate the role of the sequencers. Actually, it's not correct, the provers emulate the role of the proposers, but yeah. So in the initial stage, so they would pick the highest bidding block and basically select it and allow the consensus to build on top of it. So how it will work is in this example, let's say we have three sequencers. They compute some blocks, each of them, and propagate the bits. The bits is comprised of the amount that they're willing to pay and the hash of the block, because if they reveal the block, the prover can just steal that block.
00:14:14.906 - 00:15:06.748, Speaker A: It doesn't really. There's no disincentive from doing that. And so they propagate the bits. With the hash of the block, the prover selects the highest bid and basically propagates the signature that it selected the highest bid. So let's say if in this case, sequencer one is the one that was picked, it would sign the hash of that block and propagate it back to the sequencer, and then the sequencer would reveal the contents of that block that it committed to, and the consensus would proceed, working as usual. And the advantage here is twofold. So, one, it ensures that the distribution of the protocol incentive is somewhat done in a fair way, and also it's defined by the free market.
00:15:06.748 - 00:16:05.944, Speaker A: So essentially you can think of it as we don't actually define what the incentives are to age, but the market decides what the incentives are. And because in an auction, you're incentivized to bid as much as you can afford to bid, it's likely that in most scenarios, most of the profits are going to go to the provers the way it should be, because the provers are actually much more costly to run. So, yeah, this is the general idea. Bear in mind that I don't think anyone actually implemented it and things might change, but that's the direction where I think we should go in terms of sequencer improvers operation. And all of us have the same goal, is to build a protocol that is succinct, universal, gas efficient, off chain neutral, decentralized network. Yeah. Thank you for listening, and if you have any questions, feel free to ask.
00:16:18.554 - 00:16:45.334, Speaker B: Hey, thank you for the talk. I wanted to ask about the provers. How does the whole prover game work? Do you have a centralized prover at the beginning? Basically, how do you ensure that the prover can create a proof in time? What are the incentives there? Or how do you think about it if this is like on the roadmap for the future to have some redundancy or decentralization there?
00:16:45.714 - 00:16:51.094, Speaker A: You mean to ensure how the prover submits once it's decentralized?
00:16:52.154 - 00:17:11.214, Speaker B: I mean, you described how to make sure that you were talking about sequencer proverbs, but then the prover needs to generate the proof for the chain to progress. So how do you think about it? So we don't have just one centralized prover or three or so.
00:17:11.554 - 00:17:53.366, Speaker A: It's the question of incentives. First, the prover, only in the design that I described the second decentralization model. The prover only needs to compute the proof when they're actually selected, so they don't really waste any energy unless they're actually computing the proof. And the other time they're basically idle. And so the costs are not high. And because you share the incentives, MEV incentive profits with the provers, there's actually quite a big incentive to become a prover. And as I described, the way we work is, let's say if you, the prover time for you is, I don't know, 510 minutes, you create a slot that is 15 minutes just to allow some leeway.
00:17:53.366 - 00:18:17.794, Speaker A: And the prover has those 15 minutes to submit their proof. And if the proof is not submitted, then anybody else can submit the proof for that particular batch and just steal their reward. And that way you ensure that even if the unsigned prover for a specific slot fails for one reason or another, you always have a fallback where others are incentivized to actually steal their reward from them.
00:18:18.174 - 00:18:30.868, Speaker B: Okay, but the prover is somehow preselected, like nobody else can submit the proof, just the one prover for a specific block. Or is there like arrays of multiple provers to submit a proof for the certain?
00:18:30.956 - 00:18:59.484, Speaker A: So initially it would be one because it's inefficient and also it's quite centralizing to have multiple. And then in case the prover fails to submit the proof within a certain period of time, then people can are free to compete and submit. The first one who submits wins because then it's more time critical, whereas in the first slot you have a certain given amount of time. So there's no reason to let people compete and waste resources and create a centralizing vector.
00:18:59.864 - 00:19:11.240, Speaker B: And do you want to have more provers later on? Like, oh yeah, for sure, five or what do you think is a reasonable number? Like obviously one if its performance should be enough, but you want to have more.
00:19:11.392 - 00:19:53.268, Speaker A: So the problem here is because if you have one, you need to compute the proof sequentially. And currently the proof times are higher than the block times. So you can see if you compute the proof sequentially, the finality times grow super linearly to your block numbers. And basically you end up with a system that in a few months might have a finality time of, I don't know, a few weeks, which is not great. And so the idea is that if you use the per slot design that I described, you can assign different provers to different slots and they can compute those proofs in parallel. And basically instead of having one proverbs compute proof and then the next prover. Wait until the first one computes and then compute the second.
00:19:53.268 - 00:20:23.554, Speaker A: They compute all of them in parallel and then just submit them all at once. And that allows us, assuming that there are no other bottlenecks, your throughput essentially linearly grows with the number of provers that you have in the network. So let's say if you have 20 provers you can compute a certain number of proofs and therefore you can have a certain number of TP. And then if you have 100 provers, the number is not going to be five x, but you can see how it will grow maybe a bit sublingually, but it will roughly follow the number of provers.
00:20:25.574 - 00:20:26.798, Speaker B: Yeah, thanks a lot.
00:20:26.966 - 00:20:27.874, Speaker A: Thank you.
00:20:32.534 - 00:20:43.514, Speaker C: Hey Doggirl, thank you for your speech. I noticed there's one design that you didn't mention in this is scrolled. Considering becoming a DN roll up or not for now? No, not for now. All right, thank you.
00:20:52.934 - 00:22:14.354, Speaker A: And the design sounds a bit like GaSPa style actually. With the ability to submit proofs without waiting for the predecessor for the parent, where exactly do you see the difference? So the main difference is that essentially you can submit the proofs on chain and not verify them because there might be a scenario where let's say the parent proof has not been submitted yet. So what you do, there are two benefits here. One, it allows the system to work and two, it's much more gas efficient this way. So you would aggregate the proofs on chain like keep all of them without actually verifying them and then assign another prover to aggregate them, compute one aggregate proof from, let's say, the hundred proofs that were submitted for 100 slots and then verify one proof unchanged. So in this case, instead of paying roughly 350,000 gas per proof verification, you're just paying 350,000 gas for the batch of aggregate proofs that you computed. Thank you.
