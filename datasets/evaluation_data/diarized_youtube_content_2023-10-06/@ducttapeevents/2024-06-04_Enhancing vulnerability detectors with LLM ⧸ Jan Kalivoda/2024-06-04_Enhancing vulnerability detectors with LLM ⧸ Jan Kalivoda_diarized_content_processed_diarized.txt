00:00:10.440 - 00:01:05.102, Speaker A: Okay, so hello everyone, I really appreciate that you come into my talk enhancing vulnerability detectors with large language models since there is currently collision with Vitalik's farsight chat. So I really appreciate it. So my name is Ian and I'm security researcher at a key blockchain. We are doing blockchain security audits and also developing some open source tooling about one of the tools we will be speaking today. Also we are onboarding developers to blockchain at the local university or we are occasionally doing some online courses. We work in paths with clients such as one inch Lido safe or XLR. So that's just the interaction about myself and the company.
00:01:05.102 - 00:02:05.536, Speaker A: And we need to start with static analysis, then we dive into its interconnection with the AI. So I would just like to ask please raise your hand. Who knows something about static analysis? Awesome. And who's actively using that during the development? Yeah, maybe we will get to the reasons why it is in this way. So static analysis is basically about extracting some information from the code and we can do it in two ways. And first one is detectors where we are basically searching for some patterns that can lead to vulnerability or some code quality issue. And the second way is just about printing some useful informations about the code, such as you want a printer which will print you all only omnibodifiers in your codebase.
00:02:05.536 - 00:03:27.604, Speaker A: And you can see all the invocations and the respective functions of these modifiers. So it can be handy when you are trying to understand some code base and so on. Yeah, so there is example of some detector, if you know Slater or other static analysis tools or trans detectors, usually the first one which is implemented. And this is example of the output where you can see that at some position in the code there can be some possible entrance and you see there that sub detections which could be the attack vectors. Then example of a printer is for example for storage layout where if you are doing contract upgrades then you can print your storage before and after upgrade and you can see if there are some clashes in the storage. And this brings us to the tooling. So the static analysis tooling is mostly based on some sort of intermediate representation model that is built on top of the aSt which is produced by the solidity compiler.
00:03:27.604 - 00:04:14.256, Speaker A: So when you want to have some static analysis tooling you will probably need some IR model. And there are currently two leading frameworks. And this is the wake and slitter which build OpenapI over their intermediate representation model. So example, this is just a big picture. We don't need to dive into much, but this is how the iron nodes and the voltry of the model looks like. So if you will just stay at the top, we have their contract definition in the green square, and after that is function definition. So we can check it on the example where we have some contract.
00:04:14.256 - 00:05:28.922, Speaker A: For example, that contract has two functions, get token storage and write token storage, and we want to write a printer. So if we will be using wake, we can just simply add this one function, which is a special function called visitor function. And this visitor function is just jumping over all the nodes in the intermediate representation model. And this visit function is jumping over the function definition nodes. So in the body of the function we have two brains, where we print the name of the node and then the parent. Here you can see that if we will run the printer over the contract which is above, we will get printed the function names and the contract where they are placed. So this brings us to pitfalls of static analysis, where the biggest problem is that the full algorithmic approach is not always sufficient, because the traditional static analysis does not understand code semantics.
00:05:28.922 - 00:06:54.474, Speaker A: So it means that you can have in one code base there can be some valid detection for one specific detector, and in the other one where it is almost the same, it will not be an issue, because let's imagine that in one codebase you are working always with one token, and in the second one you can have any token to interact. And so in the first case it won't be a valid detection because the token will be always the same. And for the one case it is safe, but for the multiple cases it won't be. And we can recognize this easily with the iteration of static analysis, even when we are using some heuristic techniques and so on. So if we want to do a safe approach where we will report all the possible detections, instead of just don't report it, we will get to high false positives numbers. And that's probably the reason why a lot of developers are not using it, because it will just pollute the output of your terminal and so on. And you don't want just to dive into all the false positives and check it.
00:06:54.474 - 00:08:28.118, Speaker A: So we need some sort of human check, and this is where the AI steps in, and we want to do it this way like you can use it generated for some recommendations, what can be, for example some attack vectors, and just discuss the contract with GPT in general. But we want to be smarter, we want to combine the techniques of the static analysis and the power of these large language models. So the human check can be performed by these language models by combining some specific prompts with algorithms in static analysis. We were testing that on GPT 3.54 and cloud, and basically on some specific cases, GPT four was just enough to be very successful. So to explain it better, we have a case study and it will be about a detector that is called ketchup 256 different hash, which can be kind of confusing name, but what it is doing is pretty simple. The detector can recognize difference between expected hash and the hash that is used in the code.
00:08:28.118 - 00:09:41.804, Speaker A: So there is like a developer convention. When you are using some magic numbers in your codebase, then you have their codecommand which explains the magic constant. Do not confuse any code reviewers, other developers, so just do not add some issues in a future development. So exactly when you are using, for example, hashes for storage locations, then usually there is a code command which has some sort of string that explains the preimage of the hash. And the detector is already implemented. So on the screenshot you can see the output of NVS code where it can be seen that there is a mismatch in the last symbol. So how to implement it? Some nice implementation could be like detect code comment near the variable declaration, then extract the expected hash preimage and calculate the hash from the preimage and compare with the hashes in the code.
00:09:41.804 - 00:10:59.058, Speaker A: So, but there is an issue that you can use some regexes, heuristic techniques and so on, but you won't probably catch all the cases. So there can be like a wallet solidity code, there can be some redundant symbols, there can be wall of text in which there will be hidden some text explaining what is the preimage and so on. So if the human will be reading the code, he will probably find out what was the idea, how was chosen specific hash. But it's hard to catch all the edge cases without some more personal check. So this is what we are going to address. We want to extract this preimage from the codecommand with the language model, so the implementation will be pretty straightforward. We just want to start with some pre processing part where we use static analysis to find bytes, 32 constants in the codebase.
00:10:59.058 - 00:12:04.924, Speaker A: There is just a snippet which does all the logic for that, and it's very similar, as we shown at the beginning, how we can iterate through the function. So we just need to find not the functions, but that bytes for the two declarations. When we have that, we can extract the nearby code command. So find the relevant text and send it to language model, then we get some output, and that's the thing, we need to somehow parse it. So if you have constant expression evaluation engine then you are probably happy. Or you can insert the output to solidity template, which you can just imagine like a contract where you will get some valid solidity code and you will place it into the function in the contract. So then you are able to compile it and get the value.
00:12:04.924 - 00:13:10.446, Speaker A: And the third option, probably the most straightforward, is just to interpret the output as a python code and use save eval, where the third option is implemented in the snippet on the right. So it's not so complicated. So, but it is possible that some of these approaches fail, that you will get for example a long sentence from the GPT or some invalid output. So you want to send the error back, explain it to the language model, so the language model will attempt to fix it and process is repeated. When the evaluation succeeds, the hash calculated from the code command is compared with the value in the code. So just a high level. The architecture is that we will start with compilation, which will generate intermediate presentation model for us.
00:13:10.446 - 00:13:54.184, Speaker A: Then we will do the detector preprocessing, so we don't want to send all the source codes to the language model, just some small strings which are enough. And this will be sent to the language model that will try to extract the preimage. And then there is the post processing part where we are trying to evaluate it. If we fail, we will send a fix request, which is another prompt. So they are figuring two prompts for the language model and then it will try to extract the pre image again. And in the end we will compare the hashes. So deprovating.
00:13:54.184 - 00:15:03.804, Speaker A: When you start developing dlanguage models and get more into it, it can be pretty hard just to force to language model to answer with your code. It's still doing things like I apologize for the confusion and pollute the output with some sentences and so on. So you need the best way what we found that you can place some special sentence in the end of the prompt, which is the last thing that the language modeler read. So he will keep it in byte and can you send the output. What you need and other thing is to get the stable and consistent outputs. So you will send 100 times the same preimage or the same text and he will always extract it in the same way. This can be done when you change the attribute called temperature to zero, which is you can imagine it as emotions of GPT, so the output won't be so emotional and so human.
00:15:03.804 - 00:15:41.874, Speaker A: And then there is some. Also important thing is to cache the responses. So for example, when you are developing the detectors rerun with all the code change and you don't want to send the request all the time. So for example, if you will be working in the team, you can have some cache for the team or it depends on the level of what you need. But caching is important. And I have there the special sentence for you. This one is exactly what we are using in decode.
00:15:41.874 - 00:16:13.394, Speaker A: And it starts with a take a deep breath. It might sound funny, but these words are really working. Take a deep breath, calm down. Or from the other side you can say you are paid for that, do it or you will be fired. And sentences like that really works. And then we get to some other use cases. For example, uncheck return value detector.
00:16:13.394 - 00:17:19.420, Speaker A: It is also one of the detectors that are polluting the output with a lot of false positives. So developers sometimes don't want to check for the return values intentionally. And this can just help to somehow filter these false positives cases. So may the rate lover. And the second case can be for example for underfloor overflow detector, which can help you to discover some more complicated scenarios. For example, when you are combining assembly solidity and you are getting some very complicated statements which can for example go wrong in the places where you wouldn't expect to, then the language can again help you this time to bring the bigger positive rate. So the summary is that in our opinion, language model can help you definitely to make more precise detectors.
00:17:19.420 - 00:18:27.634, Speaker A: And also it opens doors to the word of the detectors that we won't be able to create with the traditional static analysis. So it can be definitely effective for a specific task rather than running it over the whole codebase as we shown on the one slide, which without any fine tuning, or at least for now, it won't be usable. But maybe after a few years we can just run it over the code base and we don't need a job and that's it. So if you will be more interested in tooling, then after this there is a session about versus code extensions. Otherwise in the end of the day we have their fuzzing workshop and in the Sunday there is a talk about wake, which is the tool what we were using for developing of these detectors. And that's it, that's the talk. So thank you.
00:18:27.634 - 00:18:49.294, Speaker A: Any questions? Okay, just please wait for Mike.
00:18:53.194 - 00:18:59.450, Speaker B: Have you tried to use something except GPT four maybe, I don't know, some custom LLM?
00:18:59.602 - 00:19:14.254, Speaker A: Yeah, we've tried and we've tried it, I think, like a year ago, and it doesn't have so good outputs like GPT. But maybe if you will try it today, it's rapidly evolving. Maybe it will be better.
00:19:15.514 - 00:19:21.490, Speaker B: Sorry, did you try, did you try to train your own model on your example data?
00:19:21.602 - 00:19:22.454, Speaker A: Not it.
00:19:23.434 - 00:19:24.694, Speaker B: Okay, thanks.
00:19:31.014 - 00:19:32.414, Speaker A: Okay, that's it. Thank you.
