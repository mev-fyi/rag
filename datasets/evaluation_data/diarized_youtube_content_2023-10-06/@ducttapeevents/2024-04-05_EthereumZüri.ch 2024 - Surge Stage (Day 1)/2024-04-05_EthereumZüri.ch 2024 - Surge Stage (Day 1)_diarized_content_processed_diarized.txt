00:05:33.074 - 00:11:54.290, Speaker A: It, it, it, it. All right, so welcome, everybody, finally, for the opening of Ethereum Zurich 2024. It's a great feeling to be here again after one year. So this is the second time Ethereum Zurich is happening here. Today we are at this beautiful campus of Erkel. We switched from last year because the conference grew in the number of people and hopefully also its significance. I won't be long.
00:11:54.290 - 00:12:38.246, Speaker A: I just basically want to start off with thanking all of you for being here today during the opening. I know it's tough to beat a early riser. I also want to thank all of the sponsors that enabled us, namely Dfinity, the Internet, computer, ACi, blockchain, and their project Wake, anoma Swarm, and also pon. And of course, I want to thank the University of Zurich, the blockchain center of the University of Zurich, and namely Professor Claudio Tisone, who's going to now tell you about the conference and how we got here. So thank you again for being here. Thank you, Joseph. Thank you, everybody, for joining us.
00:12:38.246 - 00:13:50.566, Speaker A: Well, I think I have the same words as Joseph was saying to me and to the University of Zurich as a whole, if I am allowed to be the representative here. It's a pleasure to welcome once again the Ethereum community and of all the ecosystem related to blockchains and platforms have been building on top of this like we had last year. Ethereum is perhaps the first example of how Switzerland became such an appealing place to do blockchain. And that's why we at the university are very happy to be leading the center exactly in this country and doing it because we are able to do a lot of academic activities, but also industry related activities, thanks to the great momentum that exists in the space here in Switzerland at the moment, that, in spite of market sentiments, has always been quite positive. The Blockchain center of the University of Zurich is a competence center in our university. We were established in 2017, formally in 2019. But one important thing that we stand is that in order to understand blockchains, we really need interdisciplinary approaches.
00:13:50.566 - 00:14:52.334, Speaker A: And that's why we have been coordinating activities in this space, be it research, be it education, but also interacting with the. What gives breadth to this space, that is the people that constructed that. It is you. And that's why we are really happy to have this event now here. One thing that is important is that we are a public university. So for us, it's very important to always have a unbiased understanding on how this ecosystem works, understanding what are the positive sides, what are the downsides, because this is exactly what is going to nurture it in, in the future and project it and within the mandates that we have, because, you know that we're a university, so we have our own set of rules between the mandates that we have from the university. One of the things that is very important is, as I said, being a single point of contact with industry and support the growth of the ecosystem, always based on this unbiasedness that is so important for us.
00:14:52.334 - 00:15:41.432, Speaker A: The last thing that I wanted to say is I wanted to thank the people from duct tape, the people from pound, because working with you like last year, has been a pleasure with Josef and Nico, Dan Voight, etcetera, because you have been a pleasure, a very fruitful collaboration, and I hope that we continue it in the next years. Thank you very much. Welcome to the conference. We will have a very long program these three days. Thank you. Thank you so much, Professor Tissoni, ladies and gentlemen. As you can see, we are now going to proceed with the fireside chat with Professor Tisone and Alexander Bruner.
00:15:41.432 - 00:16:33.232, Speaker A: I'm sorry, we still have to get some of the chairs ready. Sorry for that. As you know, Professor Claudio Tizone is professor of blockchain and distributed ledger technologies at the informatics department here at the University of Zurich. And he's also the chairman and co founder of the University of Zurich Blockchain Center. Alexander Bruner joins us here today as the president of Homeofblockchain Swiss, which is a public private partnership fostering the swiss blockchain ecosystem collaboration and promoting Switzerland as a place for blockchain technology in Europe and beyond. Thank you very much. Thank you.
00:16:33.232 - 00:17:04.088, Speaker A: And Alex Handicover, because I'm very bad host. Thank you for having me. It's a wonderful location. It's a big improvement towards last year. Thank you for having me. Good morning, everyone. So in the next 20 minutes, we want to discuss one of the questions that after regulation pops up everywhere is like, is me car or Meca? The upcoming EU blockchain regulatory framework? Is that the Holy Grail? And in the last couple of years, you've sort of seen three approaches to blockchain or crypto.
00:17:04.088 - 00:17:53.840, Speaker A: The first one is the american one. The american one is like, we do not agree what it is, but we will sue companies being active, and that's still ongoing. So that's the american approach. You even have two agencies battling who's in charge? I think after FDX and all that we've seen, that's probably not the best approach, but a much older and long term approach has been here in Switzerland, it's been very pragmatic by actually having discussions between the government, the regulatory bodies and the industry. Just recently you saw that about the staking guidelines. For a while, famous said, if you do staking because of how in the case of bankruptcy, you basically need to be a bank, if you do staking, it's a security. There was a discussion and it was updated and I think at a moment it's more workable.
00:17:53.840 - 00:18:49.174, Speaker A: So it's a very pragmatic approach here for the last ten. The crypto value is ten years old, and now you have the european approach and the european approach. As you guys know, there's this old saying, the US innovates and the EU regulates, and the EU really likes regulations. I must say I'm a bit surprised. Mecar only has 166 pages, if you want to read it. Why? I'm saying that the EU AI act has thousand pages, so I don't know if that says anything about crypto versus AI, but the question is here is, and that's a discussion we had many times, is this the holy Grail? Will it now attract all the companies Coinbase circle? They're in Germany, in France. Will Switzerland be left out? Is that the better approach, the approach or the Swiss still having a leg up? How do you see that? What's your experience with EU and rail regulation? And I know you have some first hand experiences.
00:18:49.174 - 00:19:33.544, Speaker A: Tell us. Yes, thank you. I think that in general framework, it's very important at the end of the day to have some sort of regulation. I think that exactly what you said before. The challenge is that us is having, and also listening to what is happening in business models that are still being developed in the US. And the lack of clarity of what happened all the time with a centralized exchanges, etcetera, has made the country to lose a very interesting opportunity in this space. And Switzerland, even it's a small size, as I told you multiple times, I am always mesmerized to see the extent of the ecosystem here in Switzerland compared with the same experience when you go abroad.
00:19:33.544 - 00:20:31.284, Speaker A: There are very few places that have this level of activity. And I say this, that to me some sort of regulations is important. The point is, excessive regulation as well is something that is detrimental for the growth of a BB ecosystem. You have an ecosystem, you have platforms interacting with each other, you have users wanting to use the system, et cetera, when there is no friction between all these processes taking place. And I think that Mika is trying to do too many things at the same time, but in too many different jurisdictions. And one thing that I like of the ecosystem here is how approachable different people of different stakeholders at all levels. You were talking about the statement about Finland related to staking, how closely knit together all the people is in this space.
00:20:31.284 - 00:22:00.666, Speaker A: With respect to Europe, I think that a very simple example of what was happening in the past years was the following. We're very interested here in our university, like in most of universities of the world, we are very interested in identification of our students, because, for example, you know that there is a lot of administrative burden when we need to certify students that are incoming. The previous studies that they have had, specifically, if they are from other countries, even as simple as if it is a student coming from a european country as well, there is a lot of paperwork involved because the processes didn't evolve for the last eight years. And we at the University of Zurich were running some pilots, actually with people, with industry stakeholders here with Swisscom, and trying to have a certification system for our own university. And the important thing here is this is certifying identities of students is a standard process that the university needs to have. But because it is a standard process, what you need to do is to do it at the level of the administrative people that is involved in charge of doing this. And we run this pilot.
00:22:00.666 - 00:22:38.076, Speaker A: We had in our faculty issued a lot of certificates in blockchain for one year. We're trying to think to expand it and expand it to a national level. And this is something that is very simple here, because there are only 20 universities. When I. When we were discussing the results of the pilot, I came to discuss the same issue with stakeholders of other european universities from EU. And what they told me was, look, what happened with digital certification is that we started having pilots in 2017, 2018. We ran it locally inside of our own labs.
00:22:38.076 - 00:23:59.490, Speaker A: We saw that it was feasible, but at the point in which we reached administration all the time was, it is not possible, because we do not have the right to do that. We need first a regulation on how these identities are going to work out. And when they came out after a few years, the inherent complexity of what were the processes and how it would be implemented ended up creating a situation in which after five years, all the universities are completely stalled at the european level. In this part of identification, you see that even now we are discussing with a network of which the University of Zurich belongs to, of european universities, all the process of digital certification regressed, if I have to say, for the past five years, and nobody knows how to do a process of this simple and that is in the benefit of everybody else, as it is this one. So it's the technologies here, but the processes are bogged down by bureaucracy. That doesn't sound too surprising, if the good way of summarizing it. One of the interesting things, when you talk about regulation, it's everyone says, we want to have regulation, or the right or the smart regulation, but what it basically is, when you talk about regulation, it's always a trade off.
00:23:59.490 - 00:24:37.190, Speaker A: Regulation is like, you provide rules, but you also provide enough freedom to innovate, because innovation, at the end of the day, is freedom to experiment. Now, if you stop that very early on, it does. Doesn't happen. So one of the questions, I'm also active with other universities. One of the things I hear a lot is, now the EU has this regulation, you can debate it. Apparently there are whole, certain things are not covered, but what about the knowledge and the expertise? Is that available? I heard one story, like, for example, for Germany, that Bafin, the regulator in Germany, has little expertise. So it's really challenging.
00:24:37.190 - 00:25:08.414, Speaker A: And I went to a presentation recently by FInMa here. They said the same thing. They said, it's very hard to understand the technology. It's for them as a regulator. They're not developers, they're no coders. So I think Switzerland at least has ten years track record. Do you see it as enough education, expertise in Europe, or will that be a hindering point? No, I think that you have to see that the scale of the things are completely different, because we are talking about a jurisdiction that has ten times, sorry, 20 times the population of Switzerland.
00:25:08.414 - 00:26:37.374, Speaker A: So here, one thing I have, some team members have participated in discussion routes with Finma related to this staking thing. And actually, the questions that Finma was asking were very pertinent because they have been exposed to space for a lot of time. And we humbly at the University of Zurich, but also high salut, ETH, et cetera, have been producing knowledge to practitioners as well, having in discussions with practitioners for the past seven years, because we are very active. I think that we are now writing a very interesting initiative together with a network of universities here in Switzerland. I am really happy to see the level of activities that in a lot of faculties and from completely different disciplines, how seriously this topic is being taken by ourselves, by academia as well, not trying to do knowledge transfer into industry. And this is something that I am a little bit biased here, but if I have to see how the same groups react or behave in other places of the world, in other jurisdictions, they are much more marginal to the discussion also with the stakeholders. And this is what you are asking now, I think that here there is a nurturing of the ecosystem, but also of government and regulators, in which they are really trying to acquire the knowledge that is important in order to foster the acceleration of the ecosystem.
00:26:37.374 - 00:27:09.814, Speaker A: And that's interesting, because recently FInMA, if you want to read that, they published an annual report, and they talk about crypto, among other things. And they say whenever they have fintech applications, it takes them, on average, two months to respond, on average. Maybe some of you have had different experiences, but it's quite responsive, and I heard different numbers. If you want to try this in Germany or in France, you can multiply those months by three, four, five times. So it's quite a fast. And a lot of that has to do that. They have been exposed too many things.
00:27:09.814 - 00:27:59.188, Speaker A: But one thing that FEMA came up, and one of the things they said explicitly, and coming back to your education question, is that at the end of the day, as a regulator, when you get a white paper, buy some pro project for some coins, whatever it is, you basically, at the end of the day, as a regulator, have to sort of understand the code, and that makes it much harder. They're not used to looking at code. Regulators are used to interact with entities with the bank. How do you see that? Have you had experience? You've been exposed to FEMA. How would you rate their, you know, understanding of code and algorithms? Is that is exist? Is it lacking? Do you need more education? Well, obviously it is. I think that the, let me say the following thing. Pinema has been very open all the time to try to better understand the algorithms that are behind the system.
00:27:59.188 - 00:28:59.930, Speaker A: I was involved in a very early stage of the swiss ecosystem in a project, and actually, they were looking at the logic of the algorithms that we were trying to implement for a marketplace in a very detailed manner. And I think that this is experience that they in general have. And I think that they gathered, if they also have very low response times nowadays, is because they understand far, far better what they are trying to, what they are looking after. And this is a very interesting thing. And also in the discussions of the, that I had with the people that went to discuss with them, I think that, as I say, the questions were far more relevant than the ones you would usually think of regulators. Someone may criticize the speed at which the clarification related to different incarnations of a staking, for example, is being done, but that's a completely marginal thing. I think that the steps are going to the right direction also bringing clarity to the space constantly.
00:28:59.930 - 00:29:45.138, Speaker A: And they are acquiring their knowledge. And I would say that it's not only Finma. If you talk to people from SNB as well, they know very well what all these systems are about and which are the potentials that they also bring to them. One of the, I think, standout features of the swiss regulation, the Swiss actually legal framework, is it's often very pragmatic, it's business friendly and spaced on dialogue. Like, I'm involved with one company and that is getting a license and you can have an exchange with Finma. Do you get a response? You get feedback. In your experience, have you seen that in Europe as well, or other different mindsets to rare regulation? Like one friend recently said here, you still looked at as sort of an equal as a business partner.
00:29:45.138 - 00:30:48.990, Speaker A: If you try the same thing in Germany, you first looked upon that you're doing something very suspicious or incriminate criminal. So their mindset is very different. Have you experienced that in Europe as well? What kind of mindset do you. Yeah, but also, in a way, I may be saying something, perhaps I'm generalizing too much. Another thing that this also led to, the ecosystem is basically trying to do what the EU or the countries want, because you have two options there at the european level. Either you try to do your own business model and then you face a lot of challenges because people do not understand, people have a lot of prejudices about this business area, and then the option is you go into a minefield. The other option is you just try to do business at the country level or at the european level, completely aligned with the government or with other stakeholders.
00:30:48.990 - 00:32:18.106, Speaker A: And this is exactly one thing that is completely degrading the level of innovation that you have in this stage, because you are not using your slogan, you do not have the opportunity to do what you really want to do or leverage on the opportunities that these new platforms allow you to develop. As we slowly approaching the end of our session, what's your outlook? Is Mikael going to be a boom for the entire industry? Will Europe become the leading light when it comes to blockchain? How do you see that? And also in comparison to Switzerland? I also want to hear your opinion about this. But before that, to me, I think that the problem here is that Mika forces people to get into this regulatory framework in very short timeframes at which you do not have the knowledge, capacity to absorb and to make rational decisions in the timeframes that have been imposed. And so what I fear is that as it is currently implemented is going to actually create a buffer zone in which people try to become compliant. There is going to be not a very elegant situation with respect to how these things are resolved. And we will have a time period in which innovation is most likely going to decrease because people will just try to accommodate to the regulations and the decision makers will not be there to do the right decisions. And I think that this is a very challenging thing.
00:32:18.106 - 00:33:03.336, Speaker A: I think that Switzerland has a very interesting lead on this, but what's your opinion? I think to summarize what I've explained, I was a member of parliament for seven years, so I've seen the government side, and as I mentioned before, when it comes to innovation, innovation is creativity and it means freedom. And you need certain guardrails. You have to strike a balance between let people try things and fail and sort of prevent bad outcomes. And if you are too freewheeling, I think we saw that in the US with FTX, it can be really bad for the outcome. So that's probably not the best approach. I feel like the U is, as always, very administration bureaucracy focused. And to my understanding, I don't know if you agree, but innovation doesn't happen because government is very much involved.
00:33:03.336 - 00:33:37.066, Speaker A: And I've seen that the government, even if it gives money, certain governments are throwing out money. They always want to interfere, and that's not the best. Governments are not innovators. And I think Switzerland still has this balance between a framework. We have crypto banks, we have crypto brokers, we have exchanges, we have everything custody. There's a lot of clarity on that side, but you can still innovate and there's still some empty space. I still believe that Switzerland will, when it comes stability and predictability, which are important for businesses, Switzerland is a good place to be.
00:33:37.066 - 00:34:07.536, Speaker A: The only caveat that there is, as we have in financial banking, is market access. How do you access from Switzerland? European market. And I know there's a lot of talk about doing this through Liechtenstein and there might be ways of doing it. So we'll see. I think that will be, for me, the big, big question mark. Will Switzerland be shut out from the european market? Will it be so strictly enforced or will we have access? And the ecosystem here is still very strong. And I know that predictability is attractive.
00:34:07.536 - 00:34:24.360, Speaker A: There's a few companies who are still looking at Switzerland and a lot and to finish because. And also I'm happy to be here at the university because of the developing talent and the research being done. This is really appreciated on a global scale. Because it's about knowledge and expertise. So we'll see. Fantastic. Thank you very much, Alex.
00:34:24.360 - 00:34:58.963, Speaker A: A pleasure. Thank you. Thank you. Thank you so much, ladies and gentlemen. We still have a few minutes for questions, if there are any questions. Yes, we are a little bit ahead of time, which is obviously a good thing. Are there any questions? Sorry for that.
00:34:58.963 - 00:35:37.168, Speaker A: We are a bit short on microphones. Thank you very much. Thanks for a great conversation. The question is around a comparison of regulation across different jurisdictions. If you can compare EU, UK, Switzerland, the Menas, UAE, US, Singapore, where would you start a company today if you're starting a startup? No, I can answer in two jurisdictions. Singapore, I know very well since Luna Terra Crypto's dead. I think that's a very short and brutal summary and that's just the case.
00:35:37.168 - 00:35:55.122, Speaker A: And it's a good showcase what happens if there's a big fall out. The government didn't like that. They were in the press. Singapore government doesn't like that and they're all owned citizens. Very impacted. And the interesting bit, I think we have a lot of discussion is about do Dubai, obviously recently the first time. But the interesting thing about Dubai, yes, you can get easily licensed.
00:35:55.122 - 00:36:12.946, Speaker A: It's interesting, it's good to start up. It's a really thriving ecosystem. But they have the same problem as we just discussed, whatever. They have like 20 plus free trade zones. Some license are just for one free trade zone. Some are in the region. So it's a very small market.
00:36:12.946 - 00:36:42.180, Speaker A: So obviously it has attracted a lot, but the audience or the client base is still very small. So I think Dubai is still faced by the same problem we have. Where are the big consumers? And there's obviously Asia. So I think obviously it attracts a lot of attention. And on the US, as I said before, I see a lot of companies trying to leave the US because it's just very, very unfriendly. It's just a lack of clarity. And again, it's a good showcase that for a business, that's actually the worst.
00:36:42.180 - 00:37:13.410, Speaker A: If you have bad, bad rules, okay, but if you have no rules, and I mean, just look at SeC now got the permission to go after Coinbase because of the staking. I mean, this is terrible. It's insane what's happening. So that's my take on those three regions. I don't know if you want to add on. No, I go pretty much in line with what you are saying. I think that also Liechtenstein is a very interesting bridge, but it is also exactly.
00:37:13.410 - 00:37:56.834, Speaker A: Now, the fact that Mika was implemented as well at the European Union level also changed as well, the original way in which all the digital assets were treated as well. And listens. I think that the sort of independence that Switzerland has in that space makes it interesting. Maybe just add on one interesting question is where do you go in the EU? It's sort of, you can go to Germany, to France. And one of the recent countries that popped up is interesting is Austria. That seems to be, some people perceive that's the most crypto friendly or understanding of the crypto industry. But I'm interested to hear if anyone has views on that, please.
00:37:56.834 - 00:39:20.214, Speaker A: First of all, thank you very much for your presentation. It's great to be here and thank you for the conference. My question is about, I mean, it involves a little bit doubt to the negative approach to European regulation because mica is more institutional. I mean, service provider, company focused regulation, as I remember it, explicitly states that purely decentralized models are out of the scope of the regulation. And when we look at the structure of mica markets in crypto asset regulation, it is a kind of identical, not identical, but similar copy of the markets, the directive of markets in financial instruments. So basically it regulates similar institutions who act in crypto industry. So my question is that if you put aside libertarian propaganda, do you think that regulating third parties or regulating crypto asset service providers a good idea? I mean, is it a good idea to regulate them strictly? Thank you.
00:39:20.214 - 00:40:33.240, Speaker A: If I understand right, what I would say is I think it shows a problem of doing a two targeted regulation. I think that the pragmatic approach to the regulation is exactly what you want to do that covers more general cases. And I think that the fact, exactly that these kind of platforms that are actually the breadth of why all these ecosystems are interesting, I mean, this fact of having decentralized governance, we are talking about completely revolutionary ways of running organizations, and we are still sticking our, or some regulators are sticking themselves to traditional models, shows the short sightedness and the problem of trying to be excessively tight on which are the mechanisms in which the conditions under which people can operate or platforms can operate. And it's also a lack of understanding of what these technologies enable. Maybe just a very basic observation. I've been in crypto for many years, almost, I don't know, 80 years. And one of the conundrums of this decentralization debate is it really goes against whatever the government believes in, because if I get up now and just punch you in the face and break your nose, someone will call the police and say, hey, I did an action against you.
00:40:33.240 - 00:41:30.854, Speaker A: So how we live in our society is someone takes an action, you're responsible for the action. So all the regulation or actually criminal law, all sorts of law, is what kind of actions do people do? So the idea of there is no one or a very vague group of people taking actions really goes against whatever the government is believing in, what he called Dao or Defi, whatever. And FIMA recently said, you know, from a legal perspective, they have to regulate entities and entities that are run by someone that can be held accountable. So the moment you go into this DAO discussion, it just doesn't reflect, it's not that FEMA doesn't want to do it. They just, by law, they can't. And I think that's a conundrum that will exist because it goes against the basic principles, how we live in. Having said that, I've been in this industry and even a former politician, I like this discussion a lot because I like to call it, you have to poke the bear every now and then and just get this discussion going.
00:41:30.854 - 00:41:52.500, Speaker A: I don't know if you want to. I want to live in a world of daos. I'm not sure. But the idea and the discussions and the thoughts about that is very refreshing. And I think Switzerland and has those discussions, and I think it will stay with us for a couple of years. There's no quick solution. One of the answers, FEMA always said when there recently, this defi panel is that we have to stick to what the law says.
00:41:52.500 - 00:42:25.440, Speaker A: If you want to have Dallas or more decentralized, you have to talk to the lawmaker, to parliament. You have to convince politicians that this is a good idea. It's not a regulatory topic, but it's a wonderful discussion to have. And as a person who likes liberal ideas, one should poke the bear or the government in a friendly way. There's another final question. I see. Please keep it short.
00:42:25.440 - 00:43:08.518, Speaker A: The next session will start in just a few minutes. Oh, over here. Well, thank you so much for this talk. I would like to follow up on the political issues, which is basically, as you mentioned, we have to convince politicians to be like, crypto friendly somehow. How do you see the landscape regarding political lobby? Maybe blockchain interest in some parties or candidates? Do you think we will see in the future stuff like the blockchain party of stuff like that? I mean, there used to be a private party and privates. Pirates. No private pirates party.
00:43:08.518 - 00:43:40.290, Speaker A: And it was run by Alex Rouss. Roussel. He's one of the key figures in the bitcoin community. If you should follow him in telegram. So there has been approaches like that. I mean, I think what happened with the ICO guideline and also the DLT framework, we had this unique opportunity where there was a lot of enthusiasm about innovation, and parliament passed that law without any opposition, which was amazing. Having said that, there's a space blockchain federation, a lolloping group, there's a parliamentarian group of digital assets.
00:43:40.290 - 00:44:09.426, Speaker A: What I have seen, I don't think you see that everywhere. I think the bandwagon has moved on to AI. When you talk to politicians, that's what they're worrying about at a moment or think about. And sort of crypto and blockchain has been bit toned down. So at the moment, I can't see a part like that happening and anytime soon. And last but not least, I think it's what, again, what I've seen is in Switzerland, you can have those open debates. There's different party members that open to it.
00:44:09.426 - 00:44:35.128, Speaker A: But it's a very tricky topic for politicians. I've been in this space for many years, so I'm claiming I understand a little bit. I'm not claiming I understand. I'm not a developer, but those discussions are very tough for politicians, and AI is as tough as crypto. So making those topics available to politicians is also an educational benefit. You know, I think something for the universities. Yeah.
00:44:35.128 - 00:44:58.954, Speaker A: The only thing that I would say is related to this idea is the problem is having the full platform for such a political organization as well. It's very difficult. I think it's only one angle. I think that either you. It's the difficulty of trying to completely change a system from within. Okay. And that's why my conundrum appears.
00:44:58.954 - 00:46:35.578, Speaker A: Thank you so much, Professor Tisone. Thank you so much. Alexander Pruner. Ladies and gentlemen. In a moment, we will start the session on the crypto native economy. Please just give us two minutes to set up the technical system here. Ladies and gentlemen, please take your seats.
00:46:35.578 - 00:48:21.534, Speaker A: We're about to start with the next session on the crypto native economy. Ladies and gentlemen, we're about to start with our next session on the crypto native economy, presented by our very own Joseph J. Founder of Pondao and one of the driving forces behind Ethereum Zurich. Right. All right, so I'll start, and I'll let the gentleman set up the monitor for the slides for next speakers. But, yeah, the slides are there, so we should be good. So, hello, everybody.
00:48:21.534 - 00:49:09.896, Speaker A: I kind of took the liberty to have, like, an early session so I can have the rest of the conference just to enjoy and talk to people. My name is Joseph. Can we get the slides on full screen? Because I'm not sure this will work. Oh, this actually works. But okay, if you go to. I'm good with this. Awesome.
00:49:09.896 - 00:50:00.946, Speaker A: So yeah, they say it's always good to introduce yourself, but I was also told it's better to just say a story rather than to just mumble about what you did previously. But I'll do a bit of that. So just to give you too long to read, I was born at zero years old and fast forward, I turned 18 and started studying logic, which is kind of merge of pure math and philosophy. And I was an idealist. Then I was 2021 and for some reason I got accepted for a job in Middle east where they were opening a new data center for the central bank of Oman. So I had to cut my dreadlocks and move to Middle East. I was there for a year and something, and I started a company which had nothing to do with blockchain.
00:50:00.946 - 00:50:32.004, Speaker A: While I was studying, I discovered Silk Road and crypto and all of that. But it didn't interest me really. It was just this magic Internet money. It was like I didn't care about finance money or anything. My job in mind was fairly technical, but I got a sneak peek into how the technological aspects of banking works. And yeah, I was just doing my thing. But then with Ethereum, and I'm also, again, reiterating on what Claudio said before, I'm super grateful for the Ethereum project starting.
00:50:32.004 - 00:51:24.484, Speaker A: And it's also a funny coincidence, as this was ten years ago here in Switzerland. And then finally with Discovering Ethereum 2016 2015, it started coding in solidity. I was a freelance deaf and auditor for about two years. Then I joined the Ethereum foundation in 2018 and my life flipped again from kind of doing the traditional type of thing into diving into these new waters of actually living in crypto and not just like being on the sidelines. So while I was at DF, I decided to get my salary in crypto. And that's kind of one of the merits of what I want to cover today because I was earning crypto, I was like getting ETH every month and I had a regular contract. I was about to get 30.
00:51:24.484 - 00:52:10.904, Speaker A: I was like, hey, let's do the adult thing and let's go to a bank and let's ask for a mortgage so I can buy the roof that I'm living under. And to my surprise, even though I was making a good buck for like, a person in my, in my age, I got rejected because, well, they essentially told me, well, it's nice that you have all these transactions, but that's not real money. I was like, how come I pay taxes on this? Like, how come this is not real money? But I was like, yeah, you have to prove your credit score. Is this risky? Whatever. So I did. So I faked having an income for almost two years, basically by transferring bits of crypto on an account, like sending it to the account of the bank, bank that I wanted to get a mortgage from. And all of a sudden, fast forward two years, I even switched my contract to get fiat.
00:52:10.904 - 00:53:00.854, Speaker A: The Internet banking showed, hey, you are pre approved for this amount. So I was like, yeah, finally went to the bank again, everything looked fine, and I got a form in front of my face which just, like, where I ticked all the boxes and was like, one extra box which asked, are you exposed? Do you trade or own any cryptocurrencies? I was like, well, work for the Ethereum foundation. There's no way I can hide this. So I take the box, and boom, that's the end of the story. And I got upset. I was like, hey, well, there is this entire world of defi. What are we even doing here? And that's how Pondao started and Pon as a project which essentially targets crypto natives and this entire cohort of people living in crypto, maybe making their salary from Daos or from other crypto companies earning cryptocurrencies.
00:53:00.854 - 00:53:48.026, Speaker A: And the target, the goal, is to give them access to the same financial instrument to mortgages. So what we do at Pon is essentially creating this setup for crypto backed mortgages. You can imagine, like, fixed rate, 510, 15 year term loan backed with crypto with no liquidation risk during the fixation. And the whole motivation was like, hey, this is my story. But there's a lot of people that I know being in the same situation, and there is this entire generation or cohort of crypto natives. And we got curious about, like, okay, so that's the assumption, right? But let's look at the data. Let's actually see if there is sufficient cohort that we could build a product for if there is actually enough demand and money in this thing that we call the crypto native economy.
00:53:48.026 - 00:54:30.418, Speaker A: So we started digging into this. So, finally, getting to the content of the talk here, I'm going to do an introduction of something that we call the crypto native economy report that we published for a third time now, and this is too long. Didn't read. This is basically my pitch to get you to look at the 40 something pages that we published on this topic. So first of all, I'm going to start with definition of what the crypto native economy is for us. Then we are going to look into the fees of infrastructure, l one's, l two s, dapps, and then finally say something conclusive. So what is the crypto native economy? We basically looked at available on chain data.
00:54:30.418 - 00:55:06.216, Speaker A: So basically looked at how much, what's the economic demand? Like how much are users willing to pay for using the different protocols, the different dapps. So we separated the report into several categories. First look at blockchains as such. Then look at the actual DApps, like Dexs, liquidity, provider fees, bridging fees, marketplace fees, royalties and so on. That's already enough data for it on its own, but it definitely wouldn't. You know, I'm sure there is a lot of economies sitting here and it would be like, hey, that's not the whole economy. It's definitely not.
00:55:06.216 - 00:56:06.744, Speaker A: But this is what we are keeping capable of doing just now. So what we skipped is basically all of the revenues of centralized exchanges, consulting and like DEF payments, which is also something that's attached to the crypto native economy. Dao paid salaries, dow treasuries. That's also something that comes from my interest from EF, because I was looking into how much money it's actually needed for the core development to be secured. And you can see some of my older talks, which will give you a glimpse of how much money there is actually sitting in the ecosystem and how much is needed to basically cover the entire core development. We also didn't consider yields from basically lending, even though we actually built a lending platform and something that we didn't include, but won't include because we don't think it's actually the right fit, is mining and token rewards. So basically block rewards, validator rewards, which are from other source than actually user paid transaction fee.
00:56:06.744 - 00:56:50.434, Speaker A: There are some other limitations which you can read in the report itself. So basically, all the numbers that I'll be presenting are denominated in us dollars. And because a bunch of these fees were paid in these different cryptocurrencies, the conversion is basically considered at the time of the transaction. This also means that a lot of the data that you will see will be influenced by the price movements that you see on macro level. We also excluded some of the minor categories, but it doesn't really matter, as you will see a bit later. So just a quick glimpse into the 2022 numbers, because we did the report last year already. So on l one s, the combined, let's say, revenue was $5.4
00:56:50.434 - 00:57:26.996, Speaker A: billion. 80% of that was, if you remain net, the Dapp fees were around the same ballpark, $5.98 billion. And the top 20 dapps were responsible for almost 90% of all of these fees, majority NFT marketplaces. So basically that was the year where OpenSea made over a billion dollars just on people yoloing into buying nfts. And the other bigger cohort is decentralized exchanges, mostly uniswap. So this is the rough split.
00:57:26.996 - 00:58:29.810, Speaker A: And again, you can see this in the report itself, so I won't stay on this slide. Now, finally, 2023 numbers. So most people that I know of are kind of more on the side of like us being as an industry, being in a bull market, which kind of sparked last year. So we actually looked into like, did anything change between 2022 and 2023? Can we say it was a bull market start or not? So what has changed? So we basically saw September and October being kind of the bottom afterwards, like things sparked back and the narratives kind of changed. So what picked up was lsds, like with staking derivatives, nl two s, and the narratives that kind of faded away. Not completely, though, were metaverse and NFT. So basically, part of the reason is decreased demand.
00:58:29.810 - 00:59:13.602, Speaker A: Part of the reason is that there was this race to zero from all these NFT marketplaces that basically cut the fees. What we added as well is looking back into 2022 and comparing these numbers across the board. So finally l one s, there is a slight drop, 17% drop from $2020 to $4.5 billion. Something changed though. Something significant change is that Ethereum wasn't as powerful as the previous year. So as you can see, in September, end of summer, Ethereum was basically overtaken by all of these other elements combined, which I mean, still means that Ethereum plays a major role in the ecosystem.
00:59:13.602 - 01:00:05.884, Speaker A: But something definitely changed there, and we'll get into what that was a bit later. So here's just the long term comparison. You can see that, like 2021, 2022 was definitely a much, much bigger year for Ethereum, mostly because the NFT hype, which is kind of interesting thing to say, even also another thing which is quite important, has changed, which is good to be aware of, which is, well, a bunch of l two s started 2023. So this is just for an anchor. So you can see the user base of Ethereum was actually pretty stable despite the l two s that picked up and the fees. I mean, it seems like, it's not. But the fees stayed relatively stable throughout the year as well.
01:00:05.884 - 01:00:44.064, Speaker A: Now this is the important graph. So these are the l two monthly transactions versus Ethereum transactions. So as you see, Ethereum transactions stay stable. L two s are picking up big time and basically surpass Ethereum around February last year. Bitcoin is another interesting, interesting phenomenon, which even last year is like basically maintaining a stable base. It's kind of past all of the hypes. There is definitely, definitely like a price hype or the hype that follows the price increase, but otherwise the user base is fairly stable.
01:00:44.064 - 01:01:30.468, Speaker A: Any guesses what happened in November last year? Why the fees picked up ordinals? Yeah, so that's why nfts weren't actually that bad. And this was actually quite significant for bitcoin as such. Did I click the right button? Yeah, this is just. Okay. Yeah, this is just so you can see the user base of Ethereum actually stays fairly stable. How we explain this is basically the move of users to l two s. What was quite interesting is looking at projects like Tron, because they actually seem to be reporting increasing user bases despite the bear market and increase fees as well.
01:01:30.468 - 01:02:28.744, Speaker A: And there seems to be an interesting traction will be a subject for another report on its own. I'm definitely not a person following Toronto as much. Bitcoin is basically kind of caught up with the rest of the ecosystem, essentially thanks to ordinals. Again, the user base has the most rigid user base as it seems. Another interesting project, BNB chain, also one of the ones that has the most users, or at least that's what's reported, the fees were dropping and didn't caught up with the rest of the ecosystem. So essentially, if your favorite chain isn't in this table, I'm sorry, but these were the only meaningful ones that is actually worthwhile to report on. The numbers here are basically the user paid fees, the revenues on these chains from different chains.
01:02:28.744 - 01:03:06.594, Speaker A: So it's essentially ethereum, tron, bitcoin, BNb chain avalanche polygon, Falcon, Solana from l one ecosystems. This doesn't touch DL two s. You'll be actually quite interesting to see how these l two s would play with this. Anything below Solana is basically another order of magnitude lower. So yeah, you can say, well, fees are super cheap there as well. But, well, the reality is there actually aren't as many users either. So dapps finally, and I'll be slightly over time, sorry about that, we started late.
01:03:06.594 - 01:03:43.934, Speaker A: So Dapps, there was a bigger drop, 48% drop over 2022, $3.3 billion. So if ethereum used to be the chain from all the l one s, then uniswap is the dex of DeXs. So you can see a graph comparing uniswap versus all of the other dexs combined. And these are the LP fees. So this was like mostly prior to the fact when Uniswap started charging fees. And again this is a teaser for you to actually look into the report and find the tables with the numbers.
01:03:43.934 - 01:04:41.726, Speaker A: So we also don't have as much time. Liquid staker fees. So this is an area that was basically new but started having interesting movements, started to mostly pick up towards the end of the year. Anything weird on this graph? Any hunches? Lido isn't there. And the reason why Lido wasn't in the graph because if it was you wouldn't see anything. This basically is heavily occupied by Lido and just like a 50 x from all of the other LSD's combined from last year, the space has changed a lot already in this quarter, so next report will look quite different. Lending platforms despite the bear market like January February, you could still see overall demand and using lending protocols and platforms.
01:04:41.726 - 01:05:20.454, Speaker A: Towards the end of the year when the speculation picked up again a slight increase. NFT marketplaces again we mentioned the reasons mostly decline. You can basically see the 2022 compared to 2022 in this graph here. You can spot the peak basically getting to around like $2 billion. L two s are definitely in error, which will be interesting in the next report. And as you see already, a bunch of these l two s can basically compete with a whole bunch of the l one s. So there's definitely an area of interest.
01:05:20.454 - 01:05:43.324, Speaker A: Yeah, I think we're on time now. So again, this was just a teaser. Please go to cryptonative Pond XYz or I'll show you a link later on to read the entire report. Give us feedback. What you like to see. What do you feel is wrong even? We try to validate the numbers from two sources. The main sources token terminal.
01:05:43.324 - 01:06:49.494, Speaker A: The other one I don't remember, but it's listed in the sources. And please also mention how do you wish this report evolved? So the 2024 numbers so far suggest that 2023 was the bottom for now. Ethereum is migrating to l two s and the l two space definitely is interesting. It's not just the hype around it, the narratives have switched from the metaverse and NFT hype into the actual core infrastructure. Had a very interesting conversation in London a few weeks back with a guy from percolapse and basically noticing that majority of the people around were actually working on infrastructure and like our own problems, but not really like the consumer facing stuff, which is another area, which I'm sure is like discussed on other conferences as well. The user based, although it's fairly stable despite the price drops, the difference largely is in the USD denomination. So the amount of fees actually kind of corresponds to the price drops.
01:06:49.494 - 01:07:51.578, Speaker A: And yeah, also, as you saw from basically having these top ones, or you can say the vendor take all situations with Uniswap and Lido and so on, there still is a huge opportunity space. Some of these things are very early and some of these races have started or are ongoing, especially on the l two side, but there's still a lot of rs where people can build interesting stuff. Special thanks to Victor, this french guy right here. This is a picture we took when we left KitKat in Berlin after NFT Berlin though, so you can see they wouldn't let us in dressed like this, I guess. And he basically spent a few hundred hours on the report. So thank you, Victor, even though you're not here. And then finally, if you wanted to learn more, if you had to this link, there is a link tree with links to the 2022, mid 2022 report or like mid 2023 report and the final report.
01:07:51.578 - 01:08:19.730, Speaker A: And yeah, you can follow us on Pondao on either farcast or Twitter. And if you want, if you. I'm already over time. If you have any questions, I'll be here for the three days at our booth, so just feel free to come and talk to me about this. And yeah, of course I'm expecting there will be a lot of people from treadfy here. So if you don't think this is true, just please talk to me, convince me this isn't true. I've lived through this.
01:08:19.730 - 01:08:44.864, Speaker A: So thank you. Thank you so much. Joseph. About the crypto native economy, ladies and gentlemen, as you heard, Pon XYZ and cryptonative economy XYZ. Please visit both. We're now getting ready for the next session. Please don't leave the room.
01:08:44.864 - 01:10:52.074, Speaker A: The next session is on speculative denial of service attacks in Ethereum. Ladies and gentlemen, please sit down. We're about to start with the next session there, so welcome, ladies and gentlemen, to the next session, speculative denial of service attacks in Ethereum. The speaker, Arthur Chowe, is associate professor at University College London, focusing on information security blockchains and decentralized finance. He's also the founder of of D 23, a decentralized intelligence company, and he has been in the blockchain space for over twelve years. Ladies and gentlemen, Arthur Joey. Thank you so much for the very kind introduction.
01:10:52.074 - 01:11:22.264, Speaker A: I use this clicker, right? Okay. All right. So happy to present our more academic work today. So this was work with my colleagues here, here from various institutions, actually. Aviv Yaish, he just finished his PhD at the Hebrew University. Kayua Chin just finished his PhD in Imperial College. Lee is still a student of mine, and Aviv is a dear colleague in Jerusalem as well.
01:11:22.264 - 01:11:38.254, Speaker A: This work was also partially supported by the Ethereum foundation. So thank you very much for this, for helping us to do good research. Is this working?
01:11:46.154 - 01:11:47.054, Speaker B: This one?
01:11:48.074 - 01:12:10.286, Speaker A: Okay. Got the right clicker now. Sorry about that. Okay, so everybody knows transaction fees are a thing you need to pay to execute code on blockchain. So that's nothing new for you. So there's a certain amount of gas that you need to pay. And this gas is typically, you're typically liable to pay gas when a transaction is mined, right? So when it's included in a block.
01:12:10.286 - 01:12:47.964, Speaker A: Now, transaction. Transaction executions on, for example, the EVM layer can revert. So a smart contract may have a condition that is not satisfied and then the transaction execution will not touch the state of the blockchain's execution. But these reverted transactions, they still have to pay fees. Right? And that is largely due to the fact that we want to avoid DNA of service attacks. So we don't want somebody to just spam transactions that then revert and do nothing and don't cost anything. So that's why they typically do cost a certain amount of money.
01:12:47.964 - 01:13:38.600, Speaker A: Now, is this enough? Do we need any other fee mechanism? Probably not. Okay, good. Probably not. However, so one of the issues is you can pay transaction fees for transactions that are not included in blocks. At least current designs do not look for such a setting. So just to give you a simple example, right, if you send out a transaction and it will never be included in blockchain, because maybe it doesn't pay sufficient, a sufficient amount of fees, then it will not pay transaction fees. Do you have any idea, any other case where it could happen that a transaction will not make it into a chain? Nonsense.
01:13:38.600 - 01:13:54.106, Speaker A: Okay. Some missing nonce, outdated nonce, for example. Any other issues? Zero gas. Zero gas. Okay. Right. That's also an option.
01:13:54.106 - 01:14:08.002, Speaker A: Any other ideas? Reorgs. Reorgs. Reox. So typically transactions are. That are, that are, that were in a fork, right. They are put back into the mempool so they can still be mounted afterwards. But it's.
01:14:08.002 - 01:14:20.506, Speaker A: That's a good point. Right? So it's a delayed inclusion, so to speak. Very good. Yeah. Any other idea? Why would transactions not be mined? It's such a weird concept. No, sorry. Reverted execution.
01:14:20.506 - 01:15:02.924, Speaker A: Reverted execution. So then they're still included, right, but they're included, executed, but reverted. So they don't affect the state of the blockchain, but technically they are included. Okay, let me spoil it. So we have censorship. So since the OFAC sanctions, we have this concept that certain transactions we will not include in a chain. And you may ask then, ok, where do we have censorship? Like where on the blockchain stack in this architecture will we have censorship? So here I just drew like a high level diagram of the technical architecture with these different layers, p two, p layer, consensus layer, application layer.
01:15:02.924 - 01:16:10.534, Speaker A: So there may be quite a lot of censorship here, there may be some censorship here, but also on the application layer, if you use for example, USDC or USDT, they can blacklist addresses. That's also a form of censorship. So you actually have it on multiple layers in the system stack, but you can also have it on the order flow part of the entire blockchain architecture. So for example, in ethereum, we have this concept called, called proposal builder separation, where searches are, for example, finding profitable transactions, they're forwarding them to the builders, the builders build blocks, the relayers take those blocks and forward it to the proposer or validator or miner, or however you would like to call it. And so in this setting, we do have quite some censorship going on, as I will show you in a bit. But to give you a little bit more of a definition of what is actually censorship or why did this came about? Well, there's the US office of Foreign Asset controls. So the commonly referred to the OFAC, and they have a list, an SDN list, especially designated nationals and blocked persons list.
01:16:10.534 - 01:16:50.244, Speaker A: So. And there are hundreds of blockchain addresses. There are actually also bitcoin addresses on there, and most of them are externally owned accounts, but there are also some contracts and there are also some testnet accounts that were sanctioned. So that's actually the rough concept. So you might ask, okay, I mean, has this any impact? I mean, we have a permissionless, censorship free blockchain network, right, without any central party. Why would that have any impact? Well, we did measure the impact. So the OFAC sanction here hit at this particular date in 2022.
01:16:50.244 - 01:17:10.488, Speaker A: And if you look here on the y axis, we have the number of uncensored blocks on the x axis at the time. And here we have different mining pools. So at the time we're still in proof of work. So. And you can see here. Right, ether mine had quite a lot of tornado cash transactions. Right? So TC stands for tornado cash.
01:17:10.488 - 01:17:58.198, Speaker A: It's a privacy mixer, which was. Yeah, I should add, it was sent. So the tornado cash addresses went onto the SDN list of OFAC. And you can see here there's a stark impact of what then happened in terms of transactions that were mined or rather number of uncensored blocks. So these sanctions, they have a stark impact even on these very decentralized systems. Okay, if you want to have a little bit more updated numbers on censorship, we just discussed the PBS system here, Tony. He was so kind to create one of his beautiful websites that really quantify the current state in Ethereum.
01:17:58.198 - 01:18:35.824, Speaker A: So, for example, on censorship, censorship pix, you can see 10% of the validators are censoring and roughly 60% of the builders are. So why are more builders censoring than validators? Any idea? Yeah. Builders may be more concentrated. That's a good point. Yep. Any other reason? Think about the. Yeah, they're doing the block building.
01:18:35.824 - 01:18:53.120, Speaker A: Exactly. They may be legally charged. Absolutely. And that was a bit earlier in the food chain. Right. So the searchers sent the transactions to the block builder, to the relayer, and then to the validator. So the idea is in PBS is that the validators, they actually sign off on the blocks without even knowing what's in the blocks.
01:18:53.120 - 01:19:11.728, Speaker A: So maybe, and I'm not a lawyer, but maybe the majority of the liability actually goes onto the. The builders. Hence they are the entities that are more sensory. Okay, just a little side note. So what are the security implications of censorship? Right. That's the core of the talk today. Why does it matter? Censorship.
01:19:11.728 - 01:19:43.680, Speaker A: We know it matters in terms of censorship and free speech and so on. But what about security? Well, we've seen that there's quite a stark impact on the average inclusion delay for tornado cash transactions. They went actually in August 2022. They went from 15.8 to roughly 30 seconds on average. So transactions still get included, but it takes quite a bit longer. And the second biggest point is it opens up new DNA of service attacks.
01:19:43.680 - 01:21:03.480, Speaker A: So a note that is censoring transactions has to do some work to execute the transaction, but there's no fee that can be paid for that transaction. And if there's only one thing you take away home from this talk, I think that's it. So if you censor a transaction, you may still need to execute it to know whether you should censor it and executing it, but not including it in a block implies that you cannot, this transaction will not pay fees, and that's a denial of service vector. Okay, so getting a bit more into the technical details. So there's a concept also, especially in the PBS system, where actors are executing transactions speculatively, right? So when building a block, for example, you may execute a bunch of transactions, but if another transaction comes along and this one is suddenly more profitable, well, you're going to switch it, right? And then TX prime here will not pay any transaction fees, but you had to execute it. So there's this concept also of speculative execution of transactions, even though it's not clear which transactions will finally make it into the block. So in our work that we did with our colleagues at the Hebrew university, we devised three types of attacks.
01:21:03.480 - 01:22:09.024, Speaker A: And these three types of attacks touch on three different system layers. So we have one attack where we have a conditional exhaust on the execution layer of the EVM. And this is what I will talk about today. In the later slides, there are two more attacks, one where we kind of trick the heuristics of a mempool into dropping interesting transactions so we can cheaply evict profitable transactions. And the third attack, it's targeting a reputation system of some of the PBS actors, and so where an adversary can lower the reputation of other actors and thereby weaken their ability to make a profit. So for the later two, I'm happy to refer you to the paper, if you're interested to know, really the nitty gritty. I will talk more about conditional exhaust, but all attacks are really fully described in the paper here.
01:22:09.024 - 01:23:29.510, Speaker A: This was also accepted and peer reviewed as USNC security 2024. So it will also be presented there. Okay, so what about conditional exhaust? Well, the idea is you have an attacker, and the attacker sends transactions only when the upcoming proposer, the miner here, the validator, when this proposal is ofa compliant or aims to be offer compliant. So what would the attacker then do? Well, the attacker sends a transaction, and this transaction here has two execution steps, depending on the context, and it will check is the proposer that's now upcoming executing this transaction in a compliant way. Right. So what does it mean to be compliant? I'm using that word so casually here, but just to be explicit, any idea what I mean by being compliant? Sorry, exactly. So, being compliant, and I use it as a very loose term, is to not mine a transaction if the transaction is involved, so sends or receives money from an address that is part of the SDN list from OFAC.
01:23:29.510 - 01:24:35.934, Speaker A: Thank you. Okay, so we can do these two things, right? So if it is compliant, we execute some complex code, and then we transfer one address to some sanctioned address, which, for example, transfer one way, so just some amount to the sanctioned address. And if it's not compliant, then just execute some simple code, and that's it. So that means that nodes that actually this will trigger quite a lot of computation to execute here, this complex code, and that's a denial of service vector that targets specifically compliant actors in the space. So, to give you an example, I'll actually give you solidity example in the next slide. So this executing here, the simple code would probably, I mean, for example, can take roughly 23,000 gas, which is only 10% more than the simplest ethereum transaction we can think of. But the complex code here can take a lot of gas, for example, 30 million gas, if you'd like to.
01:24:35.934 - 01:25:34.174, Speaker A: So these are really like the kind of like two branches that we can design and therefore target specific actors that are censoring. So how does this code look like? Well, it is rather simple, I would say. So here, for example, we have this contract, we have a constructor where we define which addresses to dos. And what's interesting here is, I think the way that we do this particular version is that we look at what's the address of the future of the next proposal. So we assume that the adversary in this particular case knows who is the next proposal, which in proof of stake, we typically know, and the addresses also of the validators are fixed until they withdraw from the proof of stake chain. So this is a possible way of carrying out such an attack. Does anybody here.
01:25:34.174 - 01:26:13.496, Speaker A: So, and this is a bit more technical, does anybody have an idea on how we could, what other mechanisms we could do to execute this attack or to target, like the next validator besides the coinbase address. Any other ideas? Random? Yeah, but then sometimes we have to pay fees, right, for the, for the 23,000 gas, and sometimes we're successful. So we can do a bit better than random. Yeah, timestamps, not bad. And there's something that's very close to timestamps. Block number, block height. Yes, correct.
01:26:13.496 - 01:26:37.332, Speaker A: Absolutely. So we have, we have this, we described the second version, actually, in the paper, so feel free to check that out if you're interested as well. Very good. So we did an evaluation on our machine. With 64 cores and quite some ram, the flashbots mean specs are much weaker. So we took really a beefy machine, right. That is kind of like the victim node that we will be attacking.
01:26:37.332 - 01:27:12.914, Speaker A: And we found that a one shot of 140 transactions. So we sent 140 of these sensor exhaust transactions. One shot would kind of. Yeah. Propose would force the proposal to mine empty blocks, even if the proposal is mining 100 consecutive blocks, which I understand this is unrealistic, but it's a very favorable case for the proposal. So the cost of this attack is less than $1,000. So I guess for the impact on liveness of the blockchain, this cost is really negligible.
01:27:12.914 - 01:27:34.910, Speaker A: Yeah. So there's a call to arms. Right. The consensus and transaction fee mechanisms, I would say, are relatively well understood. Obviously, there's always room for improvement and more and further research to be done. But for transactions that do not enter blocks, that's really the Wild west. So we have quite some work to do.
01:27:34.910 - 01:28:13.214, Speaker A: PBS and the separation of roles certainly add complexity and introduce novel attack vectors. So it's really complicating the system. And, yeah, I think in prior work, right. In prior work, people said proposes, future proposals can be attacked because we know who is the next proposal. For example, we can dos them. But what I really want to show in this work, the futures proposals can also attack. So if we ignore the censorship part, there are also attack variants which we described in the paper where the proposals can attack themselves.
01:28:13.214 - 01:28:52.290, Speaker A: All right, thank you. I hope this was an interesting talk for you. Thanks so much. Thank you so much. Andrew Sherwe, are there any questions? We have time for one or two short questions. Any questions? Sorry, who's asking the question? I think I saw your hand first. Okay, thanks.
01:28:52.290 - 01:29:48.240, Speaker A: I will ask first, what if the decision, if the proposer is using of a compliant builder is not known prior because the proposer is using more builders? Let's say one compliant, one not compliant. Sorry, I didn't. How would you execute this attack if it's not known if the builder will be of a compliant or not? So if the, if, let's say if a builder is receiving a transaction, right, and the builder has a certain policy, am I compliant or am I not compliant? Right. But the question is if the proposer is using more builders. If the proposal is, it is not known if the one that will build a block is compliant or not. You cannot know that. Well, you do know whether the validator, right? From historical data, you do know whether the validator, the next validate.
01:29:48.240 - 01:30:16.464, Speaker A: So first, you know the next validator, right? That's proof of stake. Then the second thing is, from historical data, you know whether the next validator, whom, you know whether this validator is censoring or not? I don't think so, but okay, I mean, we have the data, right? We can go here. Oh, I cannot. I cannot switch anymore. But we have this. 60% of the builders are censoring. 10% of the validators are sending.
01:30:16.464 - 01:30:49.624, Speaker A: So we do know who is censoring. And then the builders, what they typically do is they know. Okay, this validator is sensoring. So I'm going to have to censor also, right? It's kind of like the builder has no intention of building a block that is not compliant for a validator that wants compliance. Thank you so much. Thank you. Thank you also to the audience.
01:30:49.624 - 01:35:03.264, Speaker A: Please stick around. Our next session will be an insightful panel discussion about future use cases of digital identities. Please stick around. It will start in approximately five minutes. Ladies and gentlemen, please go to your seats. We're about to begin shortly. Ladies and gentlemen, welcome to future use cases of digital identities.
01:35:03.264 - 01:35:58.214, Speaker A: Digital transformation, as you all know, is paramount, meaning secure, reliable, and user centric. Digital identities are more critical than ever. We are very pleased and excited to announce an insightful panel discussion featuring the web three Foundation, Deloitte, Switzerland and Polymac foundation. And please welcome our speakers. So, good morning, everybody. We really want to welcome everybody to this interesting, really interesting panel discussions about the future of the future use cases of digital identities. So, thinking about technologies like bitcoin, this entails that we have evolving concepts of identities in the new digital era.
01:35:58.214 - 01:36:41.972, Speaker A: When we think about what happened in the last couple of years with big tech, all with their big data silos, a lot of leaks. I mean, I've just recently read that presumably Facebook was basically letting Netflix peeking into their DM's. We have deepfakes. So there is a lot of things happening, and clearly there is a signal that there is a need for digital freedom and a transition towards digital identities and or on chain identities. So today, we really want to cover what's happening in this space, what to expect in the future, and how to solve those before mentioned issues. My name is David Bach. I am in the lead of business development and partnerships at the Polymac foundation.
01:36:41.972 - 01:37:42.738, Speaker A: With me, we have Misha bitterly from Deloitte, Switzerland. We have Bill Laboun, who is director of education and governance initiatives at the Web three Foundation, and Luca von Wittenbach, who is a co founder of the Polymac foundation. With that being said, I want to open the discussion with Bill from the web three foundation. And, Bill, from your perspective, why are digital identities crucial for the blockchain ecosystem nowadays? Yeah. So I think traditionally we have seen with blockchain that your identity is just a series of numbers or some very large number. And what you have done in the past with that number essentially on chain, there's never been really a good way of connecting a person to an identity in a serious way on an ongoing basis and in a way that's modular and can be private. And so what we've seen is then a lot of problems because of this lack of identity.
01:37:42.738 - 01:38:27.074, Speaker A: There are a lot of benefits to this pseudonymity, but also a lot of drawbacks. And it's been difficult. People haven't had the option to have the benefits of some of the benefits we do have in a more web two world. We see this in terms of endless civil attacks. We see this in terms of fake people. So not just deepfakes, but there are a lot of bill labones out there on the Internet asking people for things that are not me. I think having this ability to have a cryptographically verifiable digital identity is going to give us a lot of the benefits we have with web two, but still have all the benefits of a web three experience.
01:38:27.074 - 01:38:52.430, Speaker A: Cool. Thank you very much. So. And as mentioned before, we have Misha here from Deloitte. And with Deloitte, the interesting thing is Deloitte has recently, last year, ventured into creating an on chain digital identity service. So before getting into deeper discussions, can you quickly please explain what is that? Thank you, David. Yeah.
01:38:52.430 - 01:39:31.664, Speaker A: In fact, what we try to do is we bring the old world into the new world, because not everything in the old world was bad. Let me use an example from our partner kilt, which explains that quite nicely. Right? In the past, you had your passport or id and you went to a shop and you showed your id and you got a beer or whatever. You just had to prove that you're above 18 and the passport had multiple things on it. You can prove that it's a right passport. You can compare the picture that's the same person, right? And you know it's an authority which has issued it. So I can trust that the person is really 18.
01:39:31.664 - 01:40:19.318, Speaker A: Now, in the new world, I have my Google account, and with my Google account, I go everywhere, right? So my Google account is no longer a passport, which is with me. It is somewhere with Google, right? And Google knows exactly what I do, which also comes with this beautiful stuff that if I buy something, I get one month of merchandising about the stuff I just was buying. Right? So that's not really working well. And more worse, if Google decides to cancel my login, I will have a huge problem with a lot of services. Right? So what we try to do is to give that back in an electronic way. So we give you a kind of an id which is in a wallet and you decide where you share your information and what information you share. It's not that you put all your details into a form.
01:40:19.318 - 01:40:50.644, Speaker A: It can be sufficient in case of the shop that it just proved that you're above 18. So you have a wallet. In this wallet, you have information which gets approved by Deloitte. So Deloitte has then liked the role of a government issuing an id. And there is information in it which simply says you're above 18 and the shop doesn't need to have more information about you unless you want to ship it and its address and whatever. Right. So that's what it is.
01:40:50.644 - 01:41:40.114, Speaker A: It's bringing the control of the data back to the user. That's awesome. And I have a question that's motivated by my professional career as I'm coming from a big four as well. Thinking about the service that you have basically introduced now, what were kind of for a professional big four audit firm, what were challenges you had deploying such a credential? Oh, you don't want to know. You don't want to know. Yeah. We are a classic web two company, right? So now working with web three companies is not the easiest thing you can do, right? So one is the basic product we today at Steeloid and in my team and with Tony, which sits over there.
01:41:40.114 - 01:42:10.904, Speaker A: Hi, Tony. We issued today reports for banks. So we do deep background checks on persons with huge reports. Right. And for us, it was rather than printing the result into the PDF report, we write it into a wallet. So from a technical perspective, it was not that huge, but as a big four, we have a lot of internal processes to ensure reputation of Deloitte. And I said classic web two business.
01:42:10.904 - 01:42:29.864, Speaker A: So now we deal with a lot of web three companies. Now the question start, what is the company? Is there a company? We even have potential clients. We are talking. It's not even a company. It's a group of people doing something in the blockchain. Right. And we need to do a contract.
01:42:29.864 - 01:43:33.144, Speaker A: So what is the counterparty in a contract? We have topics like independence. Can we issue a certificate to an audit client of Deloitte? So it was a lot of legal work to put around that to make it happen that big four can actually issue a trusted certificate to the white public. Very cool. Thank you very much. So you were just explaining the credential itself, what it is, what it contains of, and with that, I would like to ask a question to Luka, because, okay, there is such a credential in your wallet, but how does that work in practice? Being more specific, how does Polymac utilize Deloitte's KYC credential and what does it enable for Polymac's or our operations? Thank you very much, David. So maybe to follow up on the two last points raised, one by Bill and also by mija, we at Polymac, we enable web three fundraising infrastructure. Very simply put, we enable web three startups to raise funds directly on chain.
01:43:33.144 - 01:44:21.034, Speaker A: Now, why do we need a digital credential, a digital identity? It's very, very essential with where we're going. A lot of people probably know it. The good old farmers, airdrop farmers, liquidity farmers. The most important aspects for every protocol to scale and the web three ecosystem to thrive is real user adoption and also adaptability of use cases. The problem is that if I have no clue how many users I have, I cannot focus which feedback I have to adhere to and adapt for the protocol. So I do not know how many users I have. Now, for us at Polymac, this is a very critical point because the startups that are raising funds, they need to know their stakeholders, they need to know who their investors are, the right target audience, who have the investment to use the application thereafter.
01:44:21.034 - 01:44:56.134, Speaker A: So how we use that specifically for our protocol is it enables us a clear path to have end users being able to invest in web three startups without disclosing their private information. So any one of you does not have to disclose who they are, but they have to adhere to all the eligibility checks from the project. For example, I would like to raise funds, but I exclude us investors. You don't have to disclose your name. You don't have to send your id card. Nevertheless, you are in adherence to a full KYC. But the only information you disclose on chain is that you're not from the US.
01:44:56.134 - 01:45:34.324, Speaker A: So very simply put, we enable a way where you're in control of your data and you can choose which information you're sharing with whom. That enables a path to distinct who's a real human being behind a wallet transaction and in the web three format in general. Thank you very much. Quick follow up question. We worked with Deloitte. We implemented the solution to use. But thinking about, I think we have a good sense of how this digital identity looks, works, where it can be applied from your personal perspective, where do you see digital identities to be used more in the future? It's my first question.
01:45:34.324 - 01:46:34.284, Speaker A: And how does Polymac itself plan to expand? You know, the utility of this new technology. I think actually just what we just discussed outside, tried just to Google something of when does certain talk start? Today. And my phone doesn't. Trust me, being a human being, right, I have to do the capture checks every time, a little brain check if I'm still there. And it's the little things in life, we cannot go online any day because even our own devices, our hardware devices, do not know if we're real human beings. So the use cases where they can go, in the end, everything we use as of today, every store, you buy something, everywhere you order something, where you need your address, every financial market product, every financial market transaction, every interaction in web two, in web three, that requires normal Internet connection. You will need the urge to have a proof, simply that you're a human being and that nobody can steal or copy your identity.
01:46:34.284 - 01:47:30.796, Speaker A: Cool, thank you. So now I would like to ask another question to bill, because we've heard about the solution, but we also know there is quite some other projects thinking about Metamask ID, polygon ID, Coinbase ID. So what do you think? What are the biggest differentiators and what do you, or do you think? Is there a problem why those solutions could not be used for, for example, rwas? Yeah, so there are a lot of different identity platforms out there and say they fall into sort of like two sections or two groups, right? There's like sort of the traditional, like Ens style of just. All right, let's take it on faith of what you say that you are and what you exist that doesn't have a lot of complexity behind it. There's not much you can do. It's not verified. There's the more like the did style, the decentralized identity with VC's that some of these verifiable credentials that some of these use.
01:47:30.796 - 01:48:13.434, Speaker A: But I think there's two issues with some of the ones that you mentioned. So first is sort of their boolean nature. There's a lot like Coinbase ID is essentially all right, is this person verified with Coinbase? You can't do some of the things that were mentioned earlier, just like just share that you are in fact older than 18. So this means that you're really. It's just, it's not granular enough for a lot of use cases out there, or you have to share all of your information with some of these, and then you're sharing all of your information with everybody out there, which we've seen with some of the hacks you mentioned earlier. That's also a bad idea. So I think specifically with this approach, a big thing is the fact that you.
01:48:13.434 - 01:48:52.690, Speaker A: This modularity of what you can share with. I know modularity is a big word in blockchain development now, but also the fact that it is Deloitte offering these things that it is a company that governments all over the world, people all over the world respect and they trust the verified credentials there anyone can make. I could have bills, verification service and I mean, I look trustworthy, right. But probably people won't trust me as much as a big four agency. So I think these are two big issues that differentiate this approach. Cool. Thank you.
01:48:52.690 - 01:49:23.408, Speaker A: So I think we've learned now that DL is quite an innovator in this space. But I think what most people would like to know, I mean, there's Polymac as a first use case, but what is there in the pipeline? What are use cases? You might be. Could be giving us some insights what to expect, like what is coming in the next couple of months. Okay. First of all, I don't know who looks more trustworthy or me. We are not allowed to talk about pipeline. There is still a big four.
01:49:23.408 - 01:50:25.640, Speaker A: But potentially I can talk a little bit about projects and future, what we could see, which goes a little bit in what you said, right? Potentially a bit more real and closer on what we look at. I think there will be a move from the digital world back to the physical world where you could think that you have credentials on a device. The device that you are you. That is what you said. The device doesn't know that you are you, it will know that you are you. And that gives you a lot of use cases, right? You can open indoors, you can walk through smart cities and you decide. And that's very important with our credentials, what information you share, if you want to know, if you go to the next shop, that they already propose you, whatever goods you are interested or not, I think that will be your decision in the future.
01:50:25.640 - 01:51:02.676, Speaker A: And there are other use cases. We get very important when we look at fake news, deepfake doing things like I sign my picture with my personal id, that this is my picture, a company signing a document, that it's actually the document of the company, so that you can have. Can identify that this picture is not from you because it doesn't have a signature from yourself. I think that's where it also goes in the future. That also, if you have bots and whatever they need to sign, that it's actually them. It's not a human being. So we can start to separate and there is a lot of others.
01:51:02.676 - 01:51:43.440, Speaker A: I mean, the obvious ones are investment cases for whatever platforms where you need AML requirements. Because we don't only do the identification of persons, we also maintain the certificate. So a certificate from us is always valid for a year. And that's also a problem you have with a lot of other services, right, is you do the identification, then it's valid for a day, because tomorrow I could marry and change my name. Right. So we maintain it and we also do structured screening in the background, right. What we have seen before with off track lists and stuff, stuff like that.
01:51:43.440 - 01:52:29.976, Speaker A: So that Polymac knows it's not a sanctioned party trying to invest. So we keep the group of people in the investing platform clean. And the good thing is, as the platform knows in this case, Polymac, that all the data is proven and there are sanction checks in the back. They wouldn't even need the information, just the pure facts that the certificate is there means this is a good person and that's how we can achieve that. We still can only share the information we want and not always everything, while bringing security and comfort into the ecosystem. Thank you. So I also have another follow up question, because there might be a little bit of a misconception sometimes.
01:52:29.976 - 01:53:18.274, Speaker A: So we are here at Ethereum Zurich, we talk about blockchain, but Billy. So thinking about also the solution, do you see any further use cases where this could be applicable? Because this is not purely focused on web three. Yeah, I mean, I think that some of, there are a lot of traditional web two approaches anywhere where you would want to have an identity. This is going to be useful, obviously, polymec, with investing, being able to access certain websites that you may be geoblocked from. Personally, as very much an on chain nerd myself, I see a lot like, I think these web two mechanisms, a lot of them are sort of a one to one mapping. But I see a lot of cases in web three where we could immediately start using this. Right.
01:53:18.274 - 01:53:44.632, Speaker A: Think about airdrops. And there's no proof of personhood. There are airdrop farmers all over the place. They're Sibyl. If we think about daos or just individual contributors. Right, this munchables attack on base, right? Base, yeah, base recently, what happened? There was someone whose identity was essentially a fake identity, actually. So like a sort of a funny story.
01:53:44.632 - 01:54:00.720, Speaker A: I'm in a dao Kappa sigma mu, which is secured by proof of ink. So I have a tattoo that includes a. I'm a part of a human blockchain. It links back to the previous person who got a tattoo. And there are 163 of us. And there was. I know, it's awesome.
01:54:00.720 - 01:54:31.028, Speaker A: Kappa sigma moo. You should join. And if you want to be cool like me. So. But there was an issue like, okay, well, how this is proven is pictures of the tattoo and there was sort of a little scandal, like someone tried to get a second tattoo on a different part of their body. How do you prove that in a web three way that this is someone different knee than someone else's arm? So I think, again, it's sort of like a silly example, but I think that's the thing. We have a lot of these direct cases of.
01:54:31.028 - 01:54:56.492, Speaker A: I'm using an id for something now, but in a web three world, there are just so many more use cases that it's really hard to think about. Just like it's hard to think about whatever like cryptokitties would have been in 1985. Awesome. So before wrapping up and we potentially have time for a question or two, if there are any. Wait a second. Because we. My big forebrain kicked in and said we're not allowed to talk about pipelines.
01:54:56.492 - 01:55:38.212, Speaker A: We're not allowed to talk about pipelines. But there is a project we can talk about, which is Kyx, it's know your cargo. That's a huge project we are currently working on with different companies like PowerPoint, Nexiot, Kilt, which is ensuring that a delivery of a shipment of a package is safe on the entire way. And more important, that if it goes into a container and it's shipped on a global way, that nobody is adding trucks into the container. Illicit trading and stuff like that. Right. It is about expected time of arrival of goods.
01:55:38.212 - 01:56:40.524, Speaker A: It's an entire platform we are currently looking at and plan to build together with our partners. Exciting. Thank you. Any other closing words from Luca or bill before I'm wrapping up? I think from my side only. There is a cheaper way than getting your body inked to join a proof of person in the Internet. So coming back to the on chain credentials, I think it's a very natural transition of where we have to move because everyone that is following all the developments ongoing with AI, I'm sure we have more experts in the room, is that it's a trivial perception that having the best software to filter if something is being faked or not is always behind the newest AI development. So the only path to secure a verifiable version of yourself or proof of human being, or however we want to call it, if you don't want to ink your body, I think a proper way would be that you own your data and you have a verifiable credential where you can choose to share which fields you want to disclose with your counterparty at any given point in time.
01:56:40.524 - 01:56:59.284, Speaker A: Yeah. Just to conclude, I do think that would be easier than getting yourself inked. Yes. Cool. Thank you. So thank you very much for being here. I think at the end of the day, digital identities or on chain credentials are here if you want to have a look.
01:56:59.284 - 01:57:25.170, Speaker A: I think the solution Deloitte is offering is unique. It's super nice. They have a really big first mover advantage. If you want to have a closer look at it, I think it's www dot Ken KYC minus credentials.com for anybody. All of the builders here in the room. Feel free to reach out to us if you have any questions about implementation, how it works, if you need any kind of documentation.
01:57:25.170 - 01:57:46.664, Speaker A: We're happy to support wherever we can. And with that being said, yeah, was great. Thank you for being here. I think the future is bright when it comes to this topic of digital identities. We're super excited. If you want to also learn more about it, you're happily invited. We have a meetup once per month in Zug in collaboration with the Web three Foundation.
01:57:46.664 - 01:58:09.304, Speaker A: And Deloitte is also there from time to time. So this is a great place to exchange also on those topics like digital identities. Thank you for listening. And if any questions. Yeah, feel free. Are there any questions? We still have a few minutes over there. Oh, sorry.
01:58:13.364 - 01:58:56.518, Speaker B: Thank you. Thank you for the insightful conversation. I just have a question. You were talking about from the private sector point of view of the digital identity. And my question is, okay, I choose your solution, and he chooses the solution of the other and so on. Is there an interoperability that you will grant? I mean, you will guarantee that there will be an interoperability among all of these solutions? Or could we think about national solutions? Like, every country will decide, okay, let's get into digital identity. And we won't have these biometric passports anymore.
01:58:56.518 - 01:59:04.134, Speaker B: We will have something new, a digital identity, national digital identity that we will use within this cyberspace.
01:59:05.674 - 01:59:36.514, Speaker A: Sure. I mean, there can be only one solution. No, I'm kidding. You know, they're already e identities, and they have one big problem. They're national because with our service, we have a lot of people, or we have some people coming with e identities which we will be more than happy to accept, but you can only check them if you are local. So only local companies are allowed to check the validity of a nigerian e identity. Right.
01:59:36.514 - 02:00:25.286, Speaker A: The good thing is if you have a solution like the Deloitte one, this is globally available, right? So what we would like also to do is to get the e identity to save a lot of work by proving the data and whatever, reducing the cost for the credentials because we don't need to do all these checks and do a global certificate on the top. I think that's also where it goes a little bit because in Europe we now do one identity which goes over the entire European Union. I think that's already an improve. And they, by the way, go all to the web three standard. So they're all based on the same standards as we are. And the rest is then a question of a validator. Because the validator needs to judge what are the issuing companies, which he trusts.
02:00:25.286 - 02:01:09.630, Speaker A: Because if Luka just says and he puts that onto a blockchain, I'm not sure if Polymac would accept that. So you will have some trusted parties that will issue credentials and those will be trusted. And the rest you can do. Technically you can generate the credential on the killed blockchain whenever you want. But the problem is, the question is always is the service accepting the credential, accepting the fact that somebody else was approving it? That is the main question at the end. This is distrust part which comes into it. But I think there will be more than one solution, sadly.
02:01:09.630 - 02:01:44.122, Speaker A: But, yeah, that's how well it is. Right. We have time for one more question. Is there one more question? Is the solution platform agnostic or can you describe a little bit how it is multi chain, multi platform? I mean, at the end, what it is? It is a credential. The data is in your wallet and the credential is hashed. The hash is written to the kilt blockchain together with some metadata. Who is the.
02:01:44.122 - 02:02:18.934, Speaker A: The tester of the information? Is the certificate valid and stuff like this? Right. And then you get a little validator software you can put on a website wherever you want, which reads out the certificate from the wallet up, an approval from the user. Right. So you need to prove that you actually reshare the data. When you get the data, you prove against the kill blockchain that the data which was just shared is correct and it's approved and still valid. And then you do whatever you want with the data. So it's independent.
02:02:18.934 - 02:02:46.168, Speaker A: Thank you so much. Thank you so much to our panelists, Bill, mija, David and Luka. Thank you also to the audience. Please don't leave yet. At 1205, we will continue with the session presented by Antoine Spahnberg. Native account abstraction use cases. Yeah.
02:02:46.168 - 02:09:43.218, Speaker A: Thank you. Thank you. It, it, it. Ladies and gentlemen, please take your seats. We're about to continue with our next session. Bye. Ladies and gentlemen, thank you very much for joining us in this session.
02:09:43.218 - 02:09:57.766, Speaker A: The case for native account abstraction presented by Antoine Spahnberg of Argent. He's ecosystem lead at Argent. A smart contract wallet. Thank you. Thank you. My check. My check works.
02:09:57.766 - 02:10:23.770, Speaker A: Yeah, I believe so. Hello. Yes, so I'm indeed from Argent. My goal today will be to explain you and maybe even convince you of the reasons why. At Argent we have decided to primarily build on Stacknet, and this with a very particular focus and angle, which is account abstraction. So very shortly. So I'm the ecosystem leader at Argent.
02:10:23.770 - 02:11:13.462, Speaker A: I have a business background and I have a very strong conviction, which is that blockchain technologies today won't be able to reach mass adoption because the UX is too bad. And, you know, you cannot just push a technology from a very small group of early adopters that we all are, to mass adoption. If you have bad ux, that simply doesn't work. Luckily, I also have another strong conviction, which is that this can actually change. So we can improve UX, but it also requires us to, as an industry, make some very bold and big moves. So I think it's important to understand a little bit the history of Argent, to understand where we want to go. So Argent is actually already quite a pretty old crypto company.
02:11:13.462 - 02:11:52.534, Speaker A: It was founded in 2017, and back in 2018, we released our first products. As you can see there, it was a mobile wallet, and I think it was probably the best Ethereum wallet back then, thanks to account abstraction. So together with safe back then, we were the only ones to do account abstraction. And the experience was just very good. So the app was very sleek. Argent would sponsor transaction fees, users would not have seed phrases, so they can actually sleep at night, because we came up with this new concept of guardians and social recovery. So, you know, it was a very good product.
02:11:52.534 - 02:12:40.236, Speaker A: Argent today in 2024, is the best stacknet wallet. So we are the biggest wallet on stocknet. We have 75% of market share, and we actually have multiple products on Stacknet. And so, you know, this legacy product that we have on Ethereum, we still maintain it, but 100% of new development innovation is now done on stagnant. And then if we look a little bit forward, let's say Argent in 2030, the idea there, the vision is to become revolut, but without the bank, right? So to have this kind of super app where you can do anything, pay for your groceries, exchange value, invest in crypto, invest in stocks, book a flight, book an Airbnb, this kind of stuff. And we want to do this and we are doing this. We are building this based on Stacknet.
02:12:40.236 - 02:13:30.554, Speaker A: We use Stacknet as our backbone. And I think it's important to understand why we want to get there using Stacknet. It's important to understand what actually happened between 2018 and 2024 in terms of account abstraction, innovation. So most of the problems we have today with ux in blockchain and on Ethereum, because we talk about Ethereum today mainly actually comes from the onset of Ethereum, right? So at the time it was deemed that we would actually have two different types of accounts on Ethereum. On the one hand, externally owned accounts, also called eoas, I call them basically dumb accounts, right? They cannot do anything. They cannot do shit. The only thing they can do is actually originate a transaction, initiate a transaction.
02:13:30.554 - 02:14:05.370, Speaker A: And we said, okay, well, these accounts, these dumb accounts, they will actually be used by users. And then there is this other category of accounts, contract accounts. These are smart accounts. These are smart contracts that can execute codes, and you can actually add a layer of intelligence to these accounts so they can do all kinds of nice stuff. And we said, these accounts, they will be used by protocols. And the only trick there is that these accounts, because they are used by protocols, they cannot initiate a transaction. So they need an EOA, they need an externally owned account to first initiate a transaction before they can do anything.
02:14:05.370 - 02:14:57.230, Speaker A: And so from the very beginning, users have been stuck with these bad UX eoas. Consequence of that is that users are turning to centralized solutions. Instead of using these bad UX eoas, which are not nice to use, they turn to much nicer to use solutions. Why? Because, well, centralized solutions, they offer familiar web to UX. So you have easy onboarding, easy recovery when you lose access, no seed phrase, you can sleep at night, risk detection, and two fa, everything done in one click. You don't need to erc, approve everything or even do some kind of stupid infinite approvals. And so at argent, we believe that eoas actually are a danger to self custody because they impose that trade off between self custody and user convenience.
02:14:57.230 - 02:15:40.964, Speaker A: And if we want this industry to move from this small ecosystem that we have today to move forward, we need to have this good user experience and user convenience. So the question is, can we offer the best of web two, Ux in web three, or can we offer the best of web two Ux, while also preserving self custody. And the answer is yes, let's use contract accounts for users. They are smart. You can build all kinds of logics on top of them to improve those UX issues that we have. But remember, from one of the first slides, the issue, the big issue there is that contract accounts, they cannot initiate a transaction. So it's difficult for them to be actually owned by users.
02:15:40.964 - 02:16:30.096, Speaker A: And so some smart people, they came together and said, well, let's emulate transaction origination from a contract account. And that is actually what smart accounts used to look like back in 2018. So on Ethereum back then, you would have two different types of accounts, eoas and smart accounts. If you use an eoa, well, you initiate a transaction, it goes to the manpool, it's being picked up by a node and then sent to the blockchain for execution. If you use a smart account, well, actually, remember, with smart account, you cannot initiate a transaction. So instead of initiating a transaction, what you would do is send a message to something that was called a relayer, which is essentially an eoa. And that relayer would put that message into a transaction.
02:16:30.096 - 02:17:06.430, Speaker A: It's an eoa, so it can initiate a transaction, and it would send the transaction to the mempool, where then it's been picked up by the node and then sent to the blockchain. And, you know, it's pretty good. It worked pretty well. At Argent, we used to have our own relayer, but we are in crypto and blockchain, and we have this fundamentals, ideas that we love, such as immutability and decentralization. And the issue with that model is that it's not very decentralized, right? Because the relayer is actually centralized, point of failure. The relayer is operated by a company. So at Argent, we had our own relayer.
02:17:06.430 - 02:17:55.068, Speaker A: And then what happens if the relayer goes down? What happens if the company goes down or disappears for some reason? How do I then use my smart account? Right? People did not really like that. It's not a perfect solution, but it used to work. Then some other smart people came together, and then they said, well, let's then decentralize that relayer. And we will call this ERC 4337, which you have probably heard a lot about in the last year. And so, yeah, that was smart accounts in 2018, and this is now smart accounts on Ethereum, ERC 4337. And so while you see, you still have these two different types of accounts, eoas and smart accounts, if you use an eoa, well, your transaction just still follows the same flow. Now, if you use a smart account, it's a bit different because that relayer has been decentralized.
02:17:55.068 - 02:18:50.774, Speaker A: So we have introduced the concept of bundler. So the bundler basically does the same as the relayer, but anyone could theoretically run a bundler. So if a bundler goes down, there is another one that is running run by someone else that will take over. Right? And so now your smart account still cannot initiate a transaction, but it will send, it's not a message anymore, it's what is called a user operation to a mempool. And there in that mempool, the builders will come pick up many user operations, bundle them together into a bundle transaction, and push it to the normal mempool where it's picked by, by the nodes. But there is a trick, because the user operations of many different users are all bundled together in a transaction before reaching the blockchain for execution. That bundled transaction actually needs to go first through a smart contract, which is called an entry point contract, where it's going to be unbundled.
02:18:50.774 - 02:19:46.344, Speaker A: Basically all the user operations are unbundled and then sent to the blockchain. But of course still not perfect, because in this model, where does the account abstraction capability happens in two places. First place is the smart account. So you have many smart account capabilities that are managed by smart account. I'm thinking about, I don't know, social recovery typically, but then also in the entry point contract, because that's where user operations are unbundled. And so typically, if you want to run AP Master, for those who don't know, AP Master enables Dapps to sponsor transactions for the users, or users to pay gas fees with whatever token they would have in their account. So it's a way to abstract gas fees and gas costs.
02:19:46.344 - 02:20:50.974, Speaker A: If you want to run a paymaster, a paymaster needs to interact with the entry point contract. We can think of other use cases, session keys, this kind of stuff, they will need to interact with the entry point contract and so on. Ethereum with ERC 4337 on the slide there, it appears very clearly that if you use an eoa, at no point you will have smart account or account abstraction capabilities, right? Your transaction flow won't go through the entry point contract. So typically with ERC 4337, if you use an eoa, you cannot benefit from paymasters, as an example. And actually this is a problem because from a Dapp perspective now I'm building a decentralized application. I now need to actually identify whether my users are on eoas or on smart accounts, that is pretty easy to do. If they are using smart eos, then I will just offer them the normal flows that everybody knows with normal UX, which I think is pretty bad.
02:20:50.974 - 02:21:16.204, Speaker A: But if they are on smart accounts, then it opens many possibilities. I can completely rebuild the experience. I can bring all these account abstraction capabilities to users. And that means I probably need two different uis. I also probably need two different contracts. Actually, I'm building two different products. One product that is tailored for eoas and one product that is tailored for smart accounts.
02:21:16.204 - 02:21:53.542, Speaker A: And the issue is that dapps don't do that. They don't do it because they have no short term incentive to do it. Today. Still, 99% of users, even more 99% of users are still using eoas. So they have no incentive to do everything, you know, build everything twice and build these very nice, sleek experiences for smart accounts if all their user base is still on EOS. And it's a chicken and egg problem, because users have no incentive to switch to migrate from EOS to smart accounts because actually the experience today is not better on a smart account. And actually it bears the cost to migrate from EOS to smart account.
02:21:53.542 - 02:22:40.518, Speaker A: Typically you need to give up on your transaction history, which we all know we don't like to do. And so yeah, we are kind of stuck in this chicken and egg problem. So yeah, why would I build two user flows if 99% of my users are using eOas? And so, you know, some other people came up with this idea. Well, actually what we need is to integrate smart account natively at protocol level. So we will do this on a Zk roll up for scalability. We all know that l one is too expensive and we will integrate account abstraction natively at protocol level. So basically what it means is that now smart accounts, because I'm building a new layer too, I can decide the rules from day one.
02:22:40.518 - 02:23:08.040, Speaker A: Smart accounts can actually initiate a transaction. So they have now this power and we have now a single mem pool and transaction flow. So smart accounts are finally first class citizens. That is what Zksync era did. So if you use zksync era already. So basically this is what it looks like. So you have living together on zksync era, eoas and smart accounts, and you have a nice transaction flow.
02:23:08.040 - 02:23:55.550, Speaker A: And you might notice that actually when initiating a transaction from an eoa, the transaction flow actually goes through the entry point contract, which is called a bootloader on zksync. And so you can use a paymaster with an EOA on the casync, which I think is pretty interesting. So you have some account abstraction capabilities on the casync era, but still you miss a lot of them when you use an EOA because you don't use a smart account. So all the capabilities that are dependent on the account social recovery, which is a very nice one, you cannot do them. Yeah, but still it's not enough. Still we have that issue that daps on the K sync era. And believe me, I have tried to push them to do so.
02:23:55.550 - 02:24:30.548, Speaker A: Many of them, most of them, they don't build these account attraction specific experience. Why? Because still most of their users are using eoas. Because most of these Dapps era is an EVM compatible chain. So most of these Dapps actually come from other chains, right? Just integrate Dksync as another blockchain, another network. And on all these other chains there is no native account attraction. So they don't bother just creating a new very nice experience for the Dksync era users, while they actually have so many users, users on other chains. And so we are still stuck with this issue on the K sync era.
02:24:30.548 - 02:25:34.082, Speaker A: And the message here basically is technical feasibility isn't enough. And I like to shield native account abstraction on Twitter. And two weeks ago people have started to answer to me that actually native account abstraction has been happening on Solana for years already. So they already have this system where they also have two types of accounts on Solana and one type is kind of a smart account and they can initiate transaction, but they have been stuck on Solana with basically what Zksync era is experiencing now and they've been there for years. It is still the problem that Dapps don't build these specific experiences. Users are still using EOAS. And just to give another example of what inertia looks like, if you want your Dapp to be compatible with smart accounts, you need to be compatible with and support something that is called EIP 1271, which has something to do with smart contract signatures.
02:25:34.082 - 02:26:23.546, Speaker A: So typically, if you are not compatible with this EIP, that means that users that are using smart accounts, they can even connect to your Dapp. This was released in 2018. It's now 2024. We still have many prominent dapps on Ethereum that do not support EIP 1271. There is actually an interesting website called EIP 1271 IO with a list of all these prominent apps that you cannot use if you are using a smart account because they are not compatible. Just to give you an idea of how inertia can actually make things last for a long time. And so yeah, coming back to this, basically at Argent, we think that account abstraction, ERC 4037 account abstraction is not going to happen before a long time because of that inertia problem.
02:26:23.546 - 02:26:57.374, Speaker A: People don't migrate to smart accounts and you have no specific experience for smart accounts. Maybe on Zksync era it's going to happen, it's kind of middle ground, it's going to happen a bit faster, but still it's going to take a long time. And that's why we decided to focus on Stacknet. And Stacknet did something very bold that just said, well, the problem we have here is actually eoas. They provide a bad experience. Why do we keep eoas? And they just got rid of eos. So this is what the transaction flow looks like on Stacknet, you just have smart accounts.
02:26:57.374 - 02:27:50.944, Speaker A: That means that 100% of users on stack net are on smart accounts. That means that Dapps can now really focus on bringing new kinds of experiences to users and maybe to new users as well, because users have no choice anymore. So yeah, actually on Stacknet, Stagnet is still a pretty young network, but account abstraction is already a reality. So I just put there a few examples of stuff we have implemented with Argent. So Argent Shield, that is two fa. So you can use a second factor to add a second factor to your account. If your seed phrase is, I mean, leaks for some reason, you know, the attacker won't be able to move your funds without having access to that second factor, which I think from a security perspective is a very nice feature, transaction review.
02:27:50.944 - 02:28:23.724, Speaker A: So your wallet will tell you if you interact with a website that is a bit shady or that we don't know, or if you interact with a smart contract that does something a bit weird. Typically when doing a swap, sending the tokens to another address. The third one, I really like it. So on Stacknet, all transactions are multicols. So you don't do ERC 20 approvals anymore. Right, this thing where you need to approve your tokens. And first you had infinite approval and now you can actually put there a number, but actually you put very high numbers because you don't want to do this every time.
02:28:23.724 - 02:29:06.368, Speaker A: This is all done in one call, in one transaction on Stacknet, so we approve exactly the amount of tokens that you need to swap and then the approval is removed once it's done. So the Dapp never has access to all your tokens. And then last example is an email, email and password signing, which I think is also great to reach new audiences. So yeah, basically, you know, I like to say that Starklet is a blue ocean in the l two scopicats landscape. Every two weeks you have a new EVM compatible l two that is launching where there is basically no innovation anymore. All the Dapps, all the protocols on these l two s, they are just copycats of what is happening somewhere else. Innovation has been completely killed.
02:29:06.368 - 02:29:32.864, Speaker A: And Stacknet did, I think, these two very bold moves. So they have a new programming language which is optimized for ZK proving and heavy computation. And that also opens new use cases. And there is this native account abstraction, but without EOS. That's what I call 100% native account abstraction. So that's going to be it for today. So I hope I convinced you of a few things.
02:29:32.864 - 02:30:08.424, Speaker A: The first one is that blockchain ux is very bad, so we need to change it. Second thing is that it's possible to change it. We need account attraction. And third thing is that account attraction won't happen before a long time because of this inertia on most EVM compatible chains while it is already happening on Stacknet. And that's why as a smart account wallet, we have decided to use Stacknet as a backbone because we want to provide users and also new users with better capabilities. Thank you. I won't take questions now, but I will be around, so don't hesitate.
02:30:08.424 - 02:34:30.628, Speaker A: I'd be very happy to discuss. Thank you. Thank you so much, Antoine. Thank you. Ladies and gentlemen, please stick around. The next presentation will begin at 1230, building a shill free developer community for wallet innovation by Francesco Andreoli. It it ladies and gentlemen, we will start in one or two minutes.
02:34:30.628 - 02:37:47.680, Speaker A: There's a slight delay. J we are very sorry, there's a slight technical hiccup, but it should be fixed in a moment. All right, ladies and gentlemen, we are sorry for this slight delay. The session is about to start. Please have a seat. Francesco Andrioli, the speaker, will be talking about building a shield free developer community for wallet innovation. Francesco is currently leading developer community efforts at Consensys.
02:37:47.680 - 02:38:10.572, Speaker A: Consensys, as I'm sure many of you know, the maker of Metamask. Thank you, Francesco. Cool. Awesome. Okay, cool. Let's see if this is working also. Anyway, how many of you guys heard about snaps? Okay, cool, cool, cool, cool.
02:38:10.572 - 02:38:28.912, Speaker A: Okay, some people. Are you excited about Ethereum Zurich? Are you excited about Ethereum Zurich? Yeah. Can I hear some energy? Cool. I know it's Friday, but that's definitely super exciting. So I will talk about snapshot. I don't know how many dev rels are in the room or many developers are in the room. Cool, cool.
02:38:28.912 - 02:39:18.704, Speaker A: Awesome, awesome. So I don't know if you guys ever use snaps or you build a snap with flask, but it's definitely super exciting. This talk is all about permissionless innovation, why we're here, how snaps is today, and why we kind of got this virality happening. So welcome everybody. And, and yeah, so I also want to speak about frameworks a little bit. If I saw some guys raise their hands for Devril and I currently build different frameworks for speaking literally the same language for activation. And we will basically see what's going on with this builder hierarchy of needs and trying to understand why the virality of enabling developers to build on top of specific open source source code is key.
02:39:18.704 - 02:40:08.278, Speaker A: Right? Why snaps? Why permissionless innovation need to be adopted to every product and company out there? So about myself, this is actually in February that I did like an ETH Denver talk and before that they made me put this famous buffy corn. I don't know if many of you guys did in eat Denver, but it's a big thing, right? So today I'm in Metamask. I've been building before enterprise, blockchain, technology stack and I also been building a lot of Devrel activation and today I'm leading the entire offering around our consensus product. And so obviously I think everybody's using Metamask day to day. But maybe you guys deployed within Fira or even you deployed on linear latest GKe VM. Yeah, I'm just. So why snaps? Why this talk is all about snaps.
02:40:08.278 - 02:41:04.614, Speaker A: I want to basically emphasize everything around permissionless innovation and I also want to try to, I was very involved at the beginning when we launched snaps on fully open with the open beta and today, yeah, things are going well and we have a lot of developers excited about different use case and I will let you know a little bit more later. So just a quick recap, right? So for anybody that never heard about snaps, think about like a plugin ecosystem that you can actually build on top of the extension and mobile today. So you can actually do different cool things. The TLDR of also like the coolness is that when you build your own snaps, you can also have access to external APIs. So you can also build your own product snaps. Before launch, building a snap was basically like going through like Flask is the Canary version of what Metamask is today. It's basically like a version of Metamask for developers.
02:41:04.614 - 02:42:00.858, Speaker A: And basically you could build anything, right? Right now, if you want to be on the main directory, this directory on Snaps Metamask IO, you basically need to go through some audit process between the team and I don't know if you saw guys, but in e Denver we also launch a fully permissionless marketplace with karma. Three Labs is a full attestation focus service and basically you can attest snaps and basically pre audit snaps before actually going on the main directory. I will not bore you with what you can do, but there are different use case. Today we have five categories in the directory, but you can actually build specific things. The most famous one that you probably saw, maybe you're using on a daily basis are the non EVM compatible snaps. So today you can actually add a BTC snap or a Solana snap and do interaction with other chain. Decentralized identity is actually quite interesting.
02:42:00.858 - 02:42:29.020, Speaker A: People are starting to build frames with basically far caster frame on snaps. It's actually quite interesting. Notification snap is interesting for game developers and then you have also account recovery like NPC snaps. They're actually quite interesting. They're not involving any email recovery. They're actually working with different specific extension feature and it's actually quite interesting. Transaction inside is also quite interesting.
02:42:29.020 - 02:43:17.026, Speaker A: I don't know if you guys used transaction insight before, but we actually pushed in a lot of hackathon transaction inside a bounty and we will have actually this weekend in Paris blockchain week, the hackathon. We will have also some bounty around transaction inside if you are, if you guys are also coming. Cool. So the goal is not just saying what are snaps. The goal is also trying to make you guys understand why a product centric roadmap in your organization will never succeed as having like a developer centric and a community centric roadmap in your organization, right? So everything started with when we actually like launched snaps. Everything started from like a small project led by Dan and Eric about adding like a Solana ecosystem plugin, right. Was actually an add on and things kind of like moved.
02:43:17.026 - 02:44:31.570, Speaker A: And so, okay, they presented this demo at Defcon in 2019 and then actually they said how we can actually enable developers in the community to build top feature. At the time, there was like two or 3000 open repo issues that we could not solve. Like we had also like even today we had a lot of companies trying to come with different feature requests natively, right? So we cannot keep up with the innovation. So basically what happens is we try to build this kind of like open source snaps developer tooling that you can actually build your own extension. And that's basically like why enabling permissionless innovation is key today for getting not just the community excited, but also having a way for developer in other company or individual developers to build their own features and actually benefit on the amount of users on the daily basis that they're using the extension today. Right? So let's see if the pointer is working. So this is why we need the developer ecosystem, right? And important just TLDR is that, you know, product is, is not the main thing, especially right now.
02:44:31.570 - 02:45:58.064, Speaker A: I don't know if you saw like the big, what's happening a little bit with, with Farcaster and social media, but basically like what do you want to build this community around that? So you want to build tooling around that, but also like enable developers to build cool, cool products, right. The second big thing is that, and I mentioned in the third point here is that right now we're speaking always different language between organization consensus is a big company, right? We are 800 plus employee, but a lot of time even when, so I started this Devrel Telegram group, right? And the cool thing is that a lot of time we're speaking about different developer activation on different language with different kind of matrix, right? So we need to speak, we need to have some standardized framework, right? And I think that's what kind of, I concentrate myself for the past probably like four or five months and actually I published like a small paper for Eve Denver and that's what I'm talking about in the next slide, right. These are some standardization, right? So we want to also understand hackathon, what is effective, what are the key things that you want to actually get back from hackathons and why they matter. Always take care about feedbacks to your engineering team. I think that's key. For Devry incentives and Devra frameworks. There are two main areas that we're always speaking about.
02:45:58.064 - 02:46:41.926, Speaker A: We're speaking about the development side on the API, on the docs, on the tooling, and then the other side is all about the relationship side. This is also one part that I'm a big advocate right now around. Basically Devrel organizational framework is all about dividing the team in two main things around advocacy and evangelism. So evangeline is all about content creation, flywheel. So this could be encompassing stuff like technical blog post, tutorial and specific content framework that you're providing with your content team inside the company. And then you have advocacy. Advocacy is all about non traditional marketing to developers.
02:46:41.926 - 02:47:10.848, Speaker A: That could be from video tutorials, could be developer meetups. So different kind of way to reach out developer in a non traditional manner because obviously we know that we can't reach out on a traditional way. Okay, cool. So now we're getting to this builder hierarchy of needs. So we need a standardization. This is what I mentioned before. And we need a framework, right? We need to start speaking the same language and trying to understand.
02:47:10.848 - 02:47:40.674, Speaker A: Okay, those are the activation that are important. And most important, this framework that we will be using need to be agnostic to any problem product related activation. You remember how many of you guys remember this Maslow hierarchy of needs? I don't know if I can use a pointer. How many of you guys heard about the Maslow hierarchy of needs? Cool. I think a little bit. Everybody. So basically what I did that was last November, I basically transformed this to Francesco's a hierarchy of builder needs.
02:47:40.674 - 02:48:43.320, Speaker A: What does it mean? Those are five levels. Each level is basically like very tailored to what are we doing today around developer activation and developer metrics that we want to standardize with. So think like for example, what is exactly needed, for example, psychological need. What is the must have thing in order to have a good developer experience when you're onboarding. So first and big part is obviously be present at some flagship hackathons. But the most important part here that I want to state is that if your docs are not in place or if you don't have an evergreen, let's say developer funnel, things like that we started called academy for example, you will never be successful. You need to also when we're speaking about access to resources could be also your 30 minutes getting started guide or your 1 hour tutorial that people can just scrape from docs or some dynamic content like videos.
02:48:43.320 - 02:49:34.346, Speaker A: Second part is this fourth level there is that you cannot shield developers your product. This is the worst way and method you can do. And we will see the funnel later that also, that's the second framework that we build. But basically what you want to do before actually putting in front of developers that they need to build on your tech stack is actually the opposite. You want to build a vertical based use case and this is how you're doing is building a safe space around this ecosystem. So you want to build ecosystem meetups, you want to actually speak about the topic. We are running verticalized AMA Twitter spaces and specific ecosystem community call monthly and this actually like helped us to gather the attention around developers.
02:49:34.346 - 02:50:19.628, Speaker A: Example like you know, on next week we're doing an attestation space. In three weeks we will do like a risk taking builder night in during token 2049 and the cool thing is that we're building ecosystem players together to speak about the topic. After you're getting this safe space, then you can actually do conversion based initiatives, right. Third one is all about smaller hackathon fine tuning workshops and mostly commute engagement. And then you're going to actually things that you can get as extra. So you want to basically convert yourself as an organization that put the product in face of developer to actually be the platform to distribute and amplify community contributions. This is extremely important.
02:50:19.628 - 02:51:06.608, Speaker A: So how you want to do that is you know, to, you know, to virtual to virtual organization, you want to push leaderboards, you want to push like stuff like, you know, ambassador program and creator programs. And those things are all extra important here to understand is the first two levels need to be addressed before actually going over. Right. And you can use that as reference for anything. So this is actually the paper slash, like, you know, description of what much more the levels are done and I can share the information a little bit later. Cool. So what basically we're talking about here, we need to transition to a full web three narrative, not just including also web two specific tooling, but there are two main pillars that those are really care.
02:51:06.608 - 02:51:41.820, Speaker A: So you want to involve community partners, you want to involve other organization to actually push a narrative that is around the vertical and not product. Then the second thing is all about relationships. So you cannot start, I'm hearing probably every day startups or even big companies, they're going to mainnet in four months. So they're not doing anything until they're going to Mainnet. So you need to first build all this relationship with your developer ecosystem. And that's part of also the game. And I think this is one of the biggest lessons learned back in the days.
02:51:41.820 - 02:52:16.570, Speaker A: I was organizing ten, maybe four hackathon per weekend, per continent with a company called Angelac, right? And we were running basically topics and bounties that were all different. What was always similar is that we were seeing the similar faces and developers were sticking around because they were excited on the format, they were excited about the hackathon format. That's why verticals, for example, were a big win. Right. And that's why it's relevant. Right? So engaging developers successfully is part of the process. You want to be authentic.
02:52:16.570 - 02:53:48.850, Speaker A: Right? We started builder nights and I don't know if you guys ever been one, but basically are those topic, topic focused events. But builder knights are going viral because people, they just come, they want to hear a specific tech stack, but I want to hear about specific topics right. And yeah, I think like the lesson learned here was really like be authentic and push initiatives that are agnostic first to your product and then you can do product conversion, right? So this is like the big, the second big framework about funneling and basically like the lesser learning is that when I try to see what other ecosystem we're driving today in the, in, in web three is that I was actually seeing that only the lower part were actually driven. All this product specific conversion was 90% of people that were actually driving. All the budget was spent on product specific conversion. Right? And if you actually think about, this was an interesting topic because at the beginning we said, okay, no, let's actually build, do only snaps workshop, right? And then we actually think about, okay, how we can get our developers that are excited about the use case around snaps. So we actually retro engineer, we actually say the inverse logic, right? And we actually approach the onboarding of developer based on topics people want to build more aa snaps, right? People are actually like trying to build game snaps use cases, right.
02:53:48.850 - 02:54:43.772, Speaker A: We approach on the other way. We said, okay, let's actually speak about game development suite, right? We build all the, we talk all the game development company together. We said, okay, we have metamask SDK. What are the other tooling that are interesting for game developer to actually build? So that's where kind of like we thought about, okay, all this value driven onboarding that today is not really being done. Let's actually set it out in some specific framework. And all this part, and this is the most important part is that this is vertical focus, is a no shield space and is actually like very focused to drive developers awareness in order to build this relationship before actually speaking about your product itself, right. So you will run ecosystem communicable before running a snaps community, call specific.
02:54:43.772 - 02:55:41.900, Speaker A: You will run stuff like ecosystem newsletter, you could run even like ambassador program that are vertical focused and not developer tooling focus, right? And I think that was the big lesson learned that I want to share a little bit today. And that's also like one part, right? So always remember your docs are the most valuable assets. You want to also like strategize, strategize your onboarding based on different developer Persona, not just beginner, intermediate, advance, but also based on people where they're coming from. You know, a lot of like web two devs coming from a rust background. You have a lot of like, you know, devs that are coming from other ecosystem. You have devs that want to go in other ecosystem and then maybe they're returning back, as we saw like in the electric capital report, we had what, 32,000 steady developers. They're running every six months, like specific steady repo.
02:55:41.900 - 02:56:19.680, Speaker A: And actually that's quite crazy if you think about 32,000, right? It's super small. And one big lesson learned that I want to share is that we, for snap specific, we use bounty at hackathons. So we did very like engineering education iteration on hackathons. So last year we did like maybe six hackathons every two months. And every two months we were pushing the engineering team to come with different bounties on specific, like, you know, snaps features. And this helped us also like pushing ourselves internally. So that was definitely like a big lesson learned.
02:56:19.680 - 02:57:10.000, Speaker A: And if you guys are building a product or you're planning to activate developer through Irele events, that's a good way also to, to optimize your budget. So we spoke about a lot about one shot activation, both IRL and non IRL. But think about all the other parts, right? How you are building evergreen initiatives around the ecosystem, stuff like virtual hackathons, stuff like consensus academy. So things that are always evergreen and they are funneling developers to contribute no matter if you are on the ground or not. So this is one big part of the game. So you want to actually build small mvp, try it out and see what actually is sticking or not. Right? And I think we did three iteration of the ambassador program.
02:57:10.000 - 02:58:11.134, Speaker A: And one big part that at the beginning was missing is kind of like this kind of evergreen initiative because people are being there in your court of the ambassador, but after graduation they don't know how to contribute. And I think one big part was actually putting all this way to contribute. So I mentioned before that your role as a company, as a product owner is amplifying and distributing community contributions. And that's one of the biggest part that probably you think is irrelevant but is extremely important, right? Think about people that are building snaps and they are part of like they just want to be amplified and distributed. So we were basically doing a full post hackathon activations that were building, inviting them, shoutouting them on different community calls and basically fostering all the relationship around this. Cool. So I don't know how much time I have.
02:58:11.134 - 02:58:47.766, Speaker A: It's almost going. But important is that focus on building the relationship with non web, two marketing, basically strategy. Think about those framework when you are building it. This is one example about builder knights that I recap on building these verticalized developer meetups. And yeah, remember, build a safe space for developers before actually pushing the developer suite product that you have. And yeah, just to conclude here, this is the Snaps directory. So think about that.
02:58:47.766 - 02:59:31.484, Speaker A: You can actually download every day your snap using for specific security reason, for non EVM compatible reason. And yeah, that's actually quite exciting. What is happening? If you are a developer and you want to build your first nap on flask, check out this guide. We have also some snap wishlist that we also would like the community to be built. And yeah, big thanks to Dominic. I don't know if you're in the room and all the team to invite me and yeah, and let's chat more. Deborah, let's standardize the framework, but more important, let's try to enable your developer community to build modularity around your ecosystem.
02:59:31.484 - 03:01:21.566, Speaker A: That's where we always right from the user perspective and from the developer perspective. So thanks everybody. Thank you so much. Francesco, please give us two minutes while we're getting ready for the next session. Scaling Ethereum the power of unspent transaction output by Sandowski Good, thanks. It's a long day for you, right? That's true, yeah, I just need the mail. We have this thing where we play this music and then I will say a few words and then you can start.
03:01:21.566 - 03:02:00.554, Speaker A: Okay? Okay. Thanks so much. Welcome back ladies and gentlemen, to this talk. Scaling Ethereum the power of unspent transaction output by Sandowski Sandowski is developer relations engineer at Fuelabs. Sandowski, thank you so much. Thank you so much guys for being here. Thank you so much to the staff team for making this possible.
03:02:00.554 - 03:02:20.822, Speaker A: So, scaling Ethereum, the power of UtxO. We are going to understand why we need to scale Ethereum. We are going to to understand what's new TXO. But this is Vitalik six months ago. This is 2023 later, two days in Istanbul. Bulletproof. Vitalik did an amazing talk that date.
03:02:20.822 - 03:02:48.652, Speaker A: Two key takeaways. The first one is Vitalik talking about plasma, which is quite crazy, and Vitalika talking about Utxo, which is today's topic. So we are going to talk about the state load and decentralization issues. We are going to talk about race, condition and execution. Execution issues. We are going to talk about Utxo indeed. We are going to have a quick view the history of Utxo by the work of Hal Finney.
03:02:48.652 - 03:03:34.504, Speaker A: We are going to talk about the state minimized design. We are going to talk about predicates. And we are take a very quick look to some projects using Utxos right now. So state load and decentralization, state bloat is the situation where increasing state in the blockchain leads to increasing hardware requirements for the validator nodes. Basically, increasing hardware requirements threatens decentralizations because more hardware mean more money. And not everyone can run a node, so the network gets centralized. This is a chart portraying how ethereum states is growing nonstop since the beginning.
03:03:34.504 - 03:04:16.994, Speaker A: But not every kind of estate is the same kind of estate. So we can understand like basically three kind of estate growth issues. The first one is estate growth, which is new accounts, new by code, new storage. We can see history growth, which is new blocks and transactions. And we can talk about estate access, which is the issue of reading and writing operations for building and validating blocks. Today we are just focusing on estate growth, which is new contracts by code and contracts. Right now the Ethereum state is up to 245gb on a red node, which is a lot.
03:04:16.994 - 03:04:55.874, Speaker A: Mainly we have accounts, which is 14%, then we have by code, which is 4%, and then we have contract storage, which is 81%. A lot of memory spent there. Seems like it's fine, but it's not. It's strengthening the whole Ethereum ecosystem. And as you can see, somewhere around 48% of current Ethereum storage is basically ERC 20 and ERC 21 transactions. So it's like we are spending a lot of resources just to tracking assets. So summary state bloat is bad.
03:04:55.874 - 03:05:59.974, Speaker A: Then we have another issue, which is the race condition. Some of the computer nerds out there might understand this concept, which is this is like a block header for an Ethereum block. An important this, ooh, an important except might be the state root, which is the root of the merkle tree of the state of the block. And basically Ethereum storage looks like this is a dictionary where you have account names and ETH balances. And for the case of contracts, you have ETH balance and you have another storage root representing the contract storage, which for years 20 might be like just accounts and balances of that specific token. So the question might be, why is not parallel? Like what's the issue behind the account model to make the parallelization impossible? Well, because of race condition. Race condition is a situation where two processes or threads are trying to access and update the same slot of data.
03:05:59.974 - 03:06:55.244, Speaker A: Looks like this, basically, and Ethereum did some work trying to solve this issue. This is a screenshot from the EIP's GitHub. Basically this is Vitalik proposing somehow some kind of of access control list for execution. So basically the node can know beforehand which storage slots are being updated. There's a lot of interesting information in this issue. One of the cool takeaways is that accounts can have arbitrarily large status trees and we can load that into memory. And then we have another funny stuff which is we might require nodes to have 512gb of ram, which is completely nuts.
03:06:55.244 - 03:07:32.816, Speaker A: But basically the issue was closed when they realized that this cannot be done. It's another bottleneck. And the conclusion was we should move to stateless clients or sharding basically so you can do parallel execution in Ethereum. So stateless is the way. I don't dig into many of the available solutions to solve the state blow on Ethereum, just the state minimized design. But right now I stick with account model is somehow bad. Then it comes Utxo as a blessing.
03:07:32.816 - 03:08:11.582, Speaker A: This is Hal Fini inventing Utxos and helping us to solve these issues. Basically Utxo is an approach where you don't need a root state, so you don't have centralized state, so you don't have race condition and so on. Utxo look like this, they stand for unspent transaction outputs. So basically this is how bitcoin works. And it's like you don't have a big spreadsheet of balances from each user, but each user has some amount of money locked in some kind of program. And then you have like a bunch of programs with money that is not spent yet. So it's like a piggy bank.
03:08:11.582 - 03:08:41.696, Speaker A: Basically you put money into a piggy bank and then you have your account with a bunch of piggy banks with different amounts of money inside them. And when you want to spend some of this money, you just destroy as much as piggy banks. You need to fulfill the desired amount. Then you create a new piggy bank with that amount of money and send that to the recipient. Another analogy might be cash. So basically when you pay with $100 bill, let's say you are willing to pay 30. So you have 17 change.
03:08:41.696 - 03:09:16.284, Speaker A: Well, the cashier might be burning that $100 bills and creating two new bills from scratch. $30 bill for themselves and $70 bills for you. So it may look something like this. A bunch of Utxos are going to be merged into one transaction to create a new Utxo. A more accurate depiction might be this one where you have 50 bitcoins, you send them into a transaction and split them 0.5 into another recipient and 49.5 for yourself.
03:09:16.284 - 03:09:52.320, Speaker A: Another account might be the same. So it's going to merge a couple of Utxos in order to create a couple of new Utxo. So it's like an ongoing process forever. So why is this parallel? Well, because you remember race condition where you're trying to access the same slot in memory where you don't have that problem because of atomic state. Basically you are having pieces of storage, pieces of data flying around in the blockchain so beforehand you know they are not colliding together, which is amazing. Utxo killing the wood state. Wait a second.
03:09:52.320 - 03:10:09.192, Speaker A: I'm Zandoski, I'm developer license engineer at Fuel Labs. You can find me like this on Twitter. We are building the roll up OS for ethereum so it's pretty cool. Go to fuel.net work to check it out. Please skip the selling. We were talking about Utxo.
03:10:09.192 - 03:10:55.914, Speaker A: So good, right? But this has a history behind and it's a pretty cypherpunk history. This concept kind of was invented by Hal Finney back in 2004 and basically comes as this concept reusable proof of work. This is a screenshot from the archival website which is started at the Nakamoto Institute website. And they even have the slides, so I'm going to reuse some of the slides. This is based on on top of hash cache. Hashcache was built in 97 by Adam Bach and basically is proof of work. They invented a way to tokenize expensive computation in order to avoid email spam.
03:10:55.914 - 03:11:55.304, Speaker A: The issue with this is like you cannot exchange them. And Hal Finney had this theory that if you're receiving a lot of hash cash, proof of work means that you're a real human being. Spammers often don't receive emails, therefore they don't have proof of work related to their own account. So they say if you already have a computation proving that you are a real human being, you should be able to spend that tokenized computation in order to perform all your actions. And this is basically the Utxo model. You have proof of work tokens, you have a data structure that allows you to spend that tokenizing computing and then you have a network of decentralized protocol and servers willing to verify that information. This is basically bitcoin almost five years before of the white paper.
03:11:55.304 - 03:12:37.692, Speaker A: So thank you so much Hal Finney, for building this technology that makes us so happy. Now we go into a state minimized design where basically we are trading off bandwidth with a state. In ethereum we have different kind of storage information. We have like cold data and memory, which is like ephemeral information. Memory is like the scope variables you are using within a function. And call data is read only information that comes as form of parameters within the transaction and then you have a storage, which is permanent. Information storage might be like going to a restaurant.
03:12:37.692 - 03:13:16.304, Speaker A: So you are like walking so lightweight. You go there, they have the food, you eat, you leave, and then you have cold data, which is like keeping your own lunchbox wherever you go. So basically for storage based execution, you have all the information in the contract, and then you send lightweight transactions to avoid taking so much ban with it. And then you have call data, where actually you are sending big transactions to pretty stateless contracts. That's why it's called state minimized. Uniswap B three is doing this. Actually, this is like the staker, a section of the staker contract.
03:13:16.304 - 03:14:01.762, Speaker A: And basically you see that they are just storing a derivation of some data structure. And each time someone is willing to get information from that incentive, they are basically computing back again the information. And it seems counter intuitive because we learned that we want to avoid computing things many times. So we build caching and all of this other stuff. But actually for blockchain, this makes sense, and it's called a state hydration. So basically you are keeping track of small pieces of data in the blockchain and then using computer power to reiterate that data and have meaningful value. And then it's where perkins come into action.
03:14:01.762 - 03:15:13.732, Speaker A: Basically, this is a stateless account abstraction. It's a new kind of primitive built on the fuel network, and it's something really cool that I would like you to see. So we began talking about Utxo, and we end up talking about predicates, but in the middle there's pay to script hash. This is something coming from, I think it's beep 16 by Gavin Anderson. So what the fuck is p two? P two script hash? Well, you know the standard kind of account in bitcoin and everywhere else, which is private key, elliptic curve, and then a public key. So you deposit money or associate money to that account, and then a private key is able to expand those resources, where in the case of pay to script hash, you have signatures and you have a redeem script, which is some kind of puzzle designed to define the conditions, whether Utxo can be spent or no. So basically you are having a public key that doesn't come from a private key, but from a script.
03:15:13.732 - 03:15:52.688, Speaker A: So that's like the idea. You can imagine that as like the lockboxes you use for Airbnbs. So it's not a person giving you the asset, but a device in the world having some kind of puzzle and then having some kind of asset that you can unlock. So if you know how to solve the puzzle, you know how to get the assets. That's the whole idea. But what's the issue with pitoscript hash? Why is not like so widespread use? Well, basically because bitcoin is dumb. Like you can do almost anything with bitcoin so far.
03:15:52.688 - 03:16:38.724, Speaker A: That's why we went into Ethereum. Ethereum decided to go with the account model rather than Utxo model. So that's where predicate becomes so useful, because predicate, so powerful, so good, so gentle, are some kind of mix between the Utxo standard approach. Powerful vms, such as the fill vm high level expressive, unuseful language such as sui Lang, which is a DSL build base on rust. And the philosophy behind pay to script hash. That's where you get predicates. A predicate is just a pure function that you hash and get a public address.
03:16:38.724 - 03:17:21.934, Speaker A: So it's like the source code is the private key, and this is basically a stateless account abstraction. Why? Because you are not storing this code in the chain, you are just keeping track of the hashes and whatever who has the input script is able to spend the money that is there, but not with the code. Only you have to fulfill the conditions of the code. So a predicate may be basically a functor, this for the computer nerds. So predicates are like wallets. They can receive assets, they can hold assets, and they can spend assets. Predicates are like wallets, so they can be deployed and they cannot be deleted.
03:17:21.934 - 03:17:54.152, Speaker A: So yeah, so they are pure functions with a Boolean return. And if the result of the predicate execution, we call it evaluation. If the result of the predicate evaluation is true, well, you can unlock the assets within that predicate. A slide about reusing proof of work. I'm reusing the slide spending predicates. So basically you need the predicate code. As I told you, you need a transaction that fulfills the condition of the predicate.
03:17:54.152 - 03:18:26.838, Speaker A: You need the full client and the full Utxo set. This is basically like the blockchain, and then you have a new Utxo. So it's basically, it's like Utxo sending a transaction between them, but adding an additional step, which is solving a puzzle so you don't have to pick. Keep the money yourself. You can keep the money in a stateless, obstructed account, and you can spend that by fulfilling the conditions of the predicate. Differences between contracts and predicates. Predicates won't access on chain data.
03:18:26.838 - 03:18:59.784, Speaker A: Why? Because they are pure functions. They can read data from smart contracts. They are not aware of time and they can read hash numbers or block numbers, but they can read input and output coins of the transaction. They have access to transaction script and transaction bytecode. This chart is coming from our own guide on predicates, which is predicates 101. Building stateless defi applications. You can check it out just right here.
03:18:59.784 - 03:19:31.284, Speaker A: But I'm getting the QR again. I'm going to struck all DM's and Utxo. This is another project right now leveraging on the Utxo approach. We have aztec privacy solutions, we have nervous which is like intent based network. We have finder out which is some kind of ethereum sidechain. We have legume and only one of them has powerful predicates which is fuel network. So summary and a few conclusions.
03:19:31.284 - 03:20:01.194, Speaker A: State load is bad for decentralization. Account model can be parallel. Utxo is so good. Hal Finney, thanks so much. And predicates are so amazing. So if you want to start building stateless defi applications, you want to mess around with predicates, you want to build this crazy off chain on chain secure functors, well, come and join this QR code. I promise that is not a security for you.
03:20:01.194 - 03:20:26.874, Speaker A: I just need a tracker for the event so I can show it to my manager. But yeah, that's it. Thank you so much. Do we have time for questions? We have time for one or two short questions. Are there any questions? Is there anyone? However, we can meet outside and keep talking about computer science stuff. However, thank you so much for attending and I hope you have a great time building with predicates. Thanks.
03:20:26.874 - 03:25:19.002, Speaker A: Thank you so much, Sandowski. Thank you. Please stay put for the next session, which is about to start in approximately five minutes from now. Sequencer level security in circuit presented by Jan Gorsney. It it. Hello. Okay, thank you, ladies and gentlemen, let me introduce you to our speaker, Jan Gorsny.
03:25:19.002 - 03:25:40.096, Speaker A: He came all the way from Canada to talk about sequencer level security in circuit. He is both an experienced researcher and the founder of Circuit, but he will tell you a bit about himself. Yeah, thank you. Thank you. Thank you. So, hi, my name is Jan. I'm co founder at Zurkit and technical lead.
03:25:40.096 - 03:26:31.276, Speaker A: So I'll tell you a little bit about myself. I've been in the space for a while. I did a PhD a while ago, got some grants from the Ethereum foundation, started working on roll up related topics, and then decided, you know what, let's just try to build one so we can explore our own ideas and one of the ideas that myself and my co founders are really excited about was this concept of sequencer level security, which I'm going to talk to you a little bit about right now. So you're here, you probably know some things about blockchains, but I got to get you all on the same page to motivate why we're looking at the specific topic, the specific feature of a roll up. And so let's take a step back for just a second and say, what does a blockchain actually sort of do to guarantee security on it? Right. The node does a bunch of things. It'll check the transactions are well formed, that things have signatures, and that things should be included in blocks.
03:26:31.276 - 03:27:17.780, Speaker A: So if you saw Arthur's talk earlier, censorship's sort of an issue. And actually, we'll talk on a related topic. But essentially, it does all of these things to make sure that the chain advances in the nice way. So the chain is doing something correctly, always predictably correct, according to some rules. And this is very, very important, obviously, so that you have a chain that you can trust and rely on to do a whole bunch of really cool things. But what it doesn't do is actually secure your funds. So if the chain includes a transaction that might negatively affect your balance because you deposited in some untrustworthy defi protocol or something else where your funds are lost or hacked, this is not ideal, right? And you actually can't fault the chain for doing this.
03:27:17.780 - 03:27:54.726, Speaker A: If you believe code is law, you certainly think that this should be the expected outcome. But probably you're a little bit more attached to your funds than just saying code is law and sort of washing your hands with the funds you lost, and you want to do better. So we were looking at this and thought, can we add some value in some area of this ecosystem? And getting ethereum do this is not desirable. We don't want to do that. But it doesn't mean that other things, other people can't do something about this, namely layer two s. If you have a particular concern about this, you can deal with it. And in particular, we're going to do this on circuit, which is a new zero knowledge roll up that we're building.
03:27:54.726 - 03:28:31.438, Speaker A: That's going to put some level of security that can guard against, not just that sort of sequencer level, traditional protocol implementation concerns that you would normally have. So are things signed properly and do transactions execute all of that? But also, are your funds going to get hacked or lost? Obviously, only on the l two. We can't do anything about the l one. But what this means is you can actually design some stuff to guard your funds, even when sort of things are going away. And this won't be for everyone. This is going to be sort of a hard pill to swallow for some people, because we're going to have our hands in the mempool. We're going to start looking at things and say, oh, that should be or shouldn't be.
03:28:31.438 - 03:29:02.910, Speaker A: And what that means is interesting, challenging research questions and research directions. But there is almost certainly value in doing this, right. If you can guard your funds and you're honest actors, and you can discourage dishonest actors, this is something you might want to do. So, before we talk about how this works or what we're doing on Zurkuit, how do transactions flow in a roll up is important, because we're going to be changing that flow. So let me go through this very quickly. So what happens in a roll up? You have some layer one blocks. These are the orange ones at the bottom.
03:29:02.910 - 03:29:26.126, Speaker A: And you have a mempool, which is a sort of text document on the screen. And there's a sequencer. Sequencer is essentially just a centralized privileged node, not always centralized. In the future, everything will be decentralized. But generally, right now, everything is centralized. And it takes some things out of the l two. Mempool constructs an l two block, which actually posts first to the l one as a batch to get a soft commitment.
03:29:26.126 - 03:30:06.704, Speaker A: And then from the l one commitments, you actually rederive the l two state. So the sequencer can do this optimistically, and it has a good vision of what's going on in the l two state all the time. But, you know, you probably don't trust it until things are hit on the l one. And then there's some notion of finality. There's, you know, either an optimistic roll up where you're waiting seven days to make sure that no one has proven that the sequencer didn't do anything wrong, or a zero knowledge roll up, which is the case for circuit, where there is eventually a proof that it's the chain that says, actually my state transition followed some rules, and we're all good. So that is generally what a sequencer does. It constructs l two blocks by posting data on l one.
03:30:06.704 - 03:31:13.628, Speaker A: Xurkit's gonna do a little bit more than that. What we're gonna do is add this level of sequence or level of security, where we're gonna go into the l two mem pool and change how l two blocks are constructed in order to safeguard users. And so what's gonna happen is we're gonna add this security oracle. So we're using Oracle in the computer science sense now, not in the price oracle sense that you're probably familiar with in web three that asks for every transaction. Is this a safe transaction? Is this a good transaction? I'm not going to define for you what a good transaction is right now, because that's reasonably challenging, but you can envision it as is there a flash loan that's wiping out a defi protocol? And you can define what this looks like, at least at some level, for your needs, and we'll talk about how we're going to do it. But essentially there's some notion of good and bad, which we're going to have, and we'll have an oracle that will determine for a given transaction on the current tip of the chain, is this good or bad? And if the answer is yes, we're going to put that into the batch. We're going to process the transaction just like it normally would be, perfectly in sync with standard notions of an l two, if it's bad though, we're going to kick it into quarantine, or if we think it's bad, we're going to kick it into quarantine.
03:31:13.628 - 03:32:11.754, Speaker A: And what this means I'll talk about in a minute, but essentially it will be ignored, at least temporarily. And so what you end up having is an l two chain where everything in that l two chain actually went well according some definition of, well, obviously that is still something we should talk about. And exactly how does this happen? Well, first, how does it do it? So what does it mean to put something in transaction? Can it ever get out? What are the technical details? What happens in it? Can something leave it? Then the hard questions of how do you tell if a transaction should be quarantined? That's going to be probably the most controversial, the most challenging thing. And then there's some technical corner cases you have to also deal with. Some transactions originate on l one, for example, these are called deposit transactions. And then how do you make sure that other people who are running l two nodes to replicate the node, if not acting in this privileged sequencer position, can also come to the same conclusion of what the l two state should be. So let's start by talking about what is the quarantine.
03:32:11.754 - 03:32:51.362, Speaker A: So the quarantine is going to be this place where we put things to determine if they're good or bad in particular. If we think they're bad, we'll put them in there until we can validate them or for some other criteria that we'll talk about. But essentially what happens inside of a sequencer, especially one that's based on geth, like most of them are including circuits, you've got two cues there. You get a mempool and you have a queue. And the queue is for sort of things that could be right. It could be transactions that have nonce that are too far in advance, or the good ones that are ready to process right away. And if they're right away, if the nonce is sort of the one you'd expect for a user, the gas is all appropriate.
03:32:51.362 - 03:33:23.710, Speaker A: All of that, you can put them into the pending queue, and from the pending queue you essentially build l two blocks right away. That's nice. And it's exactly that flow where we're going to interrupt and put this oracle. So what we do is we say, well, before we process it into a block, is it good or is it bad? And that's a simple question. And so it naturally flows right in there. And that means quarantine essentially just says take it out of the ability to go into that pending queue. So we're just going to put it somewhere else.
03:33:23.710 - 03:34:03.724, Speaker A: We could call it a different queue, a quarantine queue if you wish, but essentially it's not going to that pending queue, and then the rest of the transactions go on. Okay, so let's talk about what happens when we actually put transactions into the quarantine. They could stay there forever. If they're a bad thing that everyone agrees is objectively bad, they lock a lot of funds somewhere. It could stay there forever, and we'll talk about exactly how that will work. Or they could leave for some other reason. They could leave because there's been a time delay, right? Maybe there is a nice rule on the chain that says you can actually leave the queue a little bit later, or the user could cancel them.
03:34:03.724 - 03:34:44.456, Speaker A: You can replay transactions with the same nonce with a different gas price and a different content of the transaction to actually have your transaction be kicked out of the queue. Because you'll put a transaction that isn't bad onto the l two and then rendering any transactions with the same nonce invalid, namely the ones in the queue. So you can actually have users cancel them, which is kind of cool. This is helpful. If someone thinks they're doing something good, but actually they're doing something bad accidentally, they can cancel that transaction. Or if it starts to fail, we can then sort of not care about it anymore. If we can prove that at a given time, we're playing it out of this quarantine queue would cause it to revert.
03:34:44.456 - 03:35:26.652, Speaker A: Well then no harm done there. Except I guess the person who submitted the transaction will take their gas fees. Or there could be other really cool things you can do, right? You can release it subject to economic considerations. So what you could do is say, I don't know what good or bad really is, and you're telling me it's a good transaction, but I don't believe you. I think it's going to do x dollars worth of damage. So you could have them stake x amount of dollars and say, look, I'm going to play your transaction, but if something goes wrong, I'm going to slash your stake and they're going to use it somewhere else. So there's a bunch of things you can do once you start looking in this design space, but the most simple one is also just the administrative concern.
03:35:26.652 - 03:36:17.804, Speaker A: So if you think something is bad, you put it in the queue until someone actually sort of traces through the transaction, figures out and says, hey, this doesn't look that bad. Maybe we can let this go. It's probably the one we'll start with, but there's a whole lot of areas you can do with this design space once you start doing this. And what you notice is actually these things can do actually a little bit more than that and we'll talk about that next. But essentially if you have decided something comes back out of the queue, for example, because it was a false positive in terms of badness, it was actually something that was good, but just complicated, you can put it back into that pending queue from two slides ago and everything works as perform. So the next question is, how do we know something is bad? And we're going to leave this largely up to the oracle. But the oracle has a lot of information at this point in the game because you're not doing something very sort of academic in the sense of like trying to prove that all transactions can go wrong.
03:36:17.804 - 03:36:43.830, Speaker A: Like when you're auditing code, you're not saying that there's, you know, something could go wrong with a smart contract. You're saying this transaction will do something bad. You can simulate it, you can actually play, play it, play with it. You can play it off chain, you can simulate it. You can use things like AR models to detect patterns in previous hacks and see if this one corresponds it. And you can do a whole bunch of really cool stuff because you have the information, you're not dealing with hypothetical attack vectors. You're given a transaction that you want to determine an outcome for good or bad.
03:36:43.830 - 03:37:14.324, Speaker A: And so this is in fact what we're going to do down the line. It might be even nicer to do something better, or maybe not better per se, but different, which is to say, define custom invariance. You could tell me what is good or bad. You have a DAP, you're concerned about losing its value. That's important to you, but the next step doesn't care about that property. So you each define specific invariants, and the sequencer enforces those in different ways for different apps. And you can do this to actually change the semantics in a blockchain, which is also really, really cool.
03:37:14.324 - 03:38:07.744, Speaker A: For example, you could have invariants that aren't sort of goodness or badness, but actually just enforce things more efficiently for you. From a gas point of view, you could say that any transaction hitting a specific function on one of your contracts is bad, unless it came from your address. That's effectively implementing the only owner modifier that people write in smart contracts, which say you can only upgrade it if you're the owner. But now you can take that modifier out of your smart contract code and put into the sequencer, and you now saved yourself a little bit of gas when you run that function call. That's probably a pretty contrived example. You're probably not doing that very often to have that warrant the extra engineering effort. But you could envision taking these other things in smart contracts and putting them into invariants that are enforced by a sequencer, which we think are really cool.
03:38:07.744 - 03:38:52.862, Speaker A: For now, what we're going to do is research this idea quite a bit. We've got some partners, both in academia and industry, where we're going to figure out how to look at these models, how to test them, how to simulate them, and how to use back testing of previous hacks so we know what things have gone wrong in the past on chains. Obviously. Everyone probably remembers with the DAo hack and all that, there was a whole bunch, right? There was $2 billion lost in bridges over the last couple of years. There were, you know, tens of hacks over each of the last months. We can play around with those. We can figure out what looks bad and start to tailor the system to start at least to counter transactions that look exactly like those or similar to those, so that we can prevent at least those disastrous outcomes.
03:38:52.862 - 03:39:25.614, Speaker A: Because none of those seem to be upsetting to the people who pulled off the attack. They all knew that they were pulling off attacks. Okay, so we just have a couple more minutes, and there's actually some small technical considerations that I'm going to dive into just for a bit. In case you're wondering about some of the more detailed stuff, we'll just to avoid questions about it later. Sorry. L one deposit transactions are a bit unique. So l one deposit transactions are generally the transactions say, put my ETH off of ether, off of Ethereum, and onto a roll up, say, Zurkuit.
03:39:25.614 - 03:40:05.984, Speaker A: The term's actually been conflated to say, say any transaction that affects l two state that starts on l one. But essentially this is a little bit of a technical issue because the sequencer has less control over these transactions. Right. The protocol for most sequencers says, look at the l one inbox or the bridge, and if there is a transaction there that puts something on l two, I sort of just have to deal with it. So I can't kick it out of my mempool because everyone looking at the l one state and trying to reach, derive the l two state from it will look at that deposit transaction and actually realize, oh, that should be in there all the time. So what you actually need to do is you need to have a different way to check and quarantine them. Sorry, I thought that was a slide.
03:40:05.984 - 03:40:41.832, Speaker A: And you can do this, but what you have to do is you have to go back to the l one chain and write to the l one chain saying, this deposit transaction is going to be ignored, put in quarantine, essentially because of some reason. You can say the reason or you can just mark it as ignored. And what this also means is to rederive the state. You have other nodes watching the l one, and it's got to have this sort of special functionality. So I think that is actually the end of my talk, which means I'm a little five minutes early. But if you have any questions, please find me. I'll be around here outside or sitting or at chain science, I think.
03:40:41.832 - 03:41:01.724, Speaker A: I'm also on a panel on Sunday, and I have a talk on at chain science tomorrow. But come check us out. Zurkit's a new Zk roll up. The staking thing is actually over a billion now, so it's grown quite a bit since these slides were made. But check us out. Ask me questions and thanks. We actually have time for one or two questions.
03:41:01.724 - 03:41:51.382, Speaker A: Are there any questions in the audience? So do you imagine Oracle to be deterministic or can be randomized? I mean, you're going to be. I don't have a good answer for that. I mean, I think you'd want it to be deterministic so that you could show someone that, like, hey, this transaction didn't go into the chain because of this reason. If there was a compelling reason to introduce randomization there, I'm not sort of opposed. What we envision long term is to have the ability to sort of have multiple of these oracles. You could also envision like a two of three majority where your oracle might be randomized and might say, this is not good, and someone else's oracle is a little bit more deterministic and gives a different answer and you take the majority of them. I don't really know.
03:41:51.382 - 03:42:34.960, Speaker A: Would you consider llms or like AI based things randomized? Probably also not, but they're probably not necessarily deterministic, depending how you train the models. I mean, if Oracle is deterministic, malicious attacker can also check in advance and design transactions so that it passes. Yeah, yeah. I mean, the hackers, I think, probably always have a little bit of a leg up here because they're trying to skirt this. I don't know how much we're going to be able to counter that. And the best answer there is probably that custom invariance. If you tell me as a Dap developer what shouldn't go wrong, and you do that in a way that's careful, then the attacker should never be able to break that invariant.
03:42:34.960 - 03:43:19.370, Speaker A: If I can simulate it. And if we do that, then there's sort of no advantage, I think, to the attacker abovehand until we get to that point. I think, yeah, people can come up with new attacks in the same way that if you're a smart contract auditor, you've probably been looking for a re entrancy all of your life, but now there's a whole bunch of new classes of attacks that you also need to look at because you didn't think of them as concerns before, but now you do, right? In particular, things like read only reentrancy. So ideally we catch all of those as well. But if the invariants are defined, it's a long winded answer. If the invariants are defined well enough, even new attacks can be canceled. So if you just say, don't wipe this protocol, it doesn't matter how the attacker tries to do that.
03:43:19.370 - 03:44:01.574, Speaker A: If it wipes the balance of the protocol, it's going to be up. Any other questions? One last question, maybe? Anyone? For now, it's us. For now, it's going to be an AI based model using the previous hacks that have gone on. Ideally, we would love that you were the Dapps to say, this is what I care about in addition to the AI, because you might have gotten it wrong or the AI might have gotten it wrong, but together we can do it better. Well, yeah, every roll up is sort of a middleware. So. Yeah, this is.
03:44:01.574 - 03:44:34.726, Speaker A: Yeah, I mean, we haven't open sourced the idea as just yet, or the tech. We will probably at least the infra for the roll up. I'm not sure the LLM will go or the I stuff will go public, but the idea you can definitely take and implement. Thank you so much, Jan. It was a really interesting presentation. Probably went way over my head, I have to admit. I think everyone saw how to contact Jan.
03:44:34.726 - 03:49:23.974, Speaker A: If you have any questions, please stick around for the next session, which will start in approximately five minutes from now. Bridging EVM and non EVM assets, a practical approach by Michael Bucher it welcome back, ladies and gentlemen, to the session. Bridging EVM and non EVM assets, a practical approach. Our speaker, Michael Bucher is here today at his alma mater. He studied here at the University of Zurich. He has a background in computer science and he's currently the lead engineer of the native bridge for Neo X. Ladies and gentlemen, Michael Bucher.
03:49:23.974 - 03:50:01.038, Speaker A: Thank you. Thanks for the nice introduction. So welcome to my talk here. This is going to be a practical approach, so it might get quite technical. So if you have any questions on pretty details, just talk to me afterwards. So we're axlabs team of five people, three located here in Switzerland. We've been developing tools for web three for a long time, for about six years now, and even also before we've worked with Web three.
03:50:01.038 - 03:50:47.524, Speaker A: So we have quite some experience today. So we've worked with various VC's, corporates, Binance and OKex, for example, use our tools to transact NIO, for example. And we're also a core community of the neo blockchain, and we're in close ties as well with the hashgraph association. So here's a quick overview of what we're going to talk I'm going to look into here. So let's dive right into. So the neo ecosystem is quite a dinosaur in crypto. It's been here since 2014.
03:50:47.524 - 03:51:50.234, Speaker A: Some of you might not recognize it because previously pre 18, it was called nShares, and it did a rebranding in 2017. It aims to build a smart economy and its main part of the neo ecosystem is its blockchain, neo n three. It has the neo VM, which is quite feature rich and dynamic, flexible, but it's not EVM compatible. We'll get into that later. It has a DBFT consensus, which means it has a one block finality. So whenever you issue a transaction and you see it in a block, you don't need to wait for further confirmations, it's done. It provides native oracles and file storage as well, so you can easily store your files on a sidechain that's connected to Neo.
03:51:50.234 - 03:52:59.734, Speaker A: It supports multiple languages, so you can, for example, develop your smart contracts in C sharp, or in Java, or in Python, whatever you like best. It also has a special dual token model, so we have two tokens here. One is native gas token, which is used for utility things like transaction fees, and Nio is the governing token. So when you vote with Nio and hold Nio, you earn gas. So you just need to hold gas and you just need to hold Neo and you get your gas that you need to pay for your transactions or pay for file storage, etcetera. So as you might know, the current situation is quite challenging for non EVM chains. It is not the main standard like EVM has large community already, so it has first mover advantages.
03:52:59.734 - 03:54:37.494, Speaker A: So if you're a non EVM chain, it's just difficult if you would have EVM compatibility, it's just easier for integrating with central exchanges or issue a stable coin, or also just generally to to connect to institutions. Because most companies that start with web three, they will start with EVM compatible solutions. On the other side, non EVM chains have also some R and D possibilities that EVM doesn't have. EVM is a bit slower, where non EVM chains can just have a faster pace, right? So the neo ecosystem wants to have both in its world. So EVM compatibility to connect to everyone that's out there that's already in the game and still keep this feature rich neo vm that it already has. So introducing Neox, which will be the new EVM compatible sidechain in the neo ecosystem, it will adapt the n three's DbfT consensus, so you will also have a one block finality there. And more importantly it will add a very great feature which is anti mev.
03:54:37.494 - 03:55:51.904, Speaker A: So on Neo x you I'm not sure if it will become the standard, but it might that all transactions will be enveloped, so they will be encrypted and only after the ordering of the transactions is finalized they will be decrypted. Besides, all that gas will be the native token on that chain. So usually some projects they come up with a new chain and they will issue a new token. But Nio doesn't want to do that they want to keep the current dual token ecosystem and they want to use gas because why? Why come up with something new if you have something that's already working? So the gas token on neo x should not be new gas. So no new gas should be minted on that chain. Everything that's on neo x must have come from n three. That's the goal.
03:55:51.904 - 03:56:37.808, Speaker A: And also the gas production ultimately just stays on n three. Also important is that gas will be burned on neo x to keep the ecosystem flourishing. Yeah. Further utilities for gas on neo x will be governance as well, which they don't have. So it will just extend the utility base of the gas token in general. So we need a bridge, we need to bring these gas tokens from neo n three somehow to neo x. Right? So why do we need it? It's really simple.
03:56:37.808 - 03:57:40.444, Speaker A: It's not rocket science. We just need to move it and connect neo x to neo n three in this whole neo ecosystem. So when we were asked to help with that development, we had some requirements. So first one is it should be a low risk solution, of course, because you all know bridges are pretty drawn to hackers in the recent past. So there shouldn't be a single point of failure, right? So there should be no single key holder. And also we want to decentralize the trust. Another thing is we don't have that much time to develop this, so we can't, you know, do much R and D.
03:57:40.444 - 03:58:15.714, Speaker A: We need to do it, but not too much. So we need to do more engineering. So here are some solutions that we considered. We consider Neo X as a layer two solution. And the solutions for the bridge would be an optimistic roll up or a CK rollup. However, with the requirements that I just mentioned, it will take a long development time for some of them. For optimistic rollup.
03:58:15.714 - 03:59:18.650, Speaker A: For example, we would need to emulate the EVM because remember, Neo VM is not EVM compatible, which will take quite some time. And Ckrollup, it's just a high complexity and we don't have the skill set. So we would need to onboard new people that already know this or otherwise we would need to learn it. But this takes a long time as well. So we went to the direction of a sidechain. So we need to bridge something without having this challenge option, right? So let's just take the most trivial approach that we could have. We could have just a central entity that just listens to one chain, sees an event, a bridge event, and then creates a transaction on the destination chain and does that.
03:59:18.650 - 04:00:23.934, Speaker A: But that requires a lot of trust, right? So before we will talk about what exactly we will bridge, we'll talk about how we will do that. So the goal here is to reduce trust. And also what's not on the slide is increase security. As you can see here at the bottom are multiple validators. So our solution includes multiple nodes that are disconnected. They will listen to the chains if there is a bridge event, and then they will assign individually what happened, forward this to the relayer, and once the relayer has enough signatures of those validators, it will relay this to the destination chain. So the validators here act kind of like a multisig.
04:00:23.934 - 04:01:21.256, Speaker A: In current stage, it's a five out of seven multisig. So with that said, let's go into what we want to bridge, right? Because just bridging every single deposit that happened to the other chain, it could lead to validators not signing something. They could just leave something out. So what you would want to do is to keep track of what happened in the past as well. So you can trust that the validators actually signed everything that happened in the past. So there are two approaches here that we considered. One is a state proof, which takes minimal computational effort, because on the origin chain, when you deposit something, you could just do the transaction.
04:01:21.256 - 04:02:16.010, Speaker A: And in a smart contract, it would just store that data, and that data would ultimately be included as a leaf, you know, in the whole Merkel Patricia tree. In the state proof, however, on n three, the state root is not in the header, so it doesn't really have final consensus. On the other hand as well, to prove that on the destination chain is quite expensive. So remember, Neo X is a new chain. So if a user wants to bridge something, he doesn't have any gas yet on new X, so he can't claim. So what we really want is an automatic bridge. So we want to include the whole transfer, the distribution on the destination chain, to be in the hands of the relayer.
04:02:16.010 - 04:02:59.870, Speaker A: We can do that on a smart contract level. So we can force, whenever we update the route on a destination chain, or on either chain, you need to provide data. You need to provide the bridge data in order that you can do that. So that will be automatically. And we also considered using Merkle trees directly on a smart contract level. So we did actually a proof of concept for this, but it's just expensive. And another interesting problem we, we discovered was that there were variable transaction fees, which is a problem on a UI level.
04:02:59.870 - 04:04:08.984, Speaker A: So if a user bridges something, provides some funds to pay for the transaction, and some other person comes with his bridge action before him, then maybe the transaction will just cost more and your funds will not be sufficient and fail. So then we asked ourselves, what do Merkle trees provide that we can actually afford to lose? And keep in mind what I just said before is we want to have something automatic. So we want to have the relayer prove what happened directly when updating the root. So we don't need the feature of merkle trees that someone can come after 20,000 deposits and prove this, leave this deposit was included. We don't need that, right? Because we do it on the go. So we don't actually need a tree. So what could we do? We can just chain it together.
04:04:08.984 - 04:04:54.434, Speaker A: So it's no rocket science here, chaining it together will just keep track of the whole history with having lower complexity, we don't need to claim. And a downside though is there's not really a proven record for that, but let's go for it. So here's the ordered binary hash chain that we use. So on the bottom right, maybe I can point here. You see this is a deposit. So that represents deposit data. So let's say I want to bridge something, I send my funds to the bridge contract.
04:04:54.434 - 04:06:12.034, Speaker A: This will be the data, then it will be hashed, and then it will be connected to the previous route that is stored on chain and then hashed again and will be the new root. So it goes just up like this instead of a tree form. So let's go into challenges that we discovered when implementing this. So on a high level, of course, we have different smart contract languages. For example, the contract for Neo X was written in solidity, but we can't write the neo n three contract in solidity, so we need to write that one in Java, for example. But it has essentially the same needs, the same functionality on a business perspective. Right? Then standards, as of now with gas, we don't really have the problem yet because gas will be the native token, right? You can consider gas on new x, like ether on ethereum, right? But neo n three doesn't have erc 20.
04:06:12.034 - 04:06:56.434, Speaker A: Neo n three has another standard for fungible tokens. So you would need to include if you were to support fungible tokens, for example, you will need to map that. And there are other differences. For example, catch arc 256 is not supported on m three, so you can't use that hash function. So for example here we used sha 256. Then on a lower level you have big endian versus little endian. From evm's side, you probably don't even know about that if you're not working on a low level because everything is big endian on the evM.
04:06:56.434 - 04:07:35.784, Speaker A: However, on neovm everything is little endian. So it's reversed. The bytes are reversed and then evm has a fixed size data structure on vm level while neo has dynamic structure. So on evm, for example, you have un eight, un 16, un 64 and so on. On neo you have just int and the data, the bytes will be as long as they need be. So you don't have the limit of un 256 or whatever. So let's go into a quick detail about this.
04:07:35.784 - 04:08:09.446, Speaker A: So what happens if I deposit on n three? We have nonce here. Then I want to bridge this amount, of course with the decimals. And this is the recipient address. So what happens on chain now in the contract is this is in hex, it's zero one, that's the hex for this amount. And then the address remains the same. Then we concatenate this data and then we hash it and we get this hash. Eight, b e four, etcetera.
04:08:09.446 - 04:08:51.488, Speaker A: Right? Now, when the relayer reproduces this on the destination chain, on Neo X, he will provide this data here and then the hash will be recreated to check whether that data is correct. However, on Neo X, the data in bytes looks like this, lots of zeros. So the message will be different. So the hash will be different. So if you run easy, recover on that, it will return another address and not the one that actually signed it. Right. So we need to fix that.
04:08:51.488 - 04:09:40.458, Speaker A: So what we can do, since Neo VM is flexible, we can adapt to that and we pad the bytes to the same length that EVM requires. So in order to get to the same message and get ultimately to the same hash, then we can recover the same signer again. One thing that's not on the slide is what I said before, big endian, little endian. These bytes here, actually there's another step in between. They're in little endian on the Neovm. So this is actually in reverse on NEo vm. So on NEoVM, you need to reverse and then pad concatenate and you have the same message.
04:09:40.458 - 04:10:09.320, Speaker A: So you need to consider a lot of things on a really low level. So yeah, that's it. We are live currently on Testnet t two. We're developing as I speak, t three. We'll release that soon. You can scan here to get to know more about new x. Some future things we want to include is that the relayer could be open.
04:10:09.320 - 04:10:56.602, Speaker A: Currently it's just restricted to one central entity. But as you know, we don't want a single point of failure, and we can also bring more validators to centralize it more. So just quick takeaways here. Non EVM ecosystems, we can't just copy, so we can just fork an existing solution and deploy it. And if you want to do this, if you want to create a bridge from non EVM to EVM, you get really good understanding of both vms to achieve that. Thanks for listening. And if there's any question, we have time for one short question.
04:10:56.602 - 04:11:26.926, Speaker A: If there's one question, you can also ask me later if you want. I'll be here. There's one. I'm sorry. So is this a nonprofit or a for profit project so far? Well, from our perspective, we're getting paid from the neo foundation, so it's not a for profit. Okay. Okay.
04:11:26.926 - 04:11:51.634, Speaker A: Thank you. That was it already. Thank you so much, Michael. I guess if anyone else has questions, you will still be around later on a little bit. Please stick around. Our next speaker will be getting ready in approximately two, three minutes. So thank you very much.
04:11:51.634 - 04:15:03.714, Speaker A: Yeah, so now it works. It it. 35 people. 35 people. That's a lot. This big audience, it doesn't look a lot, but it.
04:15:43.594 - 04:16:13.164, Speaker B: Hello everyone. Welcome to another session of our great event. So the next speech will be from Akaki Mama Geshvili, who is a research scientist at of Jane Labs and who was previously a senior researcher at ETH Zurich, where he also obtained his PhD in theoretical computer science. And his paper will be shared sequencing and latency competition as a noisy contest. Enjoy.
04:16:13.544 - 04:16:48.788, Speaker A: Thank you. Thank you for the introduction. Welcome everyone, and thank you for joining this presentation. So I will talk about shared sequencing economics, and we will look at this from the latency competition perspective, which we model as a noisy contest. This is a joint paper together with Christoph Schlegel, who is researcher at flashbots. First, let me give you some motivation. We all know that Ethereum is very good with decentralization and security, but it has problems with scaling.
04:16:48.788 - 04:17:43.752, Speaker A: And Ethereum decided to go with the paths of rollups, and it created this roll up centric roadmap. And this resulted into, at the moment, at least 50 functioning roll ups and maybe more than hundreds of announcements of roll ups that are under development. So far, so good. We can assume that Ethereum is really scaling at least, let's say, 50 times. If we assume that all roll ups have the same parameters as Ethereum chain does, and most of them do, because they are implementing evms, ethereum virtual machines, and even trying to optimize, for example, arbitrum. That is the roll up that was developed by my company, off chain labs. But it comes at a cost, and the cost is this fragmented liquidity.
04:17:43.752 - 04:18:46.176, Speaker A: So instead of all liquidity being on one chain and move on that chain freely, now you need to move them from one roll up to another roll up. And there are of course no direct channels to move money. Probably first you need to withdraw from a roll up to Ethereum and then move to another roll up. So this creates this fragmented liquidity. And you can also see it on the, for example, defi Lama. You can see that it's still the case that Ethereum has most of the liquidity, but already some roll ups have some significant amount of liquidity, at least few of them. But we have arbitrageurs that are trying to like adjust the prices of different assets in the exchanges, decentralized exchanges, and they are helping the market to be efficient, so to have equal prices on different roll ups and different domains.
04:18:46.176 - 04:19:24.122, Speaker A: But of course, it comes at a cost to arbitrageurs. Also, as I said, coordinating these arbitrage activities are quite difficult. You cannot withdraw money from one roll up and move it to another roll up through Ethereum because that's too slow. Someone else maybe already has funds on another roll up, and that arbitrage will be faster. So to solve this problem, at least partially, shared sequencing solution was proposed. And there are at least several projects that I'm aware that are solely focused on shear sequencing. These are espresso and Astrid.
04:19:24.122 - 04:20:20.154, Speaker A: And I would say that even swab, which is a chain or solution that's proposed by flashbots, can also facilitate some shared sequencing. So I will talk about what are the shared sequencers. So let's, for the simplicity for this presentation, look at two domains, for example, two roll ups. This is the simplest case, but it can be also one roll up and Ethereum main chain. And actually all the shared sequencing solutions that I have seen so far, they are offering the same transaction ordering policy, namely creating blocks from the manpool of transactions that is already live on Ethereum. So they don't go with first come, first serve solutions. So far, there are some promises given in that direction as well.
04:20:20.154 - 04:21:23.264, Speaker A: Okay, so now we are assuming that shared sequencer can sequence transactions on both domains, both chains. Okay, that makes life of arbitrageure simpler, because now it can submit one bundle of transactions and hope that they will be executed atomically on both chains. Actually, those solutions that offer shared sequencing, they also make promise that they can execute atomically so both of transactions will be executed condition that other transaction on another chain will be executed in the order that is defined by the transaction sender. But for the sake of this paper, we look only at the sequencing. So transaction is sequenced and the guarantees that it will be executed. But shared sequencer may not be able to give this guarantee. Okay, then we are analyzing what does it mean for cross domain arbitrage trading.
04:21:23.264 - 04:22:16.124, Speaker A: Okay, so we look at the arbitrage trading as a contest where if the transaction ordering is designed by shared sequencing, for example, if it's first come, first serve on both chains or one chain, then competition is who is fastest. So competition is about the latency. But if we have Ethereum style priority gas auctions, then traders are incentivized or they are competing on higher bids. So whoever has higher bid is included fastest. And we only look at this kind of competition. So you cannot. So we are not looking into cases where you are sandwich attacking some transaction, but we only look at, let's say back running competition where you just want to be fastest.
04:22:16.124 - 04:23:16.846, Speaker A: Once price changes, either outside market or inside the chain exchange, you want to be fastest to take this advantage and background the previous transactions. And this way we can see shared sequencer as a tool to reduce noise. So noise is just randomness, because if you try to be fastest on two chains, chances that you will fail on one of them is higher than if you fail on one of them, only on one of them. Okay, then we are interested what happens with the investment, how it changes the strategies of arbitrageurs. And we develop a very simple model that captures shared sequencing versus separate sequences. So we should be able to compare these two solution concepts at least along some dimensions. And we managed to do it at least for two dimensions.
04:23:16.846 - 04:23:59.686, Speaker A: So one is quite trivial. As I said, if you are competing on two chains separately, the chances are that both players or all players lose because you win some and other players win some others. So it can be that all the players are failing. So nobody wins all the competitions, all the contests, all the races. But in case of shared sequencer, it is guaranteed that someone wins. Okay, so at least user experience in this regard is better with shared sequencing. But now we are also interested in the revenue that chains extract.
04:23:59.686 - 04:24:59.436, Speaker A: Because chains need to join the shared sequencer, they need to allow shared sequencer to run a sequencing rule on them and therefore they need to get some revenue from that. Okay, so we have very simple model where we have again two players, two domains, and they are trying to be fastest on both domains, both chains and in case of winning they are getting, let's say the same value. Of course this is not always true, but for the sake of this presentation and this paper, this is the simplest model and it already shows some difference between shared SQL sequencing and separate sequencing. So I think this is fair enough. And again, they need to win both competitions. Okay, so what do arbitrageurs do? They invest some amount to create their signal. And signal will be defined in some different contexts.
04:24:59.436 - 04:26:02.392, Speaker A: And of course if you want your signal to be better, for example, if you want to be faster, you need to invest more. So we have some cost function that is increasing differentiable that we need for deriving analytic results and convex. This is quite intuitive condition to assume, but there is also random noise. So for example, in case of pure time competition, when you send your transaction to a sequencer, you are not sure how fast it reaches and there are some random factors that may delay it a bit. Okay, so the arbitrageure I wins the race against arbitrageure j. If the signal plus this noise term is larger than signal plus noise term for the other player. Okay, and if we have a distribution on the difference of this noises, then we can calculate the probability that one arbitrager wins the race.
04:26:02.392 - 04:27:01.204, Speaker A: So this just probability is given, we assume that there is this distribution, then we assume it's normal. Again, we want to have analytic solutions, but we can have arbitrary distribution and give results for that arbitrary distribution as well. Ok, so now some examples. First is first come, first serve, where your signal is how fast you are and more you invest faster you are. So that defines basically your timestamp. But there is also of course noise parameter, as I said, that you are never sure that you will be reaching the sequencer in exactly the time you have calculated. So another example is Timeboost, which is a recent proposal by including myself and Christoph and co authors from off chain Labs and Cornell University at Felton and Mahimna Calcar.
04:27:01.204 - 04:28:09.998, Speaker A: So where any user, in our case arbitrageur invests, so bids to buy a time boost which will be subtracted from the timestamp. And timestamp is some random noise, let's say in case of Ethereum type block building, you can imagine that s is how much users invest in searching and epsilon. So the noise parameter is some random factors in the market, for example if they manage to reach the builder, etcetera. But I want to make a disclaimer, we are not really analyzing this last part very carefully in our model. Okay, so now what is the difference with sheared sequencer, there is just one sequencer. So you aim this sequencer, you have one noise term, and you have one signal that you need to generate. For example, you want to relocate close to this sequencer trace where the sequencer is located at different times.
04:28:09.998 - 04:28:56.254, Speaker A: We separate sequencers, you need to target both sequencers, be closer to both sequencers. And there are two noise terms, of course, okay. But of course, in the equilibrium, and that's what we are looking into, you may invest slightly less when you have two different separate sequencers. We assume that noises are independently distributed. So sequencers, for example, are not sitting together. And they so separate sequencers, they move around independently from each other, also not always correct. But this allows us that instead of distribution function f, we can just take f squared, and that simplifies the analysis considerably.
04:28:56.254 - 04:29:48.878, Speaker A: Okay, so we have unique symmetric equilibrium of the game and equilibrium. In the equilibrium, we can find signal. So this s prime with a very simple equation. So it's basically first order condition for both separate sequencing sequencers and shared sequencers, or one sequencer. So as simple as that, we can just solve what is the equilibrium signal? Okay, so now the question is, what does it mean? Does shared sequencer bring more revenue? Because that's the first intuition of everyone that now, okay, arbitrageurs really love just one sequencer, so they will invest more to win, because at least one of them is winning. So don't forget that. And we show that this is not always true.
04:29:48.878 - 04:30:45.308, Speaker A: And it really depends on the shape of this cost function and what is the variance of the noise. So how different the noise can be. So we look at constant elasticity cost functions. So cost is just x to the power beta. And we see that if the value from the winning arbitrage opportunity is large enough, we have this number to be the investment with one sequencer. So shared sequencer and this number. So I will not read what are these values? But what we obtain is that if beta is larger than one, and for convexity, we need beta to be larger, the investment in case of one sequencer is always higher than investment with two.
04:30:45.308 - 04:31:27.024, Speaker A: And now if you go back and consider that this is just latency competition, it means that players will waste more, because that's a waste. It just goes to outsiders not to chain. So waste is more with shared sequencer. Okay, so in case of latency competition, this is a bad news. Okay, in case of time boost, I don't want to, I don't have time to go very slowly here, but. So you are generating some signal which is you are paying for time advantage. So more you pay, more time advantage you get.
04:31:27.024 - 04:32:17.758, Speaker A: And the time advantage is subtracted from your time step. So your adjusted time step will be your real time step minus some signal s, and you can get at most g advantage. So we are thinking to get, for example, at most half second advantage. And this is the cost function. And we obtain a proposition that revenue that is obtained through this time boost mechanism is higher for separate sequencers as long as this condition holds. So we can look, so do some comparative statics on these parameters. So, you know what is v is value, sigma is variance.
04:32:17.758 - 04:32:41.850, Speaker A: Basically, g is the maximum advantage you can buy. C is how much it costs to buy. So this is this parameter. And this constant is something related with two. So, two is number of roll ups or number of domains, and square root of two PI. This comes from normal distribution. So this is not just some random constant here it has some meaning.
04:32:41.850 - 04:33:18.114, Speaker A: So we can go through what does it mean. So if for example, valuation is large enough for the arbitrage, it's actually better to have two separate sequences. You don't need to have shared sequencer because separate sequencers will obtain higher revenue. So also quite unintuitive result. And also, then it depends if sigma is small, so there is small noise. In some cases, you can assume that noise is small. So you know how fast you reach the sequencer.
04:33:18.114 - 04:33:41.746, Speaker A: And there is no random, much randomness there. Then you again have that two separate sequencers is actually better. And that makes sense because if there was no noise, there would not be need of shared sequencer in the first place. Let's see. Okay, so that's all from my side. Thank you. If you have questions, please ask me now.
04:33:41.746 - 04:33:45.434, Speaker A: I think there's some time or I'll be around here. Thank you.
04:33:57.974 - 04:34:06.234, Speaker B: We still have a couple of minutes. You can ask one or two questions, actually, if you have any.
04:34:22.673 - 04:35:36.164, Speaker A: Hi, just wanted to know, do you see also or do you have a feeling or an intuition on how the results generalize to even more roll ups, like three or four? Oh, that's a good question. So, if you assume independence again, so that you can still solve the equilibrium condition, so it will be f cubed instead of squared in case of three roll ups. So, one thing about more roll ups is, I don't believe it will be often the case that you trade on three different roll ups at the same time. Because if you adjust the price between two roll ups, or centralized exchange and roll up, or layer one exchange and some roll ups, there is no need to do this cyclic adjustment, but it can happen that sometimes arbitrages are careless and leave some opportunity behind. I think all the insights will carry over. It can still be that shared sequencer for very large valuation. Arbitrage still collects less.
04:35:36.164 - 04:36:47.222, Speaker A: So we didn't do this for many roll ups in the paper, we did it for many players and there it really generalized easily and I believe for roll ups will be the case too. So more roll ups. Thank you. Very good presentation. Akaki. I have a question if there are other applications of a shared sequencer than crossrolab arbitrage? And my second question is, I mean will the cross roll up arbitrage be used more often than the arbitrage between rollups and the Ethereum mainnet? Yeah. So about second question, I believe most of the arbitrage is between centralized exchange and roll up, except for some weird tokens that are only sold on rollups or the layer one.
04:36:47.222 - 04:37:24.184, Speaker A: But there is let's say some 20% or 50% 15% of the arbitrage, which is also significant number about other uses of shared sequencing I'm pretty sure there are other uses as well. But main demand comes from arbitrageurs and these MeV searchers. But I can imagine other uses. Actually that's a good question. We should ask those companies that create such services. But I can imagine like regular retail users also using sometimes this option. That's a good question.
04:37:28.764 - 04:37:32.544, Speaker B: If there is no any other question, we thank you.
04:37:46.264 - 04:40:41.134, Speaker A: Just like you, we can try. Oh cool. Oh whatever. I use the keyboard. Yeah, yeah that's fine. Am I supposed to just search? Is there like the streaming is ending here?
04:40:53.674 - 04:41:25.274, Speaker B: Our next presentation will be on threshold cryptography for multi chain depth, which will be presented by Bjorn Trekman. Bjorn is a cryptographer and head of researcher research at Dfinity foundation. He obtained his PhD at the ETH Zurich on cryptography. Following that he worked as a researcher at University of California San Diego and IBM research until he joined Dfinity in 2019. Ladies and gentlemen, Bjorn Trachman.
04:41:28.454 - 04:41:59.674, Speaker A: Thank you very much for the introduction. So you may have seen that threshold cryptography actually actually becomes a lot more prominent in the entire blockchain space. So there have been early adopters like Thor chain or ICP was also pretty early. And now recently more and more projects seem to join that train. So XLR is a recent one. Near has just announced that they'll also be part of the threshold crew. So as a cryptographer, I thought what's the best I can do for you? And I thought I'd give you some overview of threshold.
04:41:59.674 - 04:42:27.936, Speaker A: And so I set myself two goals. The first one is for all the non cryptographers. I want to give you one or two of these beautiful aha. Moments where something really becomes clear. And that goal number two, without any potential cryptographers in the audience later beating me up for oversimplifying the beautiful things. So what is threshold cryptography? I mean, on the high level it's very simple. It's like this.
04:42:27.936 - 04:43:23.574, Speaker A: We have a group of nodes, such as the four nodes there on the left, and they together want to perform some cryptographic computation and they want to do this under two conditions. So the first one is for some threshold t, which four nodes could be three, for example, any three nodes together should be able to actually do that computation. But any less than three, or generally t nodes should not be able to do that. In particular, they should not learn anything useful for them. And so when I was a student in cryptography, I found this really fascinating. Like how do you get this property that for t minus one, you know, nothing, the nodes cannot do anything with the material they have, and yet you just add one additional node, it doesn't matter which one anyone, and they suddenly can do everything. So how is that possible? And the answer is, of course, math.
04:43:23.574 - 04:44:23.020, Speaker A: In this case it's polynomials, because polynomials that was detected by chamier, or the application was detected by Chamier in the seventies or eighties, it's pretty old. Polynomials have this beautiful property where, I mean, I've shown a degree three polynomial there. And that has the property that if you know three points, three of the gray points, say on that polynomial, you have no information at all about the pink point. The pink point could be anywhere on the y axis in that case. But if you know any additional point, any additional fourth point, you know the entire polynomial, you can use Lagrange interpolation to compute every single point of that polynomial. So this gives us this very hard gap between what happens at, in this case D and D one. And so of course we're going to use this to put the private key that we were using, cryptography.
04:44:23.020 - 04:44:51.674, Speaker A: We'll put that at the pink dot there. So it's convention that one puts it at the axis, but you could actually put it anywhere. It doesn't matter, just a convention. And. Yeah. So how do we go forward if we want to do threshold computation? I'll now specialize a bit to public key cryptography, in particular, digital signatures. We have a private key space and we have a public key space, and the two are usually mathematically very different.
04:44:51.674 - 04:45:48.382, Speaker A: So in the elliptic curve schemes that are widely used today, the private key space is just numbers and the public key space is points on the elliptic curve. And so there's functions between the two and it's easy to go from a number to a point on the elliptic curve. It's practically infeasible to revert it. Okay, so what we're going to do with the threshold scheme is I've now replaced the gray by three different colors here, so it's easier to distinguish. What we're going to do is we give three parties these three colored dots, and together they kind of know the pink dot on the right, the key where we want to go to. And so now we can ask the question, how do we compute the public key? And if we have the pink one, that's easier, right? We can just do the standard computation from the number to the elliptic curve and we have the key. And of course we also have this lagrange interpolation.
04:45:48.382 - 04:46:19.834, Speaker A: So if we have the key shares of all the parties, we can interpolate and get the private key and from there we can go to the public key. Unfortunately, that's not really useful because that means we have to compute that interpolation in some way, right? Someone has to compute it and they will know the pink dot. And once they know the pink dot, they know the private key. And so that's bad. No single node should ever know the private key. So this is not something we can do. So we are not going to use Lagrange here.
04:46:19.834 - 04:47:12.806, Speaker A: We're not going to compute this thing, and in particular, we're not going to compute anything from it. Instead, what we realize is that the colored dots are mathematically very similar to this pink dot, right? They're the same thing. They're also just a number. And so we can interpret them as private keys and we can compute private keys from these public, sorry, public keys from these private keys. So every node that has such a key share can compute the corresponding private key corresponding to its key share. And since we have a nice homomorphism property, which is mathematical speech for it preserves structure, we can also do the Lagrangian interpolation on the level of public keys. And so this is the idea, this is the core idea of most threshold cryptography protocols.
04:47:12.806 - 04:47:54.036, Speaker A: We share the private key, but whenever we need to compute something, we go to a different space and we compute in that space. So here for computing the public keys, and if we think about threshold signatures, we basically do the same thing for the signature. So we can just replace all the public key stuff by signatures. It's easy to compute the signature from the private key, it's hard in the other direction. And every party can compute using the private key a corresponding signature share. And again, there is such a homomorphism property so that these parties can use the same type of interpolation to compute the signature. Now this has to be taken with a grain of salt.
04:47:54.036 - 04:48:36.284, Speaker A: So there is some cryptographic schemes such as BLS, which if you know the proof of stake Ethereum, you probably have some had some exposure to. BLS is super nice in all the properties. And so for BLS this diagram works exactly, and you can think about it in exactly that way. If you have somewhat less nice signature schemes such as schnor, EDDSA, ECDSA in particular, it still kind of works in an intuitive sense. But the protocols are a lot more complicated than I can show here. Now, okay, now we have the threshold signature scheme, and it's exactly that. If we have sufficiently many nodes we can sign, otherwise we cannot.
04:48:36.284 - 04:49:25.654, Speaker A: Now you may ask, wait wait wait, isn't that a multi signature? If sufficiently many can sign, then it's going to be accepted, and not quite. And I think there's a nice duality between the two that it makes sense to explore, and that's as follows. So for threshold signature, every party, as I've shown before, has some kind of a share of a key. So you can think about it as every party owning some part of the key. And if enough of those parties come together, they kind of know the entire key and they can proceed. So it's pretty complex on the sender side, but it's very easy on the verifier site. So for the verifier this looks and smells and feels like a standard signature, and the verifier just has to run standard signature validation in order to, to validate this threshold signature.
04:49:25.654 - 04:50:16.728, Speaker A: And multi signatures do a very different trade off in multi signatures. Every node on the left hand side has their own independent key. They just generate it and they sign as if they were just providing a single signature. And the work in that case is very much on the verifier side. The verifier now has to know all the public keys, not only one, it has to validate all those signatures, and it has then to decide whether the signatures it saw comply with the policy it's supposed to implement. And so I've tried to make it to exemplify it a bit in this table. So in a threshold signature we have the same verification as in an existing signature scheme, you can use exactly the same software where it's a multi signature, the verification is tailored.
04:50:16.728 - 04:50:54.030, Speaker A: You need to implement multiple signature verification and a policy. In threshold signatures, the policy is on the signer side because the verifier doesn't know about it and multisignatures are obviously on the verifier side. In threshold signatures, the group of signers could change and the verifier wouldn't notice. In multisignatures, if you want to change the group of signers, you have to actually change things on the verifier side. So make a transaction, for example, where it's on threshold, that's not necessary. On the other hand, for threshold signatures it's pretty much non accountable. Like all you know is, oh, I have a valid signature.
04:50:54.030 - 04:51:35.134, Speaker A: Enough nodes have signed, but you don't know which ones. In a multi signature you always know which nodes have signed. So the two are kind of related, but they are useful for somewhat different scenarios. Okay, so what can we do with them? Wait, not so fast. The problem with this is, especially as a young cryptography researcher, you think like this, right? So there's threshold signatures, and once you solve your threshold signature, all you need to do is you need to bring them to practice. And the reality is more like this. Sure you have your threshold signature, but now is where the hard work actually starts.
04:51:35.134 - 04:52:38.220, Speaker A: And I just want to give you some examples. So one is how do you actually choose the set of nodes? Like who do you give the key to? The second one is how do you actually give them the key? I mean no one is supposed to know the key, so how do you actually come up with that key? A third one is and what if one of those nodes dies and takes its key share with it? Like how do we recover? Right? And so I want to give you some hints here. So choosing the nodes, I mean here kind of the problem is that every node is the same, every node has one share, and that works very well for some cases where, for example, your consensus. In ICP that's easy. The consensus is every node has one vote and all the nodes are equal. So in ICP we could just implement threshold signatures in the consensus or on top of the consensus mechanism that we've been using. But for other consensus mechanisms such as ethereum, that wouldn't be so easy, right? Because you have very different voting rights depending on the stake.
04:52:38.220 - 04:53:14.924, Speaker A: And for threshold signatures, that just doesn't work. Of course you can make many shares and distribute them somewhat unequally, but that's an efficiency catastrophe. So that's not what you can do in practice either. And so, for example, what NiR seems to be doing based on their documentation in their recent implementation, is they have their blockchain and then they have an MPC class A permissioned MPC cluster sitting next to the blockchain. So they completely separate it. It's just something. I mean, there are solutions, but it's very important to think about what your solution then actually also means for the security of the system.
04:53:14.924 - 04:53:50.332, Speaker A: Now for the next part, how do we generate the key? And the nice thing is there's distributed protocols that are called distributed key generation that do exactly this. And they were working, this started in the nineties. So today there's efficiency improvements, but they still base on the same foundations. And the idea is as follows. On the right hand side, we have the receivers. Those are the ones that are supposed to have the key after we run this generation protocol. And the dealers on top, they're helpers, they're just there in order to facilitate the protocol and the protocol itself.
04:53:50.332 - 04:54:33.034, Speaker A: The core structure is, again, very easy. So what each of the dealers does is it just chooses a random key. So the one on the right, on the left hand side, it chooses this purple dot and creates a sharing with the colored dots. And of course that dealer knows the purple dot, right? That's how it works. But then we have more dealers, the one that does the squares and the one that does the triangles and so forth. And so what we're going to do is we're going to collect many of those dealings and every node will then basically sum up all of its colored shares it receives. And so overall, what we're going to end up with is we need three things.
04:54:33.034 - 04:55:09.442, Speaker A: The first one is we need sufficiently many honest dealers. Actually, if there's one honest dealer, that's sufficient to provide enough entropy for everyone. Okay? So we just need to make sure that we have at least one honest dealer that's going to produce a correct dealing and distribute it to everyone. And then even if all the others are just honest, as long as the honest one deletes the key, it's fine. So you probably want to have more than one honest. But anyway, the second property we need is we need the individual dealings to be correct. Now, this is complicated, so I'll not go into this, this into detail.
04:55:09.442 - 04:55:40.874, Speaker A: There's interactive protocols with the receivers complaining there's zero knowledge, proof based things. This is really where a lot of the work recently is also happening. And the third one we need to make sure is that we have agreement on which set of dealings we use. Now that's trivial. If blockchain can do one thing, it's agree on something. So that is kind of easy. Now what's nice here is now the receivers on the right hand side, they actually have a key that no one can know.
04:55:40.874 - 04:56:39.996, Speaker A: So the sum of all those keys is actually something that's unknown. Another nice property is that we can slightly tweak the protocol and use the receivers as dealers and they can reshare their keys. So instead of creating a new fully random key, they take the key they already have and create a sharing out of that. And with a slight adaptation of the math and the protocol, this allows us then to have one group of receivers that own the key, that act as dealers and transfer the key to another new group of receivers. So this is how one can deal with the problem of having nodes that die. You can replace them and then just use this protocol again to provide a new sharing. Okay, so that's what you need to add to all the threshold signatures in order to make them usable in practice.
04:56:39.996 - 04:57:20.576, Speaker A: Can we use them now? Yes, we can. And just to give you an idea, so this is my very easy description of the ICP architecture. Threshold keys are used in ICP everywhere. In particular we have subnets which you may also know as shards. And each of those has its own threshold key. Now the advantage of this is it makes interaction between them extremely efficient because all that they have to know about each other is a few hundred bytes of public key and then they get the signed messages and they can just verify and all is good. So if we have smart contracts on one chart communicating to another shard, it's basically a threshold signature.
04:57:20.576 - 04:57:48.878, Speaker A: Verifying a threshold signature, done. There's no need for any node to replicate state and so on and so forth. It also makes it super easy for clients to validate information they receive because they can just validate the threshold signature. There's no need for clients to validate anything about consensus. It's all about threshold signatures. Now what one can also do with those things is actually implement multi chain depth. So that's where the title of the presentation came from.
04:57:48.878 - 04:58:29.718, Speaker A: So you can create a signature on one blockchain network and then sign a transaction that's going to be executed by another blockchain network. So I've taken ICP as the sender side here. So ICP would generate an ECDSA signature for the transaction, pass that to Ethereum and would just be a standard smart contract call from an external account address from the perspective of Ethereum area. And you can then use this to build nice kinds of multi chain application. My simplest example here is just a timer application. So smart contracts on ISP can set a timer. They can say wake me up at 05:00 p.m.,
04:58:29.718 - 04:59:30.928, Speaker A: today, which EVM contracts cannot do, right? And so you can use this in order to implement things like limit trading or scheduled treasury management in an EVM contract in a fully decentralized way, because you just have a smart contract on ICP that would call out to this Ethereum smart contract. Of course you can do more like the ICP contract has the capability to interact with web two services. It has access to the bitcoin Utxo set. It can also, for example, read events from EVM chains. And it can all do all this and provide the data to the Ethereum smart contract. So you can basically implement a full oracle just in two smart contracts interacting with each other. Or this one is one I find particularly nice is ICP smart contracts have a pretty high capacity in terms of memory and compute they have.
04:59:30.928 - 05:00:17.440, Speaker A: So you can build daos entirely on chain. The entire voting can be fully unchained. But you may want to use this in a way to control assets or smart contracts on Ethereum. But for this you may want to actually take data from Ethereum. So a colleague of mine actually has a demo on this at 330 at a neighbor stage, showing how exactly this here works, where a governance contract running on the inner computer fetches the voting power data from an ERC 20 contract and then executes a transaction on the Ethereum blockchain. And. Yeah, so that's all I wanted to go through today.
05:00:17.440 - 05:01:08.646, Speaker A: If you're interested in this kind of thing. Colleagues of mine will give several demos and tutorials on this. So Thursday, sorry, Sunday morning is another one, so you'll find them across the schedule if you want to have a look at those. That's going to be the same in much more detail on the technical side. Thanks again. I was wondering, is there some signature scheme that is like some kind of a mutant scheme between a multi signature scheme and threshold signatures? What I'm trying to get is the property with threshold signatures. Normally you just observe the signature and you have no view on who actually produced and partially signed the data.
05:01:08.646 - 05:01:46.362, Speaker A: Is there some kind of a signature scheme where you can do that, where the signatures are actually different? The resulting signature is still valid, but is different based on who signed it? So the answer is yes and no. So there is research work on this by Dan Monet from Stanford University. And that explores exactly the space between the two. So I think it's a very interesting research direction. The problem with this is it does not work for existing schemes. So whenever you need to support something that is existing, you will run into the problem of having this non accountability. But in principle it's totally possible.
05:01:46.362 - 05:02:03.454, Speaker A: Just not with the currently standardized schemes. But it's not implemented yet. Any like with any. Not that I'm aware of. No, I think it's right now it's really at the research stage, but I think it's a very promising direction for exactly that reason. Yeah. For some application that would be super helpful.
05:02:03.454 - 05:02:20.734, Speaker A: So you know. And you can punish or reward the nodes that actually participated. Absolutely. I mean we've been implementing this and we've. We would have loved to have full accountability for the threshold signatures. Problem is the current ones don't have it. Thank you.
05:02:25.274 - 05:02:35.534, Speaker B: Any other questions? We have three more minutes, but we will have a coffee break actually starting now for 30 minutes. Also, feel free to approach Bjorn.
05:02:35.954 - 05:23:36.244, Speaker A: Yep, feel free to find me anytime. I'll be upstairs. So thank you. There. It. It? It? It. It.
05:23:36.244 - 05:37:41.704, Speaker A: It. It? It? It? It? It? It? It? It?
05:39:09.544 - 05:39:25.076, Speaker B: I talked to the organizer that they are sending the volunteers all around the people to inform that they are starting. But if you want to start a bit slowly we can.
05:39:25.140 - 06:00:56.824, Speaker A: But maybe we wait another minute. Okay. I think everything is in order. So. Yeah. Please welcome Magdalena to the stage for a talk. Yeah.
06:01:00.404 - 06:01:20.758, Speaker B: And there's a video. Okay, I'll just check. All right. Thanks for. For joining. Today's presentation is about zero gas infinite scale, the bull case for app chains and how to build them. That's me.
06:01:20.758 - 06:01:56.700, Speaker B: Just a different hair color, but yeah, I'm doing commercials at gelato, a bit of legal and finance. So before we start this presentation, let's look at how did web two scale. So web two wasn't as scalable as it is today. Always. And let's take a short trip down memory lane. So web two started with local hosting. So if you had a website, you were running it, most likely somewhere down in your basement, you had your server there, but that obviously had a lot of disadvantages.
06:01:56.700 - 06:02:39.640, Speaker B: So it's neither really scalable. If you had electricity outages, your website could be down. And if you had users from the other side of the world, they could suffer from really slow loading times. So eventually from local hosting we went to shared hosting. In the mid nineties there were providers like geocities or Dreamhost. I don't remember any of them? Maybe you do. But what they allowed you is they provided website hosting for you and that was already a great step, right, but it still had a lot of issues because mostly you shared all the resources and all the bandwidth with, with all the other websites on that server, which led to constraint scalability.
06:02:39.640 - 06:03:24.554, Speaker B: And if one website had a lot of traffic, your website could suffer, even though it's completely unrelated to the original website. And it's also very inflexible and not really customizable because usually you are bound to specific programming languages. So from there, mid two thousands aws, Google cloud, there are also others, you most likely know them. We went to cloud virtual machines. So they're not physical anymore, they're virtual. And that actually led to horizontal scaling. So what you could do now is you can spin up instances, you can scale them up and down as you need them, it's very customizable, you can build very powerful applications on cloud.
06:03:24.554 - 06:04:20.422, Speaker B: So what's the takeaway from this? The Internet is not one huge world computer, but it's actually a web of interconnected computers. And what scaled web two were actually asynchronous services. So if you were deploying servers, instances and so on, you needed for them to communicate to each other and also to the rest of the Internet. And for that you used asynchronous communication tools like APIs or HTTP. So when looking at the evolution of web three, we can actually draw some parallels and we can actually see that asynchronous cloud services are also scaling web three. We went from a few monolithic chains to an infinite number of virtual chains in the cloud. So similar slide, let's try to draw some parallels to what we've just seen, seen with web two scaling.
06:04:20.422 - 06:05:49.104, Speaker B: So with the emergence of bitcoin, we had those special purpose layer one validator networks, and that's great, right? But if you wanted to launch a new use case you needed a specialized blockchain and that always required a bootstraping, a validator set, which usually required a token because you need to incentivize those validators to join your network, which is costly, not that scalable, and usually also leads to security concerns like 51% attacks and so on. So from there we went to general purpose smart contract platforms, Ethereum, and that's already great, right? Because now you can have a lot of use cases, a lot of applications on the same smart contract platform, but still, and that's very similar to shared hosting, you still share all the resources with all the other applications on that network. So if you have an NFT drop or some meme coin or whatever, you can have your gas costs spike and the network is just congested. So not scalable can be expensive and you're restricted to EBM languages. So what's the next step here? The next step is virtual blockchains on roll up as a service. What's a virtual blockchain? A virtual blockchain is a roll up. What's a roll up? It's a blockchain within a blockchain, and that blockchain is hosted off chain by a centralized sequence like Gelato.
06:05:49.104 - 06:06:23.632, Speaker B: So what is Gelato actually doing? Gelato allows you to deploy and maintain your own roll up and actually have a scalable environment where you don't share any resources with anyone else. Very customizable, and you can build very powerful applications on Gelato. Let's maybe start with this one. It's a video. Can I somehow start this? Someone knows. Okay, no one knows. So actually this is a video and it's the gelato platform.
06:06:23.632 - 06:06:54.900, Speaker B: So if you go on the website, we have a dashboard and you can actually configure your own, own roll up on there. But then let's start with this slide. So how does it look? You usually have a base layer, so you need to choose your roll up framework. You can choose optimism, arbitrum, polygon, ck, sync, those are the ones that gelato is currently supporting. You also need to choose your data availability layer. It can be ethereum, but that's costly. So you can also go with alternatives like Celestia, Eigen layer or availability.
06:06:54.900 - 06:07:39.314, Speaker B: And then you have the base infrastructure, but that is not really built for powerful applications. So you need a gelato and also a lot of third party web three service providers. So gelato is not only allowing you to deploy your roll up, but we also have a bunch of web three infrastructure services, smart contract automation, relaying randomness and so on. Then we heard from Simon. You need indexers, so the graph, you need a block explorer like Etherscan, cross chain messaging protocols like layer zero, oracles, a wallet, a bridge and so on. Video not working, unfortunately. So let's go a bit more technical, but still very high level.
06:07:39.314 - 06:08:33.364, Speaker B: What's the like? How does the roll up design actually look like? So you have an execution layer that's your modified version of the EVM, like optimism, arbitrary and polygon. I already said that you have a data availability layer, cheaper options like Celestia, eigenvalue and so on. And then you have your settlement layer, which is Ethereum, that's really important, because the crucial thing here is that your assets still live in smart contracts on Ethereum and you just upload them to the roll up for high execution throughput. But should something ever happen to your roll up, you know, they're still secured in a smart contract on Ethereum. So let's expand a bit from there. That was the roll up design overall. But to actually build an application, you need some essential infrastructure on top of that.
06:08:33.364 - 06:09:26.764, Speaker B: So you need a block explorer to actually, you know, track, visualize all the transactions. And you also need a bridge UI or native bridge to bridge through your roll up. And that is actually what every roll up as a service provider would offer to you. This is the most minimal thing that you have or that you need. And one thing where gelato really shines is that we package up this offering with all the crucial other services that you actually need to build powerful applications. So expanding this even a bit more, we already heard this right now, the functions in Oracles, but a lot of functions, it's similar to AWS Lambda. And what you can, for example, do with Oracles, you can stream price feeds to your roll up to actually build very powerful DeFi applications, like perpdexes for example.
06:09:26.764 - 06:10:23.294, Speaker B: And on the UI side, for example, if you want to build an application that has a great user experience, where you can use, for example, Google sign in and you don't want your users to deal with metamask or setting gas prices and so on, you can use gelato relay, which provides you with an account abstracted way for users to interact with your blockchain. And they don't have to deal with any low level complexities anymore. Then again, indexers, so you can visualize your data. We talked about the rest, I think, I think here. Yeah, one important thing, interoperability is very important because you don't want to have your roll up be very isolated. You want to be able to talk to other chains, to tap into other user bases and so on and so forth. And for that you need liquidity providers and cross chain messaging protocols to actually make that happen.
06:10:23.294 - 06:11:07.986, Speaker B: Yeah, horizontal scaling, a buzzword on there, shared sequencing. So the example I'm doing is a bit flawed, but like imagine you have three different roll ups. Let's assume you're building a game and you have three different roll ups because for whatever reason you want to segment your user bases by country. So you have one in the US, one in Asia, one in Europe, for example, you have three different roll ups for that, and they all talk to each other. But they do it in an asynchronous way. Right. And if you want to have a really great in game user experience, what you want to achieve is that you have those roll ups talk to each other in a synchronous and atomic way.
06:11:07.986 - 06:11:50.250, Speaker B: And for that you would need shared sequencing. What we're doing at Gelato, we're currently researching this topic together with Espresso, who's shared sequencer. But yeah, this is still in research phase and not live yet. And what's the takeaway here? The takeaway here, as with web two, is that Ethereum is not one huge roll computer, but it's a web three of interconnected roll ups, all talking to each other. Build on Gelato. Yeah, if you want to build a roll up, you can do all of that very easily. On our platform, these are projects that are customers of ours.
06:11:50.250 - 06:12:08.454, Speaker B: So we also have infrastructure services. We work together with more than 400 projects. Most of these projects on there are using our infrastructure services from, you know, Pancakeswap, sushi swap, optimism, safe Zora, and so on and so forth.
06:12:09.654 - 06:12:10.062, Speaker A: And.
06:12:10.118 - 06:12:21.074, Speaker B: Yeah, so if you're looking for a job, we're also currently hiring. We have an office in Zouk. You can work remotely. Check out our jobs. Come talk to me. Yeah, that's pretty much it. Thank you.
06:12:28.774 - 06:12:37.534, Speaker A: Hello. Thank you so much. Any questions? Two questions. Have one mic coming that way.
06:12:45.834 - 06:13:03.024, Speaker B: So we have web three infrastructure services and we have rollups. Business model is pretty similar. So for rollups, it's a monthly subscription fee and sequencer revenue. And for middleware services, it's also a subscription fee that you pay.
06:13:05.764 - 06:13:40.282, Speaker A: We have another question. Hi, two questions. First one is really stupid, but let's say, for example, you connect Metamask. Do you have to select gelato as a separate network, or are you going to connect to optimism to get involved in the ecosystem? How is that going to work from a user interface side? And the second question is, I wanted to understand a bit more how you differentiate yourself versus the likes of dimension or old player that obviously have also come up with this kind of similar service.
06:13:40.378 - 06:14:06.904, Speaker B: Yeah, so the first question, no, you would not connect to Gelato as a network. So I'm not sure if you're talking middleware. Roll up. No, I guess roll up. So if we are launching a roll up, and for example, we launched one for rea, for tangible, for Asta, we are launching a roll up for them. And it would be Asta like it would be the roll up you're connecting to, but you would not connect to Gelato would that answer the first part of the question and the second part of the question, can you. That was.
06:14:06.904 - 06:14:45.544, Speaker B: Yeah, I think like what I showed, like this one slide where I showed this is like the most basic design of a roll up. So if you're looking for roll up as a service providers, this is usually what you get, right? That's the bare minimum chain and you get a block explorer on top, but that's pretty much it. And I think where gelato really shines and is different from all other rust providers is that we provide this like comprehensive infrastructure package. We package it all together for you and you get all those services and all the infrastructure deployed from day one on your roll up to really be able to build powerful applications.
06:14:47.284 - 06:14:56.052, Speaker A: Thank you. Any other questions? Good. I think Gelato has one of the best names products because I feel like eating with gelato after your talk. Thank you so much.
06:14:56.108 - 06:14:56.904, Speaker B: You're welcome.
06:15:00.084 - 06:21:35.384, Speaker A: So give us a few seconds to set up the next talk. It. Last year something is that presentation that you have? Yeah, that's my own as well. Yeah, everything basically bit. I want to like say fast things about English, like good experience. It's a shame. Cool space.
06:21:35.384 - 06:24:53.484, Speaker A: Yeah. But you have not come for it. I did not. I will do that. That's what no one really acts upon. Hopefully someone from to get. Thanks so much.
06:24:53.484 - 06:25:46.556, Speaker A: Hello. Hello. Welcome back. So now we have Garrett McDonald partnered at permanent ventures. Sorry about my English, not my main language. He's talking about decentralized storage landscape into 2024, the rise of native vms. So I kind of chat upt your abstract, and I got the keyword from storage protocols, native vms and web three.
06:25:46.556 - 06:25:57.096, Speaker A: So that's what chat GPT thinks about your presentation. Good luck nailing all the buzzwords. It's great. Thanks so much. Yeah. Hi, everybody. Yeah, I'm Garrett.
06:25:57.096 - 06:26:51.504, Speaker A: Yeah, big fan of retro computers and crypto. Shout out to the guy in the back with the Commodore 64 shirt. My first programming language was 6502 assembly, so it's great to see a lot of vms native to decentralized storage networks and protocols that support assembly, web assembly specifically. So, yeah, I thought we'd start by just giving a brief history of decentralized storage. Yeah, there have been a lot of great projects in the past that have come around, and a couple of them have fallen off or pivoted into decentralized compute in general, but there are a few which have stayed since the beginning and a couple others that I've stayed on. Swarm is definitely the oldest one. ETH storage is a great project that was started more recently.
06:26:51.504 - 06:28:07.916, Speaker A: Both of those two, I would highly recommend checking out and diving into. They're great teams working on interesting things, but for the purposes of this talk, I'm just going to focus on Filecoin and Arweave, just because they're the only two which have started to develop native virtual machines, which are going to unlock a lot of new possibilities in decentralized storage as well as compute. So the two kind of offerings put together by the teams of Arweave and Filecoin are AO and FEM. They both have very similar intentions and actually are causing kind of a convergence of arweave and filecoin, which are quite different in their architecture and intentions as protocols. But the native vms allow for each one of them to kind of make up for the sacrifices or the trade offs that it had to make in the original design, just because you can do so much more with or with virtual machines now. So this is really fascinating. And we're going to dive into the ways that they're converging as well as.
06:28:07.916 - 06:28:44.256, Speaker A: Yeah, kind of the. But we'll start by just going over how they are as protocols before the vms. So if you're interested more in like the protocol level comparison, this is a link to a talk I gave last year about that specifically. Yeah, basically just diving into all of those things from a protocol perspective. So just to do a quick recap from that talk. Yeah, these are the main differences between the two protocols, not considering the vms, of course. Yeah.
06:28:44.256 - 06:29:52.980, Speaker A: So there's different implementations, there's different kind of hardware that's required for each of them, different tokenomics, different goals. This fourth one is probably one of the most important differentiations. Arweave by default as a protocol without a vm, is geared only for perpetual storage, where filecoin is just for term based storage. So yeah, this is probably the biggest differentiation between the two, but today we're going to talk about the virtual machines that each of them are going to be able to run, ethereum virtual machine versus webassembly WASM, as well as general virtual machines, which each of the two protocols, both AO and FVM, are working on implementing their mechanics. And then we'll go through all of these topics on applications. I wanted to cover a lot more, actually, but I didn't think we'll have time. So I will put a link to this presentation at the end, which then I'll update probably by the end of next week.
06:29:52.980 - 06:30:47.944, Speaker A: Going into a lot more depth with different applications, because I think there's a lot to cover there. Yeah, so we'll start with virtual machines. So the great part about both of the offerings so far is Filecoin virtual machine is started out by offering just an Ethereum compatible virtual machine underneath it. And of course it inherits the same properties, advantages and limitations as every other Ethereum virtual machine out there. You can think of it a lot like l two that settles down to the Filecoin blockchain and then webassembly is the big advancement that a lot of other networks are moving to. It's a lot more free in what you can do. You can program in a lot of different languages as long as it compiles down to waSm.
06:30:47.944 - 06:32:14.824, Speaker A: It's amazing in terms of what kind of memory you can throughput in applications, things that just wouldn't be possible due to gas fees on EVM implementations. And particularly interesting, both AO as well as FVM will eventually be able to support more than just 4gb of memory, which will be super fascinating because then we can load entire AI models, like very large and complex ones as well into virtual machines that are running natively on these networks, leveraging data stored on their respective networks. So this is like a massive, massive step forward for decentralized compute as well as autonomous agents, which is a super hyped topic these days. So yeah, just wanted to specify that, yeah, both of these are being worked on by both, and we'll dive in a little more specifically on the nuances there as well. So as well as supporting both EVM and webassembly, both AO and FVM are working on being VM agnostic. So that means as a developer, eventually you'll be able, both, both of them don't have this implemented yet, but eventually you'll be able to load up whatever kind of virtual machine you like. So instead of WASM, maybe you want to implement something else, and then you'll be able to do that and have all of the same kind of guarantees that the applications you intended are running are what's running and so on.
06:32:14.824 - 06:33:11.404, Speaker A: Inter VM communication, message passing is basically the slide after this kind of defines what like actor based computing is, but inter VM communication is a really key part of that. And yeah, so wanted to just define actor model for everybody here. Basically it just means that, you know, instead of having a global state, you have a bunch of independent actors and they're acting, they're doing out whatever they want, they're doing whatever compute they want. And the only way that one actor is aware of another is when a message is passed. So it's another way of computing that's very different from traditional blockchains that we're all used to, where we have a global state. Now we have a bunch of individual states that are evaluated independently and only communicate with one another as they're programmed to do. So.
06:33:11.404 - 06:34:10.344, Speaker A: Of course, this makes it a little bit more difficult to build, kind of like a decentralized exchange or something like that. But what it does enable is massive, massive parallel compute, which means we can do insane things that we previously could have only done in web two, but now through just independent instances and message passing, we can accomplish compute on big datasets in web three with similar trust assumptions. So, yeah, so then where it starts to get a little bit different is in the mechanics. So AO uses a single system image and there's three kind of separate units, like schedule unit, compute unit, message unit. And they all have different roles. Schedulers kind of schedule when things should happen. So you can pre program that at a certain date or upon a certain event happening.
06:34:10.344 - 06:35:11.374, Speaker A: A virtual machine can instantiate itself and kind of wake itself up to do a task. And this is something that's new that you couldn't previously do in Ethereum or EVM based systems because unless you're depending on kind of an external scheduler running on a web two server or something like that. And it also has basically AO is focusing on, you know, wasm first, and we'll have a separate network for running. That's basically an Ethereum virtual machine that is native to arweaf. And that's kind of similar to what the FVM already has. They have the FEVM and that's the thing that you can deploy smart contracts to and so on already. So yeah, the main difference is in this kind of thing is the FVM is prioritizing being more like a traditional blockchain first, where AO is prioritizing message passing first.
06:35:11.374 - 06:36:20.854, Speaker A: The advantages of each are the trade off is mostly because filecoin, I think, wanted to build a system which can be used really, really quickly for Filecoin Defi. So that way they can have liquid staking lending facilities for the different miners, because miners are required to stake tokens. AO is being more experimental. They're creating this whole new paradigm of decentralized computing by focusing on message passing first that will be eventually implemented in FBM, but for now, AO is leading on that side, while FBM has more secure traditional approach. Yeah, so so far, both protocols will allow you to start or renew storage deals and we'll dive in a little bit more on the AO side. How? That's really interesting, because previously that wasn't relevant because arweave is by default a perpetual storage protocol. So whenever you upload data to it, there are mathematical guarantees that it'll be there for a very long time.
06:36:20.854 - 06:37:28.514, Speaker A: And both of course will have native storage access. So this is huge, because in the past you would need an oracle like Chainlink or some other to grab data from a decentralized storage provider or centralized storage provider and bring that down onto whatever kind of compute layer you're using, if it's ethereum or whatever else. So having that kind of native storage access where smart contracts will be able to, able to grab data directly from the system and they don't have to trust some kind of third party oracle will be huge as well as just because they'll be able to ingest massive amounts of data into the memory of the applications. Whereas in Webassembly one which is using 32 bit memory pointers, it's limited to 4gb of memory, but that will expand to much, much larger as soon as, as WASM 64 is implemented. And tooling gets better for that and so on. A big feature of both of these things as well is compute. Bandwidth is what costs the most when doing compute, especially on large datasets, of course.
06:37:28.514 - 06:38:22.974, Speaker A: So having the compute facilities at the same place as the storage facilities just makes a lot of sense for large use cases, particularly AI. So yeah, this is a really big point as well. So for AI inference, AO is going to create kind of a decentralized compute marketplace where anybody can join in as a compute unit. Some of them can be specialized in running GPU's that are good at AI inference. FVM has this nice kind of probably, I don't know if it's an advantage, but it's just design difference there where currently they happen to have a lot of miners which have GPU's in the same location as storage. So yeah, those facilities already exist in the same data centers most often. So it's really exciting to see how that compute will be leveraged for something really productive as well.
06:38:22.974 - 06:39:27.530, Speaker A: Yeah, and here's, I think the most interesting part of this whole phenomenon that's happening is where AO or arweave started as just perpetual only storage, and Filecoin started as only term based storage, meaning you agree with the storage provider for a certain amount of time, and then when the contract expires, somebody has to be there to renew it, otherwise the data is just deleted overwritten, whatever. Now the other can be implemented in each. So it'll be interesting to see, see what the reliability is of those different trade offs because arweave and filecoin are coming from these different approaches in the beginning and now kind of stepping on each other's toes, if you will, implementing what the other can offer through virtual machines. So it's pretty interesting. Yeah. So AO will use the VM to enable temporary storage contracts on compute units. FVM will enable automatic renewal of storage contracts with the storage providers.
06:39:27.530 - 06:40:19.554, Speaker A: So yeah, I'm pretty excited about this because now arweave and Filecoin will actually be able to compete once this is up and running. Yeah. So just to summarize, key similarities, actor model, meaning it's all based on message passing. Both of them are really trying to implement this. Native storage access is going to unlock large scale compute on big datasets that wasn't possible in web three before. Each of them are trying to implement EVM WASM and the ability to implement whatever vm that you desire as a developer. The difference there is just that Filecoin is prioritizing EVM, AO is, or arweave is prioritizing WaSm.
06:40:19.554 - 06:41:09.198, Speaker A: And. Yeah, and the convergence and storage offerings, I mean, that's, that one's really huge. And the big differences are actually not so much in kind of where you want to go, but it's in more of just like the roadmap. So both of them will like, in their ideal end states, I believe, end up looking pretty similar, just with different tradeoffs because of those original, because of the design of the original protocols. So the only difference is kind of the order of what's being built and. Yeah, but it's of course going to be subject to all of those original design decisions from each protocol. So we're still really early in this and it's not very clear which will end up being the better offering for any given, given use case.
06:41:09.198 - 06:41:46.254, Speaker A: But on FEM, people are most excited, I think, about liquid staking of the tokens and automatic storage renewals with storage providers. And on AO, people are more excited about autonomous agents doing compute on large datasets. It could be the case that the other is better at either of those, but I think it's really exciting to follow these two protocols as they, as they go on this journey of convergence. Yeah, so that's all. And thanks very much. Appreciate it. And yeah, these slides will be available there.
06:41:46.254 - 06:42:11.318, Speaker A: Thanks. Thank you so much. Does anyone have any questions? There's one question for that. Yeah, yeah. Thank you. So in my limited understanding, arweave is mostly based on like permanent storage with a blockchain. Right.
06:42:11.318 - 06:42:32.634, Speaker A: And then the. Sorry, the filecoin is more. Is more based on the. What's the name of that inner planet? Ipfs. Yeah, right. With like, mixture of like, consensus algorithms. So those are like very different in nature.
06:42:32.634 - 06:43:24.364, Speaker A: How do we, like, compare them going forward? And then, I mean, besides what you've already mentioned today, and then also, what's your view on swarm? Because when I originally heard about it, as that was one of the original ethereum storages, and then after that, I didn't hear much about it and I was really rooting for it, really wanted it to succeed. So if you have maybe mentioned a few words about it as well. Yeah, yeah, for sure. So the main two questions are about how to compare the two going forward being filecoin and fvm and arweave and ao as well as swarm. So, yeah, how to compare the two, I think. I mean, it's going to be. It's going to be really interesting, I think.
06:43:24.364 - 06:43:50.618, Speaker A: Yeah, it's a tough question, like how to compare the two. I don't know. What perspective are you asking? Kind of as a developer interested in building on one of them permanent storage with blockchain. Right. In my view, it's going to be more expensive to sustain it forward as far as, like, crypto economics goes, rather than something like filecoin with actual miners there. And. Yeah, it's kind of broad question.
06:43:50.618 - 06:44:31.770, Speaker A: Sorry. But yeah, yeah, it's really just so different in nature. Yeah, they're very different, which is why it's so interesting that now they're kind of going toward the same place with the virtual machines. But the. But arweave actually isn't a blockchain per se, it's more of a block weave, which is why it's called like, arweave, and there is like a blockchain that kind of manages the transaction history, but the data itself is stored through economic guarantees. And it's a lot of game theory that goes into that. The prior presentation that I gave there goes more deeply into the economics of arweave and how that works.
06:44:31.770 - 06:44:57.542, Speaker A: Exactly. But, yeah, happy to, you know, chat more one on one about that as well. And swarm as well. Yeah, I've been following it since the beginning and been super, super eager to see them integrate. They've tried a few different things over the years and I think, yeah, there's still kind of this more quiet team building in the background, but they've been working on a lot of interesting things recently, so I'd recommend. Yeah, check them out. Awesome.
06:44:57.542 - 06:45:11.314, Speaker A: Thanks. As long as the swarm is not dead. Yeah, I know that very well in a life, and I don't think that'll change. I mean, I think they've, you know, kept a lot of the crypto that they raised at the beginning. So, you know, I think they're there to stay financially. It's just a matter of. Yeah.
06:45:11.314 - 06:45:55.534, Speaker A: Adoption. Yeah. Thank you. Yeah. Any other questions? Thanks for the presentation. Have you seen any increase in usage driven by decentralized computer on Arweave or by liquid staking on Filecoin? Yeah, not yet on either, but that's only because you can't do general decentralized compute on either yet. I mean, AO is still in a testnet, and FVM hasn't implemented arbitrarily uploadable waSM code yet, so.
06:45:55.534 - 06:46:29.324, Speaker A: Yeah, it's too early to tell, unfortunately. Yeah. Liquid staking. Yeah, that's been driving a ton of volume in DeFi on Filecoin. That's like, the majority of it. I believe there's, I think, 500 million or so as of a stat that I read of a couple months ago staked on Filecoin, and 300 of that is liquid staking for facilitating liquid staking. So then miners can take those tokens and stake them for their storage capacity to mine.
06:46:29.324 - 06:47:29.324, Speaker A: I just wanted to ask you, what else would you like to see get built on Arweave going forward? Yeah, right. That's a good question. And the thing I'm most excited for is autonomous agents, which are automatically building themselves. I mean, we're starting to see this with different AI tools that are coding, are able to code an entire application just through a text prompt. So if we can make agent, which prompts another AI model, to code something that might be useful or profitable or something like that, we can have a whole ecosystem of autonomous agents going around in crypto land, you know, acting by themselves. This is going to really change the way that we all use crypto. And eventually, as that taps into the real world, I mean, it will change the way we interface with a lot of things.
06:47:29.324 - 06:48:04.142, Speaker A: I mean, just the robots so far have been quite limited in what they can access, but I think, I really believe that AO has the potential to enable autonomous aggregation to act on chain as well as in the real world in a really meaningful way that we haven't seen before. So it is kind of a potential for a paradigm shift. So that's what I'm most excited for. Of course, there's two sides to every coin. But, yeah, I think if we are responsible in the way we deploy these things, it can be really, really positive. Yeah. Thank you so much.
06:48:04.142 - 06:51:30.822, Speaker A: Yeah, thanks. Appreciate it. Thank you. One more minute and you'll have next speaker. Hey, are you ready for the next one? So next we have Robert Zaremba from Muoni talking about cross chain defi opportunities and challenges. Are you ready? I'm ready. Yeah, go for it.
06:51:30.822 - 06:51:51.574, Speaker A: Thank you. Hey, everyone. I hope you have a good time. So a few words about myself. I'm leading amateur at WuMI, or Ux chain. We rebranded, we're one of the cosmos layer one chains. The biggest, in fact, money market, native money market in cosmos.
06:51:51.574 - 06:52:19.252, Speaker A: Yeah. My background is for already six years in Cosmos before in ethereum. I'm also part of the New York Contract Standards committee, workgroup. I used to do lecturing at the University of Geneva. But I don't have any time anymore. That's my website, if you want to follow me. So I want to talk about this discrepancy between different chains.
06:52:19.252 - 06:53:13.862, Speaker A: We talk about layer twos, we talk about chain abstraction, account abstraction, about layer ones, layer twos, layer threes, and so on. I want to start with this slide I think everyone is familiar with, because on every chain, we want to launch another mem coin, another NFT, sell it. And this replicates, like, okay, maybe you can try on ethereum then maybe you can try on the base on Solana, an avalanche or whatever is there, and see your success. So for NFTs, for, like, this kind of images, yes. Maybe the strategy will work. Like, you don't lose any efficiency, but there are many other mem coins. And then you can ask yourself, how much sense does it make? Launch yet another mem coin, and then you need to bridge it.
06:53:13.862 - 06:53:47.752, Speaker A: Like, how do you form this ecosystem? So this causes many problems, right? Like, now, if you think from the financial perspective, for mem coins, it works. You can just launch yet another one. Right? But, you know, nobody is using memcoin for serious stuff. It's good to make money, but nobody will trade it in a traditional sense. So now, like, abstracting from that way? Yes. Like, how this works. You take the same smart contract, a template, and then you redeploy it.
06:53:47.752 - 06:55:08.748, Speaker A: And maybe you just change the catalog of the images. If this represents NFT collection, if this is like a dex, then probably you fork Uniswap or any other solidity contract, and then you redeploy it, rebrand it, you add maybe one more feature change, maybe the governance parameter and voila, maybe you have another successful protocol. And then, you know, you do it for layer two, you do it for another layer one. Soon there will be layer freeze, all of it. Then if you apply it for the defi, yes, it obviously will fragment the liquidity, right? Because your uniswap pool on Ethereum and then you need to bridge some tokens and then, I mean, you get the point, right? It fragments the liquidity and then it causes the liquidity worse because like, how do you incentivize people to actually move the liquidity? So what's very often happening is that, okay, there are these incentives yield strategies, and then all the degens will basically milk your incentives, the tokens you have. And as soon as the incentives are finished, most likely your protocol is dead. But who matters is like you made, I mean, probably few thousand if you are good, a couple hundred thousand if you are very good, maybe some millions.
06:55:08.748 - 06:56:14.628, Speaker A: So you made the job, but overall, yes, like after a few months, your protocol will be dead. So it's just like, you know, phenomena in the defi of today. Next thing is that now if the protocol, like, let's say aave or uniswap, yes. Which is lands on a few layer tools or an ethereum, and there is a brand, right? So they keep the value, but they lose on financial efficiency, right? So if you take for example, Ave, again, it's very important that the liquidity on the money market perspective is there, right? Because the money markets they are supposed to provide to be that, you know, that they're very stable, big source of liquidity to help other, other protocols. And other protocols should be built on top of that. So there is a whole strategy, right, when and why the money market will have to be replicated or not, and how it works. So again, to make it efficient.
06:56:14.628 - 06:57:49.936, Speaker A: And we have from a Dex perspective especially, yes, we have a problem with, I mean, it only works when there is arbitrage, right? If you are on, let's say, arbitrum, and you need to make some traits either, yes, you break it down into few traits until the arbitrage bots will equalize the price, which is not perfect from the user perspective, or you lose because the slippage will be bigger. So the world of today is really depending on the branding, on the people, on the trust in the protocol, sorry, teams and the arbitrage bot stand, that will be efficient consequences. So reduced liquidity, of course, your protocol is less efficient, increased complexity, because now you either wait or again, you need to apply those arbitrage bots inefficiency in capital and resources allocation and uneven access to the information. So the arbitrage buzzer is a huge money to make, but it's really for sophisticated users only. So what I want to talk here is that we should, as a community, as builders, as innovators, thinking more on the bridge powered apps. So rather than fragmenting and copying the same solution, start thinking. Especially now when Ethereum is moving to layer twos and there is no one layer two, but many layer twos.
06:57:49.936 - 06:58:16.494, Speaker A: They have to be connected, they need to work, they need to be interoperable. So it's not only about the composability, but it's the connectivity. This is still missing. If you think from the perspective again in Ethereum. Ethereum is the settlement layer. It doesn't bring what we really need, the interoperability between the protocols. Because interoperability is not about token transfer, it's really about remote procedural calls.
06:58:16.494 - 06:59:14.244, Speaker A: So for that, yes, ok, token transfer is the basic one, but we need to have the message passing. We need to have a gas abstraction, because let's say if the money market will be, let's say on arbitrum, right, you don't want to again replicate it to the optimism. You just want to quickly and efficiently talk to one another without having a user to rely on the gas and underlying complexities. Interchange tokens. Another thing, USDC, how many USDC you have? Axlr USDC, Ava USDC, Axel USDC on polygon as Axelar USDC on, on arbitrum, on osmosis and so on, so forth. So then you have all those 93 usdcs, at least it helps unification around it. I mean, those tokens have to be different because they carry and different risk.
06:59:14.244 - 06:59:52.334, Speaker A: So. But the concept of interchange tokens is, again, is how we can leverage bridge to unify it. Yes, somehow. And finally, interchange authentication that comes with interchange accounts and chain abstraction. I did a lot of work, for example, with Axelr and a few other bridges. There are usually problems because when you have multihub bridge, you lose the proof who actually is the original sender, because the bridge will usually reproduce the sender from the last leg. So you need to think also about how do we do the authentication when there are multiple legs on your bridge.
06:59:52.334 - 07:00:15.516, Speaker A: Chain abstraction. Internet computer is doing a great job with that. Started it. They want to put it as a team of this year. I think I have a slide about it. So it's more about that. Okay, you have your own wallet, and from that wallet you control all the other accounts.
07:00:15.516 - 07:01:11.654, Speaker A: On other chains. So the concept is that you use MPC. I'm not sure if Internet computer was going deep into that, but they have a lot of documentation and nice video, check it out. But very roughly that the subnet or here in case of Neo. Yes, shart derive keys from your, let's say your account here to construct transaction, to sign a transaction for other chains. That's from IC, but the idea is thresholds, cryptography and NPC to basically solve it out. So going back to bridges, this is from Axelar, the applications will be on the chains, multiple chains.
07:01:11.654 - 07:02:03.862, Speaker A: Then you have many gateways, many bridges, and then how we can organize it. So all the bridges, I think they need to have the application layer, like in the TCP IP. It's not only a communication, but application layer. Axelar allows for it, Warhol will allow for it, IBC will allow for it, and I will talk in a second polygon. Polygon is doing like something similar, but in a different way. So they introduce that Ag layer or interoperability layer, where all the messages, again messages, not the tokens, can transfer in a trustless way. Here in this case yes, they require ZK proof, so that Ag layer in this slide is interoperability layer will verify it.
07:02:03.862 - 07:02:52.920, Speaker A: So you have a zero knowledge proof that the execution was correct, which is usually better than just consensus verification. Why it's important again, because you can focus on developing on a single chain and leverage a fast interoperability layer to do it. And the other way it works. The disadvantage in case of polygon or CDK, in my opinion, is that no, it only works with EVM. If you have some custom EVM with custom opcodes, because you do more of the privacy stuff, then no, sorry, it won't work for you. Here's another view how you can look at it. Some public chains, CDK chains, private chains, again, all of it.
07:02:52.920 - 07:03:19.964, Speaker A: Then the proofs can be aggregated, merged into that concept of interoperability layer. Again, I don't want to talk about Polygon, but about the concept. The concept works. Cosmos Change IBC. IBC is perfect for developing application on top of the bridge. So who here is familiar with IBC? Ok, a few people. So IBC stands for inter blockchain communication protocol.
07:03:19.964 - 07:04:16.468, Speaker A: So it really starts like from bottom up approaches. So you have the networking layer, you have the channels, connections, channels, same as you know, you connect your web applications, web services, you have a server, the servers and the client establish a connection. Once the connection is established, you can have a channel there, multiple channels and multiplexing and then in terms of IBC, these channels are important because each channel is bind to an application, to a specific protocol. They can then multiplex the packets between the same clients. Crucial part is that it's open, it's open ended, so you can have any application. On top of that, you don't need to change. I probably should be better if I would put another slide like in novel, like in Aussie Stack, you don't need to change the channels.
07:04:16.468 - 07:04:50.096, Speaker A: The concept of like no how the trust is established here, it doesn't require you for that. Yeah, think about like from, from the CDK Polygon perspective, if you would, if there is a limitation somewhere there, like the whole infrastructure doesn't work. Yes. So we will yet to see as how we will be, if we will be able to like enable more aggregation. Again, if you change the proof system, there might be some limitations. IBC maybe is not the most efficient one today. It's really focusing on the light kind of verification, but it's open ended.
07:04:50.096 - 07:05:39.444, Speaker A: You can do like any type of the verification is peer to peer. So you as a user, again, from a chain, you can have kind of like a government or risk assessment to see with which protocols, with which chains you want to interoperate. And that's crucial. This is really the interoperability. Peer to peer, you reduce the risk and you can do it. So there are many now projects trying to leverage IBC to use the same concept, use the same API about the packets, channels, ports to apply for other stuff. So for example, union recently they launched the connecting of cosmos chains with all the EVM chains.
07:05:39.444 - 07:06:20.390, Speaker A: All the API is ABC compatible. So finally it's going open that you don't have one specific implementation between the cosmos chains, but you already have implementation for other chains as well. Notably they are using zero knowledge validation for EVM chains. Few more slides here about whenever doing that, it's super important to think about the security. There's always risk. So most of the bridges like wormhole before, now they have the new version of the wormhole connect. They were rather focused on the multi signature.
07:06:20.390 - 07:06:54.018, Speaker A: We have already seen many vulnerabilities with that. MPC is kind of interesting. That's what I mentioned if you remember about IC and near, but it's still yes, that you don't have full verification. Validators could collude to forge a signature for you and they won't be slashed, basically. Then relate to another chain. So it's not the best one, but it's okay. Consensus verification and state verification.
07:06:54.018 - 07:07:49.334, Speaker A: This is what IBC and what I mentioned before about union provides the zero knowledge validation. Full zero knowledge validation is the last one. So you verify the state, you verify that operation is correct based on the latest state of the chain. So you fully reduce the risk here. So what we should really think and develop whenever you're making decision, building the applications with Axelr, with wormhole, with union, or other bridges that will come, or IBC in general, as a concept, yes, we need to really be here, don't use that. And finally, I want to close here with few thoughts about Defi and what we are doing. So, Defi v one, it really fragments the liquidity that, you know, you have pull one, pull two second pool.
07:07:49.334 - 07:08:34.006, Speaker A: Those pools, they don't talk to another one, to another v two. Yes, you have like a protocol on liquidity. It helps because then you have like a protocol who can manage the different pools, who can do some assessment. Human is less involved into that, but none of them really solve the liquidity fragmentation problem, problem. And in my opinion, like maybe it should be v three or maybe v four, I don't know how people would like to call it, but for a few years, we already talked about unified liquidity. And in my opinion, this is like building the apps with bridge in mind. From the first perspective, rather than copy pasting, there are a few solutions that already, let's say leverage that interfluid staking superfluid.
07:08:34.006 - 07:09:10.276, Speaker A: Staking superfluid is, for example, from osmosis. You can deposit into the pool some tokens. Those tokens will be locked, bonded, and automatically, let's say staked into other, I mean, if those are native tokens. So superfluid is for native tokens. So tokens that are like layer one tokens, right. That you can stake for your proof of stake rewards. And then basically you boost your API meter.
07:09:10.276 - 07:09:46.966, Speaker A: And there's also experiment we are doing that's kind of like a bridge token that you can unify different accelerated from chain a chain BC and try to find a protocol with that. We released it a month ago. We still like need to think how to market it, but it's more like an idea and. Okay, I cannot go back. I went back. Yeah. So I want to put as an example this me token, because check it out.
07:09:46.966 - 07:10:26.344, Speaker A: I really want to promote visions or missions for anyone who wants to world with bridge in mind. So with that, yes, I hope that this will be the right solution for DeFi. I mean, everyone in DeFi wants to have better speed, better quality and lower costs. Just shipping your protocol from one chain to another is okay. It's like a short term and there will be always better chains. But if you fragment it, just copy paste, it won't help you. You can always move your protocol from other chain, but you still need to have the interoperability in mind.
07:10:26.344 - 07:11:08.804, Speaker A: So if you start with interoperability and then you find that okay, actually, for whatever reason, Optimus doesn't work for me because they don't have fraud proof. I want to go for Sui or Solana because it's more sexy now. But if the whole stack or the whole API with all the connections will still work, because with bridge in mind, you will keep, you will improve the speed. You don't break the composability or interoperability, you improve the quality, maybe because the other chain will provide you better solutions and you reduce the cost. Thank you. That's all. Thank you.
07:11:08.804 - 07:11:46.824, Speaker A: Do we have like two minutes for one question? One question. Really great presentation. One quick question. What's your view upon intent networks such as across, DBridge, DLN? To abstract the need of even having a bridge and offsetting this one, trade on one chain to another through solver networks. I'm not sure. I got a question. You asked about the intent.
07:11:46.824 - 07:12:23.496, Speaker A: Yeah. Like, what's your personal view of intent networks like DLN, dBridge, across instead of even needing to have a bridge, set this to solvers. Yeah, good question. So intent could be used like kind of a resolver. So, I mean, we have diagnosed chain abstraction is which I mentioned before, that, you know, you abstract how you can control from one low wallet and other accounts. I think that could be a good optimization for like bridges or that from a user perspective. Right, it will.
07:12:23.496 - 07:12:50.364, Speaker A: Or from the interoperability perspective, yes, it will. It will solve. Yes. Because we will have some optimizers. There will be many bridges for sure. So from developer perspective, probably if there is like a good, there is a good product that will deliver bridge based intents and resolve all the routing. For me, keeping the interoperability.
07:12:50.364 - 07:18:54.356, Speaker A: Yes, that's the future. Thank you so much. Thank you. So, 1 minute to set up our last talk of the day. Stand still, don't move. You're moving it. Hey.
07:18:54.356 - 07:19:33.444, Speaker A: Hey. Are we ready? So, last talk of the day, it's going to be Marcin from Redstone talking about multi dimensional prices. Oracles, are you ready? Hey. Hey, can you hear me? All right, I suppose you can. So thanks a lot for staying longer, I think. I believe it's the last presentation of today, so I will try to make it a bit more interactive with some of you that will be still alive to answer some of my questions. So maybe first one, who knows what's Murphy's law? Raise your hand.
07:19:33.444 - 07:20:07.938, Speaker A: Okay, not that many. So it's a law that when something can go wrong, it will fucking go wrong. And the reason I'm bringing it up today, like in my personal staff, I was juggling a couple of things and they like fell down. So I'm stressed a little bit. But I try to make sure you get some value from this presentation. So my name is Marcin, I'm co founder of Redstone Oracles and also co founder of eforeso. And as I was telling you about Murphy's law, that's also one of the reason the topic of the presentation is going to be different.
07:20:07.938 - 07:20:52.884, Speaker A: It's going to be roll up as a service. And oracles, how do they interact instead of multidimensional dimensional pricing, but I hope you will still get value out of it. Few words about Redstone. We have been around since early 2021. Raised $8 million to date from various notable venture capital firms like Lemniscap, Blockchain Capital, Compass Ventures, Stani Kulechoff, Sandeep, Alexi. Many reputable people believed in our vision of disrupting oracle space. Besides oracles, we also really like research, like wrapping our heads truly around the topic that we are advocating for and then delivering products about.
07:20:52.884 - 07:21:37.804, Speaker A: So we've delivered already, I think, five really comprehensive reports. The one about LSDFi, I think was the most successful one with coindesk indices. It got, I think like 400,000 or even 500,000 views and a lot of positive feedback. We also did one about LRT's and some other topics like stablecoins and RWA. And we are a team of 15 people, energetic, very dedicated, always with a cup of coffee, working 12 hours a day in a not busy day, of course, making sure that we can deliver what developers need in the space. So first things first, who believes that knows what an oracle is? Blockchain oracle like end to end. Please raise your hand.
07:21:37.804 - 07:22:26.266, Speaker A: Okay, sweet. That's actually more than expected, but let me still bring it down. So oracles deliver data to Dapps for smart contracts execution, right? Well, if you dig into that, currently it's mainly market data. So volume, interest rates, market cap and so on. And if you take even a step further, it turns out that actually over 95% are price feeds of assets. And within these assets are mainly crypto assets like bitcoin, Ethereum and so on and so forth. So now a question, and maybe someone will try to shout if I can ask, what is the most utilized price feed today from oracles? The first one.
07:22:26.266 - 07:23:04.854, Speaker A: So if USD, if USD, it's not so easy to estimate, but it's probably between 70% to even 95% of queries of the price feed is ETH USD. So this is the king of all the price feeds. The reason is because defi is built on what ethereum, and a lot of defi is also on l two s and l two s also depend on ethereum heavily. Right now we have bitcoin l two s. So of course this can be skewed towards the other one. But as of today, efusd is the king. What are the use cases of oracles? So oracles will have wide use cases in the web three ecosystem, but now it's mainly defi, as you might be probably understanding that.
07:23:04.854 - 07:24:01.154, Speaker A: So we have lending markets where you lock collateral and borrow assets against that CDP stablecoins, which are actually very similar in that sense. But you always, almost always borrow a stable coin. For example, liquitee the team, I think, I believe they originated from Switzerland, so should be pretty known over here. Leverage protocols where you can leverage, for example, LRT points or Athena points or whatever points there are in 2024. Derivative protocols where you can play a small hedge fund manager or casino player on your couch, like trying to long short bitcoin, ethereum and so on. Yield protocols when you leave your assets and then believe that the automated strategies and the operators of vaults are going to make the best of a job. And various algorithmic stable coins that also rely on oracles.
07:24:01.154 - 07:24:59.316, Speaker A: Another one, can you come up with two defi protocol types that doesn't need oracles? So any guesses? Randomness? Randomness, yeah, it's like one of the products that usually oracle offer dexs in general. So uniswap, sushiswap and so on. Sometimes I'm getting asked, hey, why uniswap isn't using Redstone or chainlink or any other oracle? Actually we are pulling data from Uniswap, usually as oracles. And another one can be oracle less protocols. So some protocols strictly designed to create lending without oracles, like blur for example for nfts or angia is a popular oracle less landing protocol. But there are always some trade offs if you decide not to use an oracle, it doesn't matter for roll up as a service providers. And in general, if you want to have a defi on your roll up, when you create a new roll up, you need an oracle.
07:24:59.316 - 07:26:17.910, Speaker A: Unless you want just to have these Dexs, Oracle less protocols, and maybe now let me take a step back and explain what the roll up as a service is. So, in 2023, late early 2024, it turned out that creating a new roll up on top of Ethereum is thoroughly cheap. So you can standardize that procedure using popular stacks like Opstack, arbitrary stack, ZK stack, Polygon, CDK, and so on and so forth, and create roll ups on demand with new clients. So actually, some of the, let's say, older layer ones decided, for example, to switch to l two s as a better go to market strategy because they had some other advantages than necessarily being nl one and so on, so forth. So when you need an oracle for your roll up, you take your phone, you dial a chainlink, and you tell them, hey, can you deploy in a week? And then it turns out that it's not necessarily so easy because chainlink is a huge organization. Naturally, they have to also justify cost with the revenue, because I think there are like 500 or maybe even 600 people right now at Chainlink. So it's not so easy for them to keep up with the demand.
07:26:17.910 - 07:26:55.344, Speaker A: And now, don't get me wrong, I always try to emphasize that in my opinion, Chainlink has done tremendous job in helping to bootstrap Defi over the years. But probably if you have a category like oracles, it shouldn't be monopolized, and it shouldn't be just one provider to serve all of the needs that are out there on the market. So, bottom line, Chainlink is really good for the industry. And in my opinion, they do a really good job, but you need more diversity. And also, they cannot serve all the needs that are present over there. How many chains does Chainlink support? Actually, they created this fantastic website. I think it's data link something.
07:26:55.344 - 07:27:32.240, Speaker A: If you type data chainlink, you're going to get it in Google. And there you can check all the feeds on all the chains that they're supporting. Right now, it's roughly 14. Like, don't quote me on that, but it's this kind of a direction. And as you know, with all l one s, l two s, and roll up as a service, you probably have, like, I don't know, at least 50 viable chains and probably 300 promising and so on and so forth. So the ecosystem is pretty large. And another question would be, do you know any other defi vertical that has been monopolized that much? So some people claim that steal Dexs with Uniswap, but actually there is right now more diversity.
07:27:32.240 - 07:28:28.516, Speaker A: Also, like with new Dexs and new chains, because Uniswap also doesn't go to each and every one. Or maybe wallets with metamask, but actually then you think, okay, on Solana there are some other wallets, and then you have these new l two that also sometimes offer their own native wallet. So why isn't chaining on all of the chains? Like what is blocking them for going and being truly at every single ecosystem? So the key bottleneck is on chain data storage, cost and monitoring infrastructure. So with each consecutive chain you have to have higher maintenance cost for being over there. The key consideration that they take into mind into account is update condition and cost of one update. So how many times you have to update prices on chain? And what's the cost of one update? Right, because then you can calculate the cost way more efficiently. So there are right now the two key implementations of oracles on the market.
07:28:28.516 - 07:28:56.786, Speaker A: One is push oracle, which is the chain link style. And there's also pull oracles that for example we as Redstone offer, but also it's offered by PiF. And Chainlink also has right now data streams on arbitrum. But we see probably pretty distinct two categories with push and pull. Who has ever heard of push and pull oracles? Raise your hand, help me. Okay, three people, maybe four. So now let me compare the two.
07:28:56.786 - 07:29:38.280, Speaker A: So there are many verticals upon which you could compare them, but let's focus on the four most important ones, in my opinion. So in push oracle, you create a service that shoots data on chain in intervals, and this interval is defined by time. So I don't know, every 62nd, every 24 hours and deviation threshold. So you had one update, for example, it was $1 and you have a deviation threshold of 1%. So it keeps on changing information that the value of an asset is $1. And when the value of the dollar crosses, for example $0.99 or $1.01,
07:29:38.280 - 07:30:17.668, Speaker A: then the update goes again. So you kind of accept the inaccuracy of 99.99,999,999 cent on the pool, or we tend to call them on demand oracles. You have the user interaction that is defining the update on chain. So you don't create a condition that is outside of usage, you create a condition that is upon usage. So when there is a transaction that actually requires the data feed on chain, then you update it and you can do it in various ways. For example, with Redstone we utilize cold data to pass signed data packages.
07:30:17.668 - 07:30:52.198, Speaker A: Pif is using wormhole as a bridging solution. I think data streams from Chainlink is using their CCIP or maybe automation, I'm not sure. It's closed source, so you cannot dig into that that much. Then you have gas coverage. Who covers the gas on Ethereum is a really big consideration. So on push model you have the oracle to eat the gas or the protocol that is utilizing to subsidize part of the gas of updates on chain, in the pool model you usually have the user. So the user, instead of paying for example $0.50
07:30:52.198 - 07:31:33.474, Speaker A: on the transaction, he's paying let's say 50, $0.01 or $1. So there is a margin on top of the cost of the transaction to pay for the gas, so that it's kind of like hidden from the user. He just signs and accepts the transaction, it's extracted away. That part of this cost that he's paying is actually updating the oracle, but in his interest because he wants to interact with a protocol to update the dome chain and get the most out of accurate price, it or actually protocol can subsidize that. So you can also redistribute it that the protocol is paying. So it depends whether the protocol wants to cover that and incentivize users to use protocol or wants to leave it to the users themselves.
07:31:33.474 - 07:32:06.770, Speaker A: How much scalable model is. So push is fairly non scalable. The reason is for each asset, let's take if USD at each single chain, you have to push it without any correlation. So if on Ethereum you pushed it, then you have to push it on arbitrum, then you have to push it to optimism, and then you have to push it to any other chain you want to update it at. You can have various deviation thresholds. So on Ethereum it can be 24 hours, on optimism it can be 1 hour. So you don't even update them at the same time usually.
07:32:06.770 - 07:32:41.380, Speaker A: And you have to cover the cost of all of these updates. However, in the pool oracle you usually keep data off chain as long as possible. So without touching the on chain storage and gas, and then only when it's truly needed, then it's delivered onto the destination chain. So if a user is interacting with arbitrum, then you deliver it to arbitrum. If it's interacting with Ethereum, then you deliver to Ethereum and so on. So you try to minimize the gas waste within this flow. So it's definitely more scalable, especially with new chains popping up so quickly and then the number of feeds that you can deliver.
07:32:41.380 - 07:33:21.204, Speaker A: So you remember a key consideration which was update condition and cost of one update. So actually here I should add also like number of assets you're updating, because then if you update only two assets, let's say bitcoin and ETH, fine, it's not that many. But if you want to update, let's say 100 assets, then you have to cover the cost of all of them if you update them on chain. So on push model, you usually have a very limited set of assets that, that are available in Oracle because of these costs. However, in the pool on demand model, you usually have way more versatile assets. So in hundreds or even thousands, because keeping data off chain is actually not that expensive. Still has a cost, but it's, let's say marginal in comparison to the push model.
07:33:21.204 - 07:33:54.468, Speaker A: So how does the flow of standard push Oracle look like? You have data sources from which node operators are pulling data and creating the price heat of an asset. Let's say if USD, then usually it's aggregated somewhere. So I don't want to go into this is super simplified. Like please don't quote me on that. It's just to give you a sense. Because on chaining, I know there's also this dawn, this decentralized oracle network where they do the signatures, randomness and so on. So it's a bit more complicated.
07:33:54.468 - 07:34:33.764, Speaker A: But in simple forms, the node operators pool the data from sources, then agree on it, like there is some sort of aggregation, and then shoots it on the destination chain. Let's say it's ethereum or arbitrum. And then a smart contract. For example, Aave is hitting the smart contract of chainlink to query the price. So let's say the delivery goes to on chain and the protocol itself wants to just see what's updated on chain doesn't care about, let's say the off chain word as long as it works. So it's not necessarily scalable across many chains and number of feeds. In our view, the push model can be compared to email.
07:34:33.764 - 07:35:08.506, Speaker A: Do you use email every day? You do. Is it the most efficient way of communicating? It's questionable. Is it better than 30 years ago? It is. So in our view, push model is here to stay and it's going to be like a backbone of defi, like for years to come. However, you also need to innovate and create other types of flows to serve the needs of builders. Right, to go through the limitations that are with this push model. So if you want to create truly a new financial system on top of Defi and blockchain rails, we need to innovate.
07:35:08.506 - 07:35:50.664, Speaker A: We need to do something better. So therefore there is this pool on demand oracle, where, let's say the first part looks literally the same, like roughly the same, that the nodes are aggregating data, but then the aggregation happens in a sort of a middle layer. In our case, it's data distribution layer. It's like an off chain gossip network where signed data packages are like streamed. We use a couple of services over there like our own gateways, but also streamer network gossip notes and so on. For PIF, for example, they forked Solana, it's pifnet. So they aggregate this data on their chain that they govern.
07:35:50.664 - 07:36:47.640, Speaker A: And then once this aggregation is done, you somehow have to push that data on chain when it's needed. Right, as I was telling you. So when there's a user interaction, for example, with PiF, it's the wormhole bridge that is delivering the data feed to the destination chain towards which a user is interacting with, so that the data Dapp can actually utilize that. A liquidator, for example, can just, if a specific oracle, let's say Redstone, is supported by a DAP and our price packages, signed price packages, can be used, a liquidator would just fetch data off chain and then liquidate the position if it's allowed, if it hits the liquidation threshold. Or in our case, in general, we piggyback user transactions because in our logic, our logic is like this. Transactions from users have to go because they want to interact with protocols. It's the protocols and users that want the data.
07:36:47.640 - 07:37:34.700, Speaker A: So probably you should create an update condition that is strictly attached with the transaction itself. So in every transaction on ethereum, on EVM, even so, ethereum and all tools that are EVM compatible, you have this construct of call data. So whenever a user is sending a transaction from one address to the other, you can pass some payload. Usually these are like arguments for functions. So here it's for example, two and Charlie. But you can encode over their assigned data package. And this is what our engineering team has done a tremendous job optimizing because they also created a low level code for making a very efficient gas operation so that the storage is not touched too much.
07:37:34.700 - 07:38:13.064, Speaker A: So, bottom line for user, this gas margin that he's paying for using this feed with their cold data is truly marginal. It's like almost invisible usually on the networks. And cold data actually is a very interesting topic with l two s. Like also like passing data over there right now. There was Duncan. Yeah. With blobs being fully honest, I'm so snowed under with all the topics that we are working on that I didn't follow the exact gas, let's say decrease with l two s with blobs.
07:38:13.064 - 07:39:02.024, Speaker A: But I've heard it's pretty tremendous as well. Okay. And then finishing going back to roll up the service and oracles, remember that there are a couple of verticals upon which you have to compare them. And then when you are roll up a new roll up and you want to have an oracle, you have to compare them also on the wait time. So how long does it take for a new oracle, for an oracle to deploy on a new chain? What's the cost of that integration? So if you pay, I don't know, $10,000, $10,0000 nothing. And types of applications you want to bring, because if you want to bring forks of other compound and let's say the OG protocols, they require the push model and they probably will not change that code because it's well audited and solid. So over there you just need the push oracle.
07:39:02.024 - 07:39:50.336, Speaker A: And also one consideration that people usually forget about is, hey, I have my native token. So probably I would like the oracle to create a price feed for my native token, right? If I'm telegram, open network, I would like to have an oracle for token. If I'm arbitrum, I would like to have an oracle for Arp and so on and so forth. So a good question to ask is, hey, this oracle can create a price feed for my token if we go forward with them because it depends on the sources from which they are pulling. Some of the case studies we have rolled up as a service. So for example, Manta Pacific, that in the past months accrued quite a lot of ice and tvs. It's built with Caldera as a roll up, as a service provider on top of, I think right now still opstack, but they want to transition to polygon.
07:39:50.336 - 07:40:43.858, Speaker A: CDK, another one with gelato, which is, I think Swiss team as well is lisk, that is OGl one that decided to become an L two because it turned out that SNl one, they don't have that much competitive advantage. They have other strengths. So it's just better for them to align with ethereum. Celo is in the process of transitioning to an l two and right now I really recommend to type into Google sello forum and read the posts of all the l two s comparing each other to attract sello to choose their stack. So arbitrum, optimism, zk sync, polygon, all of them wrote really in detailed posts to convince sello to pick that. And it's a bit of a beauty contest. So if you ever wondered, hey, how can I compare these between each other? I recommend to go to the selim forum because it's a fantastic source of information.
07:40:43.858 - 07:40:55.834, Speaker A: Right now. Canto is another one that is switching to polygon Cdk. Who knows what that logo is. Say it. Eos. Eos. Right.
07:40:55.834 - 07:41:12.374, Speaker A: Someone remembers them. I'm not sure if they are going to switch to l two, but maybe I believe they still have a lot of cache. So maybe a resurrection of Eos is going to going to be snl two. We don't know. Right. It just shows that there is this opportunity to go that path. Parallel is another example.
07:41:12.374 - 07:42:06.072, Speaker A: I know I'm running out of time so you can take if you're interested, I probably this is recorded so you can stop on that slide later. But basically I also created a couple of verticals upon which you should compare oracles so I don't have time to go through all of them. But you can then check the presentation online as it's recorded and stop on that part. And also I will prepare an oracle comparison in some time like Redstone, Chainlink, Tiff and others. Naturally I'm maybe not the most objective person but I will do my best. I mean like come on, you can try and then get pretty interesting feedback on Twitter, right? With some marines from one or the other project to finish. I'm also organizing if Warsaw, which is a conference in hackathon in Warsaw, Poland.
07:42:06.072 - 07:42:29.474, Speaker A: Oh. All date this year is going to be last week of August. So you're all very welcome to join us over there and thank you very much for staying so long. Thank you. One question anyone, before we wrap up today. Hello. Hello.
07:42:29.474 - 07:43:13.442, Speaker A: There was a question. What are the other oracle use cases? We'll see. Hard to say right now, like gambling can be one and prediction markets and so on. But it's not me to say what people should be with my product. What is your practical recommendation? How to reconcile between various oracles. So you have like data feeds from various oracles and how to reconcile between different, different results. I wouldn't say it's an easy question and there is no straightforward answer right now.
07:43:13.442 - 07:43:44.484, Speaker A: Like there is no one aggregator of oracles. That gives you pretty objective answer. I'm being asked pretty often. I would say the best is to ask protocols that are using because they usually do pretty deep due diligence on the oracle they are choosing. So with each project, like Venus, Pandal, morpho that they are using Redstone, approximately one month was the due diligence with them before they decided to use us. So they already have a pretty good understanding. So it's best to ask the clients directly.
07:43:44.484 - 07:44:31.744, Speaker A: But there is no like, let's say, very good aggregator of oracle information yet. And the reason is because it's not that easy to create a simple table comparison, as I tried to do over here, because then always someone is hard one way or the other. Will you have a continuation of your galaxy campaign and will you have any other campaign with points? Whether we as Redstone will have points, right? We have points. Go to Redstone finance and click expedition. There is Redstone expedition and you can earn points already. Isn't it too late for that? No, it's season one, started last week. Okay, thank you.
07:44:31.744 - 07:44:41.744, Speaker A: Thank you so much. Thanks a lot, guys. And I think it's a wrap. Thank you. See you all tomorrow.
