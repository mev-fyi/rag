00:00:07.350 - 00:00:07.900, Speaker A: You.
00:00:10.030 - 00:00:29.210, Speaker B: Welcome everyone, to the restart of the A 16 Z Research seminar series. Very happy for our first talk this fall to introduce Sasha Spiegelman at Aptos, who I think will be enlightening us on recent advances in Dag based consensus. So, Sasha, all yours.
00:00:29.950 - 00:01:11.494, Speaker A: All right, so, yeah, thank you for the intro production and please feel free to stop me at any point for questions. I think it's much better if we do the questions through the presentation and not at the end. I don't want to lose anybody. Okay, cool. So I will talk today about Dags and how they meet, where they meet BFTS, and how we can use them to solve consensus very efficiently. Before I will jump into that, I just want just a single slide of how a single Shard architecture looks like. Usually we have these, I think, three main components that each blockchain has, which is a consensus, execution and storage.
00:01:11.494 - 00:01:54.860, Speaker A: We have some more, but these are the three main parts. And if we have a pipeline architecture, for example, like the one we have in Aptos, then your overall system performance will be as good as your weakest link. So in some other work we showed that we can execute 160K transaction per second by block STM. By the way, if you have ideas for next seminars, you can invite Rati to give a talk. I think he will be interested. And magically, we got the same exact number, 160K TPS with Novel and Task when we implemented our consensus. So there must be some law of physics here with this magic number.
00:01:54.860 - 00:02:52.986, Speaker A: Okay? And today I'm going to focus on consensus and show you our Dag based approach. So, this is a joint walk with George, Neil, Edit, Lafteris, Odet and Alberto. We were all on in Aptos. It was all started in Novi Research, I think George, Lafteris, Alberta and myself who were there. Neil and Odette were our interns and external collaboration with and yeah, this summarizes our work we did in the last two years. It's published in free venues, free conferences. So we started with All You Need is Dug, where we showed the foundations how we can use a Dag in order to implement Asynchronous atomic broadcast on top of the Dag, with zero overhead of communication, we achieved optimal resilience in terms of complexity.
00:02:52.986 - 00:03:32.090, Speaker A: It is post quantum safe and it's under one latency. This was theory of this approach, then Novel and Task, which was published in Eosis this year. We actually show how to build this dug very efficiently. And Bullshark is our latest protocol, which has the same property as All You Need is Dug in Asynchrony. But it also has a fast path that reduces the latency in the common case. And we also show how to practically garbage, collect, achieve fairness and chain quality. And it's important to note, I think, that this approach is currently implemented in several blockchains.
00:03:32.090 - 00:04:26.566, Speaker A: For example, aptos. We are implementing it celo. I think Mistan already integrated it into their code base and also Somalier. Okay, so what is a Dag? What do I mean when I say a dug? What is a Dug for me? So we assume a round based Dug which was first introduced by Aleph in Aft 19 and think of it as it's a structural Dug. So we have rounds and each round has at most N nodes, at most one node per validator. And an important property here, that is each node refers to N minus F nodes from the previous round. And I will explain in this talk how we build the Dag.
00:04:26.566 - 00:04:58.360, Speaker A: But first I want to introduce what is the Dag? What do I mean when I say a dug? So this is the Dag we are considering and each node also carries the information of the transactional information to one holder. So it's very symmetric protocol. Every validator just tries to add a node in the Dag. This node has either the transaction or some metadata information about these transactions. It needs to refer to N minus F nodes in the previous round. And this is the dag. Very simple.
00:04:58.360 - 00:05:44.434, Speaker A: And now we can divide our work into two. First novel is how we build this Dag very efficiently. Okay, so this is strowman way to build a Dag is to use reliable broadcast. So I have my node, I broadcast it, I include my transactions there. Then let's say in round one this is what I do. Then I wait for N minus F, deliver N minus F nodes by other validators and then I'm ready to move to the next round. I prepare a new node, I refer to this N minus F node from the previous round, I add the information about the new transactions and I broadcast this.
00:05:44.434 - 00:06:06.758, Speaker A: And this is how it works. This is like a strawman solution. But I'm going to talk a little bit about Nagar and how we do it. But this is just to give you intuition what the Dag is. Okay, so this was nalgar. This is like half part of the job of the work. The second part is the protocols that we run on top of Nargal.
00:06:06.758 - 00:06:16.830, Speaker A: And we started with Dug Rider. Then we have Task, which is actually a practical implementation of Dug Rider. And the latest is bullshark. And all these protocols, they take the Dag.
00:06:20.210 - 00:06:37.558, Speaker B: I just want to make sure your discussion around sort of the straw man reliable broadcast, I'm just trying to connect that to the picture on the slide. So should I think about every single validator is like making a block proposal and around and everybody's voting on everything or how do I connect those two?
00:06:37.724 - 00:06:59.526, Speaker A: Yeah, sure. So there is no vote processing here at all. So I'm a validator, I just prepare my node and I reliably broadcast it. And once you deliver it, you just add it to your Dag, to your local view of the Dag. That's all. Think about it. Like reliable broadcast is to guarantee non equivocation and to guarantee that everybody will eventually receive all the nodes.
00:06:59.526 - 00:07:17.550, Speaker A: But I can simply say I send the node to everybody and everybody get the node and add it to their local view of the Dug. That's all. Okay? Yeah. Okay. So this protocol is Dug rider task bullshark. We have the dug. Each one of us has a local view of the Dug.
00:07:17.550 - 00:08:16.338, Speaker A: It can be slightly different from each other because of the asynchronous nature of how we build the Dug, because some of us might deliver a node, while others may still don't deliver this node. Right? Everything is like asynchronous. And then the magic here is that we look at this Dug. The Dug provide causal order, but it doesn't provide total order. But we look at it and without sending any extra messages, no extra communication, we can just totally order all the node of the Dug and all the validators will achieve the same total order without any communication just by looking at the Dug. What we basically do in this approach, if you think about it from a high level, we have the problem of solving consensus, and we reduce it into two problems. So first we abstract away all communication, all the system aspect, everything, into just building this Dag as efficiently as possible, which is like a well defined thing that we want to build.
00:08:16.338 - 00:09:08.994, Speaker A: And it's much easier than build the full consensus. And then after we have this, then we run some local algorithm just to order these nodes of the Dag and get the full consensus that we want. And today I want to focus on bullshark. I'm going to focus on Bullshark today. And like I said before, bullshark has all the properties of Dagweider, but it also had a fast path to reduce latency in the common case in the synchronous periods. So this is the full point and this is also what is published in CCS. But at some point we realized that that might be very interesting and people might be interested in just the simple version of the partial synchronous version of bullshark without the complexity of the fallback and the asynchronous.
00:09:08.994 - 00:09:25.018, Speaker A: So we decided to provide a standalone protocol of just a partial synchronous version of bullshark. And this is what I'm going to present today, just the simple version, but maybe the most practical one in the.
00:09:25.024 - 00:09:44.850, Speaker B: Step where you go from the Dag to the total order. So I'm wondering how you should think about that. When different nodes have slightly when some of the blocks have been received by some, but not all nodes, is it that different nodes are going to apply this local thing? And it's guaranteed to be like a prefix, one is guaranteed to be a prefix of the other or just like a subsequence.
00:09:45.590 - 00:10:00.360, Speaker A: Yeah, exactly. One is guaranteed to be a prefix of the other. And once I deliver more nodes, I will increase my prefix, but always the prefixes will be always, like prefixes of each other. Okay, yeah, that makes sense.
00:10:01.530 - 00:10:15.610, Speaker B: I'm curious about what the practical implications of the complexity associated with Asynchrony versus partial synchrony are in this case, are there effects on latency or others that make the partial synchrony more appealing?
00:10:17.630 - 00:11:03.210, Speaker A: No, it's the same. In order to grasp the logic of how we interpret the Dag, the fallback, Dasynchronous fallback, has a lot of complexity, but in terms of practical implementation, it's the same. We build the Dag exactly in the same way, just how we look at the Dag and what we explained the partial synchronous version, and it's pretty simple. But in order to add the fallback, it's just a little bit more complex. But if you understand and if you're comfortable with it, you can implement it and you will get the same performance during synchrony and you will also have the Asynchronous fallback. Thank you. I think so far, everybody who implemented it were a bit scared of the details of the fallbacks.
00:11:03.210 - 00:11:38.230, Speaker A: But yeah, we enactos we are going to do the full bullshark. That's the plan. Okay. So, yeah, let me actually describe how we do it, how we order the Dag. So, as I said before, each of the validators here is an example, right? We have different validators. They might have slightly different views of the dhug, but the important property here, which we get from Liver Broadcast and we also get from our Nargo implementation, is the non Equivocation. What it means that it's fine.
00:11:38.230 - 00:12:26.260, Speaker A: Some of the validators, like in this example, validator four, might deliver a node that validator one haven't delivered yet. Okay? It will deliver it in the future, but not yet. However, if they both deliver the same node, the non Occupication guarantee that the nodes are exactly the same. It means same transaction, same information about transactions, same links to nodes in the previous rounds. And if we apply this logic recursively, so the nodes they refer to are also the same, but the same non activation property. So what we get is that if we deliver the same nodes, then the entire causal history of these nodes in both local views of the DAC is the same. It's exactly the same.
00:12:26.260 - 00:13:11.022, Speaker A: And this is an important property because we are going to imply deterministic rules to order the Dag. So the fact that we see the same causal histories will just imply that because the rule will be deterministic, that we will alter everything in the same order. Okay? So the way we are interpreting the Dag is the following. Each of the validators again has its local view, and then we divide the rounds into OD and even and in this example, I count from one and not from zero. So in every OD round we'll have a predefined N call. Now, encore. You can think about it as a leader.
00:13:11.022 - 00:13:51.546, Speaker A: I just prefer the N call name because they don't really do anything, they don't send messages, nothing. It's just the node in the dug. So I think encore is a better name, but it's the same idea. So each old round in the Dag will have a predefined encore and the nodes in even rounds. These are the votes and the goal is here. First I want to decide which encoders I'm going to commit and then hopefully all the Validators will commit the same nodes. This is the goal, how to commit the same encode.
00:13:51.546 - 00:14:33.470, Speaker A: And then we're just going to go one by one these committed encodes and apply some deterministic rule to totally all deal the causal histories of these nodes. So this is what we're going to do and this is how it works. So the commit rule is very simple. We commit an encore if it has F plus one votes. So in this example, we cannot commit encore a one because we have only one vote. Remember, a vote is a node that have a link to the encore. Okay? However, a two, the second encore is committed because we have three votes and three is bigger than F plus one.
00:14:33.470 - 00:15:13.834, Speaker A: And in this example, N is four and F is one. So we can commit a two. So the commit rule is very simple. Now again, each Validator has slightly different view of the dug, right? So it's possible that even though Validator One never not yet delivered the node in round two from Validator Four, validator Four already delivered this node. So Validator Four sees two votes on a one and therefore committed a one. So when we go back to Validator One, to his logic, we need to make sure that he will order a one before a two. Because this is what we want.
00:15:13.834 - 00:15:35.380, Speaker A: We want all the Validators to all the encoders in the same order. Okay? So this is the goal. And of course what we are going to use in quorum is quorum intersection. This is like the main trick in distributed computing, especially in consensus, right? So of course, quorum intersection. Here is how it works.
00:15:37.910 - 00:15:44.340, Speaker B: Yes, sorry, I didn't totally follow the right half of it. So why does Validator Four commit a one?
00:15:45.270 - 00:15:55.240, Speaker A: Because you can see that Validator Four has two votes on a one, right? The node of Validator fall and the node of Validator Two in one two.
00:15:55.770 - 00:15:59.162, Speaker B: Got you. That's the F plus one, not the N minus F. Got you.
00:15:59.216 - 00:16:32.770, Speaker A: Okay, that's the F plus one. Yes, exactly. Yes, that's exactly it. So the vote is F plus one. And in order to advance round, we need N minus F. So we'll have column intersection between this F plus one to commit and N minus F to advance. Okay? So exactly, with these two properties, what we get is that if an encode, some encode, A, say A is committed by some Validator, then all future encodes will have a path to at least one vote for A.
00:16:32.770 - 00:17:12.080, Speaker A: I will in a second demonstrate it with an example. But this is what we would get. And as a result, all future anchors will have a path to anchor A. So here is how it looks in an illustration. So a one was committed and now a two. It needs to refer to N minus F in this example, three nodes, right? This is how we advanced and build the dug. So in this example, no matter which of the nodes it will refer to, either the one from validator four or the one for validator two, it will refer to a vote to a one and there will be a path between a two to a one.
00:17:12.080 - 00:17:49.766, Speaker A: All right, this is exactly the quantum intersection, f plus one to commit and minus F to advance. That's the trick here. And as a result we get this colorway that the opposite. If there is no path from some future anchor A prime to A, then no honest validator committed A. And just 1 second in this node, I showed you an example from a one to a two. But you can imagine that the same applies for any future anchor, exactly the same quorum intersection. Okay, so this is the collary.
00:17:49.766 - 00:18:22.150, Speaker A: And now with this collary, I am ready to present you the full bullshark protocol on one slide. So this is how it works. This is a local view of some validators, say validator one. And now validator one sees the Dag and now he wants to totally order all the vertex, all the nodes. So it starts from NKL one. Can he commit NKL one? No, because there is only one vote and F plus one in this case is two. So he cannot commit anchor one, ankle two.
00:18:22.150 - 00:18:50.960, Speaker A: He has zero votes to NK two. He doesn't even have NK two in his local view of the dug, so he cannot commit NK two. However, anchor three can commit. It can commit anchor three votes more than F plus one. It can commit anchor three. Now what we need to do, we apply the color from before. What we do is we simply check whether there is a path between a three to a two.
00:18:50.960 - 00:19:20.646, Speaker A: In this case there is no path. So by the color, none of the validators committed a two, so it's safe to skip a two. We are not going to order a two. However, there is a path between a three to a one, which means that one of the validators might have committed a one. So to be safe, we decide to order a one before a three. And now after we do this, in this example, round one is the first round. But imagine it was some round and there was some rounds before.
00:19:20.646 - 00:20:18.330, Speaker A: So we need to reclusively, apply the same logic and try to see if there is a path between a one to a zero and then to a minus one until we reach anchor that we already previously order in the previous prefix that we order. And the intuition here is, remember I told you that if we all deliver a three anchor tree, that all the casual history, we all agree on the casual history because of the non equivocation. So if when we will go and apply this logic that I currently apply to by validator one, all the validators will apply exactly the same deterministic order. So they will agree on skipping a two and they will agree to order a one before a three. So it doesn't actually matter if a one was really committed by some of the validators or not. We will all order a one before a three. It's just important that if somebody might have committed it, that we absolutely must order a one before a three to be safe.
00:20:18.330 - 00:20:47.670, Speaker A: Now, the hard task is we did it, we agreed on the order the anchors. Now what we need to do is just to go one by one and commit the causal histories of these anchors. So in this example, round one is the first round. So a one doesn't have any causal history, so we don't need to alter anything before a one. But then we go to a three and apply some deterministic rule to causally order all the nodes in the causal history of a three.
00:20:47.820 - 00:21:00.726, Speaker B: Sasha, quick question. You talk about deciding on an ordering of the anchors, but is it the case that they're always ordered by round and you're really just deciding on the subset of anchors that you're going to include?
00:21:00.918 - 00:21:28.370, Speaker A: Yes, exactly. They are all dealt by round. I just need to decide which I order and which I don't order. I use the all deal to decide. But you got what I'm saying, right? You just decide which to take and which to skip. Yeah, exactly. Okay, so, yeah, that's the old Bullshark protocol, the partial synchronous version.
00:21:28.370 - 00:21:34.020, Speaker A: Any questions or can I move to the forward?
00:21:34.390 - 00:21:39.670, Speaker C: I have a quick question. So how did we pick the anchors and also when did we pick the anchors?
00:21:40.250 - 00:22:00.250, Speaker A: Yes. So the anchors are predefined. We need to adjust just similar to any other partially synchronous protocol in which the leaders are predefined. Right? So here as well, anchors are predefined. There is some mapping known to all mapping function between a round number to an anchor.
00:22:00.770 - 00:22:06.640, Speaker B: It's not unlike just choosing leaders in a rotating leader kind of protocol, right?
00:22:09.330 - 00:22:32.594, Speaker A: The easiest way to do is to just assume we have a mapping, but we don't have any randomness here. So in one way or another, we will need to somehow deterministically, appliolly, agree on it. We can try to use some information from the duck to do it. But still, it's no randomness, right? It will be determined.
00:22:32.722 - 00:22:39.610, Speaker C: Are the validator rewards different for whether or not you were an anchor or it's just unrelated?
00:22:40.110 - 00:23:11.380, Speaker A: I think that's a separate question. This is like an incentivized mechanism, right? How to incentivize the protocol? I don't have an answer to this. We can come up with different approaches. This is actually an interesting question. We haven't think about it yet. Here is just a protocol we assume N validators out of which Flesantine and the rest just follow the protocol. But yeah, of course at some point we will need to solve the incentives problem.
00:23:11.380 - 00:23:23.320, Speaker A: I think it's out of scope here but it's an interesting problem to look at.
00:23:24.090 - 00:23:31.434, Speaker B: So when you say order anchors, causal histories you literally just mean like look back in the Dag and do it lexicographically or something like that?
00:23:31.552 - 00:23:34.234, Speaker A: Yeah, okay, exactly. Yes.
00:23:34.432 - 00:23:46.320, Speaker B: And then you have enough consistency that any two different nodes that have committed to a three will both have the same kind of histories back, is that right?
00:23:46.690 - 00:24:02.820, Speaker A: Yeah. The non equivocation of the Dag guarantee that we see exactly the same cause of history. Cool. Okay, thank you for very good questions. I will move forward. Yeah. Okay, so let's talk a little bit about the properties, what we get here.
00:24:02.820 - 00:25:04.918, Speaker A: So first property that we get from free for free is chain quality because every round in the Dag has N minus F nodes out of which at least F plus one at most F from Byzantine. So at least F plus one are from honest. Then the chain quality that we get is at least F plus one divided by two F plus one which is more than half. And so more than half of the nodes are proposed by honest, by honest validators and this is actually proven to be optimal in asynchronous and partial about fairness. I also want to talk slightly about fairness and garbage collection. So what I mean by fairness is that if we have this Slow Validator in Australia that nobody ever waits for, he always came late to the party because everybody already moved forward, because everybody are closed. Then it's possible if we don't do anything, then we just move forward with the faster N minus F.
00:25:04.918 - 00:25:50.658, Speaker A: And it's possible that the Slow Validator will never be able to add this node to the Dag and never be able to add this transaction to the ledger. So in order to solve it, we introduce weak links and actually we introduced it in the first paper and all you need is Doug to solve the atomic broadcast protocol problem. So a weak link is only for this purpose. We don't use it for the consensus logic that I described above. These links are just to refer to nodes that otherwise wouldn't be included in the dug. So when we go and order the causal histories of the anchor we also order the nodes that are referred by weak links. That's a very easy Tweak to guarantee fairness.
00:25:50.658 - 00:26:38.550, Speaker A: But the problem with this Tweak is that in Asynchronous during Asynchronous period, if we also want garbage collection and we do want garbage collection, because otherwise the protocol is not practical, then we cannot provide this. We cannot keep infinite dug, right? Because we need to garbage collect. It's actually easy to see that we cannot have this mechanism, this perfect fairness together with garbage collection. So what we decided to do in Bullshark is that we provide garbage collection always. We always garbage collect the Dag. We keep the memory bounded, but we guarantee fairness during synchronous periods. So we keep the weak links for a while.
00:26:38.550 - 00:27:24.670, Speaker A: And then if you are just fast enough, if you are not too slow and the period, then you will be added to the dug. Otherwise we garbage collect you. So this is the sweet spot that we found fairness during synchrony and garbage and always garbage collect. And yeah, about garbage collection. So, this is another advantage of the round based dug. If you familiar with the earlier work on dug based consensus, like Hashgraph, one of the reason they weren't, I think, widely adopted is because they have this garbage collection problem. Any node could refer to any node in the past and then it wasn't clear when you can garbage collect the graph.
00:27:24.670 - 00:28:02.250, Speaker A: So here everything is round based and together with consensus, we can just simply agree starting, we garbage collect everything below some round. And because we have consensus, we all agree on it and we all do the same. So it's safe. Very roughly, I can explain. So every node, in order to garbage collect, we'll add a timestamp, a validator. Besides all the other information, we'll also add a timestamp. And then when we come and order the causal history of anchor, then we will compute the timestamp for a round.
00:28:02.250 - 00:28:39.020, Speaker A: And we do it by taking the medium of all the nodes. This is a standard trick to make sure that the timestamp comes from an honest party. And then every round will have a timestamp. The leader will also have a timestamp from its parents. And then we just decide, okay, we will garbage collect all the rounds with timestamp which are far more than delta from the encore's timestamp. And that's all very simple mechanism. And this delta will also determine how fast this slow validator needs to be in order not to be garbage collected and included in the total order.
00:28:39.020 - 00:29:15.510, Speaker A: Okay? So just to conclude, Bullshark, I hope I convince you that this is extremely simple protocol. It has no view change, no view synchronization, none of this mechanism that add external complexity. It is only one type of message that broadcast the node. And that's all. It's little less. So it allow for perfect load balancing and network utilization while building the Dag. And it provides chain quality for free and fairness during synchrony and garbage collection.
00:29:15.510 - 00:29:24.860, Speaker A: And if there are no questions, I will move forward to novel, but I will pause for a second, just to check.
00:29:26.510 - 00:29:31.550, Speaker B: Sasha, what did you mean by fairness exactly? Was that similar to chain quality or what does that mean?
00:29:31.700 - 00:30:02.470, Speaker A: Yeah, so chain quality means that chain quality is a safety protocol, right? So by the definition of safety liveness protocol, so we check how many nodes out of the total number of nodes were proposed by honest validators. This is chain quality. Fairness means that all the validators will have a chance to be added to the dub. So if I'm a slow validator, I will be added. It's not like I will not be ignored.
00:30:04.170 - 00:30:09.110, Speaker B: And fairness, that's an optimistic property or that's sort of guaranteed.
00:30:09.190 - 00:30:33.686, Speaker A: I missed that. Yeah, so sorry. Yes. So probably I haven't explained it well. We can guarantee fairness always and then satisfy the atomic broadcast protocol. But then we will need to able to sacrifice garbage collection because we will not be able to garbage collect the dug in the worst case during Asynchrony. So we don't want to sacrifice garbage collection.
00:30:33.686 - 00:30:39.890, Speaker A: So we slightly sacrifice fairness. So we provide fairness only during synchronous periods.
00:30:40.710 - 00:30:42.340, Speaker B: Gotcha. Okay, thanks.
00:30:45.430 - 00:31:20.826, Speaker A: Okay, so I will move to Nalvel. As I said before, Narvel is the system. So far, I explained to you, given a Dag, how I totally order it. Now I will explain to you how we efficiently build this Dag. This is using novel. Okay. So if we look at current designs, so monolithic protocols like hot stuff like PBFT and others, they are sharing the transaction data as part of the consensus and try to optimize the overall message, complexity of the consensus protocol.
00:31:20.826 - 00:32:00.320, Speaker A: What it means that if we look at the typical leader based protocol that we have, the leader, the leader sends the data to all the other replicas. The replicas just sign something and send it back to the leader. And then the leader will again send another data to the other validators. The validator will sign, send it back to the leader. And what we have here, if we check the resource utilization, is that the leader works very hard. He needs to keep broadcasting the data while the other validators are almost doing nothing. They'll just sign and wait for the leader to send the next data.
00:32:00.320 - 00:32:49.946, Speaker A: So clearly, the leader is the bottleneck. And the key observation that we do in Novel is that in order to scale BFT system or any consensus system, it doesn't have to be BFT. We need to decouple data dissemination from the metadata ordering. So, on a high level, the Dag that we are building does not contain the actual data. It contains the metadata, what we call proof of availability. So what we do in a high level is that the validators, they stream the data outside of this process of building, the DAC just stream raw data to each other and then they will obtain proof of availability. Right? If I stream your data, I stream data to everybody.
00:32:49.946 - 00:33:35.320, Speaker A: And then the validators, they get the data, they persist it, and then they send me back a message with a. Signature saying that I have the data, I promised to stole this data. Now I collect a column of these signatures and this column is what we call proof of availability. And if I take this column of signature and I show it to some other validators, the validator, even though he locally doesn't have the data, he can be assured that he will be able to get the data later because enough honest validators store the data. Question, okay. And then we take this proof availability and we build the Dag with them. So with the couple, the data dissemination from the metadata all the way.
00:33:35.320 - 00:34:00.322, Speaker A: So this is the key observation and this is how it works in Novel. This is the architecture. So each node in novel has one primary and a number of workers. And the workers, they can be processes on the same machine. But we can also think of a cluster and then a primary as a machine. And each worker is a machine. This is a perfect architecture to scale out.
00:34:00.322 - 00:34:26.758, Speaker A: We can throw in more machines and we can be able to scale out because the expensive part is just to stream the data. So we can just add more workers, more machine to stream the data. Ordering the metadata is actually very cheap. So this is how it works. The wall kills. They get transactions from clients, stream of transaction and then they stream them in batches to the other validators. Just workers, talk directly to workers.
00:34:26.758 - 00:34:57.090, Speaker A: Then they will take digest of these batches and pass it to the primary. And the goal of the primary is to build the dug of these digests. And the way it works is the following. This is the primary protocol. So the primary prepares a block header. The block header has references to the previous nodes in the Dag as before. But instead of having this transaction, it has the digest.
00:34:57.090 - 00:35:34.386, Speaker A: Now the primary sends the headers to all other validators. The validators make sure they persist the data behind the digest, just sign it, send it back to the validator. The validator aggregate it into a certificate and send the certificate back to all the validators. Now, all this mechanism, this is one round on the abstract dug that I used to describe you, the protocol before. This is one round. Yeah. And this is basically the high level idea of how novel works.
00:35:34.386 - 00:35:40.820, Speaker A: And yeah, next I want to talk about liveness. So I just pause again to see if we have some questions about.
00:35:45.350 - 00:35:52.150, Speaker B: It. Does it change sort of the trust assumptions somehow you have sort of extra data availability type trust assumptions?
00:35:53.450 - 00:36:25.060, Speaker A: No, it's the same trust assumption. We have N validators F out of which can be Byzantine. The proof of availability is two F plus one signatures. Each signature is a promise that I stole the data out of which F is actually honest. F plus one is actually honest. So this F plus one are actually stalling the data. So later I can ask a column and some honest validator will give me the data.
00:36:26.630 - 00:36:32.180, Speaker B: Okay. It's not like every node has to store every single thing, is that right?
00:36:32.710 - 00:36:38.120, Speaker A: No, you just store whatever you promise to store. You get the sign, you store it.
00:36:38.730 - 00:36:42.360, Speaker B: So there'd be maybe a constant factor savings there or something.
00:36:43.770 - 00:37:20.882, Speaker A: Yes. And of course, when we implement it, we need to garbage collect it. I will not store it forever, but I think this is too low level details of how we implement the garbage collect there, but yeah, there is some details of how to garbage collect. Thanks. Okay, so now I want to talk slightly about liveness. So in the Asynchronous versions, which is the Dagwider task, the Asynchronous version of bullshark, they are Asynchronous protocols. And here, remember you asked me where I take the encoders from.
00:37:20.882 - 00:38:07.300, Speaker A: So in the Asynchronous version, they are actually not predefined. We actually use randomness to elect them. And the way it works is that we use threshold signatures and each node of the Dug, the validator can simply sign the round number. And then once I have N minus F nodes in the Dug, I can collect this signature to a threshold signature. I can hash it and guarantee that the threshold crypto guarantees that it doesn't matter which of the nodes I deliver. We will all have the same signature and the same hash, and then we will elect the same. And then if you are familiar with Laba, then it's kind of the same argument, but I'm not going to go into that.
00:38:07.300 - 00:38:41.200, Speaker A: What I do want to talk is about the partial synchronous likeness of the partial synchronous version of bullshark. There we don't have randomness. This is deterministic protocol. We need the predefined leaders because it's a partial synchronous liveness and because we know there is the FLP results. And so, of course, we have to use timeouts somewhere in the system. We cannot get away with it. This is an impossibility result.
00:38:41.200 - 00:39:26.698, Speaker A: And we considered two options and we evaluated two options of how to use these timeouts. And what we ended up doing is using the timeouts directly on constructing the Dag. So I told you before that when I advance to a new round, I first need to deliver N minus F node. In the current round, this is the same, but on top of it we need to add something. So if one of these nodes that I deliver is the ankle, then I can immediately go and continue and continue to move to the next round. However, if I didn't deliver yet the ankle, then I will wait for a timeout to see if I will deliver this ankle for a timeout. And when I deliver it, I can immediately move forward.
00:39:26.698 - 00:40:10.550, Speaker A: Otherwise I just move forward when the timeout expires. So this is what we ended up doing. And what we compare to is we actually thought of separating the physical dug from the logical dug. Intuitively, when we thought about it, we thought it is a very neat and very clever idea because we will be able to build the physical dug same network speed, never wait for timeouts. And then just to piggyback the virtual dug on top of the physical dug. So each node will have one extra bit saying am I belong to the virtual dug or not? And then the same logic we can implement on the virtual dug. Once in a while I say, okay, this is the virtual dug.
00:40:10.550 - 00:41:13.182, Speaker A: The consensus that I explained before, we'll use the virtual dug to do it, everything else is the same. It's just we're piggybacking the virtual dug on top of the physical dug. So yeah, as I said before, we thought it's a good idea, it's a clever idea, but when we implemented and compared, then we saw it's actually a bad idea, but probably we couldn't think about it before we implemented. The reason is the following. We separate the data dissemination from the oldering. So even though we might slow down slightly on the construction of the duct, because once in a while we wait for the timeout the data we keep streaming in network speed, the data is orthogonal to building the metadata. So the only difference here is that if to build the duck slightly slower, then every node will have slightly more proof availability, slightly more metadata, but the data behind it will be the same.
00:41:13.182 - 00:42:08.874, Speaker A: So the actual amount of data that we all there will still be the same. So we are not losing throughput. But on the other hand, when we use this virtual dug on top of the physical dug, so say I missed the physical node by a little, my timeout expires like one millisecond after already I broadcast the physical dug. So now I need to wait for the next round in order to piggyback my logical node and I lose this latency. This is one and the second. In general, what the insight that we learned from implementing it is that building the Dag too fast is not good. We actually want to back pressure a little bit the Dag, because then the data structure are smaller, the in memory, memory is smaller, then less thread in the system are busy, less networking.
00:42:08.874 - 00:42:21.134, Speaker A: So it's actually good to back pressure it a little bit. Okay, this was a note about Liveness and now I want to talk a little bit about implementation and show you some graphs.
00:42:21.262 - 00:42:22.226, Speaker B: Quick question.
00:42:22.408 - 00:42:23.540, Speaker A: Yes, sure.
00:42:25.030 - 00:42:34.390, Speaker B: Like this vava kind of trick that you use in the Asynchronous case. Should I think of that as just like a theoretical interest or is that also kind of practically relevant?
00:42:37.230 - 00:43:28.806, Speaker A: It is practically relevant because this is how the Asynchronous fallback works. Because everything is local, right? So it doesn't have any practical complexity, just the local algorithm that you use. If you ask, I can describe in one and two sentence. So the trick is that the randomness we apply from randomness, we use the round four for randomness to elect and anchor around one. So this is already after the adversary already committed to the structure of the Dug. So this is some simple combinatorial argument. We saw that before we choose the randomness, there are N minus F potential nodes in round one, that if we elect them, the commit rule will be satisfied.
00:43:28.806 - 00:44:18.710, Speaker A: And it doesn't matter, no matter what the adversary do, there will be this N minus F nodes. The adversary can choose what N minus F nodes by choosing the order of the delivery. But no matter what it does, there will be these N minus F nodes that will satisfy the commit rule. So first we wait for the adversary to commit on some set and then we apply the randomness. So with probability two over three, we will elect anchor which is in this set and then the commit rule will be satisfied and we will be able to commit in older one rounds. That's the varba trick. And it really doesn't matter because it's all local, we don't send any messages, it's just the way we interpret the Dug.
00:44:19.290 - 00:44:22.742, Speaker B: So you guys are going to implement this because why not? Is that what I'm hearing?
00:44:22.886 - 00:44:40.400, Speaker A: Yeah, exactly. Why not? The system is the same. It's instead of like 200 lines of code on top of naval, we will have 300 lines of code on top of nullwell. So why not? Yeah, okay.
00:44:41.570 - 00:44:55.010, Speaker C: Is the threshold signature overhead negligible, I guess. How often do we have to do extra overhead to have this randomness for anchor selection?
00:44:56.170 - 00:45:53.026, Speaker A: Yeah, that is actually a very good question because every time we change the validator set, we'll need to do a new setup and we have some ideas of how to do it, but we didn't implemented it yet. So this is actually team for your question as well, if we think it's going to work, but if it doesn't going to work, so we will not be able to do the Asynchronous fallback. This was a very good question, but we think we know how to do it to rotate the keys every time we reconfigure. And if it's going to work, then we will use the Asynchronous version, otherwise we will not be able to thanks for the question, I actually forgot about it. Yeah, thanks. Okay, let's talk a little bit about implementation. So, we written everything in Rust.
00:45:53.026 - 00:46:32.206, Speaker A: We are using a real networking, real storage, real cryptography, and you can find everything in Alberto's GitHub. We use geo distributed setups on AWS. And this is a latency throughput graph. Now, for each of you that is not familiar with the latency throughput, this is how we measure the peak performance, the peak throughput. So we keep increasing the load and then we keep increasing the throughput until the latency explodes. Like this is the me point. This is the point we cannot increase the load anymore.
00:46:32.206 - 00:47:14.510, Speaker A: And this is actually the point that we're interested in. This knee points is actually this is the peak performance of the protocol. So we compared Task and Bullshark to two versions of hot stuff. First, the vanilla Hot Stuff here we can see the throughput is 18,000 transactions per second. Then we actually implemented our version of Hot Stuff improved version, where we decoupled data dissemination from the metadata. And then the Hot Stuff only proposed digest of the data that we stream. And then we improve performance by a lot, but still not as good as Task and Bullshark.
00:47:14.510 - 00:47:53.898, Speaker A: And we can see that Task has slightly better throughput, but Bullshark improved latency because less rounds. And we also evaluated the performance under faults. And here we can see that Hot Stuff, even our improved implementation of Hot Stuff suffers while Tusk and Bullshark are still performing pretty well. Yeah. And just to conclude this talk so everything I described today is available. The papers are available if you want to read an extended version, extended summary of the papers. I have a blog post in Decentralized Votes.
00:47:53.898 - 00:48:07.634, Speaker A: I want to use this opportunity. I think ita is on the call. I want to say that this blog is amazing and you can check the blog post there. So yeah, that's it. Thank you again. Thank you very much for having me today.
00:48:07.832 - 00:48:35.674, Speaker B: Thanks, Sasha. So Andy Lewis Pie, who couldn't make the talk, but he sent me a question kind of asynchronously he was wondering about transaction repetition. So he was saying how as a user, you might want to broadcast your transaction to F plus one different servers to avoid sensor resistance, in which case it seems like maybe you get F copies of the transaction embedded in the Dag. And how do you guys think about that?
00:48:35.872 - 00:49:05.640, Speaker A: Yeah, that's again, another very good point. So in this work, it was out of scope. We did not address this problem in none of these papers. But we do have some thoughts of how to do it, how to solve this duplication. So one idea we think is like content addressable storage. So we will not store everything. We will just store, just think that we need.
00:49:05.640 - 00:49:14.440, Speaker A: But it's not really baked idea yet. It's yet another thing that we need to solve. That's a very good point.
00:49:15.610 - 00:49:17.910, Speaker B: All right, sounds like me. Go ahead.
00:49:17.980 - 00:49:33.390, Speaker C: It's a good question. Do you feel like eventually all the project is going to move to this Dag based consensus since it's faster, gets better properties, or leader based consensus still have some advantages?
00:49:34.770 - 00:50:21.594, Speaker A: I'm not objective, right, but I think this is strictly better. I just want to answer to Lera's question. One point that we need to remember when we implement it that complicated a little bit. The implementation which we don't need in a little base thing is that we need to implement an abstraction of reliable communication, meaning that in order to advance rounds, I need to receive N minus F nodes. So there is no notion of timeouts like in a leader based if I have a problem in the networking and the leader was good, but just the messages were dropped because some disconnection in the TCP, right. No harm is done. Right? We will time out.
00:50:21.594 - 00:50:45.220, Speaker A: We'll move to the next reader. We'll try again. But here, if we need to implement some reliable communication on top of the TCP like application level, because if we are not getting nodes from the previous round, we are stuck. There is no notion of moving forward. Fine. Again, we really need to get them eventually. So this is one thing in implementation that we need to take into account implementing it.
00:50:45.670 - 00:50:47.506, Speaker B: All right, well, Sasha, this was great. Thank you.
00:50:47.528 - 00:50:50.960, Speaker A: So thank you. Thank you for having me.
