00:00:10.240 - 00:00:36.030, Speaker A: Welcome everybody. Really excited to have Abdulai and Jai from NYU NYU Stern, formerly my upstairs neighbor and now my down the street neighbor, sadly, but still close by. But Abdul's background actually is a macroeconomist. But he's shifted the last couple years into doing a lot of work on crypto and today he's going to talk about gas pricing. Take it away.
00:00:36.480 - 00:00:58.050, Speaker B: Cool. Thanks for having me. It's great. Always great to come here for the summer chat with everyone. So I will say it because some people said they like the title All About Gas. So in this talk, I'm going to do two things. I'm going to talk about a paper, working paper is on my website called Blockchain Price versus Quantity Control.
00:00:58.050 - 00:01:50.980, Speaker B: So this paper emerged from some of the ideas that I presented last year. So as Joe said, I started working on crypto. My ideas are ahead of my writing speed and this schedule. But in the second part, I will talk about some new ideas as well. Thinking of different ways of looking at pricing on the blockchain, whether we're thinking of consensus, which will be just block space pricing or computation where you look at either sustained durable resources versus like storage versus non durable resources. Or when we think of execution, like when we start thinking of at the moment of execution, whether the local fee markets okay, so to bring some ideas there. So this would be our talk.
00:01:50.980 - 00:03:01.384, Speaker B: Depending on question that we have, we'll go in more detail in each piece. Overall, let's look at what I call the economics of transaction fees to set the agenda in a sense of where we are going, where the paper I mentioned fits. So there is work on transaction fee mechanisms. So this is the approach that is more game theoretical, more micro in nature, where starting from, let's say credibility, thinking of credibility, tim here has think of minor myopic incentives and off chain agreements and some other people have worked as well and others on posted price mechanisms. Really what we take here is a complementary approach, meaning that I'm not going to be focusing much on whether people are going to be biding their truthful valuation. I'm going to take these game theoretic properties for given and I'm going to start and talk about what it implies for transaction fee mechanisms from a more macro perspective, which is what I call here the market for block space. There are other papers that fit in the paper, more fits in this line.
00:03:01.384 - 00:03:48.004, Speaker B: It's looking at the market. So I'm going to start from, of course, a micro model with users having valuations, and from it I'm going to generate and think of the market for block space. So this vitalik has set an agenda around this, some sort a lot of things. I'm going to be doing some of the ideas he had it. We're going to formalize them, test them and look at implications for transaction fee mechanisms. Some papers working on looking at monopolistic pricing. What if we think of the validator being monopolists and to price block space in that sense, and multiple resources as well? And then finally, there is also other papers that look at, for example, of ethereum, since it's widely used, the dynamics of ethereum specific transaction fee mechanism, EIP.
00:03:48.004 - 00:04:04.220, Speaker B: 1559, which we're going to talk about as well. So here the contribution you will see is that this work kind of gives a lot of theoretical justification for some of the empirical results they find and some planned upgrades.
00:04:04.880 - 00:04:05.676, Speaker C: Okay?
00:04:05.858 - 00:04:43.708, Speaker B: So let's start with this. Today I'm going to be more voluminous because last time I kind of explained some of the economic intuition behind a lot of things. So I'm going to start with a model of what I think of validators. So validators, we're not going to be concerned with what consensus protocol be used for our purpose. It would just be simply they take resources that are priced at prices that are given per unit, right? So this is the, let's say the gas price, this is the gas amount excise. Or these are different resources. You can interpret them as different resources, compute bandwidth and so on.
00:04:43.708 - 00:04:53.096, Speaker B: And then this will create a total gas. Okay, and there is a max gas limit, right, because of technical considerations.
00:04:53.288 - 00:04:53.692, Speaker C: Okay.
00:04:53.746 - 00:05:11.908, Speaker B: So that's what the validators do. Now, to do this, of course, they incur costs. So these are technological costs for the validator when producing a block, okay. These encompass operation costs, other costs associating with assessing and modifying the blockchain state.
00:05:12.074 - 00:05:16.564, Speaker D: Abdul yeah. So you're immediately starting with a multidimensional case.
00:05:16.762 - 00:05:28.612, Speaker B: No. So I'm going to take this for given, right? I'm going to talk about this. First part is purely the one dimensional case. I'm going to focus on quantity production.
00:05:28.756 - 00:05:31.640, Speaker D: What is x indexed by oh, so.
00:05:31.710 - 00:05:36.568, Speaker B: This is different n one to N.
00:05:36.734 - 00:05:39.268, Speaker D: Yeah, but what is X?
00:05:39.454 - 00:06:05.012, Speaker B: So it's different resources. You have like multidimensional. I'm going to take the price for given. What all this will do is just give me a marginal cost function to think of the marginal cost of creating block space. So, yeah, so there is the multiple dimension here, but I'm going to take the prices as given. So when I look at a multiple dimension, I'm going to optimize with this choice of the prices itself. So here, I'm just going to take them as given.
00:06:05.012 - 00:06:13.348, Speaker B: These are the Guai storages, 20,000, et cetera. I'm just going to take them for given. Yes.
00:06:13.434 - 00:06:17.236, Speaker D: Is Q like the cost per block or how should we think about Q?
00:06:17.338 - 00:06:24.476, Speaker B: No, Q is the block size, right? So this is the total, q is the block size.
00:06:24.578 - 00:06:26.476, Speaker C: But you're multiplying a price by a.
00:06:26.498 - 00:07:03.210, Speaker B: Bundle to get a size. Yeah, so that's exactly so it's like you pay. Think about it. This is the unit, the unit that people are paying. You have XIs, are the different opcodes in the example of ethereum, the unit prices, of course there is a different price on there, which is the unit price, but that's the multiplier of this Q. Okay, but we're going to think of this Q has a maximum size that we'll take.
00:07:04.460 - 00:07:13.788, Speaker D: I can actually think of the I's as being opcodes. Yeah, p being that the opcode cost hard coded the ethereum protocol and x the number of times that opcode is used.
00:07:13.874 - 00:07:54.720, Speaker B: Yes, exactly. But from the blockchain designer perspective, this cost can be different from the validator because you have delay in block propagation and let's say cost of centralization, et cetera. So let's say there is some uncertainty on this cost, right. And I'm going to show you how this uncertainty, what it will make us, we'll think of it as uncertainty in supply, in a sense from some distribution ETA. So now let's talk about users. And this is where I think we kind of bridge the gap. So I'm going to consider important.
00:07:54.720 - 00:08:02.830, Speaker B: So atomistic users. So my goal is from users to kind of lead to a marker foundation on what would be a demand for block space.
00:08:03.280 - 00:08:03.644, Speaker C: Okay?
00:08:03.682 - 00:08:45.716, Speaker B: So I'm going to have users denoted by J. They arrive at random between two blocks from a personal process with parameter, so lambda times Q max. So think of lambda as the intensity of demand. If lambda is greater than one, that means not all transactions can be included in the block, right? So demand is high. And if lambda is less than one, it means in principle, all transactions can be included. Users leave the pool if transaction is not included in a NOx block, only to return. So I abstract from the mem pool, piling up with pending transactions.
00:08:45.716 - 00:09:23.412, Speaker B: I'm going to talk about that when we go into quantitative. So Nissan has written a paper thinking of this and found that the dynamics of these transaction fees are pretty similar to some of the papers I'm going to cite in the quantitative part. So I'm going to abstract from that. I'm just going to assume every period I just have these new users coming in from the same user valuation. And again, also I'm going to assume that the bids are just the true valuations. Like I said, from there, just from the distribution of user valuations, I will know how many people will be willing to pay a certain amount p. Okay.
00:09:23.412 - 00:09:45.884, Speaker B: It will be just the survival property, the survival rate. So the CDF is the cumulative distribution, above p is the survival. So one minus F. Okay? So this is to give us a microfoundation. Now, when I'm going to think of demand uncertainty, I can think of this lambda being higher, lower and so on.
00:09:46.002 - 00:09:46.332, Speaker C: Okay.
00:09:46.386 - 00:09:50.920, Speaker B: Which is the motivation for thinking of price versus quantity controls.
00:09:51.080 - 00:09:51.692, Speaker C: Okay.
00:09:51.826 - 00:11:04.080, Speaker B: So the first lemma to kind of lead to a demand curve is the following, and it's just a simple this will be handy to think of is from this user model you can create a demand curve. There is a demand curve for block space, which is that the great demand for block space is just the price have this relationship decreasing with quantity, the price for amount of block space. So price will be that cutoff limit of the lowest person who's bid, who's entering and then the price elasticity of demand. So this notion that economists talk about, so what is the price elasticity of demand? It is just if I increase prices by 1%, by how much does demand decrease? Okay, so that is the price elastic demand. So this is just this tail ratio, this statistic of the distribution of user valuations. Now this statistic will be interesting because for example, if I assume F to be paratroid distribution, I'll use this and in fact it's not a bad assumption. You find that you have this simple form, okay, this is handy.
00:11:04.080 - 00:11:19.480, Speaker B: Why? Because the simple model where you have prices, how they decrease with quantities and this price elasticity of demand will be just the pareto coefficient of this pareto distribution.
00:11:19.900 - 00:11:20.360, Speaker C: Okay?
00:11:20.430 - 00:11:29.224, Speaker B: And then we're going to have this here what I'll call the demand shifter, which is proportional to lambda times q max.
00:11:29.272 - 00:11:29.484, Speaker C: Okay?
00:11:29.522 - 00:13:00.104, Speaker B: So remember, lambda was high, we are in high demand, low, we are in low demand case, okay? So now we're going to think of this scenario, right, where you're going to have uncertainty in, let's say supply, the cost to validators and uncertainty in demand people can have, you can have high intensity where lambda is very high or lambda very low, which leads to uncertainty in this demand shifter. So now in this situation, imagine you are a blockchain designer and you think of what do I want to do? Do I want to set a strict block size limit? It's just going to be fixed, there will be a block size of 15 million or do I want to do the opposite? Let relax that and let's say just fix the price so that the block size can adjust whether we are in high demand or low in demand situations. So that's what we're going to look at. Now to go that we need to define an objective, okay? And this is kind of where some of the main challenges is blockchains is a very specific example. In a very specific scenario when you look at people have thought of this idea of price versus quantity controls in environmental regulation and so on. But you need to be specific when it comes to blockchains and what is objective. So now if you look at, let's say the protocol, if you were defined as the protocol profit, okay, so you're going to have the block reward the expectation of all the fees collected minus the cost.
00:13:00.104 - 00:13:33.810, Speaker B: Of course, these can be diverted for many different purposes, we know for incentive compatibility reasons. So this is not what is going to be optimized in a sense, okay? Because some of these fees, they can be diverted by the protocol and so on. But I argue that one objective we can consider is one in which the blockchain designer will trade off. Will trade off what? So we're going to take this block reward r forgiven, we'll trade off. What would a monopolistic validator do?
00:13:34.180 - 00:13:34.604, Speaker C: Okay?
00:13:34.662 - 00:14:13.900, Speaker B: What they would be optimizing, which would be here, these excess profits and some objective here. And I will give you some example. This can be technological consideration, this can be for welfare and so on. So Beta here, you can think about it as the monopoly, as the bargaining power of validators. For example, if you were in a where you're trying to do a monopolistic validator transaction, choose choice of price versus quantity controls, this would be a beta is equal to one. You have no considerations here. If you wanted to just have, let's say this consideration s that I'm going to give you some examples.
00:14:13.900 - 00:14:48.620, Speaker B: You're going to have beta is zero and you're going to consider this, okay? So Beta captures the bargaining power of validators, okay? Let me give you some ETA is uncertainty on the marginal cost of the validators. So this can be because it can be, let's say think of proof of work or proof of work. There is uncertainty on the price of the assets for them to do some operations.
00:14:50.240 - 00:14:52.700, Speaker D: So the expectation is over Ada.
00:14:54.480 - 00:14:57.840, Speaker B: And over PSI, the demand uncertainty.
00:14:58.980 - 00:14:59.440, Speaker C: Okay?
00:14:59.510 - 00:15:01.420, Speaker D: Because which was lambda?
00:15:01.500 - 00:15:07.250, Speaker B: That was your lambda PSI, yeah, lambda. Yes, lambda leading to yeah, exactly.
00:15:08.280 - 00:15:08.692, Speaker C: Okay.
00:15:08.746 - 00:15:45.272, Speaker B: So now let's look at those uncertainties and let me give these examples why? To convince why it is reasonable and it's not just so, for example, you can think of a blockchain designer that has a concern for welfare. So they maximize here. So they care about the utility of representative user. So the U of Q here, we can consider a technological block size target. So that is typically, if you think of the case of ethereum, that's maybe what motivates them. They tend to say there is this target of 50 milli you want to hit. So let's say you can have a loss function around that target.
00:15:45.272 - 00:15:57.400, Speaker B: It can even be a dirac that we only want to hit this target. Or it can be somewhere in between where you have normal distribution and you narrow it down to be closer to this target.
00:15:57.580 - 00:15:58.310, Speaker C: Okay?
00:15:58.840 - 00:16:08.100, Speaker B: Or it can just be validator profits. If you wanted to think of a monopolistic maximizing, the monopolistic validators.
00:16:08.920 - 00:16:09.670, Speaker C: Okay?
00:16:10.620 - 00:16:42.320, Speaker B: So now what is the problem? So Tim was asking this, which is the following. So we can consider two things, right? Because there is uncertainty and the designer want to have some way to control things. Let's look at the two things. The general problem would be just designing a supply curve. But that's complicated. Let's just look at these two extremes, price versus quantity controls. Because we see that in practice, that's what happens.
00:16:42.320 - 00:16:48.204, Speaker B: The first one will be if you were to set prices. So the quantities yes.
00:16:48.262 - 00:16:49.216, Speaker C: What's gamma?
00:16:49.328 - 00:17:20.972, Speaker B: So it's just the marginal cost. So here, like I had before, so this is just when you set the price, you fix the price. What happens now is that the quantities adjust. That's the difference with price control, quantities adjust. So they can be high and high demand, low and low demand through matching the demand curve here. So I'm going to consider a situation where I think I have it here, I consider situation. Yeah.
00:17:20.972 - 00:17:43.268, Speaker B: So S in general, so this is S of the quantity that adjusts. Okay, and what was the cost? So I have prices times the quantities. So this sigma here is a marginal cost. So think about a situation where I have a constant return to scale cost function. So this is just the marginal cost. But I'm going to think of now uncertainty as uncertainty in this marginal cost. That's the main idea.
00:17:43.268 - 00:18:02.810, Speaker B: So this is the problem of setting the price change price, which is let's have price adjust. So let's say do something like IP 59, let's adjust the price. Now, if on the other hand, the validator said, okay, I'm going to set there is a maximum block size, I'm not going to hit.
00:18:03.120 - 00:18:03.870, Speaker C: Okay?
00:18:05.040 - 00:18:27.330, Speaker B: Now, if I fix a quantity, the price would be on the demand curve. And choosing that quantity will just be maximizing this object here. So I'm going to have the price from the quantity. This is the demand, the uncertainty in demand, how it affects it and the marginal cost. Yes.
00:18:28.680 - 00:18:33.808, Speaker D: What role is the uncertainty playing here? Like if there was lambda and ada.
00:18:33.824 - 00:19:14.160, Speaker B: Were fixed, we're fixed. They are equivalent. Price versus quantity control, they are equivalent. So the really idea is so if you have a lot of uncertainty in demand, so the idea is if you have a lot of uncertainty in demand, more uncertainty in demand and less uncertainty in supply. Having prices adjust is very useful because then quantity can adjust a lot and match demand and uncertainty versus if you had more uncertainty in the supply curve, let's say, imagine it gets very costly if the block size becomes too large, then you want to keep the quantity fixed.
00:19:14.980 - 00:19:15.584, Speaker C: Okay?
00:19:15.702 - 00:19:16.704, Speaker B: So that's what we're going to find.
00:19:16.742 - 00:19:22.336, Speaker D: So when you say it would be equivalent without uncertainty, without uncertainty, choosing prices.
00:19:22.368 - 00:19:24.948, Speaker B: Or quantities would be the same, you.
00:19:24.954 - 00:19:26.992, Speaker D: Mean you solve for optimality, you get the same solution?
00:19:27.056 - 00:19:38.170, Speaker B: Yeah, the same sole solution. They are indifferent. And this is kind of another rationale behind. That's why I'm saying very complementary to thinking of these price adjustments, this situation.
00:19:39.500 - 00:19:39.912, Speaker C: Okay?
00:19:39.966 - 00:20:33.770, Speaker B: So now let me look at when you look optimize these, which one, what is more important, the value from adjusting prices or just keeping a fixed block size? Okay, so this object here, and so the proposition of the paper is the following is so I'm going to consider one case here where there is a consideration for social welfare. Again, it can capture many different situations. You put new to zero, beta to one. But the general idea remains, you see that the difference between price controls, price adjustment versus fixed block size limit is proportional to these risk factors. And what do we get is that when you have demand volatility is higher, the advantage of price controls is large.
00:20:35.360 - 00:20:35.772, Speaker C: Okay?
00:20:35.826 - 00:20:45.256, Speaker B: So like I just said now when you have a lot of demand uncertainty you want to do more. You don't want to just fix a block size limit.
00:20:45.448 - 00:20:46.190, Speaker C: Okay?
00:20:46.800 - 00:21:05.764, Speaker B: And when covariace between demand and marginal cost is low, you want also to have price adjustments because then it would be the prices would increase in those situations where quantities will increase in those situations where marginal cost is low.
00:21:05.962 - 00:21:06.324, Speaker C: Okay?
00:21:06.362 - 00:21:34.076, Speaker B: So this is the general idea and now this is kind of the formalization. It's not just the uncertainty of marginal costs but the correlation of some of these ideas behind EIP. 1559. If you look at, let's say there is the thought that demand considerations there is more incentive in demand than on the, let's say supply side.
00:21:34.258 - 00:21:34.604, Speaker C: Okay?
00:21:34.642 - 00:21:46.492, Speaker B: So this is the idea. Now, one thing that I add and I think which is very relevant is how is this robust? So in the model I can account for fluctuations in cryptocurrency prices.
00:21:46.636 - 00:21:46.944, Speaker C: Okay?
00:21:46.982 - 00:21:59.430, Speaker B: So we are getting more on off chain stuff now. But think of now people, they are bidding in cryptocurrency but their value are in either in dollars or in real terms.
00:22:00.520 - 00:22:00.932, Speaker C: Okay?
00:22:00.986 - 00:22:09.508, Speaker B: So once you account now for uncertainty, large uncertainty in prices from coming from the currency actually that lowers the advantage of price controls.
00:22:09.684 - 00:22:10.120, Speaker C: Okay?
00:22:10.190 - 00:22:27.410, Speaker B: Because then price controls, there is a lot of uncertainty coming from variation in prices and people are valuing their things both validators and people in either real terms or nominal dollar terms.
00:22:27.780 - 00:22:28.144, Speaker C: Okay?
00:22:28.182 - 00:23:06.744, Speaker B: So maybe that's also say that you need to quantify these things if you account for that. All right, cool. So now let's do something more applied and look at now what does it tell us for transaction fee mechanisms? So I'm going to consider what I call here. So this is feel free to give me feedback on the name ADT TFMs TFMs that are adaptive, transaction fee mechanism that are adaptive to a deterministic target. So I'm going to ignore the tip. Tim have shown that you can have tipless mechanisms that had good properties. So I'm going to ignore the tip and I'm going to just consider the following.
00:23:06.744 - 00:23:21.200, Speaker B: You are going to adjust the prices. Say you fix a target that is deterministic, that is fixed. I'm going to just look at the growth rate, the gap from that target and I'm going to adjust the prices from some function.
00:23:21.350 - 00:23:21.664, Speaker C: Okay?
00:23:21.702 - 00:23:55.364, Speaker B: I call these ADT TFMs example is ethereum. Ethereum one is one that is linear. And that is they set it such that the block size doubles at most in a minute. That was just chosen out of the hat of someone in the Ethereum foundation. There is no reasoning why. Okay, so we're going to try to think of these and see how would be an optimal TFM in this family. Optimal here what I'm going to mean is the following.
00:23:55.364 - 00:24:33.480, Speaker B: It's going to be in the sense of the way ethereum is thinking of, which is like, oh, we want to hit a target, so I want to hit a target quantity. How should it look like? So in the paper, I show that it should be exponential. And that adjustment parameter is tied to a very deep economic parameter, which is the inverse price elasticity demand. And it's all going to be about like how to measure it or how to approximate it on chain. So why so let's intuitively the idea is the following. So if I come back to this. So this here is the growth in prices.
00:24:33.480 - 00:24:57.240, Speaker B: This is, let's say, the change in quantities. So if I'm off the target and I want to get closer to the target, it's going to be anywhere I'm around the target. This is, let's say, some change x. And here, this change is going to be just x divided the elasticity, because the elasticity is the growth rate of quantities to the growth rate of prices.
00:24:57.400 - 00:24:58.156, Speaker C: Okay?
00:24:58.338 - 00:25:16.660, Speaker B: So when I take the logs here, so the log is the actual growth rate, sorry, the log price change is the growth rate. So then I'm going to have just some quantity. X is equal to f, x log of x is equal to f of x over that elasticity.
00:25:17.320 - 00:25:17.684, Speaker C: Okay?
00:25:17.722 - 00:26:13.960, Speaker B: So what I show is that in general, for any distribution, the optimal ease of this form, the elasticity at the target for any target that is fixed, it's exponential locally in the neighborhood. So that's what they consider is linear form. But the linear has a loss in the second order that matters when you go far away, a little bit away from the quantity target. Now, to go further. If you specialize the valuation of the user distribution the valuation of the user distribution, if you consider one that is the Parital, you find that this parameter D is going to be just. You'll have this constant elasticity of demand curve that I created earlier. And it's going to be just one over the pareto tail, one over this price elasticity, which was one over the pareto tail.
00:26:14.380 - 00:26:14.936, Speaker C: Okay.
00:26:15.038 - 00:26:18.116, Speaker B: And it takes the exponential form everywhere.
00:26:18.308 - 00:26:20.104, Speaker D: Remind me what optimal means here.
00:26:20.142 - 00:26:39.040, Speaker B: So optimal means the objective is at a target. It means if I'm off the target, let's say QT is equal to q star minus epsilon. What adjustment function gets me closer to where I'm going to be at the target quantity in the next period?
00:26:42.690 - 00:26:43.440, Speaker C: Okay?
00:26:44.850 - 00:26:49.200, Speaker B: So like I showed, the objective is to hit a target block size.
00:26:50.850 - 00:26:54.610, Speaker D: So why wouldn't it be the adjustment that just always takes you immediately to the target?
00:26:55.990 - 00:27:04.920, Speaker B: Exactly, to the target. Yeah, but it's one function. It cannot change depending on the target. It has this specific form.
00:27:09.610 - 00:27:12.710, Speaker D: So are you taking like a worst case over targets?
00:27:14.090 - 00:27:39.514, Speaker B: No, I'm just taking the locally at the target. It's going to be yeah, it depends on the target. Certainly that's what you are saying, right? It's true. This elasticity is captured at the target. The thing is, this is around the neighborhood of the target. This is what matters. So the objective function is next period, I want to hit the closest to the target.
00:27:39.642 - 00:27:41.620, Speaker D: And what prevents you from hitting it exactly?
00:27:42.230 - 00:27:55.170, Speaker B: What prevents you from hitting it? Exactly? Because say I have this imagine I'm Q star is equal to epsilon, right? Q star is imagine I'm far away from epsilon of the target.
00:27:55.530 - 00:27:56.326, Speaker C: Okay.
00:27:56.508 - 00:28:21.230, Speaker B: What price do I get? Imagine the price was on the demand curve. What price do I get? Next is defined by this price. And then what is going to tell you the quantity is the demand curve. And it's not guaranteed that you're going to hit it. Exactly. Because for any price, there will be a demand for that price. So here I'm assuming that this is around in stable network situations, right? Because that's what it should capture.
00:28:21.810 - 00:28:22.126, Speaker C: Right.
00:28:22.148 - 00:28:40.660, Speaker B: So if you have a quantity that is far off and I'm adjusting prices. This price initially is at a demand curve. And I'm adjusting prices now from this function. And this price leads me, maps me to a demand. It's not guaranteed that it's going to be the target. Does it make sense?
00:28:41.590 - 00:28:48.562, Speaker D: Possibly, sort of, but I still don't like there's there's like I mean so if you're not at the target, you'll be some gap from the target.
00:28:48.706 - 00:28:49.302, Speaker B: Yeah.
00:28:49.436 - 00:28:59.642, Speaker D: And depending on the target, you might have bigger, smaller gaps from the target. So I still don't understand how you need to have some kind of operator over possible targets to get an objective function.
00:28:59.696 - 00:28:59.962, Speaker B: Right.
00:29:00.016 - 00:29:01.034, Speaker D: To get a number right?
00:29:01.072 - 00:29:04.470, Speaker B: No, I say fix a target, that's the target that is fixed.
00:29:04.630 - 00:29:04.954, Speaker C: Right.
00:29:04.992 - 00:29:34.070, Speaker B: And I'm going to look at locally. Let's say I'm on a neighborhood of a target. What should be the function that gives me closest to the target? And so now when I said locally, what happens is imagine you are at the target minus epsilon. It's not guaranteed if you're adjusting prices like this. The way you would make sure to be at the target is let me just look at for that target. What would be the price that would be on the demand curve?
00:29:35.050 - 00:29:35.800, Speaker C: Okay.
00:29:37.450 - 00:29:53.850, Speaker B: That's what it is. But I want it to be over the whole but it's going to be here. This is the price adjustment that we are following, this constrained function that is a fixed function that only depends on the gap.
00:29:54.510 - 00:29:54.874, Speaker C: Okay.
00:29:54.912 - 00:30:39.260, Speaker B: I'm happy to talk about it later person but I think now this result and I think this is nuance to interpret is the following now I'm going to have some considerations that where we are like the minor this is a form of MMiC that Tim has written in the paper where the miner can create transactions and add them and take some of the block space. So I ask if what choice of the quantities the block the target would lead to the one that the validator wants.
00:30:39.870 - 00:30:40.282, Speaker C: Okay?
00:30:40.336 - 00:31:27.350, Speaker B: And I show that. So here I make specific choices on the demand curve. So I have this is the elastic demand curve or from the prior to valuation if I consider in a situation where the block size maximum block size limit is there to match average demand not yet that but in general the quantity target to maximum block size satisfies this equation. So this is just through solving for the optimal monopolist validator's choice of a block size. Of course, I mean this is a consideration of monopoly power. Can argue. Oh, maybe that's not the main concern but we know that Validators always try to find ways to get extra to extract extra surplus.
00:31:27.350 - 00:32:31.494, Speaker B: So if they can introduce some transactions in there, they will do it and that would to increase the price overall. And I find that this bound actually goes to one over Epsilon. If I assume that the quantity max is taken the maximum quantity limit, the block size limit is taken to match average demand which is around 37%. And the interpretation of this, I think is that, you know that ethereum was not the goal. Was there more to match a target than to match the monopolies? But if the target to max block size and the max block size much demand in the average scenario then the ratio should be less than 37%. Otherwise the miner will have incentives to introduce some transactions and reduce the effective supply and get revenue from there. Let me now quantify this numerically and I think it's useful.
00:32:31.494 - 00:33:40.302, Speaker B: So I'm going to take ethereum data. So I take a random sample of 100,000 blocks a lot of transactions the London hard fork to recently and what economists would do when you're trying to match a demand curve is you have something else it's hard to tell whether you are moving just on the demand curve or whether there is a shift, meaning the conditions have changed so often what you do is you have some random variation on the supply curve. So this is hard to come by in the blockchain example because things are set programmatically even if you look at the block size often it just changes a period of time there is nothing random in it. A dream of doing an experiment wouldn't be also that scale. So the strategy that I'm going to take and I know this has limitation or I'm going to argue it's actually informative I'm. Going to fit a paratro distribution. If you look at valuations often, you will have some people just going to pay very high fees.
00:33:40.302 - 00:34:01.180, Speaker B: So I'm going to fit a parat distribution on the empirical distribution of effective gas prices, okay. In each block. Now, this has limitations. The first one is that there is a Censoring. Because of the minimum price, you won't have those valuations at the bottom.
00:34:01.790 - 00:34:02.540, Speaker C: Okay?
00:34:03.070 - 00:34:51.830, Speaker B: And also the other thing is I'm not going to capture the MEMP pool. You have pending transactions in the mempool are not captured. But I argue and I'll argue that if you want to think of the mempool, really you need a different form of transaction fee mechanisms than this one because you need to find ways to have signals from the mempool. And I don't think it would be very robust. So this estimate, what it does is by censoring the lower transactions. In fact, what you find is when you fit a parallel distribution and you censor the lower tail, you're going to have reflection transaction, the low valuation, you're going to have a higher tail than the actual valuation of distribution. What it means is that the pareto coefficient I'm going to estimating is an underestimate.
00:34:51.830 - 00:35:39.466, Speaker B: That means that the adjustment that I'm going to get is going to be an overestimate, meaning that this is going to be an upper bound to what it should be, okay? And this upper bound will be informative. I'll show you that it's less than the ethereum one. In a sense, the ethereum one is overshooting, okay? And I perform robustness checks to censor the data in different ways. So what I will find is a lower bound because sorry, it's an upper bound of how this adjustment parameter should be, okay. And I look at the result. So because I told you that this is when you look at it locally, you want to hit this quantity target, okay? So you want to hit this quantity target. So locally, so let's look at so my favorite result is this.
00:35:39.466 - 00:36:01.220, Speaker B: And I'm going to show you other results. I'm going to look at the quantity target. And I'm going to look at blocks that match it plus or -50% that are at the quantity target. And from there I'm going to fit these pirate tails and I'm going to see on average what they lead to. And I find an adjustment rate of around 8%.
00:36:02.470 - 00:36:02.930, Speaker C: Okay.
00:36:03.000 - 00:36:37.526, Speaker B: And I'm going to show some robustness, okay? And this adjustment rate, of course, 8% is not a magical number. It shows up somewhere else. But it encapsulates here this concept of inverse price elasticity of who. I'm happy to discuss it down the line as well. I find also that before the merge, this adjustment rate should have been lower, meaning that the current 12.5% was overshooting more before the merge and after the merge, where 8.67. I don't account for those who are in the mem pool.
00:36:37.526 - 00:36:58.050, Speaker B: Like I said, but for that we need other things. Yes. Can you remind me, does the sensoring mean that your estimate of the optimal is too high or too low? So I estimate an upper bound. I estimate too high, yes, that's why I'm saying it's informative. Yeah. And estimate an upper bound.
00:36:59.830 - 00:37:00.242, Speaker C: Okay.
00:37:00.296 - 00:37:07.494, Speaker D: So now concretely there's like a one eight at the beginning of the 1559 formula, and you're saying it should be more like 112 or something like that.
00:37:07.532 - 00:37:08.120, Speaker C: Yeah.
00:37:08.570 - 00:37:12.440, Speaker B: So that one eight is 12.5%, it should be around 8%.
00:37:14.730 - 00:37:15.142, Speaker C: Okay.
00:37:15.196 - 00:38:03.030, Speaker B: So now let's see some of the empirical work and explain why. So first of all, the exponential shape is already planned future that they are going to change. So this gives some explanations for that. But here you see some work that Leonardo Bernard Ben Ados have done where they look at the dynamic properties of their TFM and they find that if you see so here I don't have number, but it's around 8% and it's really afterwards I saw it, I'm like, okay, it's interesting. It explains that you have chaotic behavior afterwards, which is like it overshoots or undershoots and you either have full blocks or empty blocks at the block size on average is stable up to that around 8% number.
00:38:03.180 - 00:38:03.542, Speaker C: Okay.
00:38:03.596 - 00:38:09.706, Speaker B: So this signals that for normal demand situations it is too high.
00:38:09.888 - 00:38:10.282, Speaker C: Okay.
00:38:10.336 - 00:38:39.442, Speaker B: The current rate, I do the same. So here, this is just to show you the target block size result. But I show that, for example, even when you have this epsilon around twelve, you already are at that limit of around 37%. Now of course, how robust is this? Let's look at different ways. Of course, because some of the transactions, some blocks are empty, so I need to get rid of them. You cannot fit anything on it. Some blocks have two transactions, you can only fit a line in there, right.
00:38:39.442 - 00:39:27.060, Speaker B: So some situations you have guys around 500 where you're not in normal situations. So let's do different ways. So here is instead of considering just blocks that are around the target, let's consider the 5% 33% or further away. So blocks that can be pretty full or pretty empty, max gas from 30 is going to be not high. Gas up to 200 means there is less sensoring, so you can have all 200. Now what happens is when you have more a larger window of transactions, the adjustment rate is lower.
00:39:27.510 - 00:39:28.018, Speaker C: Okay.
00:39:28.104 - 00:40:22.066, Speaker B: So that means even away from it, it's lower the target. And when you are compressing, I think, and this is just, I think a feature of Censoring too much up to considering blocks that have a base fee of 30 because you have compressed it so much Censoring. So you're going to have an overestimate of the parallel to tail, sorry, underestimate of the tail and an overestimate of the adjustment rate. But all in all, here, when I do all these robustness, they get me at most ten. I can do the same post merge and premerge. So I look at blocks before merge, after merge, so these are all random blocks. If I did it in a sample also that is of same size, 100,000, 100,000.
00:40:22.066 - 00:41:15.700, Speaker B: I have similar results for post merge. The numbers are larger and also this is something that they find consistent also with some of that study they find that it was more overshooting before the merge and after the merge. Now and pre merge it can be as low as 6%, so six to 1093. I think the main takeaway of this is also I did some experiments looking at whether not having a fixed target but nondeterministic target can tell you more and have to talk about it. I think maybe one thing that this can bring into the conversation is seeing this D as a deep economic parameter that we need to think of how to adjust. And I think they had a sense also that maybe it was too high for normal situation. We know that when gas spikes it's too slow but certainly afterwards you stay up there too high.
00:41:16.250 - 00:41:16.950, Speaker C: Okay?
00:41:17.100 - 00:42:01.714, Speaker B: Now for the next 15 minutes I'm going to be front running some of my ideas. I think all of these I have models and the economic concepts I'm going to be developing and I think it can be useful also for people who are thinking of this. And let's talk about all about gas. This quote from Bill Meyer is we preach about capitalism and the beauty of unfettered market forces determining price, but not when it comes to gas. When it comes to gas, we need it cheap and the president has better get it for us or else we don't care how. And I think that's often, sometimes what happens in crypto too, people say okay, let's have low gas. This is often how it is.
00:42:01.714 - 00:42:40.270, Speaker B: Okay, the network just started, there's not much. Yeah, let's have low gas but not too low for people to get used to it. But let's keep transactions low, fees low, let's subsidize them. And I want to take this as a principle of how we want to think of gas. So I'm going to kind of lay out here some applied economic principles as in principle, so really apply to think of few things. So you can think of pricing of block space which is just this is just I want to just get to consensus. So think of either pricing of flow space or the resources that will get into block space.
00:42:40.270 - 00:42:50.740, Speaker B: Often you'll think about it as just like these Opcodes or these what do you call, burst resources. This is like non durable resources that's one time paid.
00:42:51.270 - 00:42:51.838, Speaker C: Okay?
00:42:51.944 - 00:43:28.814, Speaker B: And you can think of also computation which will involve you think of storage or some what I would call durable or sustained resources. These are resources that if they incur a cost to the network, will stay and remain there. How do you think of pricing of those resources relative to others and finally execution. Imagine you have a big NFT drop market or you can condition your pricing on objects. How would you do to avoid network congestion, for example? So these are all questions that people are interested.
00:43:29.012 - 00:43:29.386, Speaker C: Okay?
00:43:29.428 - 00:44:59.790, Speaker B: So I'm going to summarize these as sort of principles and happy to discuss this offline. So I think the main, the first principle is the following, which is that contrary to that code that we just have, the primary objective of protocol pricing within blockchains is to rectify externalities rather than supplanting the competitive markets. So externalities could be either there could be a lot of uncertainty that could lead us in a very bad situation that would be a very high marginal, let's say, social cost of having big blocks or that validators don't account for how do I set pricing? Okay, so this I think is the first principle. Yeah, certainly the tools that are in control of blockchain designers are sometimes there is this thing in the virtual machine, I need to set a price on it. But I think this to me is the first. Okay, so now this is summary of ideas of what I presented on multidimensional pricing and multidimensional pricing. I think the main economic insights are the following you can think of in a multidimensional context, okay? You can show examples where the efficiency can be enhanced by increasing the price of resources that are complementary to saturated resources.
00:44:59.790 - 00:45:38.854, Speaker B: Complementary because they come together to form a certain product. So that's something that you can take advantage of when you look at the level of granularity. Now, so I argue that the level of granularity is not really the level of these intermediate inputs like the opcode level. It's too granular. And the reason is the following. It is the notion of productive efficiency. When you do that, you create all these ways of people trying to go and find out ways to adjust their code, avoid these things and that leads to loss in productive efficiency.
00:45:38.854 - 00:46:04.020, Speaker B: And the idea is really to do the pricing, what I call other end user product categories. So you will have some typical smart contract that use same operations. The economic notion here or the mathematical notion is things that go in a constant return to scale way. Say, let's say you have type of template contracts that always do the same thing. You should be pricing those the same way.
00:46:04.470 - 00:46:04.834, Speaker C: Okay?
00:46:04.872 - 00:47:01.934, Speaker B: So if you write down and think about the writer, the user thinking of what contract that they choose, the contract writer choosing to optimize, that would lead you to not wanting to distort these intermediate opcodes. And finally now when you think of now if you were to do this, I mean, of course we know that the pricing always tries to be very agnostic. But some blockchains do this, they price at the contract level. If you can do some sort of tagging. You want common transactions. If you know what is common, like a token transfer, they should be subject to the protocol based metering because these are kind of very common necessary transactions and the more complex ones to be more let by the market to determine them. Okay, so these are, I would say, the high level principles on this.
00:47:02.072 - 00:47:02.566, Speaker C: Okay.
00:47:02.668 - 00:47:11.320, Speaker B: Now let's talk about storage and thinking of this object that is more of a long term durable object.
00:47:11.930 - 00:47:12.680, Speaker C: Okay.
00:47:15.390 - 00:47:31.306, Speaker B: The idea behind so there is a room for separating the pricing of storage and other more non durable resources and having implementing a storage rebate.
00:47:31.418 - 00:47:31.694, Speaker C: Okay?
00:47:31.732 - 00:48:12.890, Speaker B: So when you write down the model, you find that you can implement a storage rebate and you can price that this rebate is going to be, in a sense, the economic notion here will be proportional to the discounted marginal value of computation that it allows. And finally, and I think the key thing is that because this storage is durable, in a sense, over the lifecycle of a blockchain, when you start having more of these durables compounding, the share of the pricing that should be these durable resources should be larger relative to the share of the non durable resources because they put a larger load on their network.
00:48:15.150 - 00:48:32.894, Speaker A: Yes, that seems counterintuitive to me in a way, because if you imagine a blockchain has a fixed life cycle like ten years, the people who store something in year one, it seems like should pay more because they're paying for ten years of storage and the people at the end are not getting as much storage.
00:48:32.942 - 00:48:39.010, Speaker B: No, but what you are storing that contract has also a marginal value that is huge at the beginning.
00:48:39.590 - 00:48:40.194, Speaker C: Okay.
00:48:40.312 - 00:48:54.870, Speaker B: And I think that's the thing, right? The storage is not just the cost. It's also it has a value. The big it has a value because you are composing from this storage to create new tools.
00:48:55.530 - 00:49:01.260, Speaker A: I mean, how do you know that? How do you know it's not just like somebody storing something that's never going to be read again, right?
00:49:02.430 - 00:50:02.330, Speaker B: Yeah. But if you can target it more, then you can make those different characterizations. But I think the pool of contracts that are there combined with new contracts to generate the thing is later down the line, you are getting closer to a storage that would make the network start getting very slow at the beginning. It's not that slow. So I think that's the idea, again, as a proportion of the total thing, it could be that fees are lower at the beginning, but I'm saying at the proportion of all right, so now let's talk about local fee markets, which is more like, I think at the execution stage, different blockchains have different architectures. At some point you get to execute.
00:50:02.410 - 00:50:02.702, Speaker C: Okay.
00:50:02.756 - 00:50:53.374, Speaker B: You need to find a way. So the idea is the following. So for execution, how can you do an execution that is better than for example, what is happening by just saying, okay, all that matters is just one auction second price, an approximation of second price of a gas on everything. Then board of yo club comes and everything is gas is a 1000. And this is kind of what I find is that when you look at a submarket that faces inelastic or relatively inelastic demand compared to others, let's say everyone is going for this NFT, they don't care how much it pays. So you should incorporate local fees that is not just going to be one auction over all of them. And this approach become increasingly robust as the market size if let's off that cube, meaning the people who are going towards that is small.
00:50:53.374 - 00:51:25.800, Speaker B: For example, let's say if you are an NFT drop, if it's an NFT drop, okay, the optimal mechanism in this situation. So with assumptions, of course it operates as a global auction. But this is the idea. So there is many lanes in New York and there is different tall ways, but some of the lanes are more congested than others. But now you can have a proxy of how big a market is by what is the total gas price that people are paying in that market.
00:51:26.670 - 00:51:27.130, Speaker C: Okay?
00:51:27.200 - 00:51:55.102, Speaker B: So you are going to discount a market by that because otherwise you will just let the NFT drop going and nothing else executing. So that way you can have some others pay lower gas because their market is less saturated and yet still be filled in. So it's a way think about you have different queues. So there is a lot of work on queuing theory and how do you manage different queues. But here this is like a proxy of how do you alternate between queues.
00:51:55.246 - 00:51:55.650, Speaker C: Okay?
00:51:55.720 - 00:52:56.502, Speaker B: And I find, and I'm happy to discuss that too, that it's the market size through the demand size that makes this choice. And also finally, so what I find also is that a uniform gas auction without local fees, just one where you just do a large gas auction is only optimal when the submarkets have high price elasticity when you go to a long term also where the long term elasticity tend to be larger than short term ones. Okay, so sorry. So only low elasticity, I apologize. Here there is a typo. So when you have go in the long term where long term elasticities matter higher, what does it mean? Because in the long term people will care more, they have more time to respond if they will not be willing to always pay a higher price if they are going to pay it for a repeated period of time, blockchain should transition towards local fees in the long run. Okay, so this is all I have.
00:52:56.502 - 00:53:01.574, Speaker B: So I thank you. So comments and questions are welcome. Yes.
00:53:01.692 - 00:53:09.180, Speaker D: For all those principles, you think you have models for which those are like theorems or that's more speculation at this point.
00:53:10.510 - 00:53:33.010, Speaker B: So I started writing a paper about what I did, and I had different models for different things. It was going to be long economic type of papers, so I decided to break them down in papers so that I dig more into the quantitatives of each piece. So I have theorems for each of those statements, actually, but it would be another presentation.
00:53:33.510 - 00:53:46.002, Speaker D: One of the ones I was sort of confused by was when you say you were advocating more coarse grain pricing, you said at the level of user categories rather than yeah. So with a low level, you mean like the Opcode.
00:53:46.066 - 00:53:52.200, Speaker B: Opcode, yeah. I said no. I said the Opcode is not the right level.
00:53:52.730 - 00:54:00.390, Speaker D: Yes. What I'm a little confused about is you could imagine having two smart contracts with the same functionality, where one is implemented well and one is implemented poorly.
00:54:00.550 - 00:54:00.874, Speaker A: Right.
00:54:00.912 - 00:54:09.950, Speaker D: And the social cost of the latter is definitely, like higher. Right. So it seems like you wouldn't want those priced the same even though the functionality is identical.
00:54:11.170 - 00:54:18.622, Speaker B: Okay. Functionality. Oh, yeah. What they do. Right, yeah. Okay. It's true.
00:54:18.622 - 00:54:27.614, Speaker B: I'm assuming that people are optimizing the code writers are optimizing for their code, but I think that's the assumption.
00:54:27.662 - 00:54:30.194, Speaker D: But it's exactly the Opcodes that incentivize them to do that.
00:54:30.232 - 00:54:30.482, Speaker A: Right.
00:54:30.536 - 00:55:26.840, Speaker B: No, well, the Opcodes, the thing is, if you price it that is not reflecting market price, then people start you're creating distortions because people will start looking at they will optimize wrongly. You're creating a distortion in a sense, because if you price it, that doesn't reflect market price, you create these distortions. Now, what I say is, let's say imagine there is an open zipline smart contract that is a standard. If you price at that smart contract, what is happening now is that price that is reflected at the market equilibrium will account for each of these lower level ones. And since they are all combined in general to get you this type of contract, you are not reducing the productive efficiency. You are not introducing productive efficiency inefficiency that's. The idea behind this.
00:55:26.840 - 00:55:55.900, Speaker B: So the mathematical statement is, if you have, let's say, a constant return to scale function of these different resources, that leads you to a contract, the price, the the you know, you should the the pricing, the relative pricing of those should be the same. All right, so, great. Thank you for having me.
