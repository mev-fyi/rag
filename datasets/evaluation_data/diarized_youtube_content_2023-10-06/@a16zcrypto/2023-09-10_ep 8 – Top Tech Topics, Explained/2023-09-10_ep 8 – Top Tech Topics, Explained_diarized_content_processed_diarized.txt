00:00:00.330 - 00:00:00.880, Speaker A: You.
00:00:04.610 - 00:01:25.798, Speaker B: Welcome to Web Three with A Six and Z, a show about building the next generation of the Internet. From the team at A Six and Z Crypto, this show is for anyone, whether researcher, developer, engineer, artist, company leader, community manager, entrepreneur, other builder seeking to understand and go deeper on all things crypto and Web Three. We're back with all new episodes the coming week, but I'm sharing yet another hallway style riff that a bunch of our researchers did turing through and defining at a high level, top of mind technical topics such as Bdfs or verifiable, delay functions, ZK rollups, synarx, and zero knowledge. In general, zipping up and down the stack and across themes such as scalability, data availability and reputation in the creator economy, including discussing applications for NFTs and more. It's a quick tour through explaining key topics that also sets the stage for the next batch of content from us and podcast episodes, as well as our newly launched YouTube channel, which you can find and subscribe to at YouTube by simply searching A Six and Z Crypto research. There you'll find videos from researchers with overviews of VDFS, zero knowledge, snarks and much, much more that we discuss in this episode and beyond. But this riff is based on a longer convo that originally took place on Twitter for a live audience on the heels of our research lab announcement a few months ago.
00:01:25.798 - 00:01:57.890, Speaker B: And it features all of our current research team except for one person who couldn't make it last minute then, but who will be in our next podcast episodes. It's moderated initially by Asics and Z crypto general partner Chris Dixon, followed by Ali Yaya. The first voice you'll hear is head of research Tim Roughgardens, followed by Chris's and then Joe Bono, Scott Commoners and then Ali. Again, be sure to subscribe to the new A Six and Z Crypto channel on YouTube for more on the topics that follow, as well as our newsletter on substac. And stay tuned for the next all new episodes.
00:01:59.990 - 00:02:40.106, Speaker C: So maybe just to add in a little bit from the researcher's side, there are areas and areas of computer science which are mature enough where you can read the latest papers from researchers and know what the most important problems are to go next. But the science and technology of Web Three is nowhere near that mature yet. And so if all you do as a researcher is look at the academically published papers out there, you're not necessarily going to know what are the most important research problems to work on over the next few years. And we really firmly believe that this is a new discipline forming before our eyes in real time. And so the opportunities to do fundamental research are just huge. I expect research done in the next year or two to already be taught in undergrad classes by the end of this decade.
00:02:40.218 - 00:02:41.406, Speaker D: Yeah, just to add to that, the.
00:02:41.428 - 00:02:42.654, Speaker C: Reason at least for me, I always.
00:02:42.692 - 00:03:06.242, Speaker D: Think about DeepMind and OpenAI and these modern AI labs as aspirational goals. What I find so inspiring about those organizations is I think of it as at least three things. One, they have very legitimate and respected academic research. They put out papers. And my understanding is they do it in almost a completely open source way, which is definitely how we want to operate, just advancing the field. But two, they also then do applied things. They build software.
00:03:06.242 - 00:03:35.198, Speaker D: But then the third leg of it, they do documentaries, they AlphaGo stuff, things like this. Our view is this is important because, to Tim's point, it's striking in academia, frankly, how skeptical a lot of computer scientists are of this field. And I think one important lever for doing that is use cases that have utility, societal value, et cetera, and that get a lot of attention. And I think those AI organizations really did that. So I think it's a really great model to aspire to. Maybe with that, we could kind of get into some fun technical topics. Tim, starting with you, if you could.
00:03:35.204 - 00:03:35.966, Speaker C: Tell us a little bit about your.
00:03:35.988 - 00:03:48.306, Speaker D: Background, particularly for those who don't know. He's one of the pioneers of a field called algorithmic game theory, which is, I think, a really cool area that combines the scale of the Internet with older ideas and game theory. Maybe, Tim, if you could talk a little about that and then we'll move on after that.
00:03:48.408 - 00:04:23.422, Speaker C: Sure, absolutely. So I did my PhD. It was like 99 to 2002, and at that time, the Internet was really just starting to explode. It was really starting to become commercialized and widely used. So it was a really exciting time to be a computer science graduate student because computer science was really changing in a big way. It was obvious to everyone across the discipline that computer science needed to be different. And one of the things that happened around that time is computer scientists realized that they really needed to understand economics and game theory to reason accurately about the new computer science problems that were coming out at that time.
00:04:23.422 - 00:05:03.658, Speaker C: And there had not been much interaction between computer science and economics prior to then. So I was lucky enough to get sort of in on the ground floor of that movement. And my whole career, my calling card has sort of been the connections and interplay between computer science and economics. So for my PhD, I was working on problems closely related to the Internet. For example, understanding the consequences of selfish routing between different Internet service providers. When I was an assistant professor at Stanford, I got really into a field of microeconomics known as mechanism design, which kind of broadly is sort of making good decisions when you don't know in advance exactly what people want, what people's preferences are. One of the main application areas of mechanism design is the design of auctions.
00:05:03.658 - 00:05:47.702, Speaker C: Auctions were becoming very prominent in computer science through business models of the big web. Two companies for real time auctions for advertising. There are also, for example, the auctions that governments run sort of sell wireless spectrum licenses. And that's another really nice sort of fertile ground for the interplay between computer science and economics. So with all of that backdrop, naturally when blockchain and cryptocurrencies came along, it's kind of like, how could I not work on this, right? I mean, I've never seen a technology where the economics are so deeply embedded in the technology itself. So bitcoin I found very striking when it first came out. It wasn't obvious to me what would be the most important research question to do at that point, but I certainly followed the space from a distance throughout most of last decade.
00:05:47.702 - 00:06:26.610, Speaker C: In general, the way I learned things is by teaching courses on it. So it's very frightening when you first teach it because you don't feel like you really know what you're talking about, but you do learn an enormous amount. And so I started teaching courses on blockchains, just really as a forcing function to get me to learn it sort of more deeply. And in doing that, by 2019 or something, I was completely sold on the vision of what I like to call a big computer that lives in the sky and sort of runs as a public good. So sort of the vision of general smart contract tech platforms. What's cool is there's super interesting open questions all up and down the quote unquote stack, both from sort of very low level implementation stuff to the application layer. So I've been trying to work in different parts of the stack.
00:06:26.610 - 00:07:06.946, Speaker C: I've been working some on consensus problems. So really developing a theory of permissionless consensus to mirror the classical theory of the 1980s that was developed for permissioned consensus and then also thinking some about mechanism design style problems at the application layer. Right? So, for example, there's a very rich theory of optimal auction theory developed by Roger Myerson. He won a Nobel Prize for that. I think one can aspire toward having an analogous theory around some of the applications that you see, for example, in the DFI space. So like an optimal theory of automated market makers would be one Holy grail to go for. Another thing that was significant in my own involvement in the space was in 2020, the Ethereum community reached out to me.
00:07:06.946 - 00:07:24.822, Speaker C: Vitalik had proposed this transaction fee mechanism, EIP 1559. And some members of the community asked me to take a close look at it. And I spent several months analyzing it and writing a 58 page report about it. And so at that point, not only was I all in on the space, but I think it was obvious to everybody that I was all in in the space. Awesome.
00:07:24.876 - 00:07:27.394, Speaker D: Thanks, Tim. Maybe let's go to Joe next. Joe Bono.
00:07:27.522 - 00:07:28.566, Speaker C: Joe, maybe if you could tell us.
00:07:28.588 - 00:07:40.490, Speaker D: A little bit about your background, like your PhD thesis, and then how you got crypto pilled. One of Joe's many claim to fames is he originally was crypto pilled, our own Dan Bennett, which is a pretty epic background.
00:07:40.650 - 00:08:23.854, Speaker E: Yeah, sure. So I started my PhD in 2008, so the bitcoin white paper came out. I think my first or second year, the main focus of IPhD was on authentication protocols for the web. So all this there'd been kind of 20 years of speculation about how secure passwords were or weren't, and all of a sudden, we had data sets with tens of millions of passwords, and you could really do fine grained. Statistics and Look At not Just How Strong passwords Are, but in A Whole Bunch Of Different Contexts for Different Types of users, for Different Types Of applications. So the main thrust during my PhD was analyzing all that data, figuring out how to collect it in a privacy preserving way, tying it to the field of crypto. But I had this side interest in blockchains the whole time.
00:08:23.854 - 00:08:55.062, Speaker E: The story I tell, we had a Friday research group meeting every week, and one week somebody canceled. And my supervisor on like Thursday afternoon at the end of the day, said, we need something for tomorrow. Can you talk about this bitcoin paper that first came out? And I never heard of it. So I was like, okay, sure. The paper is only ten pages long. And I was like, yeah, I can read this by tomorrow and present it for the group. And instead of turning into this late night affair, trying to figure out the bitcoin paper is pretty easy to read, but it leaves a lot of detail out.
00:08:55.062 - 00:09:37.890, Speaker E: So I was up reading the code and reading some of the early forum posts, trying to figure out how things actually worked in practice. So after I finished my PhD, I did two postdocs. One at Princeton, which led to this textbook that I co wrote with a group of people at Princeton. Like Tim said, teaching is really when you learn. Trying to write that textbook really required digging way deep into a lot of stuff that was really not documented at the time. I realized that a lot of professors were reading the textbook, which is a little unusual for a textbook, I guess, but a lot of people wrote and yeah, I've been meaning to catch up on this blockchain thing for so long, but it was confusing. And then I just decided to try reading the textbook, and then I got really interested in the space.
00:09:37.890 - 00:10:09.002, Speaker E: So hopefully the contribution of that textbook was convincing. A lot of academics in the mid two thousand and ten s. And then, yeah, I did a postdoc at Stanford with Dan Bonet, and that was when Dan was first getting interested. And he was slightly skeptical, but I think he was receptive. And I was like, we can find some real crypto problems. It ended up leading to the work on VDFS, which is probably in the blockchain space. The technical thing I'm most known for is starting the work on Verifiable delay functions and looking at different applications to randomness protocols.
00:10:09.066 - 00:10:09.246, Speaker F: True.
00:10:09.268 - 00:10:11.006, Speaker D: Would you mind explaining what Bdfs are?
00:10:11.108 - 00:10:47.814, Speaker E: Yeah, sure. So VDFS Verifiable delay function, it's a crypto function where the main security property is that you can't compute it in less than a certain amount of time. So it's an inherently sequential function. We don't know how to speed it up with parallelism. So the only way to speed it up is by having some processor with a really fast clock speed, which is expensive and hard to do. You can't just throw a lot of processors at the problem and it turns out to be really useful in all sorts of different protocol designs. The class of the example is there's a lot of commit Reveal protocols where everybody's supposed to commit to something, which could be random input to a randomness protocol.
00:10:47.814 - 00:11:24.082, Speaker E: It could be a bid and an auction. And then you have to have some mechanism to have people actually open their commitments and reveal. With VDFS, you just have a slow function that opens the commitments, so you don't have to rely on people actively opening them. Like anybody can open them at a certain period of time. And the time delay covers network latency and makes sure that people don't have to post it exactly the same time. But that's kind of okay because they don't have enough time to open everybody else's commitments or test out different random values they can post or whatever the security property is for this specific protocol. For crypto, it's a bit weird that you design something to be slow.
00:11:24.082 - 00:12:11.870, Speaker E: Usually we want everything in crypto to be fast. But yeah, so we identified there are a couple of applications and then kind of the way crypto research usually starts is you spec out an idealized functionality. Like if we had a function that had these three properties, then we could build some really cool stuff. And then we proposed the first construction and said, yeah, you can actually build this thing that kept the space off. And since then, a lot of other researchers have proposed different VDF constructions three or four years now, since the first paper came out, we've had something like 100 papers on VDFS and whole workshops on it. There's about ten different subtypes of VDFS with extra additional properties. So it's been a really fun crypto space to see, develop, and hopefully they'll be deployed more in practical protocols soon.
00:12:12.040 - 00:12:12.422, Speaker C: Great.
00:12:12.476 - 00:12:13.142, Speaker E: Thank you.
00:12:13.276 - 00:12:14.374, Speaker D: Scott, we'll go to you next.
00:12:14.412 - 00:12:15.099, Speaker C: If you could tell us a little.
00:12:15.099 - 00:12:17.622, Speaker D: Bit about your background and early academic career and then we could jump into.
00:12:17.676 - 00:12:19.234, Speaker E: Your interest in crypto.
00:12:19.362 - 00:13:27.694, Speaker A: Sure. I studied market design, actually, one step back. I did a lot of work at Number Theory in college, so I am also excited to be getting back into the world that borders on number theory. But my economics research work has been on market design, which is the field that thinks about how you structure and design economic systems to make them run better. In the same way that economics is a way of looking at the world and thinking about what the forces and incentives that drive different outcomes are. Market design is sort of like the engineering lens. On top of that, it asks, okay, how can we use economic theory and analysis in conjunction with a lot of technical tools, computer science and market clearing algorithms, applied math, and sometimes like a little bit of ethnography and sociology to better understand the markets we're interacting with? How can we use all of these tools together with a fundamental understanding of the incentives to make the system work better? And my PhD work was very much about fundamental research on the structure and market clearing mechanisms for what are called matching markets, for settings where you have potentially very complex transactions, where different individuals want to care about whom they're transacting with and how.
00:13:27.694 - 00:14:40.342, Speaker A: I spend some time thinking about these fundamental theories and market clearing mechanisms and auctions like Tim was talking about. But then I also spent a lot of time helping companies figure out how to use those ideas to better make their platforms or marketplaces work. So when you're two sided marketplace platform for online education or something of the sort, and you have these challenges around, to what extent do you keep pushing the content from the creators that are already well established versus bring in new creators and sort of give them a way to gain traction? And there are often tensions, right? Your short run incentive is typically to push the creator that's going to get the most views that you think are most successful. But the long run health of your platform depends on your ability to optimize for the long run to bring in new creators, to constantly enable someone to be successful on your platform. And so market design thinking is sort of about trying to understand how to balance these trade offs and how micro details like how those incentives produce the outcomes you want as a platform. And this type of thinking, of course, translates very naturally to crypto. Most crypto projects have some system of incentives that a protocol or a crypto based platform is going to create.
00:14:40.342 - 00:15:27.406, Speaker A: And you can set them down transparently. You can commit to them by putting them in code that everyone can audit and interact with, sort of, and it's guaranteed to function at a fixed state, but you still have to get the incentives right. And so what's drawn me into crypto is that so many of the businesses and public projects have at their core, a fundamental intellectual challenge. That is exactly the type of challenge that we face in market design. But I should say also early on I was aware of this stuff in the late 2000s, early 2010s, but I was actually on record as a big skeptic because so many applications that people were pitching at that time, right? Like entrepreneurs would come to my office hours and say, hey, I've got this idea for a project. But the projects weren't using the technology in ways that were fundamental.
00:15:27.438 - 00:15:27.730, Speaker E: Right.
00:15:27.800 - 00:15:54.462, Speaker A: There was a lot of, we're trying to take this thing and just now make it decentralized, but decentralization is costly, and they weren't gaining a benefit to offset that cost. And what's made me so excited about the space now is that we're now seeing huge waves of applications and again, the design and balance between degree of decentralization and what things live on chain, what things live off shape economic ecosystems that matter.
00:15:54.596 - 00:15:57.054, Speaker D: Are there particular projects that turned you?
00:15:57.172 - 00:16:55.406, Speaker A: Well, sure. So the first project that I got involved in, christian Catalini, invited me and also one of my former grad students, Ravi Jagadissan, to work with him and many others on the design of the Diem Protocol. The idea that you're going to have a fundamental transaction layer that sort of is backbone infrastructure in the same way that the networks that manage the Internet are backbone infrastructure for all of information these days. That's a case where the underlying technology is just like the only way to do it in some sense, right, where everyone can transact with anybody else in a way that's completely trustworthy, independent of how you're interacting and who you are. It sort of has to be in a system that is heavily robust and many times replicated and also baked into software in a way that people can understand and interpret and that's fixed and committed. And the other really cool thing about the idea of an infrastructure layer like that, that lets you extrapolate from places that have really strong institutions to places that have weaker institutions. Right.
00:16:55.406 - 00:17:42.718, Speaker A: There's potential to enable a stable medium of exchange in places that have weaker monetary policy because they can sort of lean on the institutions in the places that are doing the processing. So that was like, the first thing I got really excited on. And then more recently, I've gotten very, very engaged in NFT communities and consumer crypto. And what's fascinating here and again, this is an insight that Christian and Robbie and I wrote about briefly in a short paper. What's fascinating about NFTs to me is that a currency is only really useful as a medium of exchange once lots of people are willing to exchange goods and services for it. You have to build a really powerful infrastructure layer that everyone agrees is going to work and they coordinate on before everyone can gain the value from a given cryptocurrency. By contrast, NFTs are formed around small communities, right.
00:17:42.718 - 00:17:58.558, Speaker A: All that really matters initially is whether the people who might want to own that piece of art agree that the token means ownership and so this means that NFTs can establish their value among a small community of holders and then grow outwards, which is a really powerful consumer design paradigm.
00:17:58.654 - 00:18:52.598, Speaker F: Actually, I think, Scott, what you were just saying was super fascinating. Maybe for the rest of the time it'd be fun to go down the stack and talk about some of the open problems at each layer. We should start at the top, we should start at the application layer. And at the risk of keeping you talking for too long, I would love to start with you. You've done a lot of work thinking about the incentive structures around things like NFTs and around reputation systems. One of the things that I find so important about that is that one of the key problems in the space is that because we don't have a good mechanism for reputation, it's as if every interaction is forced to be a one off prisoner Dilemma style game as opposed to a repeated or iterated Prisoner Dilemma style game. And so that essentially forces you to assume that everyone you interact with in this space is an adversary which very much constrains the kinds of mechanisms that you can design because it's a very severe assumption that you have to make.
00:18:52.684 - 00:18:53.174, Speaker E: Awesome.
00:18:53.292 - 00:19:34.866, Speaker A: Yeah. So certainly one of the things I've been working with teams on the most and thinking about the most in my research is how you optimize your token design for the use case you imagine in your ecosystem. And this could be as simple as if you have a utility token. There are challenges around money supply, right? If you print too many units of the utility token then suddenly the value crashes and maybe people don't want to collect it, but on the other hand they are willing to spend it. And meanwhile, if your utility token is too scarce or people foresee too much of the utility downstream, then maybe they all hoard it and no one spends it and you have no circulating supply and so the overall ecosystem can collapse. So there are simple macro balancing questions of that form and then there are much more subtle ones. You alluded to this.
00:19:34.866 - 00:20:48.714, Speaker A: I wrote this piece on reputation tokens with one of my former students, Jad Esper, and we talked to lots of different companies that are thinking about how do you use tokens to generate a durable system of reputation, right? And they're a creator platform and they want to reward people who've contributed content or they are a job search platform and they want to reward people who are helping other people find jobs or something of the sort. And a lot of platform architects pick a token system where they are hoping their token is going to do two things. One is it's going to identify some people as having high reputation, like they're contributing a lot of value to the platform, but at the same time they're hoping to allow people to liquefy some of that value. It's a way of rewarding your contributors to the platform. What Jad and I wrote about is there's a funny paradox, which is if you want to give people a reputation that's durable but also want to make it liquid and you only have one token, then suddenly now it's not a good signal of reputation. So we propose like a two token model and we talk some about how you balance these sort of like a durable reputation token that then spins off like a liquid token that you can use sort of as a medium of exchange, but so optimal tokenomics is one. Let me just point to one other application layer challenge and big opportunity that comes with it and then let's maybe move down.
00:20:48.714 - 00:21:09.618, Speaker A: A lot of the promise of Web Three comes from the idea that we're going to enable people to use the interoperable layer that is, the blockchain to sort of create established digital assets and forms of digital ownership that we can then point lots of different applications to and people can take with them from application to application.
00:21:09.784 - 00:21:10.066, Speaker E: Right?
00:21:10.088 - 00:21:40.806, Speaker A: And that has a lot of promise in terms of competition policy. For example, consumers gain a lot more leverage if they can take the content they've created with them and move to another platform. But for all of this to work well, we really need the transaction costs of the layers to be low and consumer technology around the blockchain to be much, much more accessible. And so there's a ton of design at the consumer access level that we don't normally think of as part of the technical challenge, but it's very much part of the economic design challenge.
00:21:40.918 - 00:21:59.026, Speaker C: Scott, I'm really glad you brought up the user facing design problem. That's clearly one of the biggest challenges facing the space. Among the more sort of technical types, it doesn't get spoken about enough. But one of the things that needs to happen to reach one or two orders of magnitude more crypto users is exactly some advances in the user interface design part.
00:21:59.128 - 00:22:05.538, Speaker F: Yeah, we can all chime in with other ideas, other interesting problems at each layer and we can sort of take it from there.
00:22:05.624 - 00:22:47.522, Speaker C: One natural place to go next would be the scalability problem. So L2 solutions. So the way I think of this, back to this analogy of the big computer that lives in the sky, we want two things, right? One, we want really cool applications deployed on that computer, but then we also want that infrastructure, this virtual computer in the sky. We want it to be as powerful and robust as possible. And there's going to be a coevolution of these two things so that the better the infrastructure, like the more transactions per second it can process, the bigger the design space for applications. Similarly, once certain applications sort of catch fire, there'll be extra motivation and capital for improving the infrastructure. So it's been one of the biggest areas of intense work over the last couple of years.
00:22:47.522 - 00:23:46.818, Speaker C: Layer two solutions, including side chains, including roll ups, really trying to sort of boost the transaction throughput of a platform like Ethereum by ideally at least two orders of magnitude and hopefully eventually four orders of magnitude or more. And so here again, there's a lot of, I think, really nice competing designs. For example, in the roll up space, you've got the gain theoretic approach taken by optimistic roll ups. You've got the sort of more proof theoretic or cryptographic approach taken by ZK roll ups, just understanding better the pros and cons the trade offs between those two approaches. I think there's opportunities for fundamental research and certainly there is ongoing a lot of really amazing deep research about how to get ZK roll ups like ZK Snarks as practical as possible. For me, someone who's trained as a theoretical computer scientist, this is just the most amazing thing to see because these proof systems on which ZK Snarks are based, they were introduced in the 1980s just by mathematicians, really, just to study the mysteries of the computational universe. Really, just to understand.
00:23:46.818 - 00:24:22.522, Speaker C: What does it actually mean to prove something? And now fast forward. One thing that's been amazing about the crypto space is all of these 20th century areas of computer science have just gotten it's not like they ever went away, but they all just have these completely new lives at this point through the new applications. And so Snark switch can be thought of as a specific form of proof system which now billions of dollars is going into heavily optimizing. That's another great example of the interplay between fundamental research on the one hand and very real problems, in fact technological challenges that are literally essential for web3 to realize its full vision.
00:24:22.666 - 00:24:38.630, Speaker F: Tim, I think in the context of L2, it may be interesting to just briefly touch on the data availability problem and also on the applications of proposals like EIP 4844 and how that might significantly improve the performance of L2 solutions today.
00:24:38.780 - 00:25:19.762, Speaker C: Sure, it's a really nice sort of research question here. So let me focus in specifically to Ethereum. As I'm sure many listeners know, in Ethereum transactions cost gas. So basically there is a single metric, single type of unit which is called gas and it's meant as a proxy for all of the burden imposed by a transaction on the Ethereum network. And that burden is really multidimensional. So it includes things like computation, it includes storage, it includes bandwidth that the full nodes running the protocol are using to communicate with each other. So you have this really multidimensional cost of a transaction and yet that's projected down into this one dimensional space.
00:25:19.762 - 00:25:41.318, Speaker C: You use this one number gas as a proxy for all of those burdens. So one of the key ideas in roll ups is to solve the data availability problem. So. What is the data availability problem? So basically you have this separate blockchain, this L2. And it claims it processed 1000 transactions. It tried to help out layer one. It's like, hey, layer one, let me take some of the burden off of your shoulders.
00:25:41.318 - 00:26:49.360, Speaker C: Let me sort of process these thousand transactions and then let you know what are the end results, right? And that's fine, that's very nice. But then later on, how does anyone know actually what transactions were executed or whether they were executed correctly? And so data availability is like where do you find out the information of what actually happened? What were the transactions that were executed off chain? And so in the roll up solution that's stored as what's known as call data, so it's basically just published to L1 using a particular type of relatively cheap data. And anyway, so some of the proposals going on is like well, if having room for lots of call data is essential for solving the scalability solution, maybe we actually want to think of things in a more multidimensional way. And so maybe call data coming from roll ups at layer one we can sort of treat as cheaper. We can increase the amount of capacity at layer one to handle the publication of call data while leaving the other parts unchanged. Roll ups and call data is one particular instantiation of this. But much more generally, thinking multidimensionally about the cost of transactions and pricing them accordingly I think is a super exciting direction for research.
00:26:49.810 - 00:27:33.422, Speaker E: I've thought a lot about measuring decentralization in more of like maybe not even quantitative, but at least like a systematic way doing transactions. Layer two with Zpay roll ups, there's like a little bit of a trust placed on the roll up server. If you do optimistic roll ups, the trust model is slightly different. There's a whole lot of other L2 protocols like payment channels. There's another one that I look at a different set. There's clearly a trade off between you can get better performance at the cost of introducing some new parties into the mix that at the very least you rely on for reliability. You might also rely on them for some other properties too that you hope the bill exhibit for the rollout to work the way you want.
00:27:33.422 - 00:27:48.466, Speaker E: But we don't have a really systematic understanding of how much performance can you gain for what trade offs in the trust model and are these actually fundamental limitations or is this just like the tech we know how to build now? So it's kind of like the high level.
00:27:48.648 - 00:27:58.350, Speaker F: That might actually be a good segue to thinking about how L2 solutions on top of Ethereum compare to just alternative layer ones. Curious what you guys think.
00:27:58.520 - 00:29:09.466, Speaker E: I just say quickly, there's a really similar high level question just between layer ones what the real trade offs are. Well, we're still at the point where people don't even really agree between the existing solutions, just between proof of stake and proof of work. We don't have a really good common understanding of what you're really sacrificing in the trust model by going from proof of work to proof of stake. And it's not even clear, I guess, if the sacrifice in one direction or the other, like they each have a different trust model and we sort of send advantages to both under what conditions the consensus protocol is going to behave correctly and then all sorts of different proof of stake implementations. For example, it's like we don't really have a good way of saying, like this one's slightly more robust at a cost of worse performance, or that one is better performance but is making slightly stronger trust assumptions pretty much the same problem. We don't know how to compare the existing solutions in a way that everybody agrees on. And part of that is because there's a lot of conflicts of interest and people want to say their solutions great and kind of overlook maybe the sacrifices they're making in trust model and we certainly don't know what the theoretical limits are on.
00:29:09.466 - 00:29:18.622, Speaker E: Do you have to sacrifice a little bit? Do you have to weaken the security model a little bit to get good performance? We don't know the fundamentals of it.
00:29:18.756 - 00:29:58.266, Speaker F: It might be fun to have a brief interlude about zero knowledge proofs specifically because they are a very fundamental building block that is also very general. I feel like the implications of zero knowledge proofs being a thing may be as dramatic as the applications of smart contracts when they emerged back when Ethereum was created. I feel like we were only now starting to appreciate just how powerful zero knowledge proofs could be and they also feel a little bit like magic. And so maybe if you can unpack what a zero knowledge proof is at a high level, what it allows you to do and what you think are some of the interesting applications or implications of them as a technology.
00:29:58.448 - 00:30:26.178, Speaker C: Joe's more the cryptographer, so I'll defer to him, but Joe would be great to comment on, but the zero knowledge proof systems have been around for several decades and I wonder your thoughts about I mentioned earlier about these 20th century parts of computer science finding new life with blockchains and in crypto. And I wonder if similarly, somehow blockchains in their public nature may be really what makes zero knowledge proof systems go completely mainstream. I'd love to hear your thoughts on that.
00:30:26.264 - 00:31:08.590, Speaker E: Yeah, sure. Blockchains have been the thing that brought zero knowledge proofs mainstream. The core idea is just that you can prove that some mathematical statement is true and that you actually know what's called a witness or basically like a solution to some mathematical equation. And it doesn't have to be a math equation because you can express a lot of other things as a math equation. So the example that I give in class a lot is that you can prove that you know the solution to a Sudoku puzzle without actually revealing the solution. So the zero knowledge part is always that you're proving something's true, but not proving anything else. You're not revealing the solution or revealing what you know, but you're convincing the other party with overwhelming probability that you actually know it.
00:31:08.590 - 00:31:42.990, Speaker E: I mean, yeah, the idea has been around since the with a Turing Award winning idea. It's a pretty clear fundamental breakthrough that you can do a zero knowledge proof at all. We've known for a long time that you can do it and you can basically do a zero knowledge proof of anything. I mean, you can do a zero knowledge proof of any NP statement, which is almost anything interesting. That doesn't guarantee that there's actually an efficient zero knowledge proof protocol. Especially like a lot of the early constructions were sort of more proof of concept. It was like you could do a zero knowledge proof that it might take like 10,000 compute years to compute the thing.
00:31:42.990 - 00:32:43.754, Speaker E: So the application started with really simple things relatively like proving that, you know, a discrete log of some group element or something like that. But people have come up with generic proof systems where you really can compile any NP statement down and there's all sorts of natural applications in the blockchain space. You can prove that a transaction is valid without actually revealing what the transaction is, like who's paying who or how much. So you can prove that the result of executing some really complicated smart contract is like these changes to the state, this money moves from that party to the other, and maybe you don't even reveal the amounts because all the amounts are hidden. And that means you can really potentially solve the big privacy problem of blockchains, which is that you have to make all the data public so that everybody can verify it. If you're just verifying zero knowledge proofs of transactions rather than publishing the transactions and re executing them. Yes, obviously much better for privacy, so you can get the benefits of public accountability and also have everything be private.
00:32:43.754 - 00:32:54.820, Speaker E: Kind of a long road to get there. Technically though, the current tools that we have just aren't ready performance wise to have anything like the transaction throughput that we want for a practical system.
00:32:55.190 - 00:33:06.658, Speaker F: Yeah, that's exactly right. Tim, I've heard you actually give a pretty good breakdown of the properties of a zero knowledge proof, which may be helpful, maybe even actually just starting with what the acronym even means in the case of A Snark.
00:33:06.834 - 00:33:44.494, Speaker C: Sure, yeah, happy to talk about that. So popping up a level, and this may tie some of the different threads together a little bit is I mentioned these proof systems and there's really two different aspects that are relevant for blockchains. One is the sort of succinctness aspect and the other is the actual zero knowledge property. So when I was speaking about challenges at L2 and optimistic versus ZK roll ups there, I was focused on proofs that are succinct, by which I mean someone does a whole bunch of work and convinces you they did a whole bunch of work without forcing you to recheck their work. You don't have to redo their work from scratch. So that's what I mean by succinct. You can kind of save someone a lot of time.
00:33:44.494 - 00:34:09.862, Speaker C: You do computation and then you just sort of convince them that you did it all. That's not about privacy, just that technology. And then zero knowledge is what Joe was talking about. That's really a cryptographic aspect which says that not only do you convince someone that you did something and that you know something, but you don't even actually leak what it is. You know, you convince them that you know it, but you don't actually teach them what it is. So that's just the first thing. So ZK rollups is a bit of a misnomer.
00:34:09.862 - 00:34:48.222, Speaker C: Often ZK rollups are used to describe proof systems that don't actually have zero knowledge. And so sometimes people prefer to call them validity proofs instead of ZK proofs for that reason. And so Snarks, you can add ZK on top of a Snark. But already just the basic sort of definition of a Snark is pretty amazing. So let me pop up a level. So in a proof system, like I said originally, it was meant to study the computational mysteries of the universe, really. Like, what does it mean to prove something to somebody? And so the formalism you use is that of two parties sort of Approver who's relatively powerful and a Verifier who is relatively computationally limited.
00:34:48.222 - 00:35:41.322, Speaker C: This is actually an unbelievably good mapping on the layer one L2 interactions in a blockchain where the slow, low throughput layer one plays the role of the verifier, plays the role of the actor that has very limited computational power. And then the L2 would play the role of the prover. So that would be like a chain that maybe has higher trust assumptions but also has higher throughput. And so the idea of Approver trying to convince a Verifier something true, again, it maps on perfectly. Layer two wants to convince layer one that it did a bunch of transactions accurately. And so that's why these Snarks, which came out of the proof systems literature, sort of the perfect fit for L2s that proactively to layer one prove that they did what they said they would. In this context, usually that just means like, there's a thousand transactions, they executed all of them and they published a new state route to layer one.
00:35:41.322 - 00:35:43.370, Speaker C: That would be the typical computation.
00:35:43.790 - 00:35:44.154, Speaker E: Okay?
00:35:44.192 - 00:36:19.314, Speaker C: So as far as the acronym Snark, so the S stands for succinct, what that means is that it better not be the case that for layer one to check L2's work, it has to literally redo all of Layer Two's work from scratch. That would totally defeat the purpose. You're trying to take some of the load off of Layer One shoulders. And so the Snark should be small, should not take up much space at Layer One and it should be cheap to verify. For example, in Ethereum you shouldn't need much Layer One gas in order to verify that in fact it is a correct proof. So that's the s succinct. So basically just you can check work much faster than redoing it from scratch.
00:36:19.314 - 00:36:46.974, Speaker C: The N stands for Non Interactive. So in the original definition of proof systems in the fable that was often told back at that time was like imagine a tutor and a student having a conversation. So the tutor here would be the more powerful all knowing prover. The person being tutored would be the Verifier. And there's a definition of what kind of conversation is a convincing conversation. So you had these rounds of communication back and forth. So that was the original notion of a proof system.
00:36:46.974 - 00:37:21.974, Speaker C: But if you think about it, you don't want Layer Two and Layer One to be talking back and forth for a long time. You can't afford to have this interaction. You want it so that anybody can download these software to run the Ethereum protocol, spin up a node and immediately sort of verify everything that has happened up to this point, even if it was a Snark proof published by some L2 that was defunct as of five years ago. So Non Interactive just means it's one and done. So really the Layer Two just says here once and for all, publish to L One. This is something anybody can verify now or in the future. And then the AR stands for argument.
00:37:21.974 - 00:37:50.942, Speaker C: So that just means there really should be a proof. Meaning that the Layer Two can only generate a Snark proof if in fact it's correct. In fact it really did do those computations. And conversely, it should not be able to spoof a correct proof. So if it didn't do the computations it claimed it did, then the Verifier should not be tripped. The Verifier should say AHA, I see the bug in your proof, I don't believe you that you did all of those transactions. So that's the AR and the K, which is the knowledge part, that's what Joe was talking about, that was like the witness.
00:37:50.942 - 00:38:34.650, Speaker C: And so that again comes from zero knowledge considerations originally, but actually even in validity rollups that are not zero knowledge, that's actually a really useful optimization for compressing the Layer One footprint. For example, normally to execute transactions, you need to make sure you have the appropriate signatures from the people initiating the transactions and you can actually kind of sort of not publish those signatures to Layer One in the validity roll up because they can serve the role of that witness. So in effect, Layer Two promises that new signatures generating that sort of authorized the thousand transactions that had executed along with the final state after it did all of that execution. So that's kind of the guts of what one of these stark proofs actually looks like. And that's the meaning of all the letters in the acronym.
00:38:34.810 - 00:39:14.400, Speaker E: You might see the term starring sometimes instead of Stark. So there's a difference between a proof and a proof of knowledge. Like in the Sudoku case, I can prove to you just that this Sudoku puzzle has a solution, that it's not a broken puzzle versus proving that I actually know the solution and turns out mathematically, those are two different concepts. So there are like what are called snargs, which are arguments that don't have this knowledge. You're just sort of proving something's true without proving that you know something which can be a little bit easier to construct. And a lot of the time that's what people actually use. So I think we've gotten to the point where people say snark for everything, even if the pay is not really there, but that's kind of like an optional property.
00:39:15.010 - 00:40:10.510, Speaker F: One thing I wanted to emphasize, which is something that you both said, but I think it's just so important is just like the concrete implication of this at least one concrete implication of zero knowledge proofs for blockchains is that if you look at the way blockchains work today, every single node in the network essentially has to re execute every transaction of every computation in order for the network as a whole to build confidence that the transactions were performed correctly. And one of the implications of a zero knowledge proof is that with this technology, you can have a single node run all of those transactions, run all of those computations, and then generate a proof, a zero knowledge proof that those computations were done correctly. And then everyone else in the network just has to validate that proof. And that can be done much, much more efficiently than actually running the computation itself. That's a succinct property that Tim was referring to.
00:40:10.580 - 00:40:12.158, Speaker E: And so that has massive implications to.
00:40:12.164 - 00:40:37.030, Speaker F: The way that Blockchains get architected to the potential performance and scalability improvements that we can see on Blockchains in the future. It is also a key aspect of the implementation of roll ups, like zero knowledge roll ups that are based on this insight. So just wanted to emphasize that. Scott, I know you've done a lot of work on the creator economy and there's like a lot of overlap between that whole body of work and crypto and Web Three.
00:40:37.180 - 00:41:08.174, Speaker A: Awesome. It very much ties in with these questions because a lot of it comes down to access and infrastructure. So first of all, just go back to NFTs as a conceptual example. What are they doing? They're enabling types of transactions that have been either nonexistent or just very difficult to do in the past. So if you create a digital good sort of pre modern crypto infrastructure. There was no way to define the owner of that good absent a centralized intermediary that was actually just defining ownership.
00:41:08.222 - 00:41:08.386, Speaker E: Right?
00:41:08.408 - 00:42:14.838, Speaker A: So we had ledgers of digital goods in computer games, other sort of like closed economic ecosystems. But if you were a creator whose creative form was digital, whether it was digital art or digital music or certain forms of writing and things, you didn't have a way to sell the digital asset because there was no way to define the owner. And so one fundamental use case for NFTs is a sort of digital deed. And because they exist in the blockchain in a way that is transparent and verifiable and builds on all of this interoperable transaction architecture, one can create an NFT or can create a series of them around whatever your creative activity is. Now suddenly you can have a digital goods economy and you don't need an intermediary like a label in the case of music or a gallery, necessarily in the case of art, to certify you and your assets and people can buy and sell it. And if you can build a community of enthusiasts around your work, just like I was talking about before, you could spin up a broader ecosystem as of enthusiasts. And so we're seeing this emergent creator economy built on digital goods, and so that's one really powerful opportunity.
00:42:14.838 - 00:43:20.954, Speaker A: But of course, we've pushed trust to a different level. Right, so talking about consumer protection, how do people curate and keep safe their digital assets while engaging in this marketplace? All of these are contexts where there are incentive design challenges wrapped in technical challenges, right? Like, what can we do to give people the ability to interact their digital assets with a variety of different online applications without putting them at risk? Right now, you don't know what you're signing, and so you don't know what you can interact with necessarily. Can we create technology that wraps your assets in a way that keeps them safer and you can do more of these outside interactions with less concern? And then on the creator side, this enables creators to enter more easily, and also it enables them to be platform independent, right? If you decided suddenly you wanted to transition from YouTube to TikTok, maybe you could export your content, you could reformat it and so forth, but you can't bring that reputation with you. Web three, at least in principle, lets you do that. All of these assets belong to you and the reputation that builds up around them. Like you control it and you just point different applications to it. This semester been teaching a lot of our first ever HBS course materials on this stuff.
00:43:20.954 - 00:44:01.846, Speaker A: I wrote the first case on NFTs or the board ape yacht club case. I wrote one on the looksrare vampire attacks. Anyhow, what I show to people when I'm trying to explain to them what makes crypto powerful to me is all the ways in which crypto is either enabling new types of transactions that we always wanted to do but didn't have a technology to do, or enabling us to do existing types of transactions in more efficient ways. And all of this, at some fundamental level, comes down to if you want to have markets in something, you have to have property rights. And we didn't have property rights over these digital assets, but now we do. Or at least we have a way to create them. And so, similarly, this idea of smart contracts as trusted code it's funny.
00:44:01.846 - 00:44:15.680, Speaker A: Tim talked about going back in time into classic computer science. We're also going back in time to classic economics, right? Like the earliest models of contracts assumed trust and assumed the contract was going to behave as planned. And so now we're living that again. Or finally, rather.
00:44:18.770 - 00:44:42.962, Speaker B: Thank you for listening to Web Three. With a six and z. You can find show notes with links to resources, books or papers, discussed transcripts, and more at Asics and Zcrypto.com. This episode was produced and edited by Sonal Choxy. That's me. The episode was technically edited by our audio editor, Justin Golden, with seven Morris credit also to Moonshot design for the art. And all thanks to support from Asics and C crypto.
00:44:42.962 - 00:44:58.280, Speaker B: To follow more of our work and get updates, resources from us and resources from others, be sure to subscribe to our Web Three weekly newsletter. You can find it on our website@asicsncrypto.com. Thank you for listening and for subscribing. Let's go.
00:44:59.610 - 00:45:17.690, Speaker A: You it's.
