00:00:09.240 - 00:00:09.790, Speaker A: You.
00:00:11.760 - 00:00:26.270, Speaker B: So next up we've got Michael Shoe and Sam Ragsdale from a 16 Z talking about the Lasso and engineering heavy overview. So I'll hand you straight over and enjoy. Lovely, thank you.
00:00:33.280 - 00:00:37.712, Speaker A: Hello, I'm Sam Ragsdale, an investment engineer with a 16 Z crypto.
00:00:37.776 - 00:00:40.176, Speaker C: And I'm Michael Zoo, a research engineer.
00:00:40.288 - 00:01:11.384, Speaker A: We will be talking about Lasso and Jolt. So these are two closely related research papers. Lasso was written by Srinath SETI justin Thaler and Riyadh Wabi. It provides a faster lookup argument prover than the existing constructions it's capable of proving. Lookups into massive tables. Think on the order of two to the 128 entries and you pay costs roughly proportional to the number of lookups rather than the table size. Jolt is a closely related work which builds on top of Lasso.
00:01:11.384 - 00:01:39.056, Speaker A: It was written by Erasu Arun Srinath SETI and Justin Thaler. It provides a new paradigm for building Zkvms. It applies Lasso to an entire instruction set. Thank RISC five WASM or the EVM. It has a faster prover with more accessible devex and easier auditability in terms of an agenda. First we're going to do a little bit of background on what are lookups. A lot of people here probably know, but we'll set the stage briefly.
00:01:39.056 - 00:02:22.260, Speaker A: Then we will give a high level overview of the lineage of Spark, Surge, Lasso and Jolt. Then we'll do a brief detour into multilinear extensions. Zach did a pretty good one earlier, but we'll do that again so that everybody can follow along. And finally, we'll spend the majority of the time at building a full single instruction Jolt VM to show what that developer experience looks like. So, first, background, what are lookups? So this is a four bit bitwise or operation. Bitwise operations are ones where you take integers, you represent them as bit vectors, and then you operate over each pair of bits sequentially. So these are quite simple on silicon.
00:02:22.260 - 00:03:01.170, Speaker A: CPUs are designed to do this at the speed of light, but unfortunately they represent a lot of complexity within Snarks. And that is because we have to take our integers. Our field elements represent them as binary field elements, which can be 64 bits or even 256 bits. Decompose them, operate over each of those and then recompose them. And this is very expensive. So what can we do instead? There is a strategy known as a lookup, where we pre compute all permutations of the operands. In this case, we have two two bit operands for a total of four bits of operands, creating a table of size two to the four.
00:03:01.170 - 00:03:55.264, Speaker A: We can pre compute that, then we can commit to it and we can create simple arguments or simple operations within a Snark where we look up to make sure that an operation lives within this table. Unfortunately, for large VMs with 64 bit operands, this table grows to size two to the 128 rows, which is impractical to pre compute, much less to commit to or create evaluation proofs over. So what can we do instead? In practice, what we usually do is a chunking operation. So these are 32 bit operations or 32 bit operands. We represent them as bit vectors and rather than split them out into individual bits, we're going to split them into eight four bit chunks. This creates eight bit operands. And then we only need an eight bit lookup table or a size two.
00:03:55.302 - 00:03:56.820, Speaker D: To the eight rows.
00:03:58.120 - 00:04:34.290, Speaker A: So then we can look them up individually. Of course, this still has some overhead, but it's much less terrible than splitting it out into single bits or doing the opposite. So this brings us to an interesting argument. So most lookup arguments have prover costs roughly proportional to the table size. We like to analogize this to building a library, building all the bookshelves, filling it with books and then getting on a ladder and pulling two books out of that library and then burning it to the ground. And this is hugely wasteful, as you can imagine. So it begs the question if there's a better way.
00:04:34.290 - 00:05:36.372, Speaker A: So this brings us to the high level ideas behind Lasso and Jolt. In 2020, Srinath SETI released a paper called Spartan, which introduced a sparse multilinear polynomial commitment scheme known as Spark. A sparse multilinear polynomial commitment scheme is one where you pay costs or you only pay for entries which are non zero. This applied it in the context of R one CS circuitsat, which if you've ever done that, you'll know that those matrices have a lot of zeros in them. And so it's very wasteful to commit to all of those indices. Lasso instead has this commitment scheme called Surge, which applies a similar idea. But rather than to R one CS circuitsat, we'll apply it to sparse lookups where the sparsity is that we have this whole table and we only use a subset of the rows within that table, which Lasso is a thin wrapper on top of.
00:05:36.372 - 00:06:21.984, Speaker A: So this all brings us to Lasso. The high level idea behind Lasso is rather than use lookups as a niche tool of VM design, we can look up all operands in a VM. We can do this regardless of their complexity. So if you're doing the bitwise operations or you're doing the multiply add divide, they all have exactly the same complexity. You can create these two to the 128 size row tables freely and Lasso will do all the heavy lifting for you. So this chart here, we show 264 bit operands doing bitwise and and we show that we can prove roughly a million operations or two to the 20 in around 17 seconds. So there's two high level ideas that we want you to take away from this.
00:06:21.984 - 00:06:54.030, Speaker A: One is that Lasso is very fast and Jolt VMs built on top of Lasso will be very fast, hopefully in the future. So here we compare in the most apples to apples way we can against Halo two, both the IPA and KZG back ends, we show that around a million instructions, we are ten to 40 times faster. The second important takeaway is that Lasso is simple and the Jolt VMs built on top of Lasso are simple. They're highly modular and highly comprehensible. And we'll try to spend the rest of today proving that to you.
00:06:57.040 - 00:07:48.350, Speaker C: All right, so for some quick math background so Lausu and Jolt heavily leveraged techniques based on multilinear extension polynomials. So for those of you who may be more familiar with their univariate counterparts, we're going to give a brief overview and comparison of the two. So let's say we have some data that we'd like to encode as a polynomial. So here we have these field elements 867-5309, and what we can do is use univariate LaGrange interpolation. So basically these elements get mapped to these points where the x coordinates correspond to their indices within the vector. And then we just interpolate a polynomial on top. So this q polynomial, we have q of zero equals eight, q of one equals six, and so on.
00:07:48.350 - 00:08:33.704, Speaker C: And we can also evaluate this polynomial outside of the domain zero to six. So at an arbitrary field element, say 123, moving to multilinear extension polynomials, we have a similar situation. So the indices of the vector still correspond to the domain of the polynomial. But now instead of just a single field element, the domain is multivariate. So what we're going to do is encode each index as basically a bit vector of field elements. So where before we had q of zero equals eight, we now have q of equals eight. Before we had q of one equals six, now we have q of one equals six, et cetera.
00:08:33.704 - 00:09:18.170, Speaker C: And once again, we can evaluate outside of the Boolean hypercube at arbitrary field elements and we can obtain this multilinear extension polynomial using the multivariate version of LaGrange interpolation pictorially that looks something like this. So on the left we still have our univariate extension polynomial, on the right we have the domain for the multilinear extension, which is this Boolean hypercube, which in this case is just the Boolean cube instead of the x axis as the domain. So the multilinear extension polynomial is just going to map points on this cube to elements in this vector like so.
00:09:21.500 - 00:09:41.872, Speaker A: Cool. So this brings us to the Boolean hypercube, which we wish we could stand up here and tell you that this is the Boolean hypercube. And if you stare deep into it, you'll find the evaluations you seek. But unfortunately, this is the reality. This is the boolean. Hypercube. It's just sequential integer indexes represented as bit vectors instead.
00:09:41.872 - 00:10:39.540, Speaker A: And if you want to represent a value in the range zero to the two b minus one, you have to represent it in b binary bits. We do this so that we can use multivariate polynomials instead for use in the sum check protocol. So now we're going to do a little detour and talk about in depth what the multilinear extension of the EQ instruction would look like. The EQ instruction is one which takes two integer values, returns zero if they're unequal and one if they're equal. Because we're using multivariate polynomials or multilinear extensions, we are going to have all of the inputs be bits or binary rather than full integers. So if we wanted to represent two bits or represent 0123, we'd need two bits per operand. So this is the definition of the multilinear extension of a given operation.
00:10:39.540 - 00:11:32.448, Speaker A: The multilinear extension of an operation says is equal to that operation over the full domain on which that operation is defined. So in our case, it's all x and y operands that are length two bit vectors. I'm going to go ahead and tell you what the multilinear extension of this operation is, the EQ operation. And it's here in practice, this can be found for all instructions. So we'll expand this for b equals two for two bits. And on your own time you can plug in all values of 0123 as bit vectors and you'll find that the multilinear extension, or EQ twiddle matches the EQ operation. But the cool thing about EQ twiddle or the multilinear extension of EQ is that it's defined on a domain larger than that of the original operation.
00:11:32.448 - 00:12:03.084, Speaker A: So we can put in all length two vectors of field elements for each operand and we get another field element which gives us this distance amplified encoding that we need for sumcheck. So that's all the background we need. Now we can do the fun part. So we'll talk a little bit about the developer experience of jolt. So Lasso is out. We released that in mid August, that is, at a 16 zlasso on GitHub. Jolt is what we're still working on.
00:12:03.084 - 00:12:19.216, Speaker A: Jolt allows you to do multiple instructions within the same proofs. So the Lasso code base will allow you to do a lot of a single instruction. That's not particularly useful for building a VM. You need to be able to do multiple. So jolt will do the glue here. It's still in progress. It's on this branch.
00:12:19.216 - 00:13:10.816, Speaker A: It's highly unstable, but we'll give you a preview of it today. We think in a month or so when this is out, it'll look fairly similar. So if you were to build a jolt VM on your own, or build a Lasso implementation on your own, you'd spend many, many late nights with these two papers and the 60 pages of math in them. You'd probably eventually binary search down to this figure five, which is around page 40 of Lasso, which describes surge's polynomial IOP. The good news is that if you build on top of us, all you need to learn is this green box and what all of those symbols mean, and then you can build a full jolt VM. So there's two rust traits that you'll have to implement in order to implement a jolt VM. The first is the jolt instruction trait.
00:13:10.816 - 00:13:55.748, Speaker A: The first two functions here correspond to this function g, the collation function. The third corresponds to these T sub i, which are the subtables and each of them. For each subtable you have to define a Lasso subtable trait. And then finally there is this two indices function which corresponds to this indexing down here. For each of the subtables we have this Lasso subtable trait for which you have to define the materialize function, which is simply the evaluation over the Boolean hypercube. And the second is this evaluate multilinear extension function which is that EQ Twiddle we talked about earlier. There's also an example for and Twiddle here.
00:13:55.748 - 00:14:28.300, Speaker A: This exists for all subtables within jolt. To give a bit more detail, we're going to build a one instruction VM. For the eight bit EQ instruction. This is not particularly useful, but it'll give you the mental models you need to build out all jolt VMs. Also, we're doing eight bit EQ instructions here just because it's easier to represent it on slides. If we did 64 bit operands, nobody would be able to follow along. But it has exactly the same complexity from a developer standpoint.
00:14:28.300 - 00:14:54.848, Speaker A: So again, we'll take our EQ of integer operands. We'll convert it into bit vectors. So this is three and three. All of those bits individually are equal, so we'll return a one. Now we can do the same for eight and four. Those bits are not all equal, so we'll get a zero. So the high level mental model to come back to here is a lookup table.
00:14:54.848 - 00:15:47.236, Speaker A: A lookup table is an evaluation over the operation, over the entire domain on which it's defined. In our case, that's all bit vectors of length 16 or two eight bit operands, which means that this lookup table is of size two to the 16. We'll call this parameter n, which is the big table size. The issue that we mentioned earlier is that on 64 bit operands, this table ends up being size two to the 128, which is entirely impractically large. So we'll need to do something else. What we can do is we can split this eight bit EQ instruction into a series of two bit instructions, because bitwise operations of course are independent. So we can copy in two bits at a time into a two bit EQ prime operation.
00:15:47.236 - 00:16:27.404, Speaker A: So rather than doing EQ over eight bit integers, this does it over two bit integers. And then note here that if any of these EQ prime subtables return zero, it'll multiply through the rest. And the full big table evaluation will also be zero signifying that it's unequal. And now we have a much one other parameter to define here is C. This is the subtable dimensionality. In our case it's four. Because we use four EQ prime two bit subtables, this creates a much smaller table size.
00:16:27.404 - 00:17:03.228, Speaker A: Our subtable size is now of over four bit operands. All permutations of that would be of size two to the four, which allows us to define this additional parameter m, which is our subtable size. It's of size n to the one over c, or in our very specific example, two to the four. So that background gives us everything we need to understand the entirety of the green box I promised we'd explain. First, n is the big table size. This is two to the 16 for 16 bits worth of operands. Then we have this parameter k.
00:17:03.228 - 00:17:38.696, Speaker A: K is the number of unique subtable types you need to define a big table instruction. In our case, we only need that EQ prime subtable, the two bit subtable, which makes k equal one, which makes alpha equal to c. We can now define c as well, which is the number of subtables number of EQ prime subtables we used, which, if you remember, was four. This allows us to define M. M is the subtable size and it's n to the one over c. In our case, two to the four. And finally, we can define the collation function g.
00:17:38.696 - 00:17:53.650, Speaker A: This is how you combine subtables into a big table evaluation. And for us, we can just do the product of all of the subtables. If any of those subtables is unequal, then the entire big table evaluation is also unequal, also return zero.
00:17:57.620 - 00:18:40.830, Speaker C: All right, so returning to those two Rust traits that Sam introduced earlier, we're going to implement them for the equals instruction. So starting with the Jolt instruction trait, we have four methods to implement. First combined lookups is basically this g polynomial combining the subtable values that we just saw. So in the case of equals, that's just taking a product over all the subtable values, which is this one liner in Rust. Nice and easy. The g polydegree function returns the polynomial degree of g, which in our case is just taking the product over C variables. So this polynomial degree is c.
00:18:40.830 - 00:19:59.360, Speaker C: The subtables function is just returning the different types of subtables used. So as we saw just now, for the equals instruction, it only uses that EQ prime subtable. So we just return a length one vector containing that. But as Sam was alluding to earlier, in general instructions may use multiple different types of subtables, in which case that would be reflected in the implementation of this function. Finally, we have the two indices function, which is just converting the instruction operands, the eight bit instruction operands into the four bit indices, which we're going to use to index into the subtables. So, moving on to the Lasso subtable trait, we just saw that the equals instruction only uses the equals subtable. So for the equals subtable, the materialize function is just computing the entries of the table row by row, and it's going to do that by iterating from zero to m, the subtable size.
00:19:59.360 - 00:21:05.150, Speaker C: And at each step it's going to split that iterator into two two bit operands and just compute equals over those operands, writing down the result. And another way to frame it is we're just evaluating the eqmle over the entire small Boolean hypercube of size M and the Evaluate MLE method. Well, we already saw the MLE for Equals, which was this. And so this function is just implementing that in Rust. And this is going to allow us to evaluate the MLE on a vector of arbitrary field elements, not necessarily within the Boolean hypercube. And that's it. So we've implemented the Equals instruction and the Equals subtable both pretty concise and to sort of put it all together, if we wanted to implement an entire Jolt VM, well, first we would have to implement the individual instructions and subtables.
00:21:05.150 - 00:21:32.700, Speaker C: Once we have that, we just use a couple of macros to define the instruction set as well as the set of subtables needed for those instructions. We would set the constants CNM, which together would determine the VM word size as well as the size of the subtables. And once we have our VM to prove a sequence of instructions, all we have to do is pass in a vector of these instruction structs to the prove function.
00:21:33.470 - 00:22:26.970, Speaker A: So we hope we convinced you in the past ten to 15 minutes that building a Jolt VM is fairly simple and highly comprehensible. You'll notice that we didn't talk about Arithmetization high degree constraints, circuits, none of that stuff. All that stuff is dealt with for you. You have to define some multilinear extensions, a little bit of indexing, and some table collation stuff, but they're all fairly understandable functions. Additionally, one of the cool properties we get here is once we build out these subtables and the corresponding instructions that use them, there's a lot of high level VMs that can use the same implementations there. So you can imagine the EVM version and the WASM version of the Jolt VM sharing the same underlying instructions. This creates a shared auditing service, and many devs can sort of learn the same toolkit rather than sharding attention across very many different, very complicated code bases.
00:22:26.970 - 00:22:57.090, Speaker A: So that's it. If you have any questions, feel free to reach out to us directly on X.com. I'm at Samrags underscore. He's at Moodlezoop. Alternatively, feel free to reach out to us directly via GitHub. This is open source, and it will always be open source. Going forward, we have an Issues tab with a great range of issues, from basic stuff down to Arithmetization stuff to assembly level optimizations.
00:22:57.090 - 00:23:10.540, Speaker A: In addition, we're going to eventually have applications built on top of Jolt, and there's a lot of interesting work that can be done there. So if you want to contribute or you want to build on top, feel free to reach out or contribute directly. That's it. Thank you very much.
00:23:16.590 - 00:23:27.920, Speaker B: So we've got some time for questions now, if there are any. Oh, yeah, okay. Quite a few. Yep. One at the front here. Sorry, I'm making a tell you what, don't worry, Emma. I'm here.
00:23:27.920 - 00:23:29.760, Speaker B: There you go.
00:23:30.770 - 00:23:49.780, Speaker E: Hi. So one question is from developer point of view and developer, we deployed in some Ckevms and some of them don't support easy pairing pre compilers, so we cannot do certain ones.
00:23:50.790 - 00:24:25.940, Speaker A: Are you supporting today? So one, Jolt is not built yet. We're in the process of building it. The EVM particularly is difficult to implement given the 256 bit registers. We think we do have a strategy for it. We do have a plan to include in Jolt the concept of gadgets, which, rather than just looking up everything, you'll be able to sort of route to a specific circuit, like an optimized precompile circuit. So today we don't. We have a plan on sort of the very long term roadmap for doing so.
00:24:26.310 - 00:24:27.300, Speaker E: Thank you.
00:24:33.110 - 00:24:55.930, Speaker F: Thank you for the great talk. You gave an example of how to combine the multilinear extensions of the subtables for the quality operation. Do you have an example? Can you give an example of a more difficult operation to help grow the intuition of how to combine these subtable Emles?
00:24:58.190 - 00:25:28.726, Speaker A: The short answer is a lot of them are simple, but there's complicated ones, so all the bitwise ones are simple. For example, those basically require one subtable. Things like shift and less than greater than those get a bit more complicated. The point of the Jolt paper is to prove that you can do an entire instruction set in this structure. I think less than is a good example. This is available in the Lasso repo today. You can check it out.
00:25:28.726 - 00:25:57.838, Speaker A: But the less than instruction, for example, you have to decompose and you have both an EQ subtable and a less than subtable. And the idea is the EQ subtable continuously tracks whether the previous high order bits had already decided whether it was less than. And you multiply that more and more by the less than table to check if the high order bits have decided or not already, whether less than is true or not.
00:25:58.004 - 00:26:18.360, Speaker C: And for an example that isn't equals, that's different. We have the and MLE here. So combining the different and MLEs and this generalizes to all the bitwise operations, you would basically just be taking a weighted sum over the chunks to sort of concatenate them together.
00:26:19.130 - 00:26:22.040, Speaker F: Cool. Yeah, I think I've seen that in the repo. Thank you.
00:26:28.450 - 00:27:04.618, Speaker D: For the example of less than or greater than. I wanted to ask if you could expand on that because my current understanding, you would consult the Is less than table. And then the way that you broke down, say, a 64 bit arithmetic, 64 bits into a bunch of eight bit, three tuple, eight bit tables, you would then have to check each of these three eight bit tables in parallel. And if any one of them failed, you had an Is less than. Or could you expand on that a little more.
00:27:04.784 - 00:27:32.654, Speaker A: Yes. Let me try to do less than live. It's a fairly complicated table, but you can imagine let's take integers. That two integers that you're comparing. Let's say that the subtables are you have C equals four, which means that you have four subtables of each type. The MLE would first check the subtable of less than for the high order bits. You can imagine that being inconclusive only in the case that all of them are equal, like all the high order bits are equal.
00:27:32.654 - 00:27:55.180, Speaker A: And then you have to go to the next set. So you would take the evaluation would be the less than subtable of the high order bits plus the less than subtable of the next lowest order bits. Next highest order bits times the EQ subtable of the highest order bits and then sequentially for the remaining few.
00:27:55.950 - 00:27:57.210, Speaker D: Okay, that would make sense.
00:27:57.280 - 00:27:58.140, Speaker C: Thank you.
00:28:00.030 - 00:28:16.690, Speaker G: So my understanding of Lasso is that it's the lookup argument, right? And then jolt connects that with R one CS and as well as a memory lookup argument to prove that fusion of VM. So would this API also cover the RNCs part and as well as the memory argument?
00:28:17.750 - 00:28:28.390, Speaker C: RSU, one of the co authors on the jolt paper is working on the R one CS bit of jolt, sort of in parallel with our repository on GitHub.
00:28:28.730 - 00:28:47.770, Speaker A: You're right, though, that we do need a separate R one CS that's custom per jolt VM. We're in the process of figuring out sort of what that spec looks like generically. But the R one CS is fairly simple. It covers sort of like making sure that register updates happened correctly and that we are decomposing the indexes correctly.
00:28:48.190 - 00:29:04.210, Speaker G: Some VMs may not have registers. And so I'm just trying to understand whether this is just for Lasso, where it's like drill in general. You can also optimize. For example, I customize I have all prem stack where, like, registers, no registers versus different memories per for preinstruction.
00:29:04.550 - 00:29:07.490, Speaker A: Yeah, we think all of that is handled by the R one CS.
00:29:07.830 - 00:29:09.060, Speaker G: Okay, thanks.
00:29:10.950 - 00:29:15.390, Speaker B: Lovely. I think that's all we have time for today. So thank you very much, Michael and Sam.
