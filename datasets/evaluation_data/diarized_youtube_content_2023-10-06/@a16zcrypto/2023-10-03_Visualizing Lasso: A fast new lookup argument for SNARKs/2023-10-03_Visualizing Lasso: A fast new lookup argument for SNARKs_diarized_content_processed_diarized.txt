00:00:00.730 - 00:00:22.030, Speaker A: In this presentation I'll be going over Lasso, a new lookup argument authored by Srinath SETI, Justin Thaler and Riyadh Wabi, which enables efficient lookups over huge lookup tables, leveraging techniques introduced in Spartan as well as multi linear sumcheck. Alright, so I'm going to give an overview of Lasso in a visual map like format.
00:00:22.370 - 00:00:24.998, Speaker B: So first to start, this is the.
00:00:25.044 - 00:00:40.490, Speaker A: High level overview of this presentation. We're going to start with the motivation or setup of how we arrive at the particular statement that Lasso proves. And then we'll dig into the details of Lasso itself. As you can see from this picture, Lasso is a pretty thin wrapper around.
00:00:40.560 - 00:00:43.750, Speaker B: Surge, which is a generalization of spark.
00:00:43.830 - 00:00:51.666, Speaker A: The sparse polynomial commitment scheme introduced in the Spartan paper. And within surge there are roughly three phases.
00:00:51.798 - 00:01:00.206, Speaker B: There's the commitment phase, there's what we've been calling the primary sum check, and then there's memory checking and we'll get.
00:01:00.228 - 00:01:11.250, Speaker A: Into what each of those mean later. But going back to the motivation here, conceptually, what we want to prove is this statement. Looking up indices M of table T.
00:01:11.320 - 00:01:14.740, Speaker B: Gives you A in linear algebra form.
00:01:15.050 - 00:01:43.374, Speaker A: That looks something like this. So you have a lookup table T, which in this picture contains these entries 867-5309 and you're going to perform some lookups into that table as specified by this binary matrix M. Each row of this matrix corresponds to one lookup into this table. The first row of this matrix has a one in column three. So it's performing a lookup into the.
00:01:43.412 - 00:01:46.606, Speaker B: Third entry of T and we can.
00:01:46.628 - 00:02:00.334, Speaker A: See that the first, or rather zero th entry of the lookup results vector A contains a five. Similarly, the second row of M has a one at index two. So it's looking up the second entry.
00:02:00.382 - 00:02:04.942, Speaker B: Of T and A has the element.
00:02:05.006 - 00:02:18.374, Speaker A: Seven in its first index. In practice T is potentially very big. You can think of it being roughly two to the 128 in size. And then of course M has to have N columns to match the size.
00:02:18.412 - 00:02:21.066, Speaker B: Of T. The number of rows of.
00:02:21.088 - 00:02:57.686, Speaker A: M corresponds to the number of lookups which we'll be referring to with the variable S throughout. And that's also the size of the vector A. So assuming T is huge, m is going to be much wider than it is tall. And with this sort of linear algebra equation in mind, the correctness of A can be checked using the sum check protocol which can be formulated like this. We have the multilinear extensions of M, T and A and we're going to be summing over all the entries of the table. So if R is in the Boolean hypercube of dimension log s, then the.
00:02:57.708 - 00:03:00.534, Speaker B: Left hand side and the right hand.
00:03:00.572 - 00:03:20.726, Speaker A: Side should be equal to one another. Conceptually, if R is all zeros and ones, it would correspond to a particular lookup or a particular row of M. So for one particular term in the summation, mtwiddle would be equal to either zero or one, depending on if that particular lookup is looking at the corresponding.
00:03:20.758 - 00:03:22.906, Speaker B: Entry of the table t if it.
00:03:22.928 - 00:03:52.178, Speaker A: Is, then this Mtwiddle would be one, and that gets multiplied by the appropriate table entry. So this is a valid expression that can be proven using the sumcheck protocol. But the problem, of course, is that M is huge. M is way too big to commit or to evaluate the MLE of efficiently. So we want to reformulate the sum check, or this expression on the left hand side into something that's more tractable. Specifically, we're going to reformulate it as a summation over a smaller Boolean hypercube.
00:03:52.274 - 00:03:53.814, Speaker B: Which is depicted here.
00:03:54.012 - 00:04:11.258, Speaker A: So the high level idea is that we're going to be summing over the lookups instead of the table entries. Recall that the number of lookups is way smaller than the number of table entries. So we're changing the summation from y in zero one to the log N to j in zero one to the.
00:04:11.264 - 00:04:13.486, Speaker B: Log s. And again, the left hand.
00:04:13.508 - 00:04:15.166, Speaker A: Side is formulated to be equal to.
00:04:15.188 - 00:04:17.166, Speaker B: The right hand side when R is.
00:04:17.188 - 00:04:55.500, Speaker A: In the Boolean hypercube. This EQ twiddled term that you see here is basically a selector. So it's one if r is equal to the particular lookup index given by j and zero otherwise. So now that we're summing over lookups, we're changing T total of y to this expression. Now we have the table capital T, and we're indexing into it using this function NZ of J. So if we're looking at the Jth lookup, NZ is mapping that to the table index being read by the Jth lookup. Pictorially, that looks something like this.
00:04:55.500 - 00:05:05.482, Speaker A: So previously we had this matrix M determining our lookup indices, where the entry being looked at was specified by whether there's a one in the corresponding column.
00:05:05.546 - 00:05:07.070, Speaker B: Of A for that row.
00:05:07.410 - 00:05:25.926, Speaker A: So now we're just basically converting the rows into a single value. Since the first row has a one in column three, we put a three in the zero th entry of NZ. So here we're just writing NZ as a vector instead of a function. And row two of M has a one in column two. So we're putting a two in the.
00:05:25.948 - 00:05:29.510, Speaker B: First index of NZ and so on and so forth.
00:05:30.250 - 00:05:49.882, Speaker A: Conceptually, M was this very sparse matrix before with lots of zeros, and we're effectively densifying it into this NZ vector. This is going to allow us to do some check much more efficiently. So now we still have the problem that T might be really big, but assuming T has what the Lasso paper.
00:05:49.936 - 00:05:55.278, Speaker B: Calls spark only structure, which we're not going to formally define here, but basically.
00:05:55.364 - 00:06:41.818, Speaker A: That just means that T can be decomposed into smaller subtables. So with this decomposition, we can rewrite this statement as this. So we're replacing the big table T with this function g applied to reads into these subtables T one through TC. We're also splitting NZ into quote unquote chunks of indices NZ one through NZC. So what does this decomposition look like in practice? So, before the decomposition, we had something that looks like this. So here we're looking at three and bitwise and operations on various operands. So these correspond to these entries of NZ.
00:06:41.818 - 00:06:59.550, Speaker A: Basically, the operands are being concatenated together to get the corresponding NZ entry. And those NZ entries are being used to index into the big table T, where the table entry contains the output of the bitwise and on the corresponding operands.
00:06:59.710 - 00:07:03.806, Speaker B: So, for example, this second and operation.
00:07:03.998 - 00:07:24.394, Speaker A: Is operating over all zeros and all ones and the output of that should be all zeros, which is what's, in fact, in the table T here. And recall that this table in practice would be really big. So something like two than 128 entries. So we can arrive at the same result with a few extra steps by.
00:07:24.432 - 00:07:27.674, Speaker B: Decomposing, which is depicted here.
00:07:27.872 - 00:07:52.260, Speaker A: The high level observation is that the result of the bitwise and can be computed from smaller bitwise ands basically splitting up the operands. So here we have the same bitwise and operations. The first one has the operands and 10 one. But we can break the left and right operand into two bits each.
00:07:52.630 - 00:07:54.066, Speaker B: So the first two bits of the.
00:07:54.088 - 00:08:30.494, Speaker A: Left operand are zero one, and the first two bits of the right operand are zero zero. So that's why we have this right here. And in total, we're going to be breaking it into three chunks. So one, two, three. Now, each chunk will get its own NZ vector, which again, is just concatenating these now smaller operands and the entries of NZ are used to index into these now smaller subtables T one through TC. Just like the big table, these subtables contain the outputs of bitwise and now.
00:08:30.532 - 00:08:32.310, Speaker B: Over two bit operands.
00:08:32.490 - 00:08:34.946, Speaker A: So the table entry corresponding to the.
00:08:34.968 - 00:08:40.526, Speaker B: Index contains and of course, the subtables.
00:08:40.558 - 00:08:42.014, Speaker A: Are now going to be much smaller.
00:08:42.062 - 00:08:45.106, Speaker B: Than the big table T. And finally.
00:08:45.208 - 00:09:06.790, Speaker A: Once we have the values read from the subtables, we can recombine them into the result that would be read from the big table. So, in this case, the combination is basically just concatenating together the results read from the subtables into a single value. And as you can see, the values we end up with are the same that were found from the big table.
00:09:06.870 - 00:09:07.500, Speaker B: T.
00:09:10.110 - 00:09:41.298, Speaker A: Going back to the equation, the G function is just the function that does the combining of the subtable. Lookups, at this point, we've already introduced what NZI and Ti are going to look like, but a few housekeeping items we'll need to commit to these subtable indices. So to do that, we'll need them in MLE form. That's pretty straightforward. We're just going to define these dim sub I polynomials to be equal to the multilinear extension of those NZ sub I's. So that's going to have this domain and range.
00:09:41.394 - 00:09:44.822, Speaker B: So f log s to f just.
00:09:44.876 - 00:10:22.846, Speaker A: Subbing in dimi for NZI we get this expression. One remaining thing is that everything in the summand needs to be a polynomial for some check to work. At this point we still have these tis which are not polynomials, they're subtables, which can be thought of as vectors or arrays. So we're going to need to change those into polynomials somehow. What we're going to do is basically define these EI polynomials which combine the ti and dimis together effectively. So it's just the multilinear extension of all the subtable values red for the relevant subtable lookup indices.
00:10:23.038 - 00:10:26.822, Speaker B: So that's this expression here. And that takes us to the final.
00:10:26.876 - 00:10:46.102, Speaker A: Expression that we want, which is this. And this is what we're going to end up proving in Lasso. So now that we're done with the background, we can get into Lasso itself. So just to recap, the high level statement that we're looking to prove is that looking up indices M of Table.
00:10:46.166 - 00:10:49.258, Speaker B: T gives you a lasso is going.
00:10:49.264 - 00:11:13.106, Speaker A: To do that in four steps. So first it's going to send a search commitment to the verifier and we'll see in a bit what that commitment consists of. Step two, the verifier is going to pick some randomness R consisting of log s field elements, and this of course could be fiat shamired. Step three, the verifier is going to obtain the value a Twiddle of R by making one evaluation query to a.
00:11:13.128 - 00:11:17.970, Speaker B: Twiddle which is committed to outside of Lasso, for example, in jolt.
00:11:18.390 - 00:11:32.294, Speaker A: And finally, the proverb and verifier are going to apply the surge protocol to prove that this equation that we arrived at earlier in fact holds. And it's important to caveat here that it's assumed at this point that EI is in fact equal to the subtable.
00:11:32.342 - 00:11:35.286, Speaker B: Values red and we'll be using memory.
00:11:35.318 - 00:11:43.150, Speaker A: Checking techniques to prove that this is in fact the case. So you'll see that these dotted lines are connecting things to parts of surge.
00:11:43.970 - 00:11:46.474, Speaker B: So step one is the search commitment.
00:11:46.602 - 00:12:04.738, Speaker A: We're going to need to commit to these polynomials at the outset and this can be done using any multilinear polynomial commitment scheme, in particular a dense multilinear PCs as opposed to a sparse one. For example, in the Lasso implementation, as in Spartan, it uses Hierarchs as the.
00:12:04.744 - 00:12:06.854, Speaker B: PCs, but you could sub in something.
00:12:06.892 - 00:12:34.960, Speaker A: Else yielding different performance characteristics or other properties. So the first set of polynomials is the e sub I polynomials. Again, these are basically encoding the values read from the subtables. We're also going to need the dim I polynomials. Those are the chunked lookup indices as we saw before. And finally we'll need these counter polynomials, read counts and final counts. Those are going to be used in memory checking and we'll introduce them then.
00:12:35.970 - 00:12:37.614, Speaker B: So once the prover has sent over.
00:12:37.652 - 00:12:42.320, Speaker A: These committed polynomials to the verifier, we can move on to the primary sum check.
00:12:42.690 - 00:12:44.318, Speaker B: So the prover and verifier are going.
00:12:44.324 - 00:12:47.806, Speaker A: To apply sumcheck protocol to the summation we saw earlier.
00:12:47.998 - 00:12:51.586, Speaker B: We're not going to get into the details of the sumcheck protocol here, but.
00:12:51.608 - 00:13:04.914, Speaker A: It'S basically going to reduce this check into these checks. So the EI polynomials evaluated at some random point RZ chosen by the verifier.
00:13:04.962 - 00:13:06.854, Speaker B: Over the course of sum check is.
00:13:06.892 - 00:13:09.686, Speaker A: Equal to these VI values provided by.
00:13:09.708 - 00:13:13.114, Speaker B: The prover at the end of sumcheck. And this is for I.
00:13:13.152 - 00:13:15.162, Speaker A: From one to C. In other words.
00:13:15.216 - 00:13:18.266, Speaker B: For all the EIS, the Verifier is.
00:13:18.288 - 00:13:41.490, Speaker A: Going to obtain EI of RZ by making one evaluation query to each EI, which, if you follow the dotted line, was committed to back in the commitment phase. So at this point there's still no guarantee that like we said, EI actually encodes the subtable values. So we need to somehow prove verify that the EIS are in fact consistent with the Tis and Dimis.
00:13:42.070 - 00:13:45.090, Speaker B: To do that, we're going to apply memory checking.
00:13:45.990 - 00:14:33.570, Speaker A: This takes us to the third step of surge. So just for definitions and setup, this is how we're going to depict a memory. Basically it's a vector or an array of tuples. So the memory is indexed by these addresses, these red A values and each memory cell is going to contain a memory value and a counter which keeps track of how many times that particular memory cell has been accessed. Note that in this specific subtable that we've depicted, the addresses are going to equal the values for all the memory cells. This table is used for range checks, but in general the values are not necessarily equal to the addresses. So to take a sequence of lookups and represent them as memory access patterns, we're going to think of this as occurring in four phases.
00:14:33.570 - 00:14:36.710, Speaker A: We'll have an initialization phase.
00:14:38.730 - 00:14:44.040, Speaker B: A sequence of reads and writes and end up at the final memory state.
00:14:45.050 - 00:14:53.286, Speaker A: In the initialization phase, the memory values are all initialized to the corresponding subtable entries. In other words, each V is equal.
00:14:53.318 - 00:14:57.962, Speaker B: To T sub I of A. All the counters are set to zero.
00:14:58.016 - 00:15:00.086, Speaker A: Upon initialization because none of the memory.
00:15:00.118 - 00:15:01.758, Speaker B: Cells have been accessed yet.
00:15:01.924 - 00:15:48.826, Speaker A: This initialization step can be represented by this set I consisting of tuples, each tuple containing an address value and counter. This set I is of size M, which is the same as the size of the memory. Now each lookup into the memory is going to be represented by a paired read and write operation. So the address corresponds to this dim sub I polynomial of j. The read operation is just going to add this tuple dim I of j VT to the read set and there's going to be a corresponding element added to the write set which is the same tuple, except incrementing the counter by one. Note that the read and write sets are going to be the same size, namely S, which is the number of.
00:15:48.848 - 00:15:50.906, Speaker B: Lookups at the end of all the.
00:15:50.928 - 00:16:27.606, Speaker A: Lookups or all of the reads and writes, we're going to end up at this final memory state. Note that the addresses and values are the same as upon initialization. It's just the counters that have been changed. So the final counter value is basically the number of times that memory cell has been accessed. So in this picture, the zero th cell has been accessed twice, the first cell once, the second cell five times, et cetera. And again this set F is going to have size M corresponding to the size of memory. Now with these init read write and final sets in mind, the memory checking invariant that we want to hold is.
00:16:27.628 - 00:16:31.274, Speaker B: The following I union w equals r.
00:16:31.312 - 00:17:03.970, Speaker A: Union F. So init union, write equals read union final. Intuitively, every memory tuple that was read must have been written at some point. The init tuples you can think of as basically a special case of write and the final tuples are a special case of read. In order to efficiently check this invariant, we're going to use some fingerprinting techniques which you can think of as hash functions. Basically it would be really inefficient to check every element of each one of these multi sets to make sure that they're equal.
00:17:04.310 - 00:17:06.366, Speaker B: Instead, we would much rather check the.
00:17:06.408 - 00:17:26.458, Speaker A: Equality of, for example, two field elements. So in order to get there, first we're going to take the read solomon fingerprint of each tuple. So we have this lowercase H sub gamma mapping the address value counter to a single field element like so. But what we really want is to go from the entire multi set of.
00:17:26.464 - 00:17:28.746, Speaker B: Tuples to a single field element so.
00:17:28.768 - 00:17:30.860, Speaker A: That'S this capital H hash function.
00:17:31.550 - 00:17:33.182, Speaker B: So for each tuple in the set.
00:17:33.236 - 00:18:22.330, Speaker A: We'Re going to apply the read solomon fingerprint function to it and then subtract tau and then multiply everything together. So as you can see, that gives a single field element for the multiset. So now this invariant holds if and only if up to some security parameter. This equation holds. So the multiset hash of I times the hash of W equals the hash of R times the hash of F. And since each multiset hash is basically a huge product over things, it can be written as a binary tree circuit of multiplication gates where the leaves are the read solomon fingerprints of each tuple. This allows us to prove the value of each multiset hash using the grand product argument from Thaler 13.
00:18:22.330 - 00:18:34.826, Speaker A: The grand product argument is just an optimized version of the GKR protocol for binary trees of multiplication gates. At the end of the grand product argument, the verifier is going to need to evaluate the MLE of the leaves.
00:18:34.858 - 00:18:37.778, Speaker B: At a random point and that particular.
00:18:37.864 - 00:18:50.534, Speaker A: MLE is going to differ based on which set it's for. So for the read MLE, you're going to have one leaf per lookup. So the addresses are going to be the dim subi polynomials evaluated at some.
00:18:50.572 - 00:18:56.166, Speaker B: Index k. The values are going to be given by Esabi, and this is.
00:18:56.188 - 00:19:44.230, Speaker A: Where the read countered polynomial that we introduced earlier is going to be used. So this polynomial was committed to in the commitment phase and will be used by the verifier to evaluate this readmele. The write MLE is identical, except the counters are going to be one more than the ones in the readmele. And then for init and final, the number of leaves is instead the size of the memory. So in this address term here, we're basically converting this bitvector k into its corresponding field element representation. The value is going to be the subtable moe valuated at k and for init, the counters are all zero. For final, we're going to use the final counters polynomial that we committed to in the commitment phase.
00:19:44.230 - 00:20:02.654, Speaker A: So that concludes memory checking. We've shown that the EIS, Tis and Dimis are consistent with each other. So that means that this statement that we proved in the primary sum check suffices to show that looking up indices M of Table T in fact gives.
00:20:02.692 - 00:20:05.694, Speaker B: You the vector A and that's it.
00:20:05.732 - 00:20:22.350, Speaker A: That's Lasso. We hope you found this explanation helpful. There's a working implementation of Lasso on GitHub which you can find linked in the description alongside the Lasso paper and a 16 z crypto blog posts about Lasso and its companion work Jolt.
