00:00:10.060 - 00:01:01.024, Speaker A: So I think the way that jolt works actually counters a number of narratives or just general beliefs in sort of the community that's building and deploying these snarks. So I wanted to describe some of them. So one of them, and you see this viewpoint expressed prominently, like on Twitter all the time, is that simpler instruction sets should lead to faster Zkvms. And this is a pretty intuitive statement. It's like if each step of the VM is simpler, it should be faster to prove one step of the VM. And this sort of viewpoint is sort of taken to its most extreme in, I think, the Cairo virtual machine, which was specifically designed to have kind of very small set of very snark friendly instructions. But we saw yesterday that two things.
00:01:01.024 - 00:02:01.300, Speaker A: So one is, I guess, the commitment costs for the prover and jolt are less than existing zkvm projects. And two, the scalability of the jolt prover actually, oddly, it doesn't really depend on the complexity of the instructions. So it's something a little weird. There's some kind of threshold behavior where as long as the instruction leads to a structured lookup table, whatever structure Lasso or generalized Lasso needs, it doesn't matter how complicated the instruction is. The proverbs costs are actually determined by the size of the lookup table, which only depends on how big are the inputs to the instruction. Okay, so another narrative out there is there's a heavy focus on supporting what are called high degree constraints. So this is just like, you can think of this as a generalization of multiplication gates, where you might be sort of multiplying more than two numbers or doing other operations that don't correspond to degree two polynomials.
00:02:01.300 - 00:02:57.392, Speaker A: And for example, there's a recent work called Hypernova and Protostar which looked to take sort of a very different approach, very exciting approach to snarks distinct from anything I'm telling you about today, which previously were only known to support degree two constraints and allow them to support higher degree constraints. And people were very excited about that. Now, jolt kind of pushes all the work, the prover does almost all of it, into the lookup argument. And what's left is very simple and easily captured not just by degree two constraints, but actually a very special kind of degree two constraints called R one CS. Okay, I still think high degree constraints are important, but if your goal is a Zkvm anyway, it's not actually clear at all that it's important. So kind of what's more important than high degree constraints is being able to use a good lookup argument, basically. Okay, there's also a view that snarks over large fields are wasteful.
00:02:57.392 - 00:03:49.300, Speaker A: This is actually one of the main reasons I think people like to use Fry. I mean, it's also avoids what occurs. So it's plausibly post quantum secure. So that's nice. But in certain settings, one, like field operation over small field is much faster than a field operation over a big field, right? And I actually have a blog post coming out soon where I'll discuss why I think even if you can work over small fields in some applications, it doesn't actually make sense. Anyway, I don't have to get into that now. I want to discuss the issue about this view that we should work over small fields, because working over large fields is like wasteful, right? The rough intuition here, I think, is if you want the prover to commit to a value that's pretty small, but you have to represent the value as like a 256 bit field element.
00:03:49.300 - 00:05:19.548, Speaker A: Well, there's just like a lot of extra bits in there that aren't doing anything, and that's just a big waste, then you're sort of paying a price for the prover. And I don't necessarily think that's the case because of this issue, I highlighted that committing with elliptic curve based commitment schemes to small field elements, even if the element lives in a 256 bit field, is really fast. And in fact, you can actually do it with essentially one group operation, not one exponentiation, one operation per committed field element if the field elements are small enough. And then finally, there's this very popular approach now to Snark design, which heavily uses recursion where people will take a big circuit, break it up into small pieces, sort of prove each piece separately, and then aggregate the resulting proofs to get kind of a proof for the big circuit. Okay? And with current popular techniques, there's really kind of no major loss in performance by doing this. In fact, to get good performance is like the only way you can do it, because a lot of the Snarks people are using today have sort of super linear approver time. So it's sort of a dis economy of scale where you deal with a bigger circuit and the cost kind of per day in the circuit goes up because of FFTs and things like that, okay? And one of the reasons people are doing the breaking up, by the way, is because something like Fry has enormous space costs for the prover, like hundreds of gigabytes, even for small circuits.
00:05:19.548 - 00:06:00.856, Speaker A: And so even ignoring prover time, the prover space just you can't tolerate it at big circuits, and there's not much overhead in breaking it up into small pieces today. All you really have to do is make sure that kind of the Snark verifier, when represented as a circuit, is significantly smaller than the original circuit that was proved. So that way, when you go to aggregate the proofs by kind of proving, you know, something the Snark verifier would have accepted sort of that's a statement that was smaller than the original one. And so the overhead isn't a big deal. Lasso and jolt. You actually see economies of scale instead of dis economies of scale. So you actually would lose something by breaking things into smaller pieces.
00:06:00.856 - 00:07:09.390, Speaker A: And the way to think about these lookup arguments and we discussed this last time in response to one of the questions was you think of them as the prover kind of pays a fixed cost like table size to the one over some constant, no matter how many lookups are right? And then that cost gets amortized over all the lookups. So the more lookups there are, the better that cost gets amortized. Also, if you're using Pippinger's algorithm to compute MSMs, the bigger the Pippinger speed up like Pippinger saves like a log factor over naive multi expensentiation and log of something bigger is a little bit bigger. And intuitively, I think we shouldn't expect to see dis economies of scale, right? I mean, in the real world we see economies of scale all the time. So I think if you kind of use the right techniques in snark design one proof for something really big until you exhaust the space your approver needs or something, should actually be better than breaking things into tiny pieces. Okay, so I don't know, I hope at least these new perspectives are interesting, even if I don't think everyone's going to agree with me.
