00:00:06.330 - 00:00:09.678, Speaker A: You. Morning everyone.
00:00:09.764 - 00:00:27.390, Speaker B: Welcome to today's a 16 z crypto research seminar. I'm very pleased to introduce professor at the business school at NYU, Abdu Enjoy. And he'll be talking about a topic that's quite near and dear to my heart, namely pricing block space, in particular in multidimensional settings.
00:00:27.810 - 00:01:14.402, Speaker A: Great, thanks for having me. This is going to be a perspective of what public economics teaches us about blockchain resource pricing. Some old ideas we're going to see and also some new ideas. So the motivation for this talk is this fee schedule that you can see in this yellow paper. This is an example of Ethereum the blockchain. And I'm going to say nothing here is investment advice though this disclosure need not be maybe said in the future. But you can see there is a very extensive schedule at the very level of the operations of each of them having a price.
00:01:14.402 - 00:02:01.886, Speaker A: And here then by pricing we mean that this is something that is mattered at the protocol level. This is not let people to bid on these items, the protocol level. These are prices that are set at the protocol level. While these are coded just the Ethereum foundation decides to put a number and developers are very well aware that something for example storage is very expensive. It's 20,000 guai and there is this measure of guai that is different and these are set. So there is some form of price controls here that are set in place. And what does public economics teach us about this? So the motivation for this work is that minimum fees are needed in certain situations and this is the motivation for having minimum fees in many blockchains.
00:02:01.886 - 00:03:10.566, Speaker A: Blockchains have scarce resource blockchain transactions provide private utility to the senders but they incur a social cost to the network as a whole. So this was main motivation of vitalik when they made these changes. The protocol also might need to collect and divert a minimum revenue for security reasons. And people like Tim here who have studied the security features of the EIP 1559 for instance, find that a part of the revenue need to be diverted, either burned or used for another purpose but not give to the miners. So this is motivated by EIP 1559 Ethereum case. But broadly the lessons that we can get from the field of public economics that think of also price controls and taxes and so on for many other blockchains, as we have seen, there are multiple resources and there is a schedule level at the outcode level and there are examples of denial of service attacks when these resources are mattered but mispriced. There are examples of denial of service attacks that have been proven to work.
00:03:10.566 - 00:05:27.506, Speaker A: So a few questions we're going to address to think of this framework will be first, what should be the level of granularity when you think of multidimensional pricing? Should it be at the very low level of the Opcode or should we be at a higher level? How do we think of the goods that we are actually pricing and then what are the priorities? If you have multiple differential resources where should you focus on pricing these resources and let the others be resources on which people senders will be bidding on and finally and we'll see that measures of price varies with quantity demanded is important and these are things that need to get bigger, right? Can we find ways to have signals for price elasticities? And these signals of course need to be delayed because we know that whenever some of these measures depend on the current block they can create some very complex minor strategies. So the key insights from this talk are three. This is my preliminary thinking on these topics and I can summarizing in the following way what you want to think of what is the right level of granularity. It is the level of think of the final consumer resources level of granularity. And what I mean by that, what is the empirical criterion is just that are there some operations for which there are constant return to scale means that they are packaged in the same way to create one format of contract or operation that people are demanding when they are setting asking for transactions? For example, if you look at a mid level, maybe the contact creation is one group of such combinations or message calls is another group or bulk operations of storage and bulk operations in general. At a higher level, it can be also you can think of token transfers and one form of final good that is demanded or various types of smart contract that have their own feature that are directly demanded. Rather, it's better to set a price, a minimum price at this level than at the upcode level, which seems to be the wrong level of granularity, as these are mostly intermediary inputs and by putting a pricing across them.
00:05:27.506 - 00:06:39.626, Speaker A: That is differ you are going to lose what's called productive efficiency and you have developers who are thinking of how to switch between storage versus using merkel proofs for instance, rather than just biding the right amounts to have their transaction. The second consideration then now is once we take this forgiven, what should we prioritize? As there are multiple resources should we let some of them be resources on which senders bid and what will be the ones to prioritize? And the main message here is to matter those resources that are necessary in the sense in the following sense in the empirical criterion to measure this one is that those products that are in high demand across user balances meaning that these ones, whether it's small balances or larger balances, balance in sense of the income that is in the wallet as a proxy, these will be still in high demand. For instance, what would tell you is maybe you want to have a meter pricing of certain types of token transfers and leave complex DeFi interactions to more of something that would be bid through the market of validators. And finally, the question is when should.
00:06:39.648 - 00:06:41.878, Speaker B: The prices be computed by the protocol.
00:06:41.974 - 00:07:27.194, Speaker A: Yes versus just yes versus the elect through the tip, minor tip, for instance. Right. Okay, great. And then the last point is now we will know that price elasticities matter and right now in current forms we have of adjustments. There is no notion of price elasticity and it is hard to put such numbers that can vary within a protocol. So I argue here that and this is more of the new part that of course your comments would be very welcome would be that we can have also measures of Taton man of elasticities on chain. In a sense, what we want is you want to have some variation in supply to measure elasticity of demand and we can do this from the back.
00:07:27.194 - 00:08:05.614, Speaker A: So one way to think of this is that the process for the block size is currently some process for which we have either close to full blocks or close to empty blocks. And it's a chaotic process. So if we follow the adjustment process how it is, I'm going to show you as EIP 1559, it should lead us to a more of a mean reverting process that has bounded variation. So can we use some form of randomness to get close to that mean reverting process that doesn't jump too much? That would be the end of our talk and opening new perspectives. All right. So now I'm going to start with one dimensional pricing here. This is more of an economics talk.
00:08:05.614 - 00:09:22.574, Speaker A: So I'm going to just show some graphs to explain the intuition and insights behind what has motivated the one dimensional pricing and also give homage to some people's work who have done some old ideas here. So the idea behind most of the current thinking we have about one dimensional pricing and use the price controls in blockchains is an old idea that came from Marty Weitzman who passed away three years ago. Marty Weitzman, who was studying public economics and other ideas in also environmental economics, in fact, Vitalik, who read this paper and came up with the idea of thinking of how to use the minimum base fee. This paper, which is very famous, prices and quantities. So what is the idea behind this paper? It is to think of when are quantity controls, like for example, setting a block size limit better and when is a price control, meaning also adding prices, price minimum prices better. Okay, depending on the environment we are in, knowing that there is uncertainty in demand and uncertainty in social costs as well. Okay, so let me just introduce the model here.
00:09:22.574 - 00:10:00.374, Speaker A: So this is going to be a very simple bare bones model just to give some language, introduce some notation to capture ideas. And I'm going to show you graphs and how it will operate. So for the purpose of this part, a blockchain, when it comes to just the pricing, it will just be a network of computers that include transactions with quantity of resources, queue, okay, that is demanded. There is a private benefit, which is the benefit to the consumers which will determine the demand for the block space. And there is a social cost. So this social cost captures many things. These are not the cost only to the validator, but the whole network.
00:10:00.374 - 00:10:54.614, Speaker A: It will be the cost of operating a node, the cost of centralization, the cost of delays in block propagation, et cetera. Now, the idea is that there is uncertainty in social benefits and private costs. So there is a process, there is some randomness data in the benefit that people can have from these transactions and which will lead to demand being uncertain. And there is uncertainty in the social benefits and the private benefits and the social costs inverse. So now what is the value that we get from a quantity control? So the value of the social value here that we get from a quantity control will be expected benefits minus the social cost. Okay? And this is a unique price control here. It's in expectation.
00:10:54.614 - 00:11:43.274, Speaker A: So the price control can get it wrong. It's not going to be just what do we call the first best optimum that could be set if we knew exactly what the realization of this uncertainty is. Now, what is the value of a price control? Now, what happens is that once you control the price you said this is the price quantities will adjust as a function of the price. The quantities will adjust in the following way. The marginal benefit to the private benefit of that quantity should be equal to the price, which is how people will want the quantity. As long as this benefit is higher than the price and at the exact quantity that they will choose, it will be equal to the price and at that given quantity. Now we can find the expected benefit from this price control.
00:11:43.274 - 00:11:45.034, Speaker A: So the question now yes.
00:11:45.152 - 00:11:47.194, Speaker C: So is the private benefit just like.
00:11:47.232 - 00:12:07.554, Speaker A: The sum of every individual's benefit? Yes. So this is just one person model of X ideas. The private benefit is the sum of every individual benefit. What matters in the end is going to be this private benefit is going to determine a demand for each individual person. Okay? All right. Now let's go and look at the demand. Let me give an idea here.
00:12:07.554 - 00:12:50.240, Speaker A: In economics, we're going to consider a situation where this example is one in which the marginal social cost, which is the cost to the network, is pretty well known in a sense. It's not elastic. So the marginal social cost is just constant, but the demand is elastic. Okay. Meaning that the second derivative here, if you think of this, if this private benefit is going to be larger in absolute value than the second derivative of the marginal cost. All right, so this is the typical econ graphs that you have quantities here and prices there. This is just to capture the ideas first.
00:12:50.240 - 00:13:53.438, Speaker A: Okay, now what we're going to do, let me introduce the notion of what a dead weight loss is. So if you look at the private demand and the social cost, the point A here is going to be what will be what called the social optimal, meaning that if we were to internalize all the costs on the network, that would be the quantity of the block size, essentially, or the quantity of resources used here. But now imagine we introduce a quantity limit. Let's say here it's the lower than what the block size could be optimally. There will be B would be what is the private optimum. Because this would be the price, the equilibrium price at which from the demand and this quantity chosen, what would be the price. And this triangle here, because we are losing, we could get more quantity here and people would be willing to pay on the demand curve.
00:13:53.438 - 00:14:28.560, Speaker A: And this is the social cost. So there is all this triangle here that is lost, which is what is called the dead weight loss. Okay, so now let's think of how can we minimize the dead weight loss. That is the idea behind to gain efficiency, we try to minimize the dead weight loss. All right, so this is a dead weight loss that arises from a quantity limit, for instance. Now let's introduce some uncertainty. Yes, exactly.
00:14:28.560 - 00:14:52.694, Speaker A: Yeah, if you specify it correctly, we'll be fine. But the thing is, we don't know what the social cost is. That's the tricky part. Even if you try to get demand, you don't know what the social cost is. But here there can be uncertainty in demand. There can be times in which there is no demand, much demand for block space, and there is time that can be higher. So suppose now with probability one half and one half.
00:14:52.694 - 00:15:28.690, Speaker A: Now demand is lower, but let's say it's lower, such that even my quantity limit was fine. So I wanted to be a principle of quotient. I want to make sure that my block size is not too large, so that even if demand is low, my quantity limit is fine. But there can be demand that is higher with probability one half. Okay? So now in this situation, looking at this quantity limit, again, we can assess what is the dead weight loss when you have this uncertainty in demand. Okay, we can do it. This part, there is no dead weight loss from here, your quantity limit is right.
00:15:28.690 - 00:15:45.942, Speaker A: But when we are on this line, when demand is higher, your quantity limit is too low. This is the A is here, the social optimum. B is the private optimum. This triangle will be our dead weight loss from yes, in the block space.
00:15:45.996 - 00:15:55.174, Speaker D: Example, I guess what probably is, is it not the case that sort of block space sort of wants to determine the protocol is kind of fixed.
00:15:55.222 - 00:15:55.434, Speaker A: Right.
00:15:55.472 - 00:16:03.040, Speaker D: So if you have an perfectly inelastic supply curve, then you actually don't have much time weight loss, right.
00:16:04.370 - 00:16:09.246, Speaker A: Why? No, but if you could actually quantity.
00:16:09.438 - 00:16:16.770, Speaker D: Then you're always at the optimal quantity. If you just make that red line vertical, then there's no triangle.
00:16:17.270 - 00:16:25.794, Speaker A: Yeah, but here you could increase the quantity. You think of the block side limit as a quantity limit. That's the idea here.
00:16:25.992 - 00:16:29.686, Speaker D: If you make the blocks bigger and there's less block well, okay.
00:16:29.788 - 00:16:37.980, Speaker A: Yeah. We will see later when we talk about block bigger, it all matters whether we have more uncertainty on the demand side or on the social cost side. Okay.
00:16:38.590 - 00:16:53.946, Speaker B: The way I would interpret it often, like computer science side, we just think of it like the capacity constraint period. Yes. The way I'm interpreting what you're saying is actually think of that capacity constraint as something that's basically just made up kind of a guess as to where supply it's a quantity demand.
00:16:53.978 - 00:16:54.510, Speaker A: Right.
00:16:54.660 - 00:17:01.586, Speaker B: So then if you take that framing, you can sort of take a step back and be like, well, actually, what if we got that wrong right now.
00:17:01.608 - 00:17:09.300, Speaker A: I'm not even saying let's increase it too much right now. So just like think of that as a quantity limit. I absolutely agree with you.
00:17:10.390 - 00:17:28.630, Speaker B: It's actually a really nice change in thinking how it's usually described. Normally it's kind of like, oh, this is the maximum these computers can build, so we can't go above this. And you're saying that it's really kind of a softer thing, social cost. And then think of the capacity as just trying to get the market clearing prices.
00:17:31.630 - 00:18:00.594, Speaker A: Yeah. So think of it now as a quantity limit. Okay. So can we improve now this quantity limit, right? In this situation, can you improve in this situation where the social cost doesn't have the second derivative is not large? Meaning here is pretty flat. We don't have like a catastrophic kind of environment. Right. And can we improve over this by introducing a price limit? So that's what we do here.
00:18:00.594 - 00:18:49.966, Speaker A: Imagine now instead of the quantity limit on it, I temporarily put a price minimum, which is at that price, the equilibrium price. Now, at this price here, right, I get rid of the quantity limit now and I just put the price which is exactly what is happening. Also in EIP 50, 59, you adjust the price and you let now the quantity move, right, whether between zero times the target or twice the target. What happens now? Let's look at compute now what is the dead weight loss? So we can do it here again. So if you make this price, as I said, the price limit now, the quantity will adjust. Meaning if the demand is low, right, then the equilibrium quantity will be lower and the demand is high, the equilibrium quantity would be higher. So the quantity will adjust.
00:18:49.966 - 00:19:29.546, Speaker A: And what will be now your dead weight loss? We can also look at these doing these triangles. Again, we economists are not sophisticated. All we can think of is triangles and graphs. So in the case of the slower demand, our dead weight loss will be this triangle, right? Because this will be the new private bet optimum and this is the social optimum. So this will be this triangle. And in the case of a higher demand, this would be the new private optimum and this is social optimum. This is the triangle.
00:19:29.546 - 00:20:14.750, Speaker A: So you see that the sum of these triangles is smaller than the bigger one. Okay? By introducing now this price, which is the price adjustment, right? And then letting the quantities move, you have reduced the dead weight loss in this situation where again, in this situation where the marginal social cost, meaning that the cost of having too much of a big block size is too high. And I will show you a graph later in terms of how this thinking goes, you can improve by making price adjustments. So this is kind of one of the origin motivations by having this, adding these price controls. Of course, we know also that it has nice properties when it comes to incentive compatibility.
00:20:16.530 - 00:20:19.422, Speaker B: There are some examples where debit losses more than one.
00:20:19.476 - 00:20:42.162, Speaker A: Absolutely. So let me give you exactly now the result. So this is the result, okay? So this is the result. So I showed you now earlier the demand. So the demand is just the marginal benefit at a quantity. So now see, here we do a Tyler approximation around that quantity limit Q. So here there is a process, there is an uncertainty.
00:20:42.162 - 00:21:32.482, Speaker A: This is beta on the demand which was here plus probability, one half probability one half plus delta minus delta. What happened? This is the shift. And here we have an uncertainty on the social cost, right? This adding up to zero. So you can look at what is the competitive advantage of a price control over a quantity control, which would be the net benefit over the price control. These Q's are the Q's that adjust minus the net benefit of the quantity control. Then we have that this benefit is actually proportional to this, which is the slope here, the marginal. If the elasticity of the demand is larger, like the case that I showed you, where the demand is more elastic than the social cost, then price controls can improve.
00:21:32.482 - 00:22:01.346, Speaker A: The intuition is the following. Imagine you were in a situation where the social cost was steeper in demand. So there can be like an environmental, economic, a catastrophic situation where the block size is too high. Boom, everything collapses. Then you don't want to do price controls and let quantity adjusting. You want to fix put a cap on the quantity, a hard cap, okay? And then let the price change that.
00:22:01.368 - 00:22:07.282, Speaker B: You want to control the variable for which getting it wrong costs you to vote.
00:22:07.336 - 00:22:57.058, Speaker A: Exactly. That's the idea. If 30 megabytes is catastrophic, you want to set it at 25 and then but you know, at the same time if ten would be too much of a cost on consumers, then you want to let the price adjust. Right? So that's like one of the original thinking behind introducing the thing. So Vitalik went and read Mark Weisman 1974. So the mechanics now of AIP 1559 is how do we transform this insight into a transaction fee mechanism that has also other nice properties. As we know, first price auctions are not incentive compatible DSIC and so on.
00:22:57.058 - 00:23:23.660, Speaker A: So you consider here a target size. So here this is just a target of 12.5 million gas. You have a max block size which is double that. And you try to use whether the block is full or not having this variation, but this variation, note it is endogenous variation in the block size to adjust the price. Meaning if you are above target, that means there is a lot of demand. So you need to increase the price.
00:23:23.660 - 00:24:20.560, Speaker A: And if you are below target there is less demand. You need to then decrease the price and let the decrease the price quantity adjust. Here with this one over eight here, which is note it is just a parameter of 12.5%. What you really want to capture is some sort of notion of price elasticity and it's hard to put those on chain and we're going to come back to them later. Okay then from these prices that are computed, you add up from the schedule that I showed you earlier, the gas limit, right? And we're going to come to this in our second part of our talk. That will be the multidimensional part and then the transaction standard pays that guy's limit times this price. And a tip to reflect the notion of competition as well.
00:24:20.560 - 00:25:24.430, Speaker A: Up to a cap, the base fee is diverted whether burned or put for other use, but not given to the miner. So then we have here some questions to ponder, some I'm going to do today, some I'm going to just put to you to think through, which is are there unchained signals? Can we think of unchained signals for better price? Tatton Mars so I'm going to try to elaborate on that in the multidimensional case. So here just the 12.5% and I'm going to show you in graphs why it might not be the right way to think through that. And then given the role of the burned base fee, I know that there is some history dependence and you want to have new upgrades that are compatible with the past. How do you want to think of the block reward? And maybe this is a question to everyone because the block reward, should it maybe be more tied to the service that the validators are given rather than something like because once you think of the burned base fee and it is burned. Essentially.
00:25:24.430 - 00:26:24.034, Speaker A: Could we have a situation with a DSIC mechanism for which the block reward would be tied to the collected revenue? Essentially, of course, there is the question of backward compatibility but this is just something to put out there as something to think of. Now, I want to talk about now an aside, which is the block side limit like Anthony started thinking through, so the motivation behind it and this is a graph of vitalik that I borrowed, which is his thinking about what are these social costs. So the argument is that up to the points where we are, we are closer to this line here where the marginal social cost is the social cost is close to some the marginal social cost is one over x. So we are in a log block size type of social cost. So up to a certain point we are to one over x. So this is not the second derivative is one over x square. So not as large compared to what would be the demand elasticity.
00:26:24.034 - 00:26:44.602, Speaker A: So this is the thinking tool. In this range you want to do price controls. Once you get into the potentially chaotic range of unknown unknowns, you don't want to mess up with price controls anymore. You want a hard cap limit before getting to here, just to understand this figure.
00:26:44.656 - 00:27:02.686, Speaker B: So should I be thinking about the set of full nodes as a function of the block size? And then the point is initial regime is a high cost because you're losing all of the handheld devices. Yes, but then it's like this long stretch where Raspberry Pi's can do it. Eventually get to the point where you're.
00:27:02.718 - 00:27:14.542, Speaker A: Losing Raspberry Pi and then you're losing a lot of validators. Yes, and also it takes more time for blocks to propagate in the network and so on.
00:27:14.616 - 00:27:19.862, Speaker C: But why is it starting so high though? The blockchain loses a lot of functionality if you have such a small block size.
00:27:19.916 - 00:27:49.150, Speaker A: Right, okay, so that's a great point. I will just differ and say that I don't take any responsibility for this graph. There is no fundamental reason why it's starting from an asymptote that looks like infinity. Think of that. It's going downwards, pretty flat for the ranges in which they tried, at least in the ethereum case, the Ethereum Foundation tries to estimate these and argue that we are closer to this space in the ranges that they took like 25 megabytes.
00:27:50.550 - 00:27:52.718, Speaker B: Now we're going to the benefit term.
00:27:52.894 - 00:27:53.234, Speaker A: Right?
00:27:53.272 - 00:27:58.590, Speaker B: So the point is you have this B of Q is considered wonderful.
00:27:58.670 - 00:27:58.962, Speaker A: Yes.
00:27:59.016 - 00:28:00.546, Speaker C: So right now we're probably near the.
00:28:00.568 - 00:28:05.126, Speaker A: Trophy right now the argument is we are here. Yes.
00:28:05.308 - 00:28:12.390, Speaker D: Stupid question. If you increase the block size on steering, does the throughput of the whole system just proportionally increase?
00:28:14.030 - 00:28:15.340, Speaker A: I don't know the answer.
00:28:17.630 - 00:28:24.380, Speaker B: Like a slightly higher uncle rate. It is actually.
00:28:26.910 - 00:28:29.226, Speaker D: Kind of depends on.
00:28:29.248 - 00:28:32.490, Speaker B: How you define through number of transactions.
00:28:32.570 - 00:28:33.200, Speaker A: Or.
00:28:35.410 - 00:28:54.050, Speaker D: Number of transactions or number of equal size transactions per second or something like that. Equalize or like identical transactions. How many of that can do per second? If you assume that, then it should be like kind of linearly proportional. It could be that it's the same number of transactions.
00:28:54.130 - 00:28:57.160, Speaker B: People just do more complicated stuff right now.
00:28:59.690 - 00:29:29.934, Speaker A: All right, cool. Okay. So now let's go into the multidimensional case. And I'm going to give a reason here for why in the first place, you want to make even different prices across two goods, again, using our graphs. So think of two resources here, right? Resource I and resource J. I'm going to give you a situation where imagine for resource I, you put a quantity limit, right? Think of this as bandwidth or memory. You put a quantity limit that's low.
00:29:29.934 - 00:29:56.422, Speaker A: You want to be conservative, and you put another quantity limit for resource J. Right? Imagine, think of this as like state access, for instance. But you get it almost closed, perfectly close to right. You are close to where you are. So that the dead weight loss here is small. So this is an argument to explain why you can do cross price adjustments. So here what you can do, for example, is you can minimize dead weight loss.
00:29:56.422 - 00:30:44.098, Speaker A: So the dead weight loss is here before you can minimize it by doing cross price adjustment. The idea is, for example, for the good J, you can now put the price floor increase the price adjustment instead of having the quantity limit. What happens once you do this? Because these goods are complements. What does complements mean? Think of two goods. Complements means that when people want more of the demand for this good, the demand for the other good increases. For example, bread and butter. Now, if the demand for the other good decreases, the demand for the other good decreases.
00:30:44.098 - 00:31:05.518, Speaker A: So here when you increase this price, what happens? Now? The new equilibrium price is going to be lower. So price will be higher. Sorry. The equilibrium quantity will be lower. So there is lower quantity for this good. So let's think of this like state access, and then the demand for the other good, for which is a complement, will shift. Okay.
00:31:05.518 - 00:31:09.198, Speaker A: The demand is going to be lower as well.
00:31:09.284 - 00:31:14.210, Speaker B: Let's make sure I understand. So there are two goods, and these two graphs correspond to the two market.
00:31:14.280 - 00:31:14.802, Speaker A: Yes. Okay.
00:31:14.856 - 00:31:16.466, Speaker B: And so like Q bar could be different.
00:31:16.568 - 00:31:20.418, Speaker A: Q bar could be different for each of them. Qi bar, QJ bar.
00:31:20.584 - 00:31:28.262, Speaker B: And so you're saying with complements, if you have a drop in demand, the first good is going to be an induced drop of the source in the other good.
00:31:28.396 - 00:31:48.518, Speaker A: Yeah, exactly. Now, by doing this price change here, by doing this price adjustment, which is the cross price adjustment, you didn't even touch this price. What happens. You're introducing here the dead weight loss. But when you are close to the optimum of the dead weight loss is small. It's in the square of the difference here, the sides of the triangle. But you can get a lot of gains here because this here is not going to be side of a triangle.
00:31:48.518 - 00:32:12.534, Speaker A: It's going to be linear in the price change. Okay. And if this change is small so you're going to get will be lost. So this is an argument for having differentiated price adjustments already and how cross prices affect each other. Okay. So we need to think of then this in a broader conceptual framework. This is an argument for why we want to do this in the first place.
00:32:12.534 - 00:32:37.162, Speaker A: Any questions? Okay, so now let's go to the multidimensional pricing. So here when I started working on this, I started thinking of all what the miners or the validators will collect and then thinking of how this collection interacts with pricing or different things. And then can I ask a question.
00:32:37.216 - 00:32:51.554, Speaker D: Actually about the cost curve, the U shaped thing, I think I guess when Natalia drew that cost curve, do we think of the marginal cost in the flat region as positive, but flat or positive or zero in the sense that.
00:32:51.592 - 00:32:54.110, Speaker A: Sort of in the region where it's positive.
00:32:54.190 - 00:33:01.774, Speaker D: The question is whether yeah, my question is why? What's the social cost of increasing block size in that region?
00:33:01.822 - 00:33:32.878, Speaker A: I guess so there are multiple costs that of course these are like fuzzy measures. The main argument from what they're thinking, I think, is they argue it's in a sense in log the log the either linear or log the social cost is either linear or log the block size. But the demand, you see that clearly, it can have spikes and so on. And so that's the motivation behind they consider different form of this. We can talk about it later.
00:33:33.044 - 00:33:37.902, Speaker B: One simple story would just be imagine full nodes pay per unit of bandwidth that they consume, right?
00:33:37.956 - 00:33:38.174, Speaker A: Yes.
00:33:38.212 - 00:33:46.098, Speaker B: So then you double the block size or using twice as many bandwidth constant per unit. Maui is so complicated in that.
00:33:46.264 - 00:34:10.314, Speaker A: No, but it's not zero. But it's smaller than demand. Certainly. But again, we are getting to the limits of the Weitzman model. That's why I want to tell us that we need to think of something more because this social is fuzzy. Yes. As long as we're back on this picture, it may be a little bit off topic, but there's like two distinct resources going on.
00:34:10.314 - 00:34:23.726, Speaker A: Like there's the block size, which relates to the bandwidth and all of that. And then there's the gas, which is the amount of computation you're asking every node to do. Do they generally think of both of.
00:34:23.748 - 00:34:27.386, Speaker B: Those as being like binding and increasing.
00:34:27.498 - 00:34:56.962, Speaker A: One increases the cost or like one of those is significantly more so my understanding is that gas is the intermediary unit that you then multiply with price. I don't think they think of pricing of the gas. That's where we're going to go to how they combine these different things. Okay, so now we're starting to see, as you have probably anticipated, some limits to this Weitzman approach. A social cost seems very vague.
00:34:57.026 - 00:35:00.006, Speaker B: So does Weisman only consider single dimensional cases?
00:35:00.118 - 00:35:25.722, Speaker A: In his paper, he considers multi dimensional cases, but thinks of, again, whether you want to choose just this directional question, whether price or quantity. Whether price or quantity. Just the directional question. Now we want to think of what should be the cross price adjustments. For that, you need to think of something else. You need to have a notion of what a production function is, essentially, or how blocks yes. Produce.
00:35:25.722 - 00:35:26.400, Speaker A: Yes.
00:35:26.850 - 00:35:29.822, Speaker D: Just of all to make sure I understand what you're proposing on one day pricing.
00:35:29.886 - 00:35:30.066, Speaker A: Right.
00:35:30.088 - 00:35:44.520, Speaker D: So then your design or a possible design is block based is much more elastic. Block size is more elastic to transaction demand or something like the idea is that when there's low demand in the system, you really want block size to scale down a lot. Throughput to scale down a lot. Is that kind of.
00:35:46.350 - 00:36:08.910, Speaker A: Yeah. So right now, I just gave a justification for some of the thinking that has been the aside about block space was just do you want to think of it more broadly? For instance, we can make some Tatan Ma around it, but I don't have any proposals other than what is already put in place for one dimensional. We can discuss that privately, but I don't have any proposals.
00:36:09.490 - 00:36:11.274, Speaker D: It seems like a very personal proposal.
00:36:11.322 - 00:36:59.294, Speaker A: Right. But yeah, this is just for people to think of more broadly for the design of all spaces. I'm happy to talk about it because I want to just focus on one thing and then block size always we don't know what is the right block size, but at least one proposal is clearly is how do you want to think of this adjustment of prices? And I'm going to go to the proposal at the end. How do you want to think of adjustment of prices? Okay, so the Wedgman story was partial equilibrium. What do we mean by partial equilibrium? Just it doesn't affect the feedback responses within the system. For example, there can be revenue differences between the price and quantity control, the revenue that are collected that should be diverted. We need to understanding of what is the block production function, if it has any effects, if any.
00:36:59.294 - 00:37:38.878, Speaker A: Right. We have just this social cost and then we have no clear understanding of the marginal social objectives. And I think that's the key thing why you can make some directional assessment but not relative price adjustment between quantities. So you have the directional, whether we should have price or quantity controls, but you don't have the price adjustment. So the approach we're going to take here is going to be the following. So we're going to leave the incentive compatibility and OCA approvedness type of questions to maybe future work, work like Tim. And we're going to focus on the fact that the protocol needs to divert some revenue that we take for given.
00:37:38.878 - 00:38:14.630, Speaker A: And then we're going to ask, given this need to divert revenue, what will be the optimal way to minimize the dead weight loss given this revenue target? And find and summarize some insights that come from this problem. Okay, this is also some ideas similar to this have been worked on. So this is even an older idea by diamond. Diamond merely two papers in economics published in the same year. Anthony will probably know that this is closely impossible. Times have changed. And James merely both Nobel Prize winners.
00:38:14.630 - 00:38:49.326, Speaker A: So what is this idea? So this is all the ideas of productive efficiency and thinking of commodity taxation. So for the purposes of this talk, they would be very similar. And of course I will go to caveats and add some caveats later on. So let me introduce some notation again to fix ideas. So in this situation there will be more notation because there are multiple goods. So I'm going to define a competitive equilibrium in the following way. So I'm going to talk about taxes here just directly.
00:38:49.326 - 00:40:16.462, Speaker A: I'm going to talk about taxes to just give homage to the people who have worked on this. But you can think of here as cross price base fees is the price here P, which is the price, essentially of the market that you'd be paid through the tip and the total price that the sender faces, which is the tip and the base fee and the quantity of good such that this maximizes the utility of the agent. So here I'm just in a one agent model, but you can think of multiple agent, this utility function here, if you don't like this fuzzy notion of economies, you can just think of this is just an intermediate step to have what? A demand, to have a demand curve. That's what matters in the end, such that what the agent pays in resources and the quantity they choose is less than what is in their balance. I okay, so this is what the agent solves. Okay, so these are given the quantities, given the prices, they choose their quantities. Now the quantities that are produced, Q will maximize the revenue that is collected, the revenue of the miner, which will be just the price.
00:40:16.462 - 00:40:21.962, Speaker A: Here the tip times the quantities for each resource.
00:40:22.026 - 00:40:23.700, Speaker B: So should it be t times Q?
00:40:24.070 - 00:40:57.126, Speaker A: No, t here is our base fee. Yes, that's why I made taxes. So T P is the tip, it's the market, essentially market price. Okay, so this is the tip subject to the production. So the production function, you can think of it as a convex function, f of Q within a space. So I'm here going to assume constant return to scale. Constant return to scale means that there are certain operations that if you add more of them you're going to have more contracts or more inputs.
00:40:57.126 - 00:41:59.650, Speaker A: So constant return to scale to fix ideas you can work with others and such that the protocol collects the minimum amount of revenue which here I take forgiven. So which are just the taxes times the quantities for each resource and finally markets clear. So this is our notion here of equilibrium which is that the quantity that the agents demand of each resource should be equal to that quantity produced and added to the chain by the validators. So you can think of the validator, the validator what they are doing is a complex knapsack type of problem. But for the purpose for us, for what we're doing is just simple objective here that you can think of as a F production function of Q. Of course it can change and there could be some more work to think of this interaction between people writing contracts and the validator, choosing which ones to work on. This is the first approach.
00:41:59.650 - 00:43:11.510, Speaker A: Okay. So now let me give some results again, there is a lot of apparatus behind this but the general results that are robust when you're thinking of these type of problems. The first one robust result is productive efficiency, what we call productive efficiency which is the following is that at the optimum the allocation is not at the interior at the boundary of the production possibility frontier. Meaning that intermediate goods if you think of something as an intermediate good here it should not be distorted. Intermediate good, how do you define here which will be that there are neither direct inputs nor outputs for individual people to use as something that they consume in the end, consumption here is vague notion but you can think of it as a contract is something that they consume or a sort of a transfer. So intermediate goods for instance, if you think of a car manufacturer, you don't want to tax steel or rubber, you just want to tax the final car and you let the market take the price of the steel because you want to focus on the final good. That should be the right level of Glanority.
00:43:11.510 - 00:44:30.442, Speaker A: Okay, that's the first robust idea you get from this approach and I'm going to explain how it relates to the example that I gave. So you can think of other mid level sorts of like storage or bulk operations are right way and I'm going to give here from a survey or someone who posted certain types of contract that you have what would be one approach. The other robust idea is less simple but it has some flavor here. What I show here is I'm going to look at the demand change in demand. When I change the total price of a good I for quantity j in proportion scaled by the taxes is almost uniform. So there is one single parameter, theta that determines them. However, this change the demand goes for that good is pushed to be lower when the balance, the elasticity of that good respect to the balance that people have or income here is larger.
00:44:30.442 - 00:44:59.714, Speaker A: So this is what I meant. This has the flavor of having a tie tax on necessities, even if this goes against, in a sense, equity. But we're not just concerned much equity here. Efficiency is kind of the main argument, in a sense. If you look at this is what I call like a necessity. The criteria being that if you look at different balances and you see that this type of final good is pretty much in demand, you want to do more of the pricing metering on that good. Okay.
00:44:59.714 - 00:45:37.160, Speaker A: Because it's a necessity, you want to take care of that one first and let the other ones, like luxury DeFi interactions, to be more adjusted through the minor tip as a lesson you would get from here. Okay. So these are two of the most robust results in this setting. Now there is another result which is less robust because it needs more assumptions, but I'm just going to show it here to explain the idea of what we want to measure. When I go and think of elasticities now yes.
00:45:37.690 - 00:45:42.454, Speaker B: Do you need to assume some make some curvature assumptions on, like, f U?
00:45:42.572 - 00:46:07.614, Speaker A: Yeah, absolutely. So F, like I said, constant return to scale. So homogeneous degree one, U concave. That's like U concave, meaning that the 100th burger is going to be less good as the first one in terms of how much it adds. Yeah, absolutely. And this is where I feel like working with the asyncraties of the production function of blocks would be something that I would gain a lot from learning from.
00:46:07.732 - 00:46:14.558, Speaker B: And then you're going back between, like, equilibrium and optimum. That's just kind of like a first welfare theorem type argument. You can go back and forth.
00:46:14.654 - 00:46:32.982, Speaker A: Yes. Great. Yeah. So when I mean the competitive so here, my definition of the objectives have to be an equilibrium. In a sense. I'm looking at really the decentral, the effect on the market. I'm not looking at a planner, the planner sets, but I'm looking at the equilibrium in a sense.
00:46:32.982 - 00:46:33.862, Speaker A: That right.
00:46:33.916 - 00:46:37.210, Speaker B: Like in the last slide at the top, you say have an optimum.
00:46:38.030 - 00:46:46.640, Speaker A: Yeah. Fair. This is like constrained optimum. Right? Absolutely. Yeah. You're right. Yes.
00:46:46.640 - 00:47:15.414, Speaker A: Note that here we are not exactly in the second welfare setting because we're not having lump sum transfers. Okay, good. All right. So now this is just to capture some elements we would want to measure. Okay. Which would be like, I showed price elasticities and cross price elasticities. Whether goods are complement or how goods respond to when you change the prices will matter.
00:47:15.414 - 00:48:42.190, Speaker A: So once you introduce price elasticity, what is the price elasticity means when you increase the price? So epsilon I j means if you increase the price of J by how much does the quantity of I change? Right? So if you assume that there is no interactions between these goods, they are independent, completely isolated resources, then you have the following but this is a heroic assumption. That's why I'm just showing here as that you want to think of what you measure. Then you have that you want to tax inelastic goods, meaning that goods that don't respond much to price, you want to tax them more. So the way to capture another example, also canonical example would be like tobacco for instance, because people will not change much how much they smoke. So you want to do more of the price control in that situation. But this is not again like I said, it's another robust result. One thing that you have again here that validates our uniform taxation of intermediate goods or looking at the final good pricing point, is that if you have this bundle, essentially, if you have homogeneous degree one, if you can have a collection of resources that come together as one contract at one final good that people want and have utility on, Then you want to have a uniform base price or tax on these goods.
00:48:42.190 - 00:50:21.954, Speaker A: So again, essentially what's happening is that these goods x one to XN they are akin to an intermediate good that produces G, which is what we focus on here. So the more robust angle here is that the level of Glanyarity should be looking at really what are those homogeneous return to scale that you have? Are there like some sort of operation that you just repeat up in contracts to have some defined products? Okay, now let's go and do some discussion of the results that I showed in the last ten minutes and do some elaboration. So we want to think of final resources and I argue so from the model that I gave. One criterion that can go and looked at empirically to make this case is look at where do you have constants return to scale in resource combinations. And some examples from discussing with many people here and outside is that contact creation could be some when you think of the mid level way or message calls or some bulk storage operations and bulk operations and at the higher level you can think of token transfers or various types of smart contracts as well. Certainly casual evidence and observation of how people respond by thinking of all these arbitrage rather than just letting expressing their preferences through how much they want to pay showed that the opcode might be the wrong and a too low level of Glanority for this base fee pricing. So a survey that I show someone and this is the sources, my sources here, as you see there is not much data.
00:50:21.954 - 00:51:12.500, Speaker A: My sources are from at Nformoto on Twitter, he made a survey of the contracts on Ethereum case and you see that there is some sort of standardization that happens. For example, 70% of live contracts, around 15 million are copies of one of 15 templates. So there seem to be defined final goods. 50% are destroyed contract and they are destroyed in a certain way to release storage. There are some gas token contracts which is around 10% of the live contract and there are forwarders that the centralized exchange, central exchange do, which are around 50% of live contract. Of course this is subject to change, but as we know that Blockchains gets updated and so on. There might be interest in using some resource to study these final goods more.
00:51:12.500 - 00:52:25.974, Speaker A: All right, the other flavor of result which we have, which you think of necessity resources and the empirical criterion to go to look at here, which would be that product that are in high demand across user balances. And my presumption would be that token transfers would be a good fit for that over DeFi interactions which can be less expressed through how much people want to pay, even though we know that we want to minimize certainly the tip. All right. Okay, so now I'm going to propose now just a multidimensional TFM and what would be some changes here. So we have seven minutes left. So the multidimensional one would be like the EIP 1559, which we know has good properties. But we're going to focus now you're going to collect some subset, some subset of after this criterion of final and necessary resources, maybe a small number compared to all amount of resources n, and then you're going to have a target size for each and then you're going to have a maximum size.
00:52:25.974 - 00:52:57.310, Speaker A: Note that for now I haven't set the maximum size being double the target and we're going to come to it later. To be safe, you can put it less than twice the size or let's say up to four or whatever number will be empirically relevant. And then you do the adjustment of prices like you did before. So they have changed the schedule of adjustment to make an exponential one. This makes more sense even before I saw it. The linear one doesn't really reflect elasticity. This is closer to a price elasticity, the exponential schedule.
00:52:57.310 - 00:53:36.058, Speaker A: So gamma here is going to be a parameter to adjust. And now for each transaction, so you can think of transactions, these are quantities for which there would be a base fee, some others for which maybe we find that they are not that final or there is not as much reproducibility on them or that there might not be as necessary. We're not going to put a base fee on them. We compute the base fee. Sorry, this is just over little N, so there is no big N there. This will be directly in ETH. So there is no more notion of gas here because everything is priced in ETH.
00:53:36.058 - 00:55:17.338, Speaker A: And similarly the transaction sender pays the transaction, which is the base fee and the tip and some part the base fee is collected and then reverse and then the tip is given to the validator. Now the question is now the updating parameter gamma and the max resource are very important and this should capture some of the elasticity that we have seen. The question now, how do we measure demand elasticities in economics is that you want some exogenous random variation in supply that you can isolate, okay? Not just through natural experiments, just some random variation in supply. But the problem is that we know that the prices need to depend only on the previous blocks because if it starts depending on the content of the current block, there can be some complex strategies from the validators and the participants. So what would be one approach to this? What do we have currently? So currently what we have is these are in the price adjustment dynamics that you have for the case of Ethereum, for example. So this is by Leonardo and others, is that with the constant adjustment parameter, the block size, so you have the block size in the y axis and the demand range for W here, this is over time. When they adjust prices, the block is often full or empty and the block size is chaotic when this adjustment parameter is constant.
00:55:17.338 - 00:56:47.306, Speaker A: If this adjustment parameter was doing its job in the right way and calibrated to the right essentially elasticity, what you would have is that you would have a mean reverting process, okay? Because you have this target and you have a mean reverting process around this target and this mean reversion should be in a way in which wouldn't have that much variation, okay? So people who have studied this find that you have these dynamics where this adjustment parameter, when it's constant, you have this kind of chaotic behavior. So one approach to measure, to have this exogenous variation is the following. And this is something that an approach that I'm starting to think of, and this is my conjecture, is that you want this to be a mean reverting process. Let's generate what would be the block size that would still be not too high to avoid taking too many risks, but that would be generated through a mean reverting process, through a parameter or reversion here, that depends on the previous price. This is the realized block before. This is our target of mean reversion. And we add some noise so we don't take the block size that is fixed.
00:56:47.306 - 00:57:50.634, Speaker A: We add some noise on purpose using just only information from previous block. And this is computed at the end of the previous block and just announced, just announced before we come to the next block. Okay? So the block size announced before we come to the next block. You can compute all this before and then from there, what happens if the realized block is less than full and higher than the target? You still do the adjustment, but you're going to decrease the adjustment parameter, okay? Because the problem right now is that the adjustment parameter is constant. And so you're going either too slow and it doesn't catch up, or it's too fast and you get the zero or one block. So you're making here this adjustment parameter dependent on the behavior before. And if the realized block is too low compared to what block size you have and lower than the target, in the end you increase the adjustment parameter while you're doing the adjustment.
00:57:50.634 - 00:58:34.542, Speaker A: What is happening here is that then by the frequency, assuming that demand doesn't just change within like, let's say ten minutes, and you will have many blocks, right? Demand doesn't just abruptly change within ten minutes. You will have a sizable demand. This exogenous variation here can tell you good information about on the change in supply, can tell you good information about the demand elasticity, and this helps you adjust the adjustment parameter. So my conjecture is that this type of operation could have nice properties, and that is my proposal when it comes to quantitative things that we could do to measure these cross elasticities. So that's all I had to say. Yes. To answer your question, Anthony, I guess.
00:58:34.596 - 00:58:37.054, Speaker D: There'S a short run, long run elasticity difference here.
00:58:37.092 - 00:58:37.918, Speaker A: Yes, absolutely.
00:58:38.084 - 00:58:43.238, Speaker D: What would demand be if the throughput of the system were changed for three days versus one block?
00:58:43.434 - 00:58:54.974, Speaker A: Yeah, that's why by changing the adjustment parameter here, you're capturing this short run elasticity, which is what you're looking at here, to make it a blocks to make the price reflect that short run elasticity.
00:58:55.022 - 00:58:57.960, Speaker D: But do we want to make the price reflect short run or long run?
00:58:58.810 - 00:59:03.062, Speaker A: I mean, if you want to price things right, because of the product sorry.
00:59:03.116 - 00:59:04.854, Speaker D: I think it's a long run elasticity you want to do.
00:59:04.892 - 00:59:10.010, Speaker A: Right. So what do you exactly mean?
00:59:10.160 - 00:59:21.902, Speaker D: Maybe we have different conditions on the system. Presumably, like, some days a drop is coming and then for a day demand is high, and then some days there's no drop and for a day demand is low, right?
00:59:21.956 - 00:59:22.270, Speaker A: Yes.
00:59:22.340 - 00:59:23.726, Speaker D: So I guess sort of in a.
00:59:23.748 - 00:59:36.834, Speaker A: Sense, but I'm not trying again, what I'm trying to say here, I'm not trying to just say, oh, this is the elasticity and it's not going to change forever. That's the key thing. That's why I'm talking about short run elasticities here.
00:59:36.872 - 00:59:41.234, Speaker D: What I'm just acknowledging, I feel like you want like a longer run.
00:59:41.272 - 01:00:14.874, Speaker A: No, I mean if you are choosing one parameter, if you are choosing one parameter and looking at a policy with an infinite horizon model, you want the longer elasticity. Here what I'm doing is the adjustment of the elasticity to reflect what is the demand conditions in that the sensitive demand in that time frame. The adjustment parameter is trying to capture what is demand in that time frame. Once you hit the block size, all it tells you is to increase it. I'm just telling you. Now let me add some variation on this block size to tell you what rate should I be increasing it to capture that. And because of this, there is no elasticity.
01:00:14.874 - 01:00:38.230, Speaker A: I'm capturing elasticity in that moment condition. In that moment. There is no that's why this is my way. If you're putting elasticities on the blockchain, you're going to get it wrong. And this is my approach of thinking of how can you find a Taton mount process that is better than the current Tatan mount process. And happy to discuss this. All right, it was a lot of fun.
01:00:38.230 - 01:00:38.854, Speaker A: Thanks a lot.
01:00:38.892 - 01:01:27.076, Speaker B: Thanks. One other sort of comment, it comes up in two different places. One thing that's very tricky about blockchains, which is not there any traditional economics application is kind of deliberate manipulation, opposite mechanisms by and so like in two places I see that popping up. One is the most recent one, right, where somehow like an EF 59. I feel like the updated rule is kind of deliberately very coarse in part just to minimize the attack surface. Right. You can kind of predict the ways people might try to manipulate it.
01:01:27.076 - 01:02:24.224, Speaker B: So if you get more complicated the update rule, it doesn't worry about more manipulations. The other place was when this idea which you said, which I found very intriguing, which is somehow almost like think about price at the level of a transaction type as opposed to compiling it down to just like this raw gas and pricing the gas there. Again, I like the idea, but I worry also about sort of adversarial attacks, right. Because for example, when Ethereum works with the gas costs of various Opcodes, if they get them wrong, when all of a sudden they're sort of open to denial services tax because bad people, bad miners can assemble blocks which actually wind up taking way more resources than the gas costs would suggest. Right. So I do worry a little bit that if you just use only kind of the economics and not the engineering to sort of set those prices that there's going to be a holes that can be exploited.
01:02:24.352 - 01:03:06.252, Speaker A: Absolutely. I 100% agree with that, in fact. But also one of the things to recognize now is there is multidimensional pricing right now, but it's centralized one and that's key thing too. So there are already denial of service attacks because of the actual pricing. So I think the way to think about this is actually saying oh, we should do less pricing, we should be more robust to these things. There is already the multimodimensional pricing. What is saying is like what are dimension in which maybe you want to do it right on the point attacks.
01:03:06.252 - 01:04:05.608, Speaker A: I think one thing that kind of was coming out is here in this production function, it captures strategic behavior meaning people choosing what are they going to introduce in the block. But it's considering the block producer. And the person who is writing the smart contract as one entity. That's where I feel like there could be some engineering gains as well. And of course, we economists also should learn a lot more about how to introduce the computer science aspects in it. I will keep though the following that for the cross price adjustments I feel like some of these insights I feel like are very robust for incentives and all other stuff over the whole payment. I think that would be why where some of like one of your work did could be a lot of but we should always think of how not to set parameters in and I think this variation also on the supply by introducing in ways of course.
01:04:05.608 - 01:04:15.304, Speaker A: I know randomization is tricky, but announcing it before the block and using some of the work of Joe could maybe help. And analyzing that more.
01:04:15.502 - 01:04:32.776, Speaker D: Yes, I guess how this can be made. The first part, the 1D pricing can be made into a proposal. You think the cost curve looks like that the UC, right? I think you can approximate the UC by sort of a square U shape.
01:04:32.808 - 01:04:33.160, Speaker A: Right.
01:04:33.250 - 01:04:55.620, Speaker D: And you could have a mechanism which rather than a block size target range what that would do is have some elasticity where the block size increases when gas fees overall gas fees are higher. If you take that seriously, it feels like that pretty you could decently approximate the total optimum if you had a reasonable sense of what this function looks like.
01:04:55.770 - 01:05:08.330, Speaker A: So you need a signal of when are you and this is something I've discussed with Eddie and others to think of what would be a good signal for oh, my block size is too high. Right. And costs can change. And that is the issue you need.
01:05:08.780 - 01:05:12.156, Speaker D: Anyways because you're right now targeting a number. I'm just saying you can replace the.
01:05:12.178 - 01:05:39.430, Speaker A: Number of no, I'm not targeting but they are. Yeah, they're being advanced. No, they think it's a principle of caution. I think talking with people, what they say is that when the block size high, you can be a situation where you have a block size higher and the hash rate is higher. Because I thought hash rate would be a good measure, but you can have more specialized miners. So Eddie was thinking maybe the topology and number of miners would be a good signal. But the cost can.
01:05:41.320 - 01:05:45.984, Speaker D: Mean I'm making a simpler thing, which is demand. If the demand is higher, you want to move around the curve.
01:05:46.032 - 01:05:46.292, Speaker A: Right.
01:05:46.346 - 01:05:50.244, Speaker D: So I'm just saying that rather than always revert to same block size when.
01:05:50.362 - 01:05:54.084, Speaker A: Yeah, but what if you revert and you are here and then the social cost is very high at that moment?
01:05:54.122 - 01:05:55.748, Speaker D: We might be there anyways. We can talk later.
01:05:55.834 - 01:05:57.172, Speaker A: Yeah, we might be there already.
01:05:57.226 - 01:05:59.964, Speaker D: So the same applies to fixing one block size, right?
01:06:00.082 - 01:06:18.530, Speaker A: Yeah. Look, I agree that there is no theoretical reason why the block size has to be fixed. In fact, the example that I showed were purposefully showing sums in which the block size that is chosen is too small. But yeah, I agree that there could be more experimentation to do on this front as well. Yes.
01:06:19.940 - 01:06:24.396, Speaker C: Your first insight was that maybe we shouldn't price transactions as a sum of their opcodes.
01:06:24.428 - 01:06:24.672, Speaker A: Right.
01:06:24.726 - 01:06:51.310, Speaker C: So suppose Ethereum had like half of the transactions for token transfers and we want to make it cheaper for more people to do it. And if you decrease the cost of just one token transfer to less than the sum of its UpCodes, so what part of your analysis, what do you think will happen then? The demand for token transfers would increase because it's cheaper. What are the lessons learned from here?
01:06:54.080 - 01:07:22.640, Speaker A: Imagine there's half of token transfers, others are different. The idea is kind of you want to put a price on token transfers. The leftover there is still price by the market and competition, but you're not going to control that. Right. If you lower the base fee. Naturally, I think what would happen is there might be more demand for that. Okay, so here it's like in the example of the principle of saying you want more of these goods that are more like, in a sense, necessity.
01:07:22.640 - 01:07:26.708, Speaker A: That's where you want to put more of a price. It's not telling you should you want.
01:07:26.714 - 01:07:28.056, Speaker C: To put less of a price?
01:07:28.238 - 01:07:48.076, Speaker A: No, it depends on there is a formula that I gave. Of course you need to think more deeply on the production function. But it doesn't tell you that you should put a less price if you have equity concerns, which is not probably here, you would want to have less of a price on that. But that's not like it will be a bad thing.
01:07:48.098 - 01:07:48.428, Speaker D: Right.
01:07:48.514 - 01:07:54.176, Speaker C: If such essential function, then you'd want to make it more accessible and decrease the price rate.
01:07:54.278 - 01:08:33.848, Speaker A: Yeah, I'm saying you want to set a price on it. I'm not saying you need to have a low price on it. Okay, sorry, let me clarify. So the notion of the necessity thing, it's true, I misspoke they are essential. But if you care about that's, like kind of I would say equity concerns, if you just care about the revenue that you need to collect, it's better to collect it from there because they are essential. Right. And we don't have here a strong equity kind of motive.
01:08:33.848 - 01:08:40.990, Speaker A: We're just looking at more like efficiency. It doesn't make sense, but I agree with you. In fact.
01:08:42.820 - 01:09:07.780, Speaker C: One last thing about that. So if we decrease the price of a token transfer, by your definition, the individual benefit would increase, but the marginal social cost would also increase because people now have to consider more token transfers for the cheaper gas prices. So if you decrease the price of a very popular transaction, the marginal social cost, like the social cost would increase right.
01:09:07.850 - 01:09:13.204, Speaker A: Because no, if you decrease the price, there will be more of it. So the social cost would increase.
01:09:13.252 - 01:09:17.112, Speaker C: No, but miners would have to do the same check for a cheaper price.
01:09:17.166 - 01:09:39.800, Speaker A: Right. If you decrease the gas no, the price is not what they are. It's just the resources they're going to collect, going to be using, which would determine the social cost. They will be using more of it, so they will be using more of that resource, and there will be larger cost to it for the whole network.
01:09:39.960 - 01:09:41.432, Speaker C: So the social cost would increase.
01:09:41.496 - 01:09:41.772, Speaker A: Yes.
01:09:41.826 - 01:09:42.452, Speaker C: Okay.
01:09:42.626 - 01:10:02.950, Speaker A: It and in summary, kind of when I was talking about the cross dimensional, I think it's good to think of it in terms of production rather than social costs and how it affects production. Social cost doesn't tell us how do you do the cost differences? Okay, great. All right, thank you.
