00:00:07.560 - 00:00:08.110, Speaker A: You.
00:00:10.080 - 00:00:35.172, Speaker B: All right. Welcome everyone to this afternoon's a 16 Z crypto research seminar. Very happy to introduce Valeria, Nicolenko research partner here at a 16 Z crypto. Lara will probably tell you she's a cryptographer, but she actually knows an amazing amount about amazing number of topics which those of you who know her, of course, have already experienced. And in particular, she's recently been doing super exciting work on a very important problem, data availability, which she'll tell us some about today.
00:00:35.226 - 00:01:25.652, Speaker A: So lira, fantastic. So I will talk about distributed data storage, data availability, sampling and then sharding. And if you've heard none of these terms before, it's somewhat new developments in the blockchain space. So those ideas are being pioneered right now by Ethereum and Ethereum foundation and those effectively come to help shard the blockchain, help make the blockchain more scalable. So let's just take a little bit of a step back and kind of understand what is the problem that we'll try to solve with this new protocols. And let's look at how blockchains scale today. So you probably all know or heard this for many, many years now, is that natively the blockchains, the two largest ones, Bitcoins and Ethereum, are only supporting very small number of transactions per second.
00:01:25.652 - 00:02:08.850, Speaker A: So your Bitcoin is just seven transactions per second and Ethereum is 15. So it's confirming about 1 MB blocks every ten minutes for Bitcoin and about four megabytes for Ethereum, just because Ethereum transactions are a little smaller. But this is super low numbers compared to Visa that's handling tens of thousands transactions a second. We see blockchains natively, they don't scale very well and you can't really make them scale naively. You cannot just make the blocks appear faster, make the block larger. There is some bounds for why those are the parameters that the blockchain can sustain, otherwise it will start forking too much and you'll be running into security issues. So you can't really naively go and scale them.
00:02:08.850 - 00:03:00.944, Speaker A: You can scale them a little bit, but not by much. And the reason we're kind of focusing on these two blockchains, of course, is that those are the largest. But the market cap, if you look at Bitcoin, I think it now at 45% and Ethereum is kind of on par with Bitcoin if you also include all of the tokens built on top of Ethereum. All right, so that's kind of the problem that we will be addressing. And the fundamental reason for why blockchains struggle to scale is because everybody is doing everything in the blockchain. And the reason is that initial thinking when the blockchains were designed is that the blockchains should really survive catastrophic scenario. So when everything disappears and only one node is left to sustain the blockchain, it can still keep going.
00:03:00.944 - 00:03:50.684, Speaker A: So if everybody else except one crash faulted, since everything is fully replicated, all of the nodes have all of the transactions and are executing them in replication, this node will just keep going, keep sustaining the blockchain. So it seems like they were designed for this scenario, catastrophic scenario of all except one crash failing. But in fact, we know that blockchains fundamentally survive only up to 33%. Sometimes it's 50% of Byzantine faults. So you can't really hope to have a secure blockchain if you have more Byzantine faults than that. So we really want a blockchain that can sustain not just crash faults, but Byzantine faults. And if you want to sustain Byzantine faults, you have to assume that you have a large percent of honest nodes that help you sustain the blockchain.
00:03:50.684 - 00:04:26.744, Speaker A: So at least 66% of the nodes. And here comes an idea of why we do full replication. Maybe instead of doing full replication, we can replicate to the 66% of the onus nodes that we know are there. So maybe we can do some savings there. So instead of full replication, do some partial replication so that each of them, each of the 66% is doing some subset of the work, not replicating each other. Okay, that's the idea we're going to be heading towards. And if we look into what kind of work the node is doing, well, I'm abstracting a lot of things away here, such as consensus.
00:04:26.744 - 00:05:22.124, Speaker A: But fundamentally, it kind of stores a lot of each node, stores a lot of transactions, and it processes all of these transactions. So stores transactions and executes. And we're going to look into solutions that will help us scale those two modules of the node. We'll look at the distributed storage for storing transactions, and then we're going to look at the roll ups that help scale the execution. So the overall kind of question we're going to be asking is how to make each file there do less work, do subset of the work, not replicate each other. All right, so this is kind of I'll start with motivation because otherwise I feel like the protocols I'm going to describe next will not make a lot of sense. But you first need to understand how we scale the execution, and then we will see how to scale storage to help the scalable execution.
00:05:22.124 - 00:06:14.000, Speaker A: So let me explain you very briefly. What are roll ups and how they help scale execution? So right now, without the roll ups, all of the accounts and users are submitting transactions directly to the chain. And the validators here, they work to put these transactions, order them, put them in the block and append this block to the chain. So the validators are working on finalizing lists of these transactions and all the users are submitting transactions to the validator set. If you look semantically, what's happening, if we assume just simple money transfers, then those users are sending coins to each other. So every list of transactions kind of induces this redistribution of coins between accounts. So let's look at a cluster of the users who are transacting between themselves as well as outside.
00:06:14.000 - 00:06:50.796, Speaker A: So let's call these green transactions that are happening within this cluster inner transaction. So inner transaction list, let it be green. And then the transactions that are going out of this cluster, let's call them TX outer and put them in purple. Okay? So let's keep in mind those two kinds of transactions that are happening inside the cluster and kind of going outside the cluster. So what those users can do is they can designate a central party. We'll call it a roll up. And instead of submitting their transactions directly to the chain, they're going to submit it to the chain through a roll up.
00:06:50.796 - 00:07:52.332, Speaker A: So roll up going to relay those transactions to the chain for them, kind of packing them into something smaller than the collection of these transactions. So what roll up exactly is doing? Roll up is a smart contract on the blockchain, and its state is the commitment to the account states of all of those users. So we put account balances of all of the users. We compute a merkel tree, and the merkle root hash is a commitment to account states that stored in the rollup smart contract. So the roll up is only allowed to transact. The only valid transaction that the roll up can submit will have to prove the knowledge of a valid set of this inner transactions within the cluster that will result in the transaction that go out of the cluster to other users other than the roll up users. And such transactions also update the state route accordingly.
00:07:52.332 - 00:08:12.084, Speaker A: So this transaction inner has to satisfy two conditions. This has to result in those outer transactions that are transacting out of the roll up, and it has to update the state route to another valid state according to this transaction inner list another way.
00:08:12.122 - 00:08:15.664, Speaker B: So when you say inner, I should think of that as like l two transactions, the inner transaction.
00:08:15.712 - 00:08:41.548, Speaker A: Exactly. L two transactions, okay. And then this is an l one transaction that actually hits the chain. So the chain never sees these transactions, only sees kind of an update to the state route and a proof. It can be zero knowledge proof, can be fraud proof. I cannot differentiate here what kind of proof that is. Okay? So there are two components, a smart contract and a service.
00:08:41.548 - 00:09:41.868, Speaker A: So the fundamental benefit of the roll up, why it's not actually a centralized party, why you're not introducing a central point of failure, because the whole point of using the blockchain is it's very decentralized and you're not relying on any central party to help you do the transactions. So why roll up is not a point of centralization is because it cannot steal your funds. So every time roll up submits a transaction, it has to prove that it knows a valid set of transactions that will result in this effect on chain. And since roll ups cannot fake valid transactions, they cannot fake user signatures even from users of its own cluster. They will not be able to come up with a valid list of transactions to use in the so rollaps really can't steal funds, but they can do a couple nasty things. First, they can censor, they can just ignore transactions from one of the users completely. Maybe this is one of the sanctioned addresses, maybe they just don't like the user.
00:09:41.868 - 00:10:35.468, Speaker A: And roll ups have this additional component in a smart contract built in that allows the users to go directly onto the chain and transact out of the roll up. So if the service is not cooperating with you, you can always go on chain and transact out and keep going with the L, one blockchain. So it's an additional functionality of the smart contract. Another nasty thing that the roll up can do, it can go down and you'll just lose liveness. So the service is not doing anything. And the mitigation to that, again, you can go directly on chain to transact out, but that's going to be an unpleasant event because nowadays roll ups handle large vast amounts of users. So if roll up goes down and all of those users suddenly want to transact on the blockchain, cause a lot of congestion, very high gas fees.
00:10:35.468 - 00:11:17.676, Speaker A: So there is a better motivation which is anybody should be able to restore the state of the roll up and resume the service. So the smart cortex will stay intact, but everybody will be able to act as a roll up service in case this roll up, this current roll up goes down. It's kind of another safety feature that's built in. But what's important here is that in order to restore the state of the roll up, you have to get it from somewhere. And roll ups, they are abstracting the state away. The smart contracts are storing only the hash commitment to the state. So where do you get the state from? So you need to get it from somewhere.
00:11:17.676 - 00:12:10.380, Speaker A: So there has to be some solution that stores all of the past transactions of the roll up so that you can reconstruct its state. So here we come to the storage. Problem is how do we store transactions plus transactions of this roll up? How do we make sure they are available to users and that validators can agree that they store all of them in order to kind of help roll up be secure. And these transactions, if you notice, the blockchain only needs to store them, the blockchain doesn't need to execute over them. So we're kind of scaling execution with the roll up. But for data availability, storage, we still have to use the blockchain. And I'll show you how to do storage in a much better way than just storing it directly on the blockchain and replicating again on all of the nodes.
00:12:10.380 - 00:12:37.060, Speaker A: And by the way, you also need the state in order to allow the clients to go on chain because they need to proof their merkel, provide a merkel proof for their account state. In order to show you that you have that much balance on your roll up, on your state, in order for the roll up to allow you to transact this balance out on twelve one. So for both of these problems we need some reliable storage for bus transactions.
00:12:37.220 - 00:13:11.776, Speaker B: I think of this as kind of this extreme model where the roll up goes down and somebody just wakes up and was like, oh no, I hadn't been paying attention this whole time and I want to reconstruct from scratch. Like from Genesis, right, the state. So that's why you need like the full log of transactions to be able to look available. The other extreme would be imagine you yourself were just for whatever reason like maintaining the state yourself locally at which point you could just immediately exhibit that and it would hash to the right commitment and everybody like, oh thank God you just had the state saved.
00:13:11.808 - 00:13:12.004, Speaker A: Right?
00:13:12.042 - 00:13:14.420, Speaker B: Yeah, but so then you need someone who's online all the time.
00:13:14.490 - 00:13:14.772, Speaker A: Exactly.
00:13:14.826 - 00:13:19.060, Speaker B: So should I think of where you're going? It's kind of like how to interpolate between those two extremes.
00:13:20.300 - 00:13:51.824, Speaker A: Yeah. Well, essentially what we're going to look into is how to store this transaction in the distributed way so that we still submit all of those transactions under the Ethereum blockchain. But those are special transactions. They only need to be stored. So those special transactions Ethereum validators are going to store in a distributed dispersed way. So not everybody will store all of those kind of roll up transactions, but only in encoded way. Only store fragments of them.
00:13:51.862 - 00:13:55.808, Speaker B: But for now I should be thinking of them storing them permanently. So not all the transactions.
00:13:55.984 - 00:14:18.750, Speaker A: That's what roll ups are doing today. They're just submitting. They're doing some clever packing techniques to squeeze as much efficiency as possible. They're packing all of those transactions into some diet state but they're rolling so much you can pack. It's still very expensive for them to store them on Ethereum. But that's what they do today. They store them on Ethereum directly and that's where the bulk of the gas costs come from.
00:14:18.750 - 00:15:30.396, Speaker A: Right. So just to be a little bit more precise, so when the roll up submits a transaction, this transaction looks like the outer list of transactions that are kind of going out of the roll up to l one accounts, the updated state route. So we go from stateroute S to state root s, prime updating the inner account balances of the roll up users and a commitment to the inner transactions that the roll up is kind of masquerading from the blockchain. And then the proof pi will just prove that all of that is correct, that the transaction inner under the commitment corresponds to the valid state transition and to the valid transactions going out of the roll up. So the transaction will only be accepted if it has a valid proof and also if the commitment transaction inner under. So transaction in or under the commitment is available and we're going to look into what that means in just a second. But the way roll ups operate right now is it's a mid transaction directly into the Ethereum.
00:15:30.396 - 00:16:03.512, Speaker A: And Ethereum kind of recommits to these transactions. That's how validators make sure those transactions are there. Cool. So just having a bigger picture, we'll now have a lot of different roll ups and those roll ups are kind of abstracting away their clusters. So only few transactions hit the chain. But in fact there are many more transactions that are happening, just that those roll up transactions, they never hit the chain. And these transactions that hit the chain, they are much easier and cheaper to execute compared to all the transactions that are happening within the cluster.
00:16:03.512 - 00:16:43.480, Speaker A: So you see kind of within this 15 transactions per second that the roll ups and other users produce. In fact, under the hood, many many more transactions are happening. So we're kind of scaling the execution, more transactions actually happening, but only these 15 transactions per second hit the chain. Let's look at the roll up. The good indication to see that the roll ups indeed helps scale Ethereum is to see how much it costs to transact on those roll ups. Because the cheaper the transaction cost, it means the easier it is for the validators to handle this load. So the lower the cost, the more scalable is the solution.
00:16:43.480 - 00:17:14.460, Speaker A: So roll ups indeed have two to 30 x lower gas cost. So if you look at send simple Ether on Ethereum, it's going to be about one dollars. But say on Optimism, it's only 0.2, only $0.22. So indeed roll ups already help scaling. But if you look deep into kind of what fees you pay, this is from Optimism dashboard. So 90% of the fees still go to storing data storing these transactions.
00:17:14.460 - 00:17:50.156, Speaker A: So 90% of fees go to data fees and only a small portion goes to execution fees. This will be a little larger for zero knowledge roll ups because you need to verify zero knowledge proofs. But still one data fees will be a substantial portion there. So if we make this cheaper, we have a good chances of dropping the transaction fees by ten x, so we'll be only left with a small fraction. So we'll now focus on how to make data fees lower, how to make the blockchain store on this slide and.
00:17:50.178 - 00:18:01.088, Speaker C: On a previous one. You seem to be taking the position that roll ups have to store the data on ethereum or on the blockchain they're working on, as opposed to they just need to store it somewhere. Right.
00:18:01.254 - 00:18:02.416, Speaker A: Yeah, right.
00:18:02.518 - 00:18:07.700, Speaker C: So I guess off chain data availability would be another option, right?
00:18:07.850 - 00:18:56.900, Speaker A: Yeah, true. And some of them do data availability committees and do off chain data availability. Yeah, we'll be looking into monolithic kind of design, but true, you can maybe look into other option. It's just that this proof that transactions are available, if you are going to another chain to store those transactions, it should give you something to show here as a proof that you are storing those transactions elsewhere, like a certificate of storage or something like that. And this is tricky, right? Because you will have to do a bridge effectively to another blockchain. Maybe you will even have to verify partial consensus here. I don't know, it's kind of tricky to piece those two chains together.
00:18:56.900 - 00:19:17.080, Speaker A: Yeah. But for this talk we'll look into single blockchain trying to do everything here. Cool. Okay. So how to make validators store data distributed. So let's jump into now the main topic of the stock. So we'll be looking into solutions for distributed data storage.
00:19:17.080 - 00:19:54.710, Speaker A: Those have been started from the those are two protocols, dispersal protocol and the retrieval protocol. So the dispersal protocol distributes a block of transactions to N validators. And then the retrieval protocol reconstructs this block back from any subset of N minus F validators, where F is rebound of the number of faulty validators. So byzantine validators. So here we have five validators. Two can be Byzantine. No matter which ones are Byzantine, you still have to be able to reconstruct the original block from them.
00:19:54.710 - 00:20:34.912, Speaker A: And the techniques that we're going to be using are erasure codes. I'll kind of hint at their design. But then we're going to go more in depth into all of the steps and it will make more sense at the end. But at the high level, what we're going to do is we're going to take the block, we're going to pack it into a matrix and we have some flexibility into picking the dimensions of this matrix anyway. We'll pack into the matrix B all of our data that we want to disperse, those will be roll up inner transaction lists. But let's just call it block. And we're going to apply erasure coding techniques.
00:20:34.912 - 00:21:17.570, Speaker A: So we'll take this matrix for B and we're going to append more rows to the matrix, kind of expanding it. And those additional rows will help us reconstruct for erasures in this matrix. So we apply erasure coding. Then we give each validator one row of the resulting erasure coded matrix e. So e one through e four, e five. And then when we are reconstructing and this only works so far for crash faults, when we are reconstructing, some of the validators might have crashed. As long as there are bounded number of them crashed, you can still and there is enough data that you are recovering, you can apply erasure coding and kind of correct for those erasures and piece the block back together.
00:21:17.570 - 00:22:02.300, Speaker A: Okay, so let's look into erasure coding. Oh yeah, the fragment size here has to be well, will be, can be larger than that, but it cannot be smaller than the size of B divided by N minus F because when you will be reconstructing here you will be receiving data from n minus F honest nodes. F is number of faulty nodes, n is number of total number of nodes. So the data that you reconstruct has to be at least of size b. And that's the optimal you can hope for. Yeah, because we don't have generic data compression. So the yellow data here has to be at least the size of b in order to be able to reconstruct.
00:22:02.300 - 00:22:23.392, Speaker A: And these algorithms will achieve this kind of optimal fragment size. Cool. So let's dive into a quick tutorial on erasure codes. This is going to be super simple codes. You don't have to take a class to understand how they work. Those are pretty straightforward. Let me just explain them in a couple of slides.
00:22:23.392 - 00:22:54.620, Speaker A: So let's treat our block as a vector of integers modulo p. So, just a simple case. We'll pack our data into this vector b one, b two through b m. And then we'll need a special matrix g. The size of g is going to be n by m. So m n is larger than m with a special property. And the special property is that if you take any m rows of the matrix g, you get a full rank matrix.
00:22:54.620 - 00:23:44.528, Speaker A: So any subset of m rows will give you an invertible matrix. So let's assume we have the matrix with this magic property. Then our ratio coding will simply multiply our vector b by this matrix g, and you get back an expanded matrix sorry, an expanded vector e. So reconstruction for e, which is a larger vector. Yeah, it has n elements, more elements than in the original block. To reconstruct E, you have to have any m elements of e. And the way it's going to work is you take the elements of E that you have, and from this matrix g, you leave out only the rows that correspond to the elements that you get.
00:23:44.528 - 00:24:34.290, Speaker A: So here you have m by m matrix, by this property of this g, it's invertible. And so you can multiply by here by an inversion. So let me walk you through an example for simple error correcting code, where n is equals five and m is equal three. So the block here only has three elements, and when we erasure code it, we expand it into a larger vector with five elements. Okay, so you have three to five expansion. Then what happens is if you lose some of the elements of this vector e, so those are erasures, those have been held by Validators, that crash fault that crashed, and you don't have them anymore. So we will kind of ignore the corresponding rows from the matrix g.
00:24:34.290 - 00:25:09.160, Speaker A: Let's just forget about them. And then the matrix that you have is a square, so it's three by three by this property it's invertible, so you can invert it. Let's see. So we're piecing these things together and now we need to find b that will satisfy this relation. But that's simple solving a Flynn's system. You invert the matrix and multiply e prime with this inversion to get the block b back. Okay, let's now look into the examples of this code.
00:25:09.160 - 00:25:43.140, Speaker A: How to pick the matrix g with this matrix property. So a property again is that any mrs of g should constitute an invertible matrix. And surprisingly, a random matrix will just work if your prime q, everything works modular prime here. So if your prime p, in this case p is large enough, then any random matrix will have this property. The problem is that it's expensive to invert a random matrix. So if your matrix is of size n by n, inversion is n to the 2.37 or something.
00:25:43.140 - 00:26:19.804, Speaker A: So it's more than square and a better choice is another matrix. I'll show you in a second. It's called Vandermont matrix and it gives Ritz Solomon erasure codes. I'll let this words not scare you because this is really simple. This matrix is of this form, you take consecutive integers 12345 and you compute powers of those integers. So this is called the Vandermont matrix and you can see how to extrapolate it into more dimensions. So, what's happening here is when you multiply your vector b by this matrix is in fact you are doing polynomial evaluation.
00:26:19.804 - 00:26:53.964, Speaker A: So you can treat this vector as coefficients of the polynomial a zero, a one, a two. And when you multiply by matrix, you're producing these evaluations at points 12345 for four, it's a zero plus four times a one plus four squared times a two. So you are getting evaluations. That's basically polynomial evaluation. And the backward transformation is polynomial interpolation. And turns out that you don't need to do any inversion of the matrix for this Vandermont matrix. You can just do it all in your head.
00:26:53.964 - 00:27:21.268, Speaker A: And the transformation back and forth is quasilinear. So it's linear with some polygarithmic factors. So that's assuming that you're not using consecutive integers, but powers of the root of unity, it doesn't really matter. It's a technicality. This kind of method works as long as you have any distinct evaluations here. So both directions is really efficient to encode and decode for this code.
00:27:21.434 - 00:27:25.940, Speaker B: You said the n log n needs like an FFT like powers.
00:27:26.360 - 00:28:01.836, Speaker A: Exactly right. That's an FFT. So register coding is going from coefficients to evaluations and decoding is going back. We'll use a related code, it's very similar, but it goes from evaluations to evaluations. So it also can be represented with a linear matrix. And the reason we want this is we want to preserve the code word inside the encoded block. So we want to preserve this values inside of our encoded coded string.
00:28:01.836 - 00:28:22.196, Speaker A: And that's just in a happy path. When no validators are faulty, you can just query the top three and very quickly, you don't even need to go through reconstruction happy path. You already are getting the vectors that you're interested in back from the validators.
00:28:22.388 - 00:28:29.168, Speaker B: It's not just that you said as long as everybody's non faulty, but actually it's even as long as the top three are non faulty.
00:28:29.204 - 00:28:53.404, Speaker A: Yeah, you're right. Cool. Yeah. And this is kind of doing interpolation and evaluation. So it's kind of combining the two steps. Basically the top square matrix is just identity just because you're translating the top vector to itself. And then the bottom ones are some LaGrange coefficients, but this is also linear.
00:28:53.404 - 00:29:55.376, Speaker A: So to get y four, just a linear combination of these three elements. Cool. So this is the matrix that we're going to use and transformations back and forth are also very efficient. Okay, so I showed you how to encode like really dummy vectors. But how do you encode really large blocks? If a block is super large but you still want to encode it, say to five validators, what you do is that you put it into just matrix that's a little bit wider and depending on the size of your data, you're going to pick kind of the size, the width of the matrix, but you will still be encoding into five rows if you have five validators. So the way it's going to work is the disperser is going to erasure code the matrix B. Just multiply it by g on the left by our fancy matrix or doing some FFTs to make it more efficient.
00:29:55.376 - 00:30:52.760, Speaker A: And it will give each validator one of the roles of e and then decoding from the errors is multiplying by the inverse or again doing the FFT if you want more efficient algorithm. And this only tolerates crash faults. So if those validators are feeding you bogus data, this method won't work. You either need to go to error correcting instead of razor correcting, but this will result in parameters that are much worse, but you can still do it with error correcting tolerating some small number of Byzantine faults. But we'll do much better. We'll augment this method to have the same kind of efficiency but make it tolerate Byzantine fault. By the way, there is a parameter that you have some freedom of choosing, which is the number of rows in the matrix B.
00:30:52.760 - 00:31:35.892, Speaker A: So when the disperser wants to disperse a large file, it can choose into how many rows it subdivides this file. So it has this ability kind of free parameter V that it will choose to kind of make it smaller or larger as long as that's less than n minus F. So there is an upper bound that you cannot go above, but you can make it smaller. But making it smaller kind of increases this dimension, the width. And when you increase the width, it means each validator will store more data. So when you make V smaller, you make validators store more. But at the same time the number of pieces that you need in order to reconstruct the block is also.
00:31:35.892 - 00:31:59.508, Speaker A: V. So making V smaller allows you to talk to smaller number of validators. So with smaller V, validators store more, but you need to talk to fewer of them. So there is an interesting trade off thing you can play with. Okay, but how to protect against malicious servers. And here we'll need to use a little bit of a heavy machinery. We'll have to use cryptography.
00:31:59.508 - 00:32:17.030, Speaker A: And the cryptography that we'll need is homomorphic vector commitments. So commitment, this commitment scheme can commit to vectors. So those are K length vectors, modular Prime P. The resulting backing up and.
00:32:17.720 - 00:32:22.704, Speaker B: Talking through why what you've said so far doesn't work for Byzantine faults.
00:32:22.832 - 00:32:57.364, Speaker A: Why it doesn't work for Byzantine faults? Yeah. So the reason is that you can correct for errors in this code. It's just that you will be able to correct for twice, two times smaller number of errors than erasures. So you'll be able to correct for one Byzantine validator instead of two. Right. So that means your F is smaller. Basically, the parameters of the scheme degrade a little bit.
00:32:57.364 - 00:33:21.236, Speaker A: It's just that to handle these two validators, you have to use a larger code. So you have to add two more roles here if you still want to handle two validators, do you have to grow validator set? This solution is kind of not optimal just because it can handle fewer errors than erasures. But if you're fine with that actually.
00:33:21.278 - 00:33:24.716, Speaker B: I have some slides the decoding procedure has to change. Right.
00:33:24.818 - 00:33:26.796, Speaker A: Decoding also becomes much I was thinking.
00:33:26.818 - 00:33:30.764, Speaker B: Of erasure coding as like coding where you know where the bit flips were.
00:33:30.802 - 00:33:31.390, Speaker A: Basically.
00:33:33.120 - 00:33:39.408, Speaker B: If two of the five rows are bogus, you don't know which ones, but you would need to find the right three to invert and you don't.
00:33:39.414 - 00:34:01.156, Speaker A: Know which ones those are exactly. Yeah. So you have either to know who are the values that isn't in or kind of correcting for a smaller number of razors, just more expensive. It's not as simple. For that you need to take a course. It's a complicated algorithms to correct. But we will augment this scheme with homomorphic commitments.
00:34:01.156 - 00:34:23.532, Speaker A: So commitment to a vector is something short. We will use deterministic commitments. You commit to the same vector twice, you get the same thing and the commitment should be binding for this to work. So you cannot commit to two different vectors getting the same commitment out. So you cannot find any collisions with the scheme. So hash function is a good commitment. So you just hash your vector.
00:34:23.532 - 00:35:03.996, Speaker A: It's short, deterministic and binding, but it's not homomorphic. And there are schemes that are more complicated, but they are homomorphic. So by homomorphism I mean it's that if you take two commitments that you add them together, you get exactly the same commitment as if you added the vectors first and then committed. So homomorphism means that you can kind of move this addition operation inside the commitment. Right. And then here is what we're going to do. We're going to commit to the row vectors in our block B, creating this kind of column vector H.
00:35:03.996 - 00:35:44.248, Speaker A: So each element of H is a commitment to the row of B, and this vector commitment we're going to replicate to all of the validators. So we're not replicating B, but we are replicating this H. And this H is something much smaller than B, so that's okay to replicate. And then we do same thing as before. We erasure code B into E and split it and give each validator one row of E. So because of the homomorphism of these commitments, if you multiply the vector H by G, you magically get back the commitments. C.
00:35:44.248 - 00:36:39.180, Speaker A: That correspond to the commitments of the expanded matrix E. And that's because the homomorphism of the commitments means that you can compute linear combinations of these commitments and they will correspond to linear combinations of the vectors. So since this is multiplying by G is a linear combination, the commitments will satisfy this equation. So G times H will be equal to C. So it doesn't matter how you compute C, whether you recommit E to get C, or whether you just take H and multiply it by G, both ways you get C, right? So how the retriever is now going to work, it's going to talk, it's going to wait to receive 50% of replies and going to check that it gets consistent age. So it's kind of verifying that validators are consenting on what age they store. It's kind of consensus on the user side, so to speak.
00:36:39.180 - 00:38:02.852, Speaker A: So as long as when it gets 50 plus Epson percent of replies with consistent h, it will then request all of the fragments and it will check that the fragments correspond to the commitment. So the way it's going to check is it's going to expand this vector H with this matrix G getting C, and it will check kind of recommit each vector to check whether this matches C or not. And for fragments that got corrupted, they will not match the commitment. So those are just, the retriever can safely filter those out and treat those as erasures and only left with correct fragments, they can then apply erasure, coding erasure, correcting here. So, same thing, we added this additional filter that filters out fragments that were altered in a Byzantine way and don't conform to the commitment anymore 50%. So if you talk to less than that and say disperser is malicious, it can maybe disperse one file to one half and another file to another half. And if you're only talking to less than half, you will not be able to figure out that the other half stores something else.
00:38:02.852 - 00:39:04.630, Speaker A: And then another retriever that shows up and talks to another half will retrieve a different file. Why not two thirds? You can make it two thirds. Oh, why not two thirds? Well, why two thirds? You can do one third plus one you can do, but then you have to make them run consensus because I worry that I get exactly half of the replies. Exactly half of those are from Byzantine notes, and the other half are from honest notes. And then there is no majority strictly more than 50%. Yeah. There can still be a tie, right? Shouldn't I have more than two F plus one replies? So F is less than 50%, the number of Byzantine faults.
00:39:04.630 - 00:39:09.720, Speaker A: But don't I need two F plus one to make sure the majority is.
00:39:10.090 - 00:39:21.850, Speaker D: I think the 50% of the replies have to be consistent, like 50% of your replies with the same so if half of half are inconsistent, then you won't.
00:39:25.760 - 00:40:11.610, Speaker A: Yeah, it's not quite kind of a quorum thing from the consensus. It's kind of a non interactive check for the consistency. Is it that if there are ten nodes, you wait for five replies that are consistent with H plus one? Yeah. And then if you have those and they're consistent, you're reconstructing against that H, and you know that any other retriever will either reconstruct the same file or fail to reconstruct for some reason. So you don't want this inconsistency that different. Retrievers reconstruct different things. It's either those who reconstruct reconstruct the same.
00:40:11.610 - 00:40:15.610, Speaker A: Thanks. Okay, cool.
00:40:15.980 - 00:40:26.640, Speaker B: This feels kind of like a sort of general result. It kind of feels like you're saying through homomorphic commitments you can reduce error correcting coding to erasure coding.
00:40:28.260 - 00:40:29.170, Speaker A: That's right.
00:40:30.500 - 00:40:31.712, Speaker B: Was that known before?
00:40:31.846 - 00:40:33.856, Speaker A: Yeah. This is a long line of work.
00:40:33.958 - 00:40:34.512, Speaker B: Okay.
00:40:34.646 - 00:40:36.470, Speaker A: I have a slide at the end also.
00:40:38.920 - 00:40:44.916, Speaker B: Because we're talking about erasure coatings, like, you know where the errors are and that's like, exactly what this C gives you, right?
00:40:45.018 - 00:41:29.616, Speaker A: Yeah, exactly. So some of the more efficient commitment schemes just maybe came out the last decade, but before that, people were thinking of using homomorphic fingerprinting, those kind of information theoretic objects. So you can build this thing from that or Universal Hashing, but only recently, kind of we got this homomorphic commitments that are super efficient and easy to use, making that possible. And that's, I think, kind of the latest work by Stanford people kind of explains how to build that with our modern commitment schemes. But yeah, the concept was known.
00:41:29.808 - 00:41:46.410, Speaker B: Is it known you can't do this well without assuming existence of commitment schemes? So, like purely information theoretic. Does this overcome some information theoretic limit by having the by assuming that you have these commitments that you can't find collisions and stuff?
00:41:47.980 - 00:42:25.290, Speaker A: I'm afraid to be mistaken, but I think you can build this with information theoretic objects. It's going to be similar to error correcting that it's not going to match the same efficiency. But it's an interesting question. I need to look more carefully to give you an exact answer there, but I suspect that you definitely can't build this with information therapy techniques. I don't think it's going to match the efficiency of this. Great. Okay, so this is how you distributedly store the data.
00:42:25.290 - 00:43:14.700, Speaker A: So let me just recap what we've covered. We first kind of started with why blockchains fail to scale. And the reason they fail to scale is because everybody's replicating each other's work. We then tried to explain how rollups help scale the execution and kind of motivate why we need good solutions for distributed data storage. And then we just looked into how to build distributed data storage resilient to crash faults using razor coding and how to make it resilient to Byzantine faults using some of the heavy cryptography using homomorphic commitments. So right now I'm going to skip to the next topic. And the next topic is how to make validators agree on whether they store the data behind the commitment or not.
00:43:14.700 - 00:44:06.380, Speaker A: And just going a little bit step back of how this fits into the roll up story, as the roll ups are going to be submitting this inner list of transactions to the blockchain and they're going to disperse this block to the validators. So validators will store all of this inner transaction list in a distributed way instead of full replication. So that's how we're going to scale storage for all ops. Okay, so now how do we reach agreement? So the protocol that I showed you, it's only disperser, sends the messages to the validators and then retriever kind of retrieves something back. Validators are not talking to each other. So let me just redraw this picture. And of course, when we want agreement, we have to make validators do some interaction in order to kind of come to the decision of whether they store all of the necessary data or not.
00:44:06.380 - 00:44:48.202, Speaker A: And there is traditional way to build agreements through reliable broadcast, kind of BFT protocols that are studied for decades now. It's multiple rounds of interactions where every validator is talking to every other validator. You can make it three rounds, you can do this in two rounds. It depends also on your network assumptions and such. But largely speaking, I will not go too much into depth. If you're interested, look into the slide later on. But the Validators, as a result of this interaction, they will produce some certificate that can in principle serve as a proof of storage that they are storing this data.
00:44:48.202 - 00:45:23.326, Speaker A: So first of all, they will all come to agreement if they did store all the necessary fragments for reconstruction. And they will also come up with a proof of storage that they can later on show to the client. It's kind of an aggregate signature and the client can then maybe go breach to another blockchain and show it there. Okay, so I'm not going to focus on that. You can read, given direction, how to build reliable broadcast and consensus here, but it's well known how to do consensus on this type of systems.
00:45:23.438 - 00:45:27.222, Speaker B: That's black box, right? Any protocol for reliable broadcast, you just plug in yeah.
00:45:27.276 - 00:45:27.878, Speaker A: Okay.
00:45:28.044 - 00:45:43.914, Speaker D: And just make sure I'm on the same page. The problem you're talking about now is just making sure that among the validators, they're storing all of the different fragments, that the disperser didn't send the same thing to everybody and they don't actually have all the information.
00:45:44.032 - 00:45:59.018, Speaker A: Yeah. So basically this agreement guarantees that at least F plus one honest nodes are storing correct fragments that are consistent with age. So you know that there should be enough fragments to reconstruct back the block.
00:45:59.194 - 00:46:01.394, Speaker B: And they themselves know that fact, right?
00:46:01.432 - 00:46:34.650, Speaker A: Yeah, exactly. They themselves get to be convinced in that, and they can also prove it outside. Cool. And another kind of beautiful idea that I think Ethereum Foundation was the first to try to make it work. I haven't heard them explain the idea this way, but it's effectively trying to build an alternative consensus mechanism around data availability through data availability sampling.
00:46:35.070 - 00:46:36.506, Speaker B: Who did you attribute this to?
00:46:36.608 - 00:47:15.110, Speaker A: Ethereum Foundation Bank, Sharding. I think this is the first to kind of suggest this idea, an alternative way to build consensus around data availability. So instead of BFT style, they do longest chain style consensus. It's kind of surprising that you can do this for data availability. But let me explain first the mechanics of data availability, how it works. So, data availability sampling is a protocol that helps the client make sure that the validator set is storing the data underneath the commitment. So it allows a client to request random fragments, fragments from random nodes, to probabilistically check whether the data is available.
00:47:15.110 - 00:48:14.938, Speaker A: So the client what it does, it gets the commitment to the data. It has the commitment to the data, and it's trying to understand whether there are enough fragments that are stored by the validators. So having that commitment, assuming there are many, many validators, the client will pick a random validator, get its fragment, and check that the fragment is consistent with H. And if so, here are some parameters that are tweakable. But say there are two third of fragments are enough to reconstruct the original data, to kind of piece to do error correcting or razor coding to get back the original data. So if one third of the fragments is enough, then the block is only unavailable if two third of these fragments are absent, are missing. So the probability that those many fragments are missing yet you get a valid fragment is only one of a third.
00:48:14.938 - 00:48:47.830, Speaker A: That's a probability of your false positive, of you thinking that the data is available, when in fact it's not, when in fact that much of it is missing. So you just was lucky to hit the existing less than one third. And then you can repeat that. You can query another run evaluator, get back the fragment. If it's correct, you know that probability of the data being unavailable is one third squared. And so it goes down exponentially with T tries. And you can amplify it really quickly to get it to negligible.
00:48:47.830 - 00:49:48.714, Speaker A: And the amount of data that you'll be downloading here will be much smaller than it's not intuitive to see, but it will be much smaller than you need in order to reconstruct the full block. So the kind of communication here, you will ensure that the data is available with much less communication than you need in order to reconstruct the block. So those algorithms are efficient to making sure the data is there. So what the validators will do when they will try to agree on whether they store the data, behind the commitment is that each of them will sample every other. So the validator number one will independently request and verify fragments from randomly selected validators to get assurance that the data is available. So it will kind of locally get this assurance and assume that the data is available. And everybody doing that will eventually come to agreement that the data is there.
00:49:48.714 - 00:50:45.420, Speaker A: So I'll not go into the details explaining how longest chain consensus works, but it essentially allows validators to do longest chain style consensus. Now, I need to say that this is very much new, and Ethereum Foundation is still trying to figure out how exactly the protocol is going to work, especially the networking aspects of it, how exactly you query the validators, which validators are connected to which ones, et cetera. So they even have some call for proposals, and they gave out some grants for people to research this problem. But that's what fundamentally they are trying to do, build longest chain style consensus based on data availability sampling. Great. And in the remaining five minutes, let me quickly explain the Denk sharding. It's a future upgrade to Ethereum's, named after Duncrat Feist, who came up with the idea.
00:50:45.420 - 00:51:13.042, Speaker A: It's a proposal for distributed data storage for Ethereum. The proposal is to do consensus through data availability sampling. And it has some other clever ideas. The razor coding that I explained to you was kind of easy, but they have an extension to that. They do 2D eraser coding. And for that, you need some special commitment polynomial commitments to make it work. So we were expanding the block down.
00:51:13.042 - 00:51:43.920, Speaker A: We were kind of appending more rows to the block, but they're also expanding the block right and kind of expanding it both directions, creating a larger matrix as a result. To make it work, you need some more constraints on your commitments. You need them to be polynomial commitments to make them work. Another interesting aspect that they combine replication with the razor coding. Just because the number of validators on Ethereum is so large, it's like half a million right now. You cannot possibly expand your small block to half a million validators. It's just too much work.
00:51:43.920 - 00:52:44.930, Speaker A: Multiplying by this metric or doing FFT, it's still very expensive. So they're kind of trying to look for trade offs and erase your code to some larger set, but then have validators since the number of encoded fragments will be smaller than the validator set, smaller than 500,000, some of the validators will be storing the same fragments. So interesting kind of maybe research to be done to exactly analyze what are the guarantees here with the change in number of validators if you combine replication and razor coding and then another beautiful idea is locality of reconstruction. And that's why they have this 2d design. A missing fragment in this scheme, and then sharding can be reconstructed without reconstructing the full block. So if you're a poor validator who didn't get the fragment, you can only talk to a small number of other validators and they will help you reconstruct your fragment. You don't have to go through full block reconstruction.
00:52:44.930 - 00:53:28.394, Speaker A: Great. And Dank charging will add a new transaction type. So those are going to be transactions that roll ups will submit, they will submit as data blobs. They're going to submit this inner transaction list that it need to store. So each transaction has this giant data blob, it's of the size of around 130, small commitment, signed commitment to that blob. So this yellow part is going to be erasure, coded and distributed, split into fragments and distributed to validators. So none of the validators will store this replicated, they only stored fragments of erasure coded data but they will store replicated commitments.
00:53:28.394 - 00:54:00.730, Speaker A: So commitments will persist on the chain but this data will be distributed and N is 256. Right now the thinking is that should be enough to satisfy the needs of the roll ups. Yeah, the last slide I have is conceptually what Dank shining will allow to do is to increase the block size. So right now the block size on Ethereum is 0.1 megabytes on average. That fluctuates a little bit but not much. And then with Dank sharding the blocks are going to be massive like 300 times larger blocks.
00:54:00.730 - 00:54:56.758, Speaker A: But this additional data that will get added into the blocks will not be accessible to execution, it will just be stored in distributed way. It's not going to be replicated. So the validators will not be able to execute on this data. They will be able to execute on the commitments if they want through zero knowledge proofs, but not on the data itself. So this additional data that will get added will be just stored by the network and this will add about half a terabyte per validator, which Ethereum thinks nowadays is kind of cheap to store half a terabyte on a hard drive. So they still live up to their premise that validators can run out of laptops and that's about 30 x less than full replication. If you are to replicate the 30 megabyte blocks to all of the validators you're going to incur like 30 x blow up in the storage.
00:54:56.758 - 00:55:36.614, Speaker A: But this additional blob data also has an expiry, that's what caps it, it's 500GB. So the current thinking is that it's going to expire in one to two months. But that's optimistic. So none of the mechanisms there incentivize the Validators to keep the data for that long. They're only incentivized to distribute it during the data availability sampling, when they're actively working on achieving consensus. And if you really want to download this data, that's the moment you need to connect to the Validator set to be able to get those transactions. And then they're going to store it for one to two months.
00:55:36.614 - 00:56:12.526, Speaker A: But that's altruistic. The protocol doesn't guarantee they're going to keep up with that. So you really have to be there in the moment in order to download this data. For example, if you want to restore the state of the roll up, if you're a very active user, you have to actively be downloading this data at the moment when the data is being uploaded by the roll ups. Later on, you'll likely be able to download it from archival nodes, but it's going to be altruistic. So the data is not accessible to execution. Only commitments are accessible, main users are roll ups.
00:56:12.526 - 00:56:43.120, Speaker A: And another interesting thing is that it's going to have a different fee market. So this Blob data is going to be priced at one gas per byte, but this is a different gas than the current gas fees. So how much it's going to cost compared to ETH is yet to be seen, and compared to Call data, 16 gas per byte. But that's, again, a different that's execution gas, and this is storage gas. So it's not very clear how they're going to compare long term. Cool. Okay? That's all I have.
00:56:44.050 - 00:57:02.950, Speaker C: So actually, could you page back one slide? Thanks. So this data doesn't have to be propagated to validate the block that it's included in, or does it? Can a block be kind of validated and confirmed before the Dank Sharding data is actually distributed?
00:57:04.410 - 00:57:20.814, Speaker A: Well, you have to achieve agreement that you store the data before you finalize this commitment. So this commitment will not persist unless the Validators figure out that they indeed stored.
00:57:20.882 - 00:57:26.330, Speaker C: So there's some concern about block confirmation latency then, with the much bigger blocks.
00:57:29.790 - 00:57:32.314, Speaker A: You mean with this additional interactive protocol.
00:57:32.362 - 00:57:48.740, Speaker C: For well, it's just way more. I mean, if the blocks are seems like they're like 300 x the size you just said, so I mean, that's much more latency to propagate the block, right?
00:57:49.750 - 00:58:25.258, Speaker A: Yeah. Well, okay, so the Ethereum plans to do this proposal builder separation. So the builder is going to erasure code the block and then split it into fragments and send to the Validators. So it has to do all of that in less than 12 seconds. So that's a lot of work for the builder, but they expect the builder to be beefy in any case because the builders are doing MAV extractions and such stuff like that. So they're outside of the Validator set. They can be running on kind of good and fast machines.
00:58:25.258 - 00:58:42.020, Speaker A: So that's not a concern on the dispersion side because they assume that the builders are powerful to distribute that, but it definitely adds some rounds of communication when they try to consent on the data availability because of this data availability sampling. So it's maybe a little bit of a delay there.
00:58:42.970 - 00:58:57.990, Speaker C: My other question was all Validators have to be Dank Sharding storage nodes? There's no concept of validating, but not storing data or doing. Yeah, that also seems a bit weird.
00:58:58.490 - 00:58:59.318, Speaker A: Yeah, exactly.
00:58:59.404 - 00:59:03.002, Speaker C: In the future, they could unbundle that, maybe.
00:59:03.056 - 00:59:09.658, Speaker A: Yeah. Or locally you can maybe partner with another Validator who will be storing for you and you'll be executing.
00:59:09.754 - 00:59:27.394, Speaker C: Well, that's my concern, is that in the short run, a lot of validators will just put all their Dank Sharding data on AWS or wherever, and it won't actually be that decentralized if a lot of them don't have 300gb or whatever sitting around.
00:59:27.512 - 01:00:27.766, Speaker A: Yeah, but at the same time, 300gb is like, I don't know, probably under $100 to store to buy a hard drive to store them. I don't know. But for consensus at least, you'll need fast access to this data because nodes are requesting the fragments from you. I don't know if storing it on AWS is too slow because you'll have additional delays there, but yeah, good question. I think the whole idea is that Ethereum hopes this number is slow enough that the Validators will store this locally. And I think if they realize that the Validators are starting offloading this data to AWS, they're going to probably change the parameters to make sure validators can still keep them locally. That's my guess.
01:00:27.868 - 01:00:42.878, Speaker C: Just seems weird that it's like a one size fits all parameter for every Validator, rather than validators sort of opting into how much storage they want to do, especially since they have a floating gas cost per byte of data stored here, right?
01:00:42.964 - 01:00:54.110, Speaker A: Yeah. At the same time, I think they figured out right now it doesn't cost that much. It's still cheaper than buying a laptop. So if you're already executing transactions, you should be fine storing.
01:00:55.490 - 01:01:02.482, Speaker B: Also sort of touching on the Haggenda kind of vision, right? Sort of opt in through restaking kind of stuff.
01:01:02.616 - 01:01:03.300, Speaker C: Yeah.
01:01:04.870 - 01:01:09.222, Speaker B: Lara, could you talk a little bit about your recent work with Dan in the context of the talk you just.
01:01:09.276 - 01:01:55.334, Speaker A: Yeah, I figured it's a lot to cover. So my work with Dan is going kind of in the weeds of how razor coding is done and expansion. So Ethereum kind of proposed doing a two step expansion. So you take these vectors and you treat them as polynomial evaluations, and you evaluate at more points. You expand this block this way, and same wise, you expand the columns down. So you treat those vectors evaluations of the polynomial of degree n, and then you evaluate at n more points. So that's how you do the expansion of the block, kind of two step process expansion, right? And then expansion down.
01:01:55.334 - 01:03:02.810, Speaker A: And then we realized that that's equivalent to doing bivariate interpolation. So instead of doing these two steps, you can just directly construct a Bivariate from this block. So it's going to be x and y, it's polynomial of two variables and then you evaluate it at more points. So overall this is just evaluations of a large bivariate polynomial. And if you look into right, so if you don't know this is a Bivariate, then the way to do erasure reconstruction is to do raw and column wise reconstruction, because every row and every column is a polynomial of degree. Well, n say if this is square n, then you can find a row that has at least 50% of elements, and then you can through interpolation, you can figure out the missing erased elements in it, just because every row in every column is a polynomial of degree n. And you can substitute for n misses in your two n vector of evaluations.
01:03:02.810 - 01:04:02.510, Speaker A: And in order for that reconstruction, row wise and column wise reconstruction, to succeed, you need 75% of the elements in this matrix. So you can only substitute for 25% of erasures as kind of a pigeonhole argument for why this is true. But if you do bivariate interpolation, turns out you can reconstruct from more erasures. And in fact, in principle we are trying to argue that you can reconstruct from 75% of erasures. So with column wise reconstruction, you can only reconstruct from 25%, but with full Bivariate interpolation you can reconstruct from 75% and that drops. That helps you when you do data valuable sampling, your probability drops quicker. So you have to do fewer samples and making the protocols more efficient.
01:04:02.510 - 01:04:48.554, Speaker A: But the algorithms that do Bivariate interpolation, they are kind of expensive. So we're still in a search of a good and fast algorithm. To do bivariate interpolation, you can always go through solving a linear system because every time you need to do interpolation, you can always invert a gigantic matrix to reconstruct your coefficients of the polynomials. But that's too slow for this parameters. N is on the order of 200 or even thousands. So the matrix is just gigantic. You cannot do this through Gaussian elimination, school book methods, you have something more clever.
01:04:48.554 - 01:05:28.602, Speaker A: And so we are trying to figure out it now becomes a pure mathematical problem, how to do efficient bivariate interpolation. It's very interesting. Somehow it's understudied, it's really puzzling for why mathematicians didn't look into this problem, since kind of obvious we figured out good algorithms for univariate interpolation. There are many of them, but no good algorithms for Bivariate. Probably this problem wasn't motivated very well. Maybe it's just a hard problem, but it has nothing to do with the blockchains or cryptography, it's just pure math. So yeah, that's what we're working on right now.
01:05:28.602 - 01:05:31.500, Speaker A: They're coming up with better algorithms there.
01:05:32.110 - 01:05:34.682, Speaker B: And then you also made commitments to.
01:05:34.736 - 01:05:57.042, Speaker A: Bibaria KZ no, you. Can still keep commitments to the roles? Well, actually, that's an interesting question. Instead of commitments to the row, can you do one commitment to the whole matrix? That's an interesting problem. Maybe you can. We don't know. We don't know how to do it. That's an interesting problem.
01:05:57.042 - 01:05:57.506, Speaker A: Definitely.
01:05:57.608 - 01:06:00.046, Speaker B: So each row is a univariate polynomial.
01:06:00.078 - 01:06:14.454, Speaker A: Each row? Yeah. Each row, bivariate is a univariate polynomial because you fix one of the variables, say this is Y, you fix Y to be one, two, three, or N, and then you kind of reduce it to the univariate, and you commit to that.
01:06:14.572 - 01:06:18.734, Speaker B: But the whole matrix is a huge two variate two varieties.
01:06:18.882 - 01:06:42.346, Speaker A: Yeah. Those commitments not only have to be homomorphic in order to do the expansion kind of down to help you verify these rows, but also be polynomial commitments. Because when you're expanding here, you're computing more evaluations. You want the commitment to this expanded vector to be the same. Yeah.
01:06:42.368 - 01:06:50.586, Speaker D: And so I guess even if you manage to capture all of it in one commitment, you somehow need that one commitment to also be homomorphic.
01:06:50.618 - 01:07:01.040, Speaker A: Yeah, exactly. So it's a little more tricky than yeah, but I think we should talk more about that. Interesting.
01:07:02.710 - 01:07:16.680, Speaker E: Is there any proposal of how you compensate builders for doing this erasure coding or what's incentivizing builders to even do this in the first place? Why wouldn't they just send a normal block that's not including any blobs, and then that's cheaper for them?
01:07:17.530 - 01:07:40.430, Speaker A: Well, same as they're incentivized to include the transaction, they are getting some portion of the gas fees. Right. Okay. So for proposal builder separation, it's kind of a little trickier of how they get paid. It's kind of a relationship that they establish with the validators. Validators are the ones collecting the gas fees and the data fees, and then they're kind of paying back to builders.
01:07:43.010 - 01:07:51.342, Speaker B: Maybe one way of what Pranav's saying is, like, you already mentioned that now we're going to have these two different floating prices for the data and the execution.
01:07:51.406 - 01:07:52.020, Speaker A: Right.
01:07:52.630 - 01:08:20.890, Speaker B: One interpretation of Pranav's point is that one thing that could drive up the sort of data prices would just be like excess demand for the 48 blobs or whatever. But his point is it's also kind of lower bounded by the marginal cost of just communicating that data to other people. So if that's like, expensive, that's going to put a certain price floor on the gas price for the data, I think.
01:08:21.040 - 01:08:21.740, Speaker A: Yeah.
01:08:28.550 - 01:08:44.966, Speaker E: The builder is spending just more computation on extracting mev or something. Makes it so that it's more popular for them to just kind of ignore this, like doing this erasure coding. And I don't know, that might affect things in different ways, but I don't know if anybody's thought about what the sign up would look like.
01:08:45.068 - 01:09:00.700, Speaker B: Yeah, I think if you had these blocks that had no data blobs, then the price would sort of go up. I think they do, like, a 1559 style price adjustment. So eventually it's going to get high enough that builders are going to be like, okay, sure. You're willing to pay me that? Okay. I'll send you data around.
01:09:03.690 - 01:09:24.320, Speaker A: Yeah, but if the builders are living off, only off extracting mev, then indeed it requires some analysis to figure out if those types of transactions give you any mev or not. Yeah, I don't know. It's interesting. Probably research that in the intersection of mev and this stuff.
01:09:26.050 - 01:09:26.970, Speaker B: All right. Thank you, Larry.
