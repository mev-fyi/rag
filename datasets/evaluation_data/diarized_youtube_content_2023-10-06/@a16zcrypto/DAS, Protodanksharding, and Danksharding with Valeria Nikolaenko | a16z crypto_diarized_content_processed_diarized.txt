00:00:00.650 - 00:00:57.760, Speaker A: So in this talk, I'm going to describe and explain data availability sampling, protodink sharding and dink sharding. If you haven't heard these terms before, this is somewhat new developments in the blockchain space. Those are being pioneered by the Ethereum Foundation and those effectively will come to shard our blockchains to make them more scalable, but do so in a much, much simpler way compared to the ideas for Sharding that we had before. Since a number of projects are working on data availability solutions, I hope it's helpful to show how some of the best schemes work so that others could either improve on those and pick up perhaps an idea for research project from this work, or incorporate these ideas into their own projects. So I'll start with explaining what problem Ethereum is trying to solve here. I'll show where it fits onto the roadmap. I'll explain exactly what will be achieved by this upcoming upgrades and I'll briefly explain how this is going to be done.
00:00:57.760 - 00:01:37.894, Speaker A: So, the main goal with this upcoming upgrades is to help Ethereum scale and Ethereum L Two S or Roll ups effectively already help scale the execution of transactions. These L Two S take some subset of transactions and process them off chain. The chain only sees the post effect of these transactions. So Ethereum Validator set effectively doesn't have to execute those transactions. Those executions are happening off chain. Gas fees are typically a good indicator of whether we are making the blockchain more scalable. So if the gas fees go lower, it means it's cheaper for the validators to process those transactions, so we make the blockchain more scalable.
00:01:37.894 - 00:02:35.226, Speaker A: So, if you look at the gas fees on the L Two S, they're typically six to 30 times smaller compared to transacting directly on Ethereum. But if we dive deeper and dissect the gas costs on each of the rollups, for example, if you look into optimism, we will see that most of the transaction fees go to L One data fees. So those fees go to storing transactions. Storing data on an Ethereum L One and only a tiny fraction, only the top kind of gray area shows you how much you pay for the actual execution fees on the L Two. So we can make roll ups even cheaper if we lower the cost of storing data on the Ethereum L One. And that's what Ethereum is trying to do with this upgrades that I'm going to explain in this talk. So there were a number of proposals for how to make Ethereum scale and how to make storing data cheaper.
00:02:35.226 - 00:03:16.010, Speaker A: Some of the initial ideas were around full charting. Full charting effectively splits your large blockchains into smaller blockchains and tries to sync between them. That's a very complicated design. And the nice thing about the current state of Ethereum is that roll ups are effectively already doing part of the split. They take some of those transactions and execute them themselves and then they use the Ethereum l One in order to sync between those roll ups. So L Two S already kind of went into the direction of full sharding. So the only thing that Ethereum blockchain really needs to do for this roll ups to become even cheaper is to allow them to store data with lower fees.
00:03:16.010 - 00:03:56.166, Speaker A: So full sharding was kind of replaced with another two proposals, at least for now. Those have very similar names, EIP 4488 and EIP 4844. AIP starts for Ethereum improvement proposal. So don't mix them up. They're pretty different. The 4488 was proposing to make call data cheaper, but instead Ethereum decided to focus on another proposal, EIP 4844, which suggests to add data which will have an expiry. So the Ethereum will now store some data blobs some data fragments that it will expire and erase after some period of time.
00:03:56.166 - 00:04:21.802, Speaker A: The current thinking is it's going to be one to two months of guaranteed storage. EAP is also called colloquially protodank Sharding. So that's what Ethereum is implementing right now. It's mostly implemented. It should be incorporated into Ethereum pretty soon. So if we look into the roadmap for Ethereum and where those upcoming upgrades fit in there, the merge is already done. So the next is helping rollups scale.
00:04:21.802 - 00:05:20.290, Speaker A: So the protodank sharding will come first and the protodank sharding will follow. That still requires a bit of a research, and there are a number of open problems still to be solved. So why roll ups need the blockchain to store some data without executing them? That's kind of a one slide explanation of what a rollup is and why it needs to store some data onto Ethereum L One. So roll ups are processing transactions from the users off chain. They're kind of taking all of these transactions, executing them, and instead of relaying them to the L One, they're only posting a single transaction that reflects the effect of all of those green transactions on the right. So this transaction TX is going to update the state transition. It's going to update the Merkler root hash of account balances effectively or account states from S to S prime, and will only be allowed to do so if it can present a commitment to the transaction list on the left.
00:05:20.290 - 00:05:56.350, Speaker A: So the commitment to the user transactions and a valid proof that the state transition conforms to this transactions and then use the commitment that those transactions are valid. So the transaction is really succinct. Because the root state is small, the commitment is typically also very small. This proof can be there or may not be there, depending on the type of the roll up we're talking about. If it's zero knowledge roll up, this will be a zero knowledge proof. Then the size of the proof will depend, of course, on the type of the proof system that the roll up is using. If it's an optimistic roll up, then the proof will not be there and will be fraud proof based.
00:05:56.350 - 00:06:34.118, Speaker A: So the clients will have an opportunity to raise a dispute if they think that the state transition was done incorrectly. Those transaction lists needs to be made available in order for the roll up to be fully trustless. What it means is that the roll up should be replaceable. Anybody, if the roll up goes down, becomes malicious disappears. Anybody can replace the state of the roll up by restoring its state. In order to restore the state and get all of the account states of the users underneath the roll up, this new entity new roll up needs to replay all of the transactions. And in order to replay transactions, it needs to download them from somewhere.
00:06:34.118 - 00:07:03.182, Speaker A: So that's why rollaps are storing this transaction list on some data availability solution. And typically that's an Ethereum blockchain itself. So that's where most of the gas costs for roll ups go. And if Ethereum can make it cheaper for roll ups to store those transaction lists, it will make the roll ups fees cheaper. So if we look into Ethereum block size today, it's pretty small, only about 100 KB. So 0.1 megabyte per single block.
00:07:03.182 - 00:07:22.410, Speaker A: One block appears every 12 seconds. Now, with Protodank Sharding, the suggestion is to expand the block size with additional data. So about 0.5 megabytes of data. The main purpose of this data is availability. Validators will not have to do any processing on this data. They will need to make it available only.
00:07:22.410 - 00:07:57.326, Speaker A: So we will maintain this 100 KB data for execution, but we'll add an additional 500 KB for storage. The storage data for Protodank Sharding is going to be fully replicated between the Validators, and it will have an expiry of one to two months. This data will not be accessible to execution. Only a digest of the data will some hash of the data and the main users for this data are going to be roll ups. And then what will come next is Dank sharding. It will expand the data even further. And the hope is that it will grow as large as 30 megabytes, which is mind blowing.
00:07:57.326 - 00:08:29.402, Speaker A: So this data will have the same property as for Protodon Sharding. It will have an expiry. It will not be accessible to execution, but the Validators will be storing this data and will be serving it to the clients. The reason why we can make this second jump from zero five megabytes to 30 megabytes will be the main focus of my talk. I will explain how this expansion is possible without making the Validators store more data. So storage of this data is going to be distributed among the Validators instead of replicated. Validators will not be storing 30 megabytes for each block.
00:08:29.402 - 00:09:23.662, Speaker A: They will cleverly split this data and only store small fragments of each block. So if you look into Ethereum state size, in order to be processing transactions, each Validator needs to store about 100GB of data in order to verify and execute transactions. So Prototync Sharding will grow the storage requirement a little bit. Since we add this additional data that the validators will now need to store for a month or two months, the state is going to grow to about 200 to 400GB. And then with Link Sharding, despite the fact that we're going to increase the block size, the storage on the validator side is going to stay roughly the same because the storage data will now be split between the validators instead of fully replicated. So with Dank Sharding, validators will still keep about 200, up to 400GB of data in order to execute transactions. So let's see what Protodank writing is.
00:09:23.662 - 00:10:10.350, Speaker A: Protodank Riding introduces a new transaction type called Blob carrying transaction. Each transaction of this type will submit a long data vector and a short commitment to this vector, which is going to be signed by the user. The thinking right now is that the number of these vectors will be initially pretty small, two, three, or four. And the size of each of the vectors is about 130 KB. So that's rough sizes, I guess. Ethereum plans to see how the validators react to that and then maybe increase or decrease this number. So this data is going to expire in one to two months so that validators will drop the data Blobs, but they're going to keep the commitments, so the commitments persist, and Blob data will have separate fee market.
00:10:10.350 - 00:10:50.838, Speaker A: So cold data is the cheapest way to store data in Ethereum right now costs about 16 gas per one byte. This Blob data is going to cost one gas per one byte, but there's going to be a separate fee market for this Blob data. So we don't really know right now until it's been implemented and running. How this gas two compares to gas one, that remains to be seen. So let me explain you the commitment scheme that Protodyncarney is going to use. I was saying that we will use a commitment scheme here to commit to the data vector. Now, Hash is a good commitment scheme, so you can just hash a data vector and it's going to be a binding commitment.
00:10:50.838 - 00:11:28.290, Speaker A: But we will do something more clever called polynomial commitments, and this commitment will give us some additional properties. The commitment that Protodynx Sharning is using is KCG polynomial commitment. I'll explain it in one slide. If you haven't seen this before, this will give you a rough idea, but I highly encourage you to look into the paper. It's a very well written paper and it's easy to follow and see how everything works, and even the proofs pretty straightforward in this paper. It's a very elegant scheme. So we assume that we have three elliptic curve groups g one, G two, and GT, with a bilinear pairing that will operate on the groups g one and G two will move effectively.
00:11:28.290 - 00:12:09.130, Speaker A: An element in G one and an element in G two to the group GT. This polynomial commitment scheme requires a trusted setup that will generate a structured reference string of this form. So G one is a generator of the group G one. So the SRS is going to be of the form G one to the power tau, G one to the power tau squared, G one to the power tau cubed, et cetera. So they're going to be n powers of tau in this SRS. And the way I'm going to describe it, you only need one power tau in G two, but for efficiency reasons you may want to generate several of those to do batch proving, which I'm not going to explain now. So the SRS should be generated in such a way that nobody knows tau.
00:12:09.130 - 00:12:49.490, Speaker A: And when SRS is published, of course, in these groups the discrete logarithm problem is hard. So given G one to the tau, there is no way you can figure out tau unless you can break discrete logarithm. So KCG polynomial commitment allows you to commit to a polynomial of degree n, and this n is the same as the size of the SRS. So the polynomial is described as usual school book polynomial representation. You have a coefficient, say zero a one, a two an, and the polynomial is the sum of a zero a one times x, a two times x squared, et cetera. So all the usual nice stuff. You can commit to this polynomial with a very short element in G one.
00:12:49.490 - 00:13:38.770, Speaker A: So I put it in gray just because I don't want to go into too much detail, but this commitment is effectively G one to the power f of tau. And it's pretty remarkable because no matter the degree of the polynomial, it can be as large as you want, as long as it's smaller than the SRS, you can still commit to a large polynomial with a constant size element. So CF binds you to the polynomial. There is no way without knowing tau to come up with another polynomial that will commit to the same c and it's constant size, it's just a single element in G one. Now, to open a commitment, I can basically prove to you that the polynomial underneath the commitment evaluates to a certain value at a given point. And the proof is also very short. It's a single element in G one.
00:13:38.770 - 00:14:25.470, Speaker A: Now the committer and the opener needs to know the polynomial. Now, the verifier, in order to verify that the committed polynomial evaluates point alpha to some value beta, it only needs to know the short proof pi. So it's a very efficient way to convince somebody that the polynomial evaluates to a certain value with a very short proof. So the polynomial commitment scheme is basically these three functions, the commit open and verify. Now, this polynomial commitment is binding, so the main property is it's polynomial binding and evaluation binding. It's not possible to come up with another polynomial that will commit to the same value. And it's also not possible to open this polynomial at the same point to two distinct values.
00:14:25.470 - 00:15:29.474, Speaker A: So once you commit to the polynomial, you can only faithfully approve things about the polynomial being committed there. All right? So that's the polynomial commitment scheme, and it can be viewed as a vector commitment because effectively you can commit this way to the vector of evaluations of this polynomial f, and then the prover can open selectively any elements of this vector commitment to convince the verifier of the selective opening of this commitment. So, as I mentioned, this scheme requires a trusted setup, and Ethereum is running a trusted setup right now. It's generating four powers of tau strings. The first string has 4095 powers in g one and 64 powers in g two. The size of this whole setup is about 200. There is a larger setup, twice as much powers in the g one, so up to tau to the 8000, that's about 400 have larger and larger setups, those that are effectively two times the lengths of the previous strings.
00:15:29.474 - 00:16:36.246, Speaker A: So 400 can participate in the setups. Right now, it's basically every participant that shows up can re randomize this string and add randomness to this tau. And the whole process is engineered in such a way that as long as there is one honest participant who correctly contributes to the setup and completely erases and forgets the randomness used to participate, the whole setup is secure. So you can show that as long as one participant forgets the secrets in its contribution, nobody would know the tau. So these types of setups really benefit from a large number of participants, because in a large number of participants, more likely somebody will be honest and will correctly erase all of the secrets and not keep them around. There is a link to participate. So the way this committee scheme is used in protodync Sharding, the data is treated as a vector of 4096 elements in ZP, where P is a prime, it's an order of g one group of the BLS twelve 381 curve.
00:16:36.246 - 00:17:20.434, Speaker A: So P is about 256 bits. So the data Blob is just a vector of this 256 bit elements that's about 130 KB. We interpolate the polynomial through those values, so we create a polynomial of degree 4095 such that f of I equals Yi. And we know that such polynomial is going to be uniquely determined just because the number of points is 4096 and the degree for the polynomial is 4095. So such polynomials are always uniquely determined. And then the commitment to this data Blob is the commitment to the polynomials, the KZG commitment. And the nice thing about KZG is I can show you selective disclosure of this data Blob.
00:17:20.434 - 00:18:12.294, Speaker A: I can prove to you that certain points indeed correct values of the data Blob and that they conform to the commitment that they generated in the first place by doing KCG openings on this polynomial f. So at the high level, we have our old transaction lists that ethereum blocks contain. And then with protodank Sharding, additionally, the block is going to contain the commitment to the data blobs and the data blobs themselves. Okay? So new ethereum block is now going to consist of two parts, the old ethereum block commitments and then data blobs. So the green is the block for execution, and it's going to be fully replicated to the validator set. So each validator is going to store the green block, but the yellow data blobs are blocks for storage. Those are for protozank Sharding, also going to be fully replicated.
00:18:12.294 - 00:18:36.434, Speaker A: But those are going to have an expiry of one to two months. So the ethereum block size now comprises of two parts 0.1 megabytes of data for execution and 0.5 megabytes of data for storage. So that's proto Dank Sharding. Now let me explain you the Dank Sharding. Dank Sharding is an invention of Dankrad Feist from the Ethereum Foundation, and it's even named after him.
00:18:36.434 - 00:19:00.834, Speaker A: So Dank in dank. Sharding stands for Dankrad. So the idea is to apply erasure coding. So don't be scared. This erasure coding we're going to use is super simple. So our data blob, as I was explaining before, is evaluation of a polynomial at points one two, up to 4096. And we're going to continue evaluating this polynomial, evaluate it at zero, 4097, 98, et cetera.
00:19:00.834 - 00:19:55.446, Speaker A: So we're going to evaluate at 4096 more points, and this is going to be an extension of our data blob. Okay? So that's called read Sullivan encoding or low degree polynomial extension. So you interpolate the polynomial through the first half, through the data blob, and then you evaluate this polynomial at more point to extend the data blob to create this extension. Now, since the polynomials of degree 4095, it means that any 4096 elements are enough to reconstruct f. So even if some of the elements of the extended data blob get corrupted, you can still reconstruct the original polynomial and reconstruct your data blob. So that allows us to do the dispersion of data. What we can do is we're going to take this extended vector where there are 8192 elements, and we're going to give each validator one element of this vector.
00:19:55.446 - 00:21:05.970, Speaker A: Now, if some of the validators go Byzantine go missing, they drop their fragments. As long as 50% of validators are honest and correctly keep their fragments, we can always reconstruct all of the data from just 50% of it just because of this property, as on the polynomial, that any half of the elements are enough to reconstruct f. All right, so this is an initial idea. We're going to make it more efficient. But for now, the commitment to the polynomial f is going to be fully replicated to the validator set, but the actual vector is going to be split into fragments, and each validator is going to get one evaluation of this polynomial. The question is how the validators know that they got correct fragments? Well, they get the commitment opening proofs for their fragments from the block builder. So the block builder is going to do some heavy computation to not only extend the data block by evaluating the polynomial at more points, dispersing the elements to the validators, it will also produce evaluation opening proofs to convince the validators that the fragments that they got correspond to the actual commitments.
00:21:05.970 - 00:21:54.466, Speaker A: So how do we apply these ideas to multiple data blobs or A block? Because it's not very useful for a single vector, it's pretty straightforward. So in Dank Sharding, the number of data blobs is up to 256. So that's about 32 x increase from draw to Dank Sharding. So we're going to extend each data blob individually, the same way I explained before, and then we're going to split the extended block into columns, and each validator will receive a column of this extended block. So as long as 50% of validators are honest, they can always piece the block back together because every row can be interpolated and can be corrected for erasures if you only have 50% of elements in it. So Dank charting goes a step further. This scheme already works.
00:21:54.466 - 00:22:38.538, Speaker A: There is one annoying part where you might want to use some more complicated tools, and that's the locality of the reconstruction. So right now, if the validator is missing its column, the way to get it back is to try to reconstruct the whole block effectively. So this poor validator will have to talk to the whole validator set to try to get 50% of columns, then do the interpolation to reconstruct his missing fragment. So that's not great because the blocks in Denk Sharding are really large. Those are 30 megabyte blocks. So that's going to take a lot of effort on the validator side. And the hope is that validators can be really light, that this will not add a lot of burden on the validators.
00:22:38.538 - 00:23:17.658, Speaker A: So for that, Denk Sharding does another step. So let me explain how Denk Sharding achieves local reconstructibility. How do we make sure that the validators can reconstruct its missing fragment with few other fragments without going to the full validator set, without doing full reconstruction? Right. So right now, if the validator is missing, its fragment needs to reconstruct the whole block. But to get local reconstruction, dank Sharding needs two additional properties of the commitment scheme. It needs homomorphism of the commitments. So the commitment to polynomial f plus the commitment to polynomial G gives you the commitment to polynomial f plus G.
00:23:17.658 - 00:23:40.386, Speaker A: So you can add up the commitment, and this will cause the polynomials to add up. KCG commitments are homomorphic. Bulletproof commitments are homomorphic, fry commitments are not, for example. So we have plenty of examples of homomorphic commitments. And that's what we're going to need for this step for Denkrating. And the other property we will need is homomorphism on the proofs. And that's a tricky property.
00:23:40.386 - 00:24:21.790, Speaker A: And we only know of KZG commitments that achieve both, that achieve homomorphism on the commitments and homophobiaism of the proofs. Unfortunately, KZG commitments require a trusted setup. So that's an open problem of how to construct the commitment scheme with both properties without a trusted setup. There's some work being done in the group sofano and order, that's pretty recent. But the homomorphism of the proofs property says that you can add the proofs. So you can now take the opening proof for the polynomial f and point alpha and you can add it to the proof for the polynomial g at point alpha. And this will effectively be the same proof as for the opening of the sum of the polynomials.
00:24:21.790 - 00:25:20.162, Speaker A: Okay, so those two homomorphic properties are required to make Ding Sharding achieve local reconstructibility. So here is how this property is used. So the way I explained it before, we were using univariate polynomials and we were evaluating those univariate polynomials at more points to create this extension on the right. Now, you can also treat the whole block as a bivariate polynomial, so you can feed a bivariate polynomial d through this left part and evaluate it at more points on the right, and you can also evaluate it at more points extending down. Now, extension to the right is equivalent to the univariate case, but extension down can also be done in a univariate fashion if you feed a univariate polynomial through each column and evaluate more points. But it's much nicer to view it as a bivariate polynomial evaluation. So that's how it's done.
00:25:20.162 - 00:25:51.366, Speaker A: You basically do two x expansion to the right and two x expansion down. And now with this expanded block, each validator is going to get two rows and two columns of data. So let me just explain a little more detail. Each validator is getting from the builder all of the commitments. So c one through C 256, those are commitments to the vectors of the original block. Now, it also gets from the builder two rows of extended block and two columns. Columns will be slightly larger.
00:25:51.366 - 00:26:38.678, Speaker A: It's going to be columns of width 16. This is done in order to make this game a little bit more efficient because you can do evaluation opening proof on 16 elements and they're also going to be short. So the proof size for opening 16 elements is the same as a proof size opening a single element. It also is more friendly to the network because you're sending a little bit more data so your packets are more full. That's just a technicality for why it's 16 widths column, but you can think of it as the validator getting two rows and two columns. So that's about two megabytes per validator per block. Now, to verify the elements in the bottom half, we need the commitment scheme to be homomorphic because those extension is a linear transformation on the block.
00:26:38.678 - 00:27:13.266, Speaker A: Wherever you're interpolating the polynomial and evaluating at more points, that's actually a linear transformation on the vectors that you are doing. So you can translate this linear transformation and extend the vector of polynomials to also cover the bottom. Half rows of this matrix. And that's how you can verify that you actually got the correct row, that your row corresponds to the commitments. You can use the homorphism to extend these commitments and then check that your row. Commits to the extended commitment. But for the columns, each validator is going to get the proofs from the builder.
00:27:13.266 - 00:28:16.490, Speaker A: And again, for those at the bottom part, it's going to just check against the commitments. These green commitments for the top part and for the bottom part, the commitments are going to be extended, and you're going to be checking again against those bottom commitments. Each validator can now locally reconstruct its missing pieces. So if the validator say, is missing a column, it's going to ask other validators who are storing the rows and who are intersecting, obviously this column of the validator to supply this validator with fragments of his column. And the moment it gets 50% of elements in the column it will be able to reconstruct and fulfill the whole column and that's where we need homophism on the proofs as well. Because when you are fulfilling elements of the column, you also want to fulfill proofs for those elements and fulfilling the proofs will require homomorphism for the proofs so that's how local reconstructability works so again, the properties that we need from the commitments came from Dengueting is homomorphism on the commitments homorphism? On the proofs. And only KZG has both properties.
00:28:16.490 - 00:29:08.822, Speaker A: So the reason Denshrading extends the block both ways extends right to get resilience against malicious 50% of Validators. Because Validators are getting columns and half of the Validators can forget. Their columns you will still be able to reconstruct the original block and Dank Sharding extends down in order to get local reconstruction, in order to allow the validators to reconstruct their missing columns or rows without doing full block. Reconstruction. So let's talk about reconstruction briefly. The way it's proposed to be done in Dink Sharding is if 75% of elements in the block are there, then at least one incomplete row or column will be reconstructible, will have at least 50% of elements in it. So you'll find such row or column and you will reconstruct it, basically patching your gap.
00:29:08.822 - 00:29:42.066, Speaker A: Then you will find maybe column with 50% of elements and you'll patch that and you will repeat the iterative process until you reconstruct all the missing pieces. Now, we know that we require 75% of the block in the worst case. And the worst case is here if you're having 75% of the block. And this is the yellow part, minus some tiny strip minus Epsilon. So you're missing a little bit of this corner here. So this yellow area, including the striped one, is 75% minus. Epsilon.
00:29:42.066 - 00:30:34.130, Speaker A: So given only this area, you will not be able to patch and reconstruct this missing white quarter, because none of the rows or columns will have 50% of elements in it. And this worst case is very real because validators are getting rows and columns. So it might happen that the data goes missing in exactly this pattern and you get this worst case. So while exploring this with Denbane, we wrote a blog post, by the way, that you can check out on the A 16 z website. But we also had a proposal there of improving the reconstruction by having the validators, instead of storing rows and columns, store some randomly dispersed data from the block. So let me explain in a few words how it works. So instead of going through row and column wise reconstruction, we can go through Bivariate interpolation.
00:30:34.130 - 00:31:25.362, Speaker A: So instead of doing this iterative process where you're finding incomplete rows and columns, you try to reconstruct your Bivariate polynomial as a whole. So this new method allows to reconstruct from 25% with high probability when these 25% are randomly distributed across a block. So instead of rows and columns, the validators, we propose to give the validators random elements from the block. It remains an open question, however, how to do Bivariate interpolation efficiently. So let me show you a polynomial algorithm for Bivariate interpolation, but it's not good enough for us. So if each validator gets random elements of the block, the elements from honest validators are going to be randomly distributed across the block. So your task is now to patch for the missing white elements.
00:31:25.362 - 00:32:00.158, Speaker A: So the yellow are the elements that you have, and the white elements are the ones that are missing. So the polynomial that we are seeking for is a polynomial of two variables. It has degree n minus one in each variable, so it has n squared coefficients. And given n squared random evaluations, we need to obtain the coefficients of this polynomial. So each evaluation that we have here, those elements that are present, gives us a single linear equation on the coefficients of this polynomial. So here the coordinates XY. You can write equation on the coefficients of this polynomial.
00:32:00.158 - 00:32:36.206, Speaker A: So each element gives one equation. So now we can write it as a system of equations. So those are our unknowns, the coefficients of the polynomial. The matrix m is constructed this way from the coordinates of the evolution points I one, j one, I two, j two, et cetera. So they're going to be n squared points that are present in the block n squared unknowns. So you should theoretically be able to find those unknowns given those n squared evaluations on the left. And indeed, solving linear system, we know how to do it in polynomial time.
00:32:36.206 - 00:33:07.810, Speaker A: For general matrices, it takes time n to the 2.37 if you use the best algorithms. In our case, however, n is really large. It's two to the 32. So the size of this matrix is 128GB. So that's not a practical approach, although it's kind of an efficient algorithm just for our sizes, it's not suitable, but we know that the matrix is very structured, so the better algorithms should exist. So it would be nice to come up with a better bivariate interpolation algorithms and this will improve the reconstruction.
00:33:07.810 - 00:34:02.890, Speaker A: Now let me talk about data availability sampling and how it fits into the picture for Deng sharding. So in Denk Sharding, the blocks are reconstructible from 75% or possibly lower if we can make bivariate interpolation faster. So to understand whether the block is available, the sampling client is kind of going to throw random darts into this block matrix, hoping to find out any missing pieces. So the sampling client will request random element of the block. It will send the coordinate IJ to the validator set, and the validator set is going to reply with the value of the polynomial dij. And a proof we assume here that the client have the commitment to the block, so it will be able to verify that it got back the correct value. So the probability that the value comes back and it is correct but the block is unavailable is zero point 75 and that matches the 75%.
00:34:02.890 - 00:35:05.520, Speaker A: So imagine that your block is unavailable, so more than 25% of it is missing. Only 75% minus epsilon is present. So the probability that you hit the present element, yet too much of the block is missing for reconstruction is zero point 75. The probability that you do it again, so you do it two times in a row is zero point 75 squared. And by repeating the sampling, you can amplify this probability and goes down exponentially quickly, so you can make it negligible with small number of queries. And if with bivariate interpolation we can improve on this 75%, make it 25%, then the probability will go down even faster. So data availability sampling allows our client to verify whether the data is available without basically knowing any information about the validator set, without knowing the public keys of the validators, without verifying any signatures, by just simply throwing this darts into the block and verifying that it got back correct values from the set.
00:35:05.520 - 00:35:53.766, Speaker A: And this is also going to underline the consensus design in Ethereum because data availability or dank sharding is going to be blended into the consensus protocol. So each validator during the attestation phase in the consensus will attest to the block BN only if the two conditions are met, if this validator has sampled all the previous blocks and the sampling was successful. So with good probability, the validator is convinced that all the previous blocks are available. So it determines availability of the previous blocks. And it also attests if it received correct fragments of all the previous blocks. So there's green lines, two rows and two columns each validator gets for each block. And it needs to verify that those conform to the commitments.
00:35:53.766 - 00:36:25.010, Speaker A: If these two conditions are met, then the validator attests to the block BN. And that's happening in addition to what Ethereum Consensus is doing right now, which is verifying and rerunning all the transactions within the block. It will also verify that this additional data that we're adding with Dink Sharding is available. All right, so that's all I have here at Important Disclosures to check out. Hope you enjoyed this presentation, and please check out the blog post that we wrote with Danbane. And yeah, let us know your thoughts.
