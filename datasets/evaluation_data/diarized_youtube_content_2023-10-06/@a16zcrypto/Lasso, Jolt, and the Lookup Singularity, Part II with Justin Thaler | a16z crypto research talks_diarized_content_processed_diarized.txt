00:00:07.320 - 00:00:07.870, Speaker A: You.
00:00:09.920 - 00:00:21.996, Speaker B: Welcome to today's a 16 z crypto research seminar. Today is going to be part two again, Justin Thaler, research partner here at a 16 z crypto and also a professor at Georgetown. So yesterday we kind of heard at.
00:00:22.018 - 00:00:23.436, Speaker A: A high level how things work and.
00:00:23.458 - 00:00:32.060, Speaker B: Sort of a lot of the importance of this work. And today we're going to find out how it actually works under the hood. So Lasso jolt, no look up singularity. Justin, floor is yours.
00:00:33.480 - 00:01:15.650, Speaker A: Thanks Tim. All right, so just like a one slide recap of yesterday so a lookup argument lets approver, commit to a vector of value and then prove that all entries of that vector reside in some Predetermined Table. So Lasso is a new family of lookup arguments, high level. I think the prover is about an order magnitude faster than prior works. And this is sort of coming from two different factors. One is the proverbs committing to fewer field elements in Lasso than in prior lookup arguments. And the other factor, which is actually more significant is that all of the field elements being committed are small.
00:01:15.650 - 00:01:56.680, Speaker A: And exactly how important that is depends heavily on what commitment scheme you use. But in general, in snarks and lookup arguments the bottleneck for the prover is cryptographically committing to values. And for some really important commitment schemes, if you're committing to small values, even if those small values reside in a very big field, the commitment is really, really fast to compute. Okay? Also in Lasso sort of inherited from the fact that under the hood it uses multivariate polynomials and not univariate ones. And we'll see details about that today. For many tables there's no need for the prover to commit to the table. So in all prior lookup arguments the prover always had to not the prover.
00:01:56.680 - 00:02:51.968, Speaker A: Some honest party in preprocessing always had to commit to the table and the prover would have to kind of process that commitment on the fly as well. And finally, kind of a distinguishing feature of the Lasso family of lookups is that they can support gigantic tables. So think of size maybe two to the 128 as long as the tables have some structure to them. And we build jolt, which is kind of a new ZK VM technique out of Lasso which uses a property called decomposability of the table. And actually prior lookup arguments also could handle decomposable tables, but probably with more overheads. And then something really unique to the Lasso family is that there's a varying called generalized Lasso which exploits a weaker structural property in the tables. And I don't think you can achieve something like this.
00:02:51.968 - 00:03:49.680, Speaker A: Certainly it's not known using univary polynomials rather than multivariate ones. Okay? And then jolt is a new zkvm technique which has lower commitment costs for the prover than prior works. And the one sentence version of the main idea is every time the virtual machine goes to execute a primitive instruction, the jolt prover will sort of prove that that instruction was executed correctly just by doing one lookup into a giant table. That table contains actually the entire evaluation table of the instruction. So if the instruction takes 264 bit inputs, the table actually has size two to the 128 because there are two to the 128 possible inputs to the instruction. Okay, so I wanted to begin today before I dive into the technical details by giving a little more context for Lasso and Jolt. So I want to explain what this term lookup singularity means from the title.
00:03:49.680 - 00:05:21.256, Speaker A: So this was a term and a vision that was sort of put forth by Barry WhiteHat in a ZK research forum. Post basically means producing circuits that only do lookups. And Barry's motivations described in the post were mostly about auditability and formally verifying correctness of the snark. Generally, the idea is that lookups are fairly simple and it should be easier to audit the correctness of an implementation of a lookup argument and the correctness of specific tables the lookup argument is getting applied to versus what? A lot of projects are doing today, which is having very skilled developers, kind of hand code gadgets tailored to each specific functionality that's important in their application. So if you look at especially like Zkevm projects, you'll see like tens of thousands of lines of code basically devoted low level DSLs like Bellman or Circom or something coding up very optimized gadgets. And it's just sort of impossible to think that in 50,000 lines of Circom or something, there's like zero issues anywhere in there. So hopefully if you're just doing lookups with one lookup argument, hopefully it's much simpler to kind of make sure the lookup argument is implemented correctly.
00:05:21.256 - 00:06:30.768, Speaker A: And if you change the lookup table to make sure that the new table is implemented correctly, instead of this sort of endless stream of verifying that this particular hand coded circuit is equivalent to the functionality of the day. Okay, so Barry's post couple posts definitely acknowledges that performance considerations as well. So it acknowledges that current lookup arguments are slower than we would like for this. And also one of the comments I think highlights that for the lookup singularity to be realized, you probably do need to support lookups into giant tables and mentions things like structured tables possibly allowing for that. But it's hard to say in those posts like what else maybe Barry had in know. Well, I hope that Lasso and Jolt are coming close to realizing this vision. Okay, so I think the way that Jolt works actually counters a number of narratives or just general beliefs in sort of the community that's building and deploying these snarks.
00:06:30.768 - 00:07:10.544, Speaker A: So I wanted to describe some of them. So one of them, and you see this viewpoint expressed prominently, like on Twitter all the time, is that simpler instruction sets should lead to faster Zkvms. And this is a pretty intuitive statement. It's like if each step of the VM is simpler, it should be faster to prove one step of the VM. Right. And this sort of viewpoint is sort of taken to its most extreme in, I think, the Cairo virtual machine, which was specifically designed to have kind of very small set of very snark friendly instructions. Okay, but we saw yesterday that two things.
00:07:10.544 - 00:07:48.716, Speaker A: So one is I guess, the commitment costs for the prover and jolt are less than existing zkvm projects. And two, the scalability of the jolt prover actually, oddly, it doesn't really depend on the complexity of the instructions. So it's something a little weird. There's some kind of threshold behavior where as long as the instruction leads to a structured lookup table, whatever structure Lasso or generalized Lasso needs, it doesn't matter how complicated instruction is. The proverbs costs are actually determined by the size of the lookup table, which only depends on how big are the inputs to the instruction is one way.
00:07:48.738 - 00:08:06.580, Speaker B: To think about it. That if you have like two to the 128 possible inputs, whatever the complexity of the instruction, you were thinking of putting your instruction set information theoretically. It's just like much smaller than that input space is. It kind of like it's just dwarfed by the number of possible inputs.
00:08:07.880 - 00:08:58.070, Speaker A: I think that is a reasonable way to look at it. I'm going to give a perspective after I get through the narratives that I think might clarify the issue. So let me just defer your question a few more slides. It kind of depends on what they mean by simpler, right? If I took 2128 bit numbers, I just took the last bit and added them, then that would be a very simple instruction, right? That would be actually not necessarily because for a couple of reasons. So one is all of these zkvm projects are trying to represent each sort of primitive data type for the virtual machine as like one field element or something and actually pulling out like one bit of a field element. It's not necessarily nice. But you are right.
00:08:58.070 - 00:09:23.550, Speaker A: If a primitive operation for the virtual machine is literally like addition over the finite field or multiplication over the finite field, you don't need a lookup and that is still cheaper. But the primitive operations, even in the Cairo VM include things that are not just addition or multiplication over finite field. And the drill prover commits to fewer field elements than the Cairo prover, even though it's a much more complicated instruction set. Other questions.
00:09:26.480 - 00:09:33.244, Speaker B: All of this work focuses mostly on the prover complexity. How much do you sacrifice on the verifier side or does it stay comparable.
00:09:33.292 - 00:10:32.272, Speaker A: To yeah, so the verifier complexity in all of these, how do I want to say this? In jolt, the verifier complexity involves two things. So for each instruction the verifier has to evaluate a certain polynomial, capturing that instructions lookup table for all the risk five instructions in jolt, that's like logarithmically many field operations. So think like 100 field operations or something. And field operations are generally cheap. Like one hash evaluation is many, many field operations. Right? And then there's one evaluation proof of a polynomial commitment scheme. And Lasso's implementation right now uses a polynomial commitment scheme where the proofs are a little bit big, though actually not really much bigger concretely than some other polynomial commitment schemes popular today.
00:10:32.272 - 00:11:27.600, Speaker A: Fry in particular has pretty big proofs. Yeah, so with all of these approaches, all of the KVMs, the verifier cost does go linearly with the number of instructions, and the verifier also has to evaluate some polynomials, kind of capturing the instructions. And jolt is no different than that. Yeah, there are some more intricacies, but I think that's capturing most of them so roughly still comparable to, yeah, once at least once know, switch up the polynomial commitment schemes. And again, I actually think even though so HIRAX is what Lasso today uses because it's built in the Sparring Library, it has square root verifier costs. Actually, square root looks like polylog for small values of n, so I don't actually think it's that bad. And then the one other intricacy is Lasso.
00:11:27.600 - 00:12:35.060, Speaker A: Jolt really use multivariate polynomials, like at their core. And so you can't use KZG commitments. All known multilinear polynomial commitments have at least sort of logarithmic size evaluation proofs KZG for univary polynomials is constant size. But again, there are projects that aren't using KZG commitments, and we should be able to match their proof size, for example. Okay, so another narrative out there is there's a heavy focus on supporting what are called high degree constraints. So this is just like, you can think of this as a generalization of multiplication gates, where you might be sort of multiplying more than two numbers or doing other operations that don't correspond to degree two polynomials. And for example, there's a recent work called Hypernova and Protostar, which looked to take sort of a very different approach, very exciting approach to Snarks, distinct from anything I'm telling you about today, which previously were only known to support degree two constraints and allow them to support higher degree constraints.
00:12:35.060 - 00:13:18.040, Speaker A: And people were very excited about that. Now, jolt kind of pushes, like all the work the prover does, almost all of it, into the lookup argument. And what's left is very simple and easily captured not just by degree two constraints, but actually a very special kind of degree two constraints called R one CS. Okay, I still think high degree constraints are important, but if your goal is a zkvm anyway, it's not actually clear at all that it's important. So kind of what's more important than high degree constraints is being able to use a good lookup argument, basically. Okay, there's also a view that Snarks over large fields are wasteful. This is actually one of the main reasons I think people like to use Fry.
00:13:18.040 - 00:14:10.484, Speaker A: I mean, it's also avoids lift occurs. So it's plausibly post quantum secure. So that's nice. But in certain settings, one, like, field operation over small field is much faster than a field operation over a big field, right? And I actually have a blog post coming out soon where I'll discuss why I think even if you can work over small fields in some applications, it doesn't actually make sense. Anyway, I don't have to get into that now. I want to discuss the issue about this view that we should work over small fields because working over large fields is like wasteful, right? The rough intuition here, I think, is if you want the prover to commit to a value that's pretty small, but you have to represent the value as like a 256 bit field element. Well, there's just like a lot of extra bits in there that aren't doing anything, and that's just a big waste.
00:14:10.484 - 00:15:36.024, Speaker A: And you're sort of paying a price for the prover. And I don't necessarily think that's the case because of this issue, I highlighted that committing with elliptic curve based commitment schemes to small field elements, even if the element lives in a 256 bit field, is really fast. And in fact, you can actually do it with essentially one group operation, not one exponentiation, one operation per committed field element if the field elements are small enough. And then finally, there's this very popular approach now to Snark design, which heavily uses recursion where people will take a big circuit, break it up into small pieces, sort of prove each piece separately, and then aggregate the resulting proofs to get kind of a proof for the big circuit. Okay? And with current popular techniques, there's really kind of no major loss in performance by doing this. In fact, to get good, performance is, like, the only way you can do it because a lot of the Snarks people are using today have sort of super linear approver time. So it's sort of a dis economy of scale where you deal with a bigger circuit and the cost kind of per day in the circuit goes up because of FFTs and things like that, okay? And one of the reasons people are doing the breaking up, by the way, is because something like Fry has enormous space costs for the prover, like hundreds of gigabytes, even for small circuits.
00:15:36.024 - 00:16:17.308, Speaker A: And so even ignoring prover time, the proverb space just you can't tolerate it at big circuits. And there's not much overhead in breaking it up into small pieces today. All you really have to do is make sure that kind of the Snark Verifier, when represented as a circuit, is significantly smaller than the original circuit that was proved. So that way, when you go to aggregate the proofs by kind of proving, you know, something the Snark verifier would have accepted sort of that's a statement that was smaller than the original one. And so the overhead isn't a big deal. Lasso and jolt you actually see economies of scale instead of dis economies of scale. So you actually would lose something by breaking things into smaller pieces.
00:16:17.308 - 00:17:38.504, Speaker A: And the way to think about these lookup arguments, and we discussed this last time in response to one of the questions was you think of them as the prover kind of pays a fixed cost like table size to the one over some constant, no matter how many lookups are done. And then that cost gets amortized over all the lookups. So the more lookups there are, the better that cost gets amortized. Also, if you're using Pippinger's algorithm to compute MSMs, the bigger the Pippinger speed up like Pippinger saves like a log factor over naive multi expensentiation and log of something bigger is a little bit bigger. And intuitively, I think we shouldn't expect to see dis economies of scale, right? I mean, in the real world we see economies of scale all the time. So I think if you kind of use the right techniques in snark design one proof for something really big until you exhaust the space your approver needs or something should actually be better than breaking things into tiny pieces. Questions? So would you say it's better to have smaller instruction sets or smaller instruction sizes, but you have a large number of lookups than have more complicated instructions in your CPU? Because if you have simpler instructions then you'll have a large number of lookups and then you'll get economies of scale versus complicated.
00:17:38.504 - 00:18:23.240, Speaker A: Actually, maybe I didn't quite get that. So you're saying have your CPU have fewer instructions or simpler instructions in some sense, but that require like basically you have to do them many, many times so that you'll have more lookups into the table versus if your instruction that you have is very complicated and you only need to do one of that. I got it. Great. Yeah, I think there is a tension. So there is still like a per lookup cost here. So just adding more lookups for the heck of it doesn't help, right? So it might help amortize the fixed cost better, but then there's a per lookup cost, so there's tension.
00:18:23.240 - 00:19:19.964, Speaker A: And I think this tension, it remains to be explored more. So again, I think that you see the same tension when comparing Lasso to generalized Lasso, where Lasso applies to less general tables, although still general enough to implement jolt entirely with Lasso but needs more structure in the table. So you could think about kind of using more complicated instructions. So each instruction does like more work, as you were saying. And since there's one lookup per instruction, if you reduce the number of lookups enough, since each instruction is doing more work, maybe you'll pay for the fact that generalized Lasso has one lookup with generalized Lasso is more expensive than one lookup with Lasso. So it's just it's attention and I don't have a great sense of where's the optimal point I think it depends. With something like jolt, where the instructions? That was handed to us by RISC five, it's clear where you should be.
00:19:19.964 - 00:19:46.436, Speaker A: But with something like just some application, I want to prove knowledge of a bunch of pre images of some hash function or something. Pick your favorite hash function and no one's telling you what your primitive instruction set should be. Then it's a whole quandary. Like what should your primitive instruction set be? How complicated should each instruction be? There's a lot to explore. Does that make sense? Yeah. Tim, do you have a question I.
00:19:46.458 - 00:19:56.836, Speaker B: Was going to ask actually this second half, this is about the LBE case that you mentioned or also the decomposable case. Yeah.
00:19:56.938 - 00:20:00.664, Speaker A: The second point, this last cost yeah.
00:20:00.702 - 00:20:02.664, Speaker B: Just in general, for the decomposable case.
00:20:02.702 - 00:20:04.344, Speaker A: Is it still end of the one.
00:20:04.382 - 00:20:25.036, Speaker B: Overseas or that you're like a higher level that you're losing? Because that was like basically said there. You break up a lookup into a big table and do a bunch of lookups into smaller tables, which sort of sounds like you're breaking the computation into small pieces without any loss. Right, so I feel like that pushes.
00:20:25.068 - 00:21:11.400, Speaker A: Against I see, yeah. It is a tension between the per lookup cost and the fixed cost and the whole point of choosing C greater than one. I don't have the whole cost here. I did have it on a previous slide. Maybe this is not necessary, but yeah, so the per lookup cost grows with C actually in jolt, there's not actually a factor C in front of N of the one over C. It's like always two, there might be a couple of instructions where it's four, even though C might be like six or seven. Anyway, so you take C bigger to reduce the fixed cost, but you pay a price in the per lookup cost and so you kind of only want to reduce the fixed cost so much as it's acceptable.
00:21:11.400 - 00:21:32.150, Speaker A: So you kind of want to choose C to make C times M and N to the one over C about the same and it doesn't make sense to go any lower than that. So I don't think there's any contradiction to what I was saying. You sort of want to make the fixed cost just low enough that it's not the bottleneck, but no lower because there's another cost.
00:21:32.600 - 00:21:37.012, Speaker B: So this commitment costs are the same for both versions whether it's decomposable or LDE structured.
00:21:37.076 - 00:22:11.680, Speaker A: Yes. Yeah, I'd say the only difference is that with the LDE structured stuff, some of the CM things are big field elements. Yeah, that's the only difference. Cool. Any other questions? Okay, so I don't know. I hope at least these new perspectives are interesting, even if I don't think everyone's going to agree with me. Okay, so now I want to present a new perspective of lookup arguments as data parallel computation.
00:22:11.680 - 00:22:48.776, Speaker A: So our intern Yanuo actually raised this perspective earlier. So I think it's really nice and wanted to share it. Okay. So many previous works have studied Snarks for data parallel computation. This just means computing the same function F on many different inputs. And they consider, like, what I call a polynomial amount of data parallelism. So you're thinking of if the input size to F is like n bits or something n field elements, you're thinking of M as at most polynomial and n typically that's typically how computer scientists think, like polynomial.
00:22:48.776 - 00:23:52.530, Speaker A: And the input size is what computer scientists normally mean when they say polynomial. And these prior works, they still force the prover to evaluate F in a very specific way. That is, they turn F into a circuit, and they make the prover evaluate that circuit on each of the M inputs they want to evaluate F at. Okay? And basically, at least for the improver's perspective, they tend to only save like a constant factor by exploiting the data parallelism for the Verifier's perspective. The fact that data parallel computation is kind of inherently uniform, which is the Verifier should only have to kind of pay for one copy of F instead of M copies of F or something like that. Okay? But what I want to now introduce is a view of lookup arguments as exactly data parallel computation snarks for that, but now where there's a huge amount of data parallelism. Okay? So think of the lookup table exactly as Jolt does, where it contains all evaluations of some function F.
00:23:52.530 - 00:24:41.292, Speaker A: And the key sort of insight here, I suppose, is now if F is like a primitive instruction for a CPU, you should think of its input size as like, logarithmic. And so now exponential amount of parallelism in the size of the input size is something that a computer scientist should think about, right? I mean, this is exactly how Joel thinks about things. You've got like, F has 128 bits of input, and you're running a computer program for a billion or a trillion steps. So the number of steps M, one lookup per step M is really exponential in the input size to F. Okay? And so what like Lasso is is a really good Snark for data parallel computation from this perspective.
00:24:41.356 - 00:24:52.820, Speaker B: I want to make sure I understand that. So you're saying these AIS are like just sort of counters these AIS are.
00:24:52.890 - 00:24:57.556, Speaker A: You either think of them as indices into the table or inputs to F, because the table is the evaluation table.
00:24:57.588 - 00:25:02.936, Speaker B: Of F. Okay, so what's logarithmic and what?
00:25:03.038 - 00:25:18.668, Speaker A: Okay, yeah, okay, forget about logarithms and stuff. The table size is exponential in the input size to F. Another way of saying as the input size to F is logarithmic in the table size.
00:25:18.754 - 00:25:20.592, Speaker B: Okay, table size.
00:25:20.726 - 00:25:57.288, Speaker A: Yeah. Okay. And the cost of generalized Lasso, it's like CM plus n to the one over C. So there's this fixed cost of table size to the one over C. Okay? So for this end of the one over C cost to not be the dominant cost, you really need M to be something like polynomial in n. It might be a small polynomial, it might be n to the one over eight or something. And another way of saying that is M is exponential in log n the size of the input to F.
00:25:57.288 - 00:26:44.184, Speaker A: Okay, so what Lasso is it's a really good Snark for any data parallel computation as long as there's a huge amount of data parallelism, like way more than prior works on data parallel Snarks for data parallel computation. Consider and I thought I had one more slide here. Guess not. Okay, so I'll just say this verbally. The cool thing about Lasso from this perspective, compared to, I think, like, the prior works on data parallel computation is it kind of lets the prover evaluate F any way it wants. So whereas the prior works would make the prover evaluate F by using a specific circuit, and using a circuit is gross in general. Right.
00:26:44.184 - 00:27:20.390, Speaker A: That's why we have this whole field of front end design for Snarks. With Lasso, the prover merely just commits to the claimed outputs of F and some multiplicities, like one or two multiplicities, like per lookup or something like that. And that's basically it. So it's kind of tying the proverbs hands less tightly and as a result much more efficient. Right. It's like it's expensive to tie the proverbs hands tightly. You have to spend more cryptographic operations to do that.
00:27:20.390 - 00:27:23.430, Speaker A: Does that make sense?
00:27:24.040 - 00:27:30.424, Speaker B: But this only holds for specific apps. No, for this LDE structured apps or.
00:27:30.462 - 00:28:13.732, Speaker A: For any type of functions can you do? Yeah, so the issue is just from the severifier issue, really. So you can apply Lasso to any lookup table, the evaluation table for NEF, and the proverbs costs are whatever they are. Let me think for just a second. Yeah. The Verifier is going to have to evaluate a certain polynomial associated with the lookup table at a random point. And so if F is very complicated, it's not going to be able to do that quickly. And that's why prior lookup arguments would have somebody, an honest party in preprocessing commit to the whole table exactly.
00:28:13.732 - 00:28:50.492, Speaker A: So that the Verifier wouldn't have to evaluate that polynomial on its own. It can make the prover do it. Okay. So you can kind of think of Lasso or Jolt as like, from the Verifier's perspective, it's a reduction from executing the transition function of a virtual machine of a CPU many, many times, like a billion times or however long the prover is claiming to run the computer program for. And from the Verifier's perspective, it reduces the work of doing that down to more or less executing the transition function just once. If that transition function were really hard, that's still a lot of work. But we design virtual machines.
00:28:50.492 - 00:29:19.768, Speaker A: Exactly. So their instructions are really simple. And taking one step of the virtual machine is not hard. Seems like VDFS could be a great application of this. That's an interesting question. You're saying F would be the delay function? Yeah, potentially. Like repeated squaring or squaring modular squaring.
00:29:19.768 - 00:30:08.728, Speaker A: Yeah. So then it's just one instruction you're executing over and over again? Potentially. And you said they just have a very strong competitor in folding schemes because folding schemes are sort of like perfectly designed for capturing iterative application of a simple function. So I think, lookup, arguments could be really nice for that, but it's just got to really folding schemes are also really nice for that as well. Other questions. Okay, let me just say a couple of things about kind of pros and cons of the entire idea of designing a zkvm and then I'll actually tell you how Lasso and Lasso finally works. Okay, so jolt is a zkvm.
00:30:08.728 - 00:31:06.750, Speaker A: I do think the prover is significantly more performative than prior zkvms. I do want to discuss kind of good things and bad things about just zkvms in general. Okay, so the good things about designing a zkvm and what I mean by this is like designing like, a circuit that kind of forces the prove, like the circuit itself, like executes the virtual machine, just like runs it for a certain number of steps. So the program is written in the assembly code for the virtual machine and the circuit, you turn it into a circuit that just executes the virtual machine over and over again. So if you do this, if you go through the assembly code of a virtual machine and you use a virtual machine that already existed from outside the Snark community, you can leverage existing compilers that compile, like, high level code down to the assembly for that virtual machine. That's kind of the whole point of EVM projects and risk zero targeting risks. Five.
00:31:06.750 - 00:31:50.072, Speaker A: So this can very quickly lead to a very developer friendly tool chain where you don't have to write these compilers down to the assembly from scratch. Okay. Also often the zkvm approaches generate like a universal circuit where kind of the program that the Verifier wants the proverb to run is part of the input to the circuit. So you don't have to run the front end, which can be really expensive actually in general every time you change the program that you care about the witness checking program. Also, these circuits have a ton of structure and so they're what I call uniform circuits. Just it's doing the same thing over and over again. And so Snarks that leverage that uniform structure can be much faster for circuits of a given size than Snarks that don't leverage that structure.
00:31:50.072 - 00:31:50.910, Speaker A: Okay.
00:31:52.880 - 00:31:58.140, Speaker B: Uniformity is kind of already taken when it comes to circuits as far as the meaning.
00:31:59.200 - 00:32:32.280, Speaker A: So this is a very closely related meaning though. Yeah, I mean, a uniform circuit is one where if I tell you the label of a gate, you can very quickly determine what other. Gates it's connected to or whatever. Right. And intuitively you can do that for circuits with some kind of repeated structure. At least there's a description of for any uniform circuit in that sense. There's certainly a description that's much smaller than writing down an explicit table of here's every gate in the circuit, here's what other gates it's connected to.
00:32:32.280 - 00:33:04.380, Speaker A: So I'm using it really to mean there's kind of a hand wavy version of that where the circuit is doing the same thing over and over again. So if I tell you the uniformity, I mean here implies the uniformity. You mean this is sort of like a strong uniformity? Yeah, exactly. It's a very strong version of uniformity. Okay. Yeah. So what are the downsides? So I'd say one is if you insist that your circuit actually kind of run a VM for a bunch of steps, the circuit might be much bigger.
00:33:04.380 - 00:33:35.070, Speaker A: So even though it's all structured, it's much bigger. And maybe like a faster Snark. The Snark is sort of faster for a given circuit size because the circuit is uniform, but the circuit is so much bigger, potentially you still come out behind or head. It's hard to tell. And I'd say this compilation procedure from high level computer programs down to the assembly is like some kind of bug or attack vector. It's like a surface. If there are any bugs in that compiler, the prover is going to prove it correctly evaluated a circuit that's not doing what the Verifier thinks it's doing.
00:33:35.070 - 00:34:17.572, Speaker A: Just to highlight this first point here is a computer program for naive matrix multiplication. Looks like some of the indices got messed up by PowerPoint and sort of naive matrix multiplication. It basically is a circuit, right? It's like a straight line computer program. You know in advance how many iterations of every for loop there's going to be. And then it only does addition multiplication operations. So you can just directly turn it into a circuit. If you insisted on going through a Zkvm, the circuit would sort of have to be prepared at every step of the CPU to execute any of the primitive instructions.
00:34:17.572 - 00:34:36.400, Speaker A: So it's going to have to include logic for all of the primitive instructions. Here. You sort of know in advance at each step exactly what the instruction will be. There's only two primitive instructions and so forth. So I think with the Zkvm approach, you'll just never get a circuit that looks like this. Okay, so it's just trade offs all along. I just wanted to have that discussion.
00:34:36.400 - 00:35:20.210, Speaker A: I think jolt is like a powerful demonstration of the utility of Lasso, but I think Lasso will be useful outside of Zkbms as well. So I wanted to maybe highlight that. All right, so how does Lasso work? Oh, question. Yeah. Why would this be a lot more complicated? If it is trying to be implemented in a ZK VM, is it because I will implement special instructions that would, in turn emulate a matrix multiplication. Yeah, you can naturally design a VM that has addition multiplication as primitive operations. In fact, most of them do.
00:35:20.210 - 00:36:22.850, Speaker A: But if you sort of take a computer program, say, written in assembly, like, if you just wrote this in the assembly for a virtual machine with, like 100 different primitive instructions and fed it through any Zkvm's front end, any of the existing projects, this is not the circuit that will come out. Maybe you could redesign these front ends for the developer to somehow give it hints or for it to automatically sort of figure out that, look, only additions and multiplications are getting done, and all of the loops have predetermined bounds. And so I don't actually need to produce a circuit. But these projects, a lot of them generate universal circuits. Right. The computer program is part of the input to the circuit, and so there's no way to stop the circuit from doing all this extra work, basically not leveraging all this extra information we have about this computer program where it only does multiplications and additions and the loops have sort of predetermined bounds and things. Does that make sense?
00:36:24.040 - 00:36:27.428, Speaker B: Basically the best case and the worst case are going to be the same at that point.
00:36:27.514 - 00:36:28.470, Speaker A: Right. Okay.
00:36:29.400 - 00:36:44.776, Speaker B: So when you say that not all Zkvms produce universal circuits, are the exceptions, what you're talking about, where it kind of has this special logic in it where it can optimize the universal circuit that comes out based on some knowledge of the program.
00:36:44.878 - 00:37:13.110, Speaker A: So I'm actually not sure whether there are Zkvm projects that don't produce a universal circuit, but there are projects that won't that just don't implement a virtual machine abstraction at all. So, like some of these lower level DSLs I mentioned, okay, halo two, the developer directly writes constraints, some kind of constraints. Right. So there's just no virtual machine to be found.
00:37:13.640 - 00:37:20.672, Speaker B: To me, Zkvm kind of is it fair to think of it analogously to producing universal circuits?
00:37:20.736 - 00:37:50.324, Speaker A: Because to say that Zkvms and universal circuits are like one and the same, maybe I think one can probably come up with universal circuits that if it's one circuit that works for all, you do need a way to describe the computation you want the circuit to do, then. So how would you do that if you don't just have, like a maybe.
00:37:50.362 - 00:37:51.796, Speaker B: This is kind of like a philosophical question.
00:37:51.818 - 00:38:46.216, Speaker A: Yeah, it might be more philosophical. I don't have a great answer. Certainly with existing approaches, they feel like the same. Maybe there's some way to separate them a little bit. Is there any work that you know of that combines both the approaches of, like, universal circuit and also have the developer give constraints? So you could be like, okay, this part generated universal circuit, but like, this part I know how to optimize and just slot it in. So I think you see something very close to this with the Zkvm projects that have built ins which are just like hand optimized circuits for specific gadgets. And those are basically saying, look, if I actually just wrote assembly code for the VM to compute this particular thing, whether it's like a digital signature or a hash function that gets applied all the time, the resulting Snark or the resulting circuit would be bigger than it needs to be.
00:38:46.216 - 00:39:55.870, Speaker A: So I'm going to hand code a specific one. Whether they're a project doing more complicated things than that than just like sort of building in some gadgets for certain important things, I don't know off the top of my head I don't have examples, but it's not hard to imagine that you can do something like that. So maybe someone is, I guess I have a similar question. I know in Machine Learning Compilers they take a bunch of code and figure out the right way to collapse the matrices down to be more efficient or the matrix operation to be more efficient. I'd curious if there's a technical reason you couldn't do something like that. Yeah, so there's a space called Zkml now, which is like basically just applying Snarks to ML workloads and you just cannot afford the overheads of the VM abstraction and so none of them are going through. I guess I meant like is there a reason you can't have better front ends that could create look at this code to create the efficient circuit? Yeah, no, there's no reason for that other than you're going to have to run the front end every time you change your computer program and a lot of these front ends are very expensive to run.
00:39:55.870 - 00:40:33.720, Speaker A: Other than that, yeah, you totally can and probably should do that. We should have more front ends that do things like that, actually. More questions. Okay, so this was kind of all the high level discussion, so now we're going to dive into the technical details. So let me just remind you the definition of a lookup argument. Okay? Let's focus on index lookup arguments. So the prover commits to a bunch of index value pairs and proves that for each pair AI comma bi.
00:40:33.720 - 00:41:08.404, Speaker A: The value AI resides in the bi entry of the table and then needs to prove that. Okay, now all the Snarks I want to tell you about today, the basic Lasso and Lasso snarks, they build on what's called interactive proofs. So let me just briefly introduce that model. I think it makes sense to describe the model in the context of an application. Just kind of makes things easier to think about. So imagine somebody just like a week, they have a laptop, someone has a laptop and they want to do some heavy duty computation. So they hire a cloud computing service to do the computation for them.
00:41:08.404 - 00:41:40.270, Speaker A: So they ship a bunch of data up to the cloud. The cloud stores the data. Most interactive proofs, the user only has to keep around a very short summary of the data. As we'll see, typically the summary is actually you interpret the data as a multiliner polynomial and evaluate that polynomial at a random point, but don't worry about that yet. Then the user asks the cloud a question about the data, maybe in the form of a computer program, and wants the cloud to run on the data. The cloud comes back and says, I ran your computer program on your data, and here was the output. But the user doesn't trust the cloud for whatever reason.
00:41:40.270 - 00:42:16.700, Speaker A: Maybe there's a reason the cloud wants to trick it. Maybe the cloud is just lazy and doesn't actually want to spend the electricity or use the resources to run the program properly. So the user starts interrogating the cloud, sending a sequence of challenges and getting a sequence of responses, then eventually either accepts the answer as valid or rejects it as invalid. Okay, so it's sort of the same thing as a Snark, except there's no witness only known to the prover here. There's just some data that the user itself saw, and there's interaction. But in all the interactive proofs we're going to use today, they're public coin. What that means actually isn't important.
00:42:16.700 - 00:43:12.524, Speaker A: What is important is that means you can use some cryptography, very lightweight cryptography called the Fiat Shimir transformation, to make it non interactive. Okay? All right, so I sort of explained at a high level, like, how Snarks were designed yesterday, but I want to sort of re explain that now, specifically using interactive proofs instead of yesterday, I referred to what are called polynomial IOPS. So forget about polynomial IOPS for now. Okay, so the trivial proof in a Snark, the proverbs claiming to know some witness W, satisfying some property that some witness checking procedure would have accepted. The trivial proof is always the prover sends the witness to W, who directly checks the witness for validity. A slightly less trivial proof system would be the prover still sends the witness to W. But if checking that W is correct is expensive, you can use an interactive proof to force the prover to do that hard work.
00:43:12.524 - 00:44:02.750, Speaker A: Of running the checking procedure on W. Okay, so if the checking procedure is hard enough, you can get a work saving snark this way. But it still won't be succinct because the proof isn't any shorter. Like the proverb is still sending the witness to the verifier. Okay, so what you'll do in Snarks derived from interactive proofs basically, is the prover will cryptographically commit to the witness using a polynomial commitment scheme. And so that commitment is just like one hash value or one group element, and it will later reveal just enough information about the committed witness to let the interactive proof verifier sort of do the checks it needs to do. And in all of the interactive proofs, I guess that matter what that check will be is just like evaluate a certain polynomial that captures W at a random point.
00:44:02.750 - 00:44:45.252, Speaker A: Okay. All right, so let me give a brief history of Snark design and sort of convey the perspective I'm hoping to convince you of through this talk. So historically, interactive proofs were designed kind of first, and then people looked at a variant called multipruer Interactive Proofs and that led to something called Probabilistically Checkable Proofs. And then finally people built Snarks from PCPs. Now these were all theory works in the mid. Interactive proofs were introduced in 85 and let's say the first Snarks were given strictly by Killian and Macaulay in sometime in the early ninety S. I forget the exact year.
00:44:45.252 - 00:44:50.888, Speaker A: Okay. Now it took a very long time to get anything resembling practical Snarks.
00:44:50.984 - 00:45:22.900, Speaker B: Quick question. So I certainly agree the characterization all that was theoretical. I still feel like between the PCPs and the Snarks, at least from my perspective, there was a shift in perspective for the first time. Right. I kind of feel like the first three of those, at least, certainly the first killer applications were all in kind of structural complexity theory, right? Sort of is for lower bounds, if you want to call it that way. Whereas Snarks the mind, even though those first constructions are so theoretical, it's like, look at this cool thing you can do in principle. Would you say that's accurate?
00:45:23.420 - 00:46:00.484, Speaker A: I think that is accurate. I think one of the early PCP works, there's a quote from it actually, and it was maybe like the last PCP work before Snarks that really had this perspective. Maybe. So I don't know if it was the first, but I think it might have been the last. Talks about monitoring a herd of supercomputers, which is exactly like basically foreshadowing this cloud computer scenario I mentioned before. The cloud is like the supercomputer and the monitor is just a laptop or something. And then you're totally right.
00:46:00.484 - 00:46:39.376, Speaker A: I think with Snarks, even though they weren't practical, these early works did have in mind the prover knows some witness and is going to prove it knows the witness. Whereas the IPS and MIPS and some of the PCP work was thinking about totally intractable problems, like peace based complete problems or even next complete problems. So there was really no thought of actually running the prover in the real world because you can't even solve the problem in the real world. So it is a unique change in.
00:46:39.398 - 00:46:49.952, Speaker B: Perspective now that I think about it. You had like BLS linearity testing, right, kind of as a PCP precursor or something, and that had a more positive framing, actually, I think.
00:46:50.006 - 00:47:50.084, Speaker A: Yeah. So I should track down the very first paper that seems to have had the positive framing. And then the framing in the PCP literature was kind of abandoned when everybody was using them for hardness approximation and then they cared about PCPs for just like completely different reasons. But fast forward to 2008 and we still did not have practical snarks from any of these, I guess. So then there was work sort of introducing linear PCPs and giving what quickly led to practical snarks from them and then only later did we get practical snarks from the precursors. And yeah, actually you see a lot of the PCP papers will in the introduction or something, talk about a very simple kind of PCP that they're not happy with. So then there's like 100 pages to make them happy with it.
00:47:50.084 - 00:48:16.668, Speaker A: And what it turns out is it's this sort of funny thing. Those simpler PCPs from the introductions are kind of what some of the most practical snarks are based on. So like Killian McAuley, because they only use merkel hashing and fiat shamir as the cryptography to get a snark really needed like a short PCP. So they really kind of needed the other hundred pages sort of these are.
00:48:16.674 - 00:48:18.800, Speaker B: The upper right earliest snark papers.
00:48:19.220 - 00:49:35.744, Speaker A: So like Killian McAuley, say take a PCP, have the prover merkel, hash it and send that to the and then the Verifier kind of acts like the PCP Verifier and the authentication paths are what ensures that the prover actually answers consistent with the committed PCP string, doesn't change the string based on what queries the Verifier makes. And if you go that route, you really do need, I mean, the modern analog is IOPS, we still don't have practical PCPs. But the higher level point here, I guess is like we do have cryptography that's not just merkel hashing and fiat shamer. And if you use that heavier cryptography you can sort of start with something simpler than a PCP or at least something that came in the introduction of the 100 page PCP paper and skipped the last 95 pages or something. Yeah, if that makes sense. Okay, now there's something really interesting about snarks from Interactive Proof. So like the thing that came very first, and this is sort of a personal opinion, but I do my very best to convince you to have the same view by the end of today, which is they minimize commitment costs for the prover.
00:49:35.744 - 00:50:43.016, Speaker A: And the key technical tool here is something called the Sumcheck protocol, which I won't mention really again till the very end of the talk and even then I'll just kind of tell you what problem it solves. But there are a lot of tutorials I've given like online, there are videos and stuff. I think it's a really beautiful protocol, very simple and is based on multivariate polynomials. These multivariate polynomials were kind of abandoned in later work like especially the PCP literature because they don't lead to short PCPs, but for snarks we don't necessarily need PCPs, which is the point I just made. And so the subject protocol forces the prover to conceptually kind of materialize intermediate values in a computation without sending them to the Verifier and without even cryptographically committing to those values. Okay? And it's sort of unique in that way. And this is very hard to describe like why and how and I'll try to give a sense of this throughout the rest of today's talk, but essentially all of the other techniques just have the Snarks derived from multipring proofs and from PCPs and IOPS and linear PCPs.
00:50:43.016 - 00:51:50.000, Speaker A: They all have the proverb just commit to more values. And that's the bottleneck for the proverb, those commitments, okay, so even Snarks that use the sumcheck protocol, some of them do have the proverb commit to some intermediate values in the computation. Example of this is Spartan and it's really based on a multi prover interactive proof, not a single prover one. And that's kind of where the second prover sort of from the right perspective is where the extra commitments are coming from. But even Spartan has the prover commit to fewer intermediate values than Snarks based on univary polynomials. And I think the comparison of the number of commitments in Spartan to Snarks derived from univary polynomials, it's not so night and day that people necessarily see, people might not even agree with this statement I'm making, but I firmly believe it's true. So I've been trying to come up with sort of starker differences where the subject protocol is really saving like a huge amount of commitment costs.
00:51:50.000 - 00:52:24.472, Speaker A: But in general, when Snarks from the subject protocol, there's none of these quotient polynomials that all of the polynomial based Snarks have the prover wind up committing to. And there are certain ways sometimes to avoid committing to quotient polynomials with certain polynomial commitment schemes. But even then you still have the prover committing to something called composition polynomials and all of the univary techniques always have the prover do FFTs, which the sumcheck based Snarks don't require. Okay, so I'm going to try to just convey throughout this talk how the sumcheck protocol minimizes these costs for the proverb.
00:52:24.536 - 00:52:41.270, Speaker B: It's one way to think of this that the sort of non sum check ones are kind of more proactively checking every single step so that you catch it as soon as there's a deviation, whereas sum check, you just check at the end. You just sort of let it play out.
00:52:42.120 - 00:53:50.840, Speaker A: So I think for the most extreme versions of sumcheck with minimal commitments, it's exactly like that what the MIPS and the PCPs and IOPS. So everything other than just IPS do. The very first thing the proverb does is commit to every value of every gate in the circuit. Whereas if you want to go through like a circuit abstraction, the interactive proof for that is the GKR protocol that goes layer by layer through the circuit and if there's any commitments at all, it's only to the input to the circuit. And so it's exactly like that where you're sort of deferring catching the prover in a lie until the very end and only then do you maybe have the cryptography. Depending on context, is it a correct statement to say that a lookup argument is a Snark from IP, because in IP, you mentioned like, we don't care about the proverbs complexity. So we don't in a lookup argument because we assume the table is somehow computed and is committed the way interactive proofs were originally defined.
00:53:50.840 - 00:54:34.970, Speaker A: People just thought about polynomial time verifiers and didn't care about the prover complexity. But in practice, we absolutely do care about the prover complexity. I'd say we're going to see that Lasso is derived from interactive proofs very heavily, and not only does it use the sumcheck protocol, it doesn't use it in the Spartan like way where the prover commits to a whole bunch of things all at once. It really uses this GKR protocol to really, really minimize the prover costs. So I don't think the lookup argument is IP based just because it's like a lookup argument or something. It's IP based because under the hood, it's literally using an interactive proof combined with as few commitments as possible. That makes sense.
00:54:34.970 - 00:54:59.756, Speaker A: All right. I do want to briefly discuss the downsides of some check based Snarks relative to other techniques. It's in quotes because I don't think they're major downsides. People are certainly welcome to disagree with me. So the main downside is really just larger verifier costs. However, so in something like Spartan, the proof size is still just logarithmically many field elements. In something like the GKR protocol, it's something like log squared.
00:54:59.756 - 00:55:32.516, Speaker A: We'll see why shortly. Log squared actually can be pretty big, but I have caveats to that in a second. But the other downside is you need the proverb to commit to multivariate polynomials. If it is committing to anything, it's committing to multivariate ones. And we don't have KZG like evaluation proofs for multivariate polynomials. We have like log size proofs instead of constant size. But many projects today are using a polynomial commitment scheme where the proof size is actually not just log squared.
00:55:32.516 - 00:56:15.160, Speaker A: I'd call it more than log cubed technically because this is the security parameter and that needs to be super logarithmic to have security against super polynomial time adversaries. So we're already using Snarks today with big proofs. So from my perspective, a lot of people are not going to disagree with this. Maybe this work might help convey it. But one of the reasons to use this big proof commitment scheme is to try to get a faster prover. That's the only reason to do it, because it has worse verification costs. That and if you care about post quantum security since it avoids lifted curves, personally, I think the right way and there's no one right way to do anything.
00:56:15.160 - 00:57:16.648, Speaker A: So let me just say a very powerful way to get fast approver is to just minimize the number and size of the field elements committed rather than committing to a lot of them and trying to work over a smaller field, if that makes sense. Okay, so hopefully this perspective just continues to slowly come across today. There are a couple of extreme examples of the sumcheck protocol minimizing commitment costs. So one is a matrix multiplication protocol that I described back in 2013 where the prover really just computes the product matrices any way at once and does a completely low order amount of extra work to prove that it's correct. So if you have N by N matrices, the amount of extra work to prove that the product matrix is correct is linear in the size of the matrix n squared. So this is vastly faster than the fastest known algorithms to just compute that product matrix. Right? So you see, the prover overhead is like under a percent even for very small matrices.
00:57:16.648 - 00:58:09.100, Speaker A: You just can't achieve this with other techniques. With any other techniques, the prover at a minimum is going to have to cryptographically commit to the product matrix or send the product matrix to the Verifier, which would destroy sysickness. So this is like a really extreme example, and some of the Zkml projects are trying to use this because ML does a lot of matrix multiplications and you just can't tolerate overhead that's bigger than this in some of those applications. And then the other extreme example is this GKR protocol I mentioned. Okay? So this is an interactive proof for circuit evaluation, meaning the Verifier knows the input to the circuit. It also gives a Snark for circuit satisfiability as long as the proverb sort of commits to the input to the circuit and just the input, not all the other gate values in the circuit. So here's the picture.
00:58:09.100 - 00:59:07.070, Speaker A: So if you have a circuit that sort of there's like a public part of the input x, which is known to the Verifier as well as the prover, and then there might be a witness W that only the prover knows. Okay? So if the prover is claiming to know a W that causes the circuit to spit out some output y, you can use the GKR protocol and the only information that the GKR Verifier needs to know about the input to the circuit is to evaluate these polynomials. I keep mentioning, but haven't told you what they are yet, the multilinear extensions of x and W at a single random point. Okay? So to get a Snark for circuit stat from the Gkrr protocol, the prover would just commit to the multilinear extension of W, and then when the Verifier needs to know W twiddle at r, some random point R, the polynomial commitment will just make the prover give it that value.
00:59:08.160 - 00:59:11.152, Speaker B: So the Verifier can evaluate x tilde itself, right?
00:59:11.206 - 00:59:31.750, Speaker A: Right. And it's known the Verifier can do that in time, linear in x tilde. And today x is going to be the lookup table might have size two to 128, but if it does, the Verifier is still going to be able to evaluate x Twiddle really quickly because we're going to be dealing with tables that structured means they have that property.
00:59:32.360 - 00:59:35.896, Speaker B: So where is the sum check coming in in this?
00:59:35.998 - 01:00:05.730, Speaker A: Yeah. So the GKR protocol under the hood is applying the sumcheck protocol once per layer of the circuit. So the verifier costs are the depth of the circuit times log of this can actually be like the width of the circuit, but I just wrote the size of the circuit. For simplicity, what's happening is for each layer of gates in the circuit, you're running the sumcheck protocol once. So that's log cost for the verifier per invocation of sumcheck, but then you apply it once per.
01:00:08.820 - 01:00:12.276, Speaker B: Did each invocation computes what so what's the computation you.
01:00:12.298 - 01:00:28.360, Speaker A: Do each layer basically like correctly evaluate every gate at that layer. So the GKR protocol is forcing the prover to evaluate the circuit gate by gate, but without using any cryptography other than if the prover committed to a witness.
01:00:31.370 - 01:00:36.470, Speaker B: So it kind of like succinctly reevaluates the circuit on the commitment.
01:00:37.770 - 01:01:32.648, Speaker A: Yeah, just think of like on the commitment the prover commits to w twiddle at the start, the small tune extension polynomial from w the verifier through the whole protocol doesn't need to know anything about w until the very end right. When it needs to know w two of R. Right. So yeah, the proverb is just proving like it went gate by gate through the circuit and evaluated if a multiplication gate said, hey, compute the product of these two other gates that it did that the prover can be implemented in order circuit size, many field operations. So this was shown via a long line of works. Today we're only going to apply the GKR protocol to actually one very specific circuit where we already knew this back in 2013, how to do this. It's all field work.
01:01:32.648 - 01:01:46.030, Speaker A: Right. The only cryptographic work is involved with this witness commitment, whereas all other snarks would the very first thing the prover would do is commit to a value for every single gate in the circuit rather than just to this witness at the top.
01:01:49.010 - 01:02:02.914, Speaker B: So these successive invocations, is there some consistency requirement? Not only did you evaluate these correctly, but you evaluated these correctly given what you implicitly said the previous layer's values were or something like this.
01:02:02.952 - 01:02:26.810, Speaker A: Yeah, that's basically how it works. So you sort of apply sumcheck to the output gate of the circuit. Right. And at the very end of that application of sumcheck, the verifier has to evaluate some polynomial derived from the gates that fed into the output. But it can't do that on its own. So it sort of checked everything up to checking the gates that fed into the output. So then it just does the same thing again.
01:02:26.810 - 01:02:59.006, Speaker A: So it's kind of reducing the proverbs claims from something about the output to something about the gates feeding into the output, something about the gates feeding into them and one layer at a time until eventually approver is forced to make a claim about the inputs to. The circuit W has already been committed at the start. The Verifier knows X, and so it can basically check that final claim on its own. It's getting what it needs to know about W from the cryptography. Okay, so I mentioned this. People normally think of X as really small. It's like one hash output.
01:02:59.006 - 01:03:36.346, Speaker A: The provers claim to know like a pre image of the hash output. These lookup arguments, x is going to be the whole lookup table. And that lookup table might have sized two to the 128, but all the Verifier needs to know about X is one evaluation of the multilinear extension. And for all the lookup tables arising in Jolt, even if they assign to the under 28, that's really quick. Okay? So I think these GKR based Snarks, they haven't seen deployments. And I think the general reason is that general purpose front ends, especially those supporting a virtual machine abstraction, use a ton of what's called this untrusted advice that I mentioned yesterday. And so that winds up in this W that I showed feeding into the circuit.
01:03:36.346 - 01:04:21.086, Speaker A: So the prover does have to cryptographically commit to all that advice. And so you don't really get a huge amount of speed up for the prover because that W is actually like a constant factor of the whole circuit. Moreover, the circuit model is very restrictive. Low depth arithmetic circuits, kind of a double Whammy. Like people don't want to deal with these circuits and you're not getting that big a speed up for the proverb, even for a given circuit size. So my view is Lasso and Jolt are trying to make these two extreme examples very general, right? So the GKR protocol applied to circuits with tiny witnesses, no advice inputs, very few advice inputs, is like spectacularly fast for the proverb. But in general, we need circuits that have big advice inputs, so it's not helpful in general.
01:04:21.086 - 01:04:58.330, Speaker A: So Lasso does use the Gkara protocol but applied only to a very, very specific circuit. It just computes what's called a grand product. So the input is like some large number of field elements. They end field elements and we want to multiply them all together. These grand products arise all over the place. They arise in all lookup arguments, they arise in Snarks for circuitsat as well. All deployed grand products, they will have the prover commit to partial products, which are basically intermediate values in a circuit computing the grand product with the GKR protocol, you cut out those extra commitments.
01:04:58.330 - 01:05:45.580, Speaker A: Okay? In fact, in the 2013 paper, I gave a very optimized version of the GKR protocol exactly for this. Didn't know it'd be useful for Snarks at the time, but this is kind of the core of many Snarks. Now, the proof size there is log squared, which is bigger than I would like, although it's still smaller than like lambda times log squared, which is what a lot of people are using today. There's a way to reduce it down to basically log. And if the prover does commit to some extra stuff, but it's like, it's lower amount of extra stuff. So concretely, say you can get this proof size with the proverb committing to, I don't know, like N over 64 field elements. So like just 2% extra if you already paid for a commitment to size N or something.
01:05:49.540 - 01:05:58.084, Speaker B: Sorry, this extra stuff that corresponds to the untrusted advice you were talking about or that's just sort of that arises from what?
01:05:58.202 - 01:06:42.290, Speaker A: It arises from the following. So the circuit that computes the grand product for us is just the binary tree multiplication gates. Now most of the gates of that circuit are near the inputs, right? But most of the layers of the circuit are near the output. So most of the layers are tiny, but most of the rounds in the GKR protocol are coming from the many tiny layers. So you kind of have the proverb commit to the values of just those tiny layers. And it's not many values. So you'll run like the 2013 protocol for just like the first five layers of the circuit and then you'll run something like more like Spartan for everything else.
01:06:42.290 - 01:06:49.052, Speaker A: But you're only applying Spartan with its higher commitment costs to one over two to the fifth fraction of the circuit.
01:06:49.196 - 01:06:52.708, Speaker B: When you say first five layers, you mean closest to the output, to the.
01:06:52.714 - 01:07:27.408, Speaker A: Input, the big layers. So you avoid applying the cryptography to the big layers. You do apply it to the small layers. You see something analogous now in recursive Snarks, sometimes I know Stark where they'll use a slow but Snark friendly hash function exactly in their merkel trees at the many small layers. It'll be Snark friendly, but they'll use something fast but not Snark friendly for the few big layers. And that way each authentication path is mostly Snark friendly. But computing the merkel tree is fast.
01:07:27.408 - 01:07:56.730, Speaker A: It's the same idea. Okay, so with all of that out of the way, I can give you the details. And really now there aren't all that many details to cover in the end. Okay, so I do have to change perspective a bit, I guess. So we're going to think of the lookup table now as a computer memory. Now it's like some fixed it's fixed, but it's not going to actually matter that it's fixed. Everything I'm going to tell you about actually would work in some kind of dynamic setting, but I won't get into the details of that.
01:07:56.730 - 01:08:44.280, Speaker A: And so, yeah, index lookups are just memory. They're just reads into memory, right? Does that make sense? Okay, so what we need to do is something called memory checking. And there's a very old literature on this. And so all the techniques I'm going to describe to you other than the Sum check protocol and the GKR protocol are from this very old literature. Of course, the Sumcheck protocol itself is also very old, but from 1990 or something. So we need like an algorithm that can check that all the reads are correct, but snarks are only good at making sure the proverb ran algorithms that basically are easy to represent in a circuit. So with some very limited exceptions, like matrix multiplication, we always go through a circuit.
01:08:44.280 - 01:09:14.316, Speaker A: So generally speaking, we want the algorithm that's going to do the memory checking to kind of only do addition multiplication operations. And moreover, since I ultimately want to use the Gkrr protocol to really, really minimize commitment costs, we're going to want the circuit to be low depth. Ideally, that's like a parallel algorithm that's only doing addition and multiplications questions. All right, so here are the key techniques. It's really just something called fingerprinting, and I need two variants of fingerprinting.
01:09:14.348 - 01:09:16.980, Speaker B: You're now designing witness checking algorithm.
01:09:17.880 - 01:09:33.504, Speaker A: Yes, but I'm going to think of it as memory checking, and I'll explain in more detail what that means shortly. But right now we're going to just view fingerprinting as just some more background before I return to details of memory checking.
01:09:33.552 - 01:09:39.636, Speaker B: Right. I can just think of an algorithmic problem given an array. I'm giving the little AIS and little BIS to give me a very simple algorithm.
01:09:39.668 - 01:09:48.424, Speaker A: Yes, exactly. Verifying that. Exactly. Yeah. Great. Okay, so two variants. So let's start with what I call the equality checking variant.
01:09:48.424 - 01:10:38.396, Speaker A: Okay, so think Alice and Bob have two vectors of length n. They want to know are they the same vector? Okay. If they didn't use any randomness, it's known like the best you can do is basically Alice sends her whole vector to Bob, and he compares them entry by entry. Okay, so is it possible for Alice to send less data than that to Bob? It's known how to achieve this with a randomized procedure. So the way that works is Alice interprets her vector as a degree n polynomial, let's say as the coefficients of a degree n polynomial. And same with they just alice picks a random input to the polynomial from the whole field, evaluates her polynomial at that point, sends the random point and the evaluation to Bob, who compares it to his polynomial evaluated at the same point. Okay, that's it.
01:10:38.396 - 01:11:12.184, Speaker A: So just two field elements, r and P of R getting sent to Bob. Okay, so I call P of R the Reed Solomon fingerprint of Alice's vector at R. It's actually just a random entry of, like, the Reed Solomon encoding of her vector. If you know what Reed Solomon codes are, if you don't, it doesn't matter. And yeah, the key claim is that if their vectors are equal, then their fingerprints will always match. And if the vectors are not the same, then with high probability, their fingerprints will not match, and so they'll output not equal. Okay, so F is zero one, and.
01:11:12.222 - 01:11:15.930, Speaker B: This is the familiar, like, random inner product kind of thing?
01:11:17.020 - 01:12:20.476, Speaker A: Yes, but for random inner product you're really interpreting A and B as a linear function, like an invariate linear function, and I want to work over a big field and interpret them as univary polynomials. Other questions? Okay, so the first claim that if two vectors are equal, their fingerprints match is just because you wind up evaluating the same polynomial at the same point. So of course it spits out the same thing. Now the second claim, if Alice and Bob have different vectors, what this boils down to is you wind up evaluating two different polynomials at a randomly chosen point. And there's this basic property of polynomials. If they both have degree at most n and they're not the same polynomial, they can agree at a most n points. So the probability they happen to pick one of those n agreement points is the most n over the size of the field you work over.
01:12:20.476 - 01:12:57.448, Speaker A: Think of n as, I don't know, two to the 30 and the field size as I don't know, two to the 256. And this is like astronomically small. Like you'll never get basically a collision of the fingerprints for different vectors. Great. Okay, so the other variant we need is called permutation checking. And this dates back to work of lipton in 89 and then was used for memory checking by an old paper of Blum and others, auburn Blum and others from 94 or earlier. So here the goal is just to determine not whether their A and B are the same vector, but whether they're permutations of each other, whether they're like reorderings of each other.
01:12:57.448 - 01:13:49.356, Speaker A: Okay, so these two things are permutations of each other. It's just the same field elements reordered. But if I change one of the entries of A to something else, there would not be permutations. So now we do the same thing, except now we're turning A into a polynomial, not by interpreting it as the coefficients, but as the roots and the same thing works out. So Alice picks a random R and evaluates her polynomial at r and sends R and that evaluation to Bob, who compares it to his. And again, the point is if A and B are permutations of each know, two polynomials with the same roots are the same, the multiplication commutes, and if they're not permutations, they're different polynomials. They both have degree n minus one.
01:13:49.356 - 01:14:21.360, Speaker A: So you evaluate them at a random point, they almost certainly disagree at that point. Questions? Cool. So I call this permutation invariant fingerprinting. All right. So now let me summarize this little paper of Blum and others. So yeah, remember, we're thinking of lookups index, lookups into a lookup table t as reads into memory. And we want some algorithm that will check that all the reads have the correct value returned.
01:14:21.360 - 01:14:49.164, Speaker A: But this algorithm should only do like additions and multiplications over find it field. Now, you saw these fingerprinting techniques only do additions and multiplications over find it field. So you see, where this is going. Okay, so we're going to change our perspective a little bit. So I'm going to adopt a perspective from the memory checking literature. We're going to think of these reads in lookup arguments. You think all the reads, the memory cells being read and the values returned by the reads, they're all committed like, all at once.
01:14:49.164 - 01:15:19.364, Speaker A: Instead we're going to think of like a weak Verifier doing the read sequentially. So at each step it determines, hey, I want to read this cell. And the prover is going to come back and say the value currently stored in that cell is blah. And what this early work was interested in was the space complexity of the Verifier. So they were thinking, oh, this Verifier has very little space. So it's going to use an untrusted prover that has a lot of space to maintain the memory, but it doesn't trust the prover. Okay, we don't care about the space complexity of our Verifier.
01:15:19.364 - 01:16:22.468, Speaker A: Our Verifier is really like an arithmetic circuit in the end, but we do care that the circuit only does addition multiplication operations over some finite field. Turns out the same solution that kept the Verifier space low also turns into like a good circuit. So yeah, this old work just has the Verifier compute a couple of fingerprints of certain values that it observes over the course of the prover sending it claimed outputs of the read operations and it can maintain these fingerprints incrementally, right? So every time the prover comes back and says, here's the result of your latest read request, the Verifier just like, does one or two multiplications to update one of its fingerprints and moves on to the next one. So it's both small space, but I don't really care about that. I care that it's just additions and multiplications and only like one or two per read. Okay? So to make the reads more easily, checkable the blum at all paper, they modify the reading procedure. So they wind up kind of storing more stuff in the memory.
01:16:22.468 - 01:17:02.488, Speaker A: So like, more stuff gets read with each so I'll explain what that means. So with each memory cell, each index of the lookup table, they're going to keep a counter. And if the approver is honest, this counter is just going to track how many times the cell has been read at any given point in the procedure. Now, every time a read operation occurs, the Verifier says, I want to read the value currently stored in cell bi. And the prover comes back, it's going to return, both the value of claims resides there and account. So we're going to sort of augment the algorithm. The Verifier is going to do a follow every read with a write.
01:17:02.488 - 01:17:48.970, Speaker A: It's just going to write back to the seller, just read the value the proverb just said was stored there, which might be a lie. And if it is a lie, it's sort of overwriting the value with what the prover said was there instead of what's actually there. And the verifier itself is going to increment the count by one. So if the prover is honest, you're not actually changing the value stored in any cell ever, because the verifier is just writing back the value that was already there, and the counter just gets incremented by one every read to that cell. So the counter really is just counting how many times the cell has been accessed. Okay. And then the final modification is once all the reads are done, the verifier makes one final pass over memory, reading the values and counts in memory at the very end.
01:17:48.970 - 01:18:19.040, Speaker A: Okay, I think my later slides sort of forgot that there's a value here, but it doesn't yeah, the protocols obviously work. I just might be missing that little detail. Okay, somehow my definition of these vectors, R and W got cut. So I'm going to verbally tell you what R and W are. Fortunately, they're pretty simple. Okay, so R is called like the read set, and W is called the write set. So R is the list of tuples.
01:18:19.040 - 01:19:00.828, Speaker A: They're each at three tuples. So it's memory cell, value and count associated with every read operation that the prover returns. W is the same thing, but associated with every write operation. Crucially, also in W is like the initialization writes, which are sort of only implicit. So that is for every memory cell, and there are capital N of them, if this is a lookup table. So for I equals one to n there's I comma t of I. So the value stored at the it memory cell comma zero to indicate no reads have been done yet.
01:19:00.828 - 01:19:25.392, Speaker A: That's R and W. And sort of that initialization aspect of W is ultimately what's going to kind of force the prover to always return the value that actually was initially written that should reside in each memory cell. Okay, so the key theorem is that R and W are permutations of each other if and only if the prover is honest throughout this whole protocol.
01:19:25.456 - 01:19:26.896, Speaker B: Specifically for the algorithm you just showed.
01:19:26.928 - 01:19:58.016, Speaker A: Us in the last slide. Yes, and I can sketch the proof. It's going to be hand wavy, but this is really how the proof can go. Okay, so after initialization, there are, like, N tuples in W, but none in R. Right, because we've sort of initialized memory with run write per memory cell, but haven't done any reads yet. So you think of how W and R, like, evolve over time. After initialization, you've done n writes to memory to initialize the memory, no reads yet.
01:19:58.016 - 01:21:10.648, Speaker A: Now each read adds another tuple to R, because R is like the set of tuples, like, returned by the prover on the read operations, but each read is immediately followed by a write, which adds another tuple to W. So now, if the prover is honest, every tuple added to R matches the last write to that cell. Because if the proverbs honest, every time a read operation is done, it returns the value and count that were last written to that cell. Okay? But then that read is immediately followed by a write that adds something new to W, okay? And then finally, the final passover memory is supposed to match everything that remains unmatched in W, kind of match every final write with the read. Okay? So that's sort of what happens if the prover is honest. And that's why r and W, if the proverbs honest, will be permutations of each other because every read, like, matches a write that already occurred. But if the proverb ever returns an incorrect value count pair immediately, that means there's a tuple in the read set that's like the first time it does it.
01:21:10.648 - 01:21:56.812, Speaker A: There's now a tuple in the read set that's not in the right set. And there are still at all times until the final pass, there are n tuples in the right set that are not in the read set, which means that the symmetric difference of them grew in size and basically you can never recover from this. So every read operation until the final pass is immediately followed by a write operation. So the best case scenario is the symmetric difference neither grows nor shrinks. And then at the very end, the final pass over memory, there's only n read operations that can only reduce the symmetric difference by n. So if it was n plus one at any point, it's never going back below one anyway. And here, these are like up to permutation.
01:21:56.812 - 01:22:48.164, Speaker A: I'm bringing these as sets instead of vectors. I guess here when I say symmetric difference oh, there's my definition of well, is it? Yeah, that was my definition. Recap. Okay, well, we've recapped the definition now. Great. Okay, so now the question is, how do we check that r and W are permutations of each other, right? Because we've said that's equivalent to the memory checking goal, which was making sure that all read operations return the correct value, right? And so what we do is yeah, so each entry of r and W is a triple, like stores a cell, a value that the proverc claim was in the cell and a count that the prover claim was also stored in that cell. I don't like having vectors or triples.
01:22:48.164 - 01:23:38.170, Speaker A: So what we're going to the verifier is just going to read Solomon fingerprint each triple to reduce the triple down to one field element. And we've explained that with overwhelming probability, there'll be like, no collisions, so you won't have two different triples that fingerprints to the same thing. Okay? Which means that as long as there are no collisions, then these sort of fingerprinted vectors, which I'm calling r prime and W prime, which each just store field elements rather than triples, they are permutations of each other. If and only if the original thing of triples were permutations of each other. Now, for each triple, we care about the order. BAC is not the same as ABC or something, right? Values are not the same as cell identifiers, not the same as counts. That's why we're read Solomon fingerprinting here.
01:23:38.170 - 01:23:56.510, Speaker A: Okay? But then to check, we want to check that r prime and W prime are permutations of each other. So the definition of permutations, we don't care about ordering. So there we do permutation invariant fingerprinting. So this is the final check.
01:23:58.640 - 01:24:06.008, Speaker B: So the capital n plus m. So the capital n is that was what?
01:24:06.194 - 01:24:07.920, Speaker A: Table size, memory size, right.
01:24:07.990 - 01:24:12.416, Speaker B: But as far as the capital w and that's like the initialization stuff that.
01:24:12.438 - 01:24:18.790, Speaker A: Shows up in it shows up in the initialization and in the final passover memory. Those are the two places n shows up. Okay?
01:24:19.400 - 01:24:26.816, Speaker B: And then everywhere little m corresponds to the reads, which are part of the actual initial thing. And then the m writes that are your kind of copies, your sort of echoes.
01:24:26.848 - 01:24:27.430, Speaker A: Okay.
01:24:31.450 - 01:24:45.114, Speaker B: Can we just review why you did this in the sense of what were the target properties? Because you take this problem that is trivial unless, you know, you want to solve it in a way that has certain very lightweight computational properties. Right?
01:24:45.312 - 01:25:02.990, Speaker A: So what I care about is different than what the old blumentov paper care about. What I care about is I have an algorithm that takes as input these triples in r and w and just through additions and multiplications figures out are all the triples honestly computed.
01:25:04.850 - 01:25:08.510, Speaker B: And so like depth of the computation or something you don't care about?
01:25:08.580 - 01:25:29.844, Speaker A: Well, I actually do because I want to use GKR. Okay. So the bulk of the computation is just these big products. And so we're going to have a binary tree multiplication gates, logarithmic depth questions. Okay, great. So finally I can tell you how lasso works. Hooray, basic lasso.
01:25:29.844 - 01:25:54.028, Speaker A: Okay, so full details. So just a reminder. So T is a lookup table of length, capital n, b is committed vector of table indices, and a is a committed vector of values that are supposed to reside in those indices. And the Verifier is just going to get a commitment to this multilinear extension of A and this multilinear extension of b. I still haven't told you what those are. Continue to not worry about it. You can commit to them with the polynomial commitment scheme.
01:25:54.028 - 01:26:36.568, Speaker A: Yeah. So we're going to think about applying the GKR protocol to a circuit whose inputs are basically the lookup table a and b, which are a is like committed values, b is committed indices, and then this extra vector c, which is just these counts. Remember yesterday I said the prover in basic lasso commits to m plus n multiplicities and that's exactly what this vector c is. Okay? There's also some other stuff which doesn't these are the initial counts are all zero. So there's n zeros and the initial indices for the initialization step is just the number zero up to n minus one. Say, really doesn't matter. You could also make it all zeros.
01:26:36.568 - 01:26:45.996, Speaker A: It really doesn't matter. Wait, no, indices can't be zeros yet. Sorry. They should be the number zero up to n minus one, saying these are the N cells each getting initialized.
01:26:46.108 - 01:26:50.544, Speaker B: So that C, those are the same as the CIS that counts in your algorithm that you just showed us.
01:26:50.582 - 01:27:16.952, Speaker A: Yes, exactly. Okay. And there's also these gammas and R's used in the fingerprinting. So what's crucial is that, and I mean it's totally standard, the prover will have already t is fixed in advance. Let's say prover has already committed to A and B. Then the Verifier picks gamma and R at random, and ultimately fiat shamir will pick them. And that ensures, you know, if the prover knew what gamma NR would be when it's choosing like A and B and C, actually it should commit to C before gamma R are picked too.
01:27:16.952 - 01:27:49.670, Speaker A: If it could choose A, B, and C to depend on gamma and R, it could easily pass the randomized check. It's like we want the circuit to run a randomized algorithm. It's important that the random issues by the randomized algorithm not be known by the prover when it commits to things. And then the circuit just computes these fingerprints that I mentioned. And so I kind of like, wrote what these vectors r prime and W prime are. It's just the read Solomon fingerprints of the tuples a, B and C. Here again, the details aren't actually that important.
01:27:49.670 - 01:28:34.132, Speaker A: Here I've replaced for the writes, like, the count associated with a given write is the count associated with the same read but plus one. And that's capturing that. In the old memory checking algorithm from 1994, the Verifier just every time a read returns a given count, it just writes back with the counts incremented by one. So the write counts are just the read counts incremented by one. Okay, so we're really just multiplying computing what are called the grand products, just not directly. The inputs to the grand product are not like this vector I wrote up here. Exactly.
01:28:34.132 - 01:29:22.644, Speaker A: They're sort of derived from it in this kind of funky way by subtracting R from each entry. And each entry is actually obtained by the Reed Solomon fingerprinting, some tuples. Okay, but what's going on is the Verifier in the GKR protocol applied to this binary tree multiplication, gates just needs to evaluate the multilinear extension of its inputs. And R prime is like this simple combination of A, B, and C. So you can kind of take the multilinear extensions of A, B, and C and sort of put them together to get the multilinear extension of R prime. Don't worry about the details. Each entry of R prime is just like an entry of B plus gamma times an entry of A plus gamma squared times an entry of C.
01:29:22.644 - 01:29:37.304, Speaker A: So the same linear combination basically gives you the multilinear extension of R prime and W prime. It's something similar. So you can take sort of the multilinear extensions of the lookup table A, B and C and quickly evaluate or.
01:29:37.342 - 01:29:40.472, Speaker B: Quickly you're saying tilde commutes with plus in times.
01:29:40.526 - 01:30:17.140, Speaker A: Right? Yes, that's another way of saying that read molar encoding is linear. Okay. So I mean, that's it. That is basic, lasso. And actually, in hindsight, this is already sort of in Spartan. Yeah, we're sort of just viewing it in the right way. And the other contribution is it doesn't seem that the 1994 memory checking paper was known to be secure previously if the prover could mess with the counts.
01:30:17.140 - 01:30:58.044, Speaker A: I believe the initial analysis was just sort of assuming that these counts were well, it actually had one global count that the verifier could maintain itself. So every read it just gets incremented by one, so it only increases the verifier space by one. That was fine for their consideration. Then Spartan switched the global counts to like a per cell count. And I think then it was possible, according to that analysis, that if the prover messed with those counts, it could actually trick the memory checking procedure. So we show that even if the prover is messing with those counts, it can't trick the memory checking procedure. And that's essential for Lasso.
01:30:58.044 - 01:31:08.710, Speaker A: It didn't matter for how the prover happened to use this sort of scheme in Spartan because an honest party was actually able to kind of commit to the counts or something like that. Great.
01:31:10.520 - 01:31:36.376, Speaker B: I still just want to sort of understand where the performance gains are coming from. One thing I feel like comes out pretty clearly here, which is what you said yesterday. The number of things you're committing to is pleasingly small and a pleasingly large number of those are themselves pleasingly small, pleasing small numbers. Right. But then you're also sort of getting savings right on sort of the simplicity of the computation given the commitments.
01:31:36.568 - 01:32:07.796, Speaker A: Yes, exactly. So here's a summary of all the savings. So if key is structured as all the tables are in jolt, no one commits to it even if it has side 228. Okay. That doesn't seem to just be possible with univariate polynomials. It seems like the associated extension polynomials of T would not have the structure even for the tables. Like jolt needs A and B sort of have to be committed by definition of a lookup argument.
01:32:07.796 - 01:32:37.392, Speaker A: It's like part of the input, like you said, C. So really the only commitments are to C and these are just counts. Cool, then. So the other savings is because anything based on univariate polynomials as well as something like Spartan would commit to the value of every gate in the circuit, which is basically like partial products in these grand products. And we don't do that either. The price for that is a log squared size proof. But I said.
01:32:37.392 - 01:32:50.164, Speaker A: With just like a low order amount of commitments. You can drop that almost to log that's. This SETI Lee stuff we talked about where you sort of commit to the many small layers, don't commit to the few big layers. Yeah.
01:32:50.202 - 01:32:57.290, Speaker B: So it seems like it's important not just that the computation uses only plus and times, but that it boils down to grand products. Is that true?
01:32:57.980 - 01:33:47.850, Speaker A: So in general, as long as it's like a low depth arithmetic circuit, you could do it with GKR, but the fact that it's a grand product makes it extra nice because there's some optimizations in my 2013 paper. Yeah, but that's it. I don't think this is actually all that novel. Kind of, in hindsight. Like I said, it's almost in Spartan up to not being expressed as a lookup argument and not showing that it's secure when the counts might be adversarially determined. Okay, I know I'm already over, but I think generalized Lasso is I can get through, I think, in maybe even five to max, ten minutes. Is that okay? Sure.
01:33:47.850 - 01:34:24.692, Speaker A: Okay, so I do need to cover basic I'm finally going to wind up telling you what a multilinear extension is. So that's nice. So first I'll start with short zipple, which is an analog of the fact that any two degree d univary polynomials can agree at most like d points unless they're the same polynomial. So the same thing sort of holds for multivariate polynomials. So the fraction of points they can agree at is at most something called their total degree. And actually total degree, exactly what that means won't be important for the rest of this talk. So let's just not worry about it.
01:34:24.692 - 01:35:07.536, Speaker A: All the polynomials I'm going to deal with, basically they're just going to have total degree equal to the number of variables, and the number of variables will be like logarithmic in the M and n, where M is the number of lookups and n is the size of the table. So anyway, this will be an astronomically small probability when you work over like a 256 bit field. Okay, so now here an extension. Polynomial is a polynomial. G extends a function f, where f is only defined over zero one inputs. When you feed G zero one inputs, it spits out the same thing as f. So G is a polynomial, so you can evaluate it at points in the whole field.
01:35:07.536 - 01:35:42.270, Speaker A: So it's an l variate polynomial. So for each of those l variables you can sort of feed into it any field element and it'll spit out something. It's said to extend f. If when you do feed into it some point where f is defined zero one valued inputs, it does the same thing as f. Okay? And for any function defined over zero one to the l, there's actually a unique polynomial that has degree one in each variable. That's called the multilinear extension. Okay, so here's a quick example.
01:35:42.270 - 01:36:14.192, Speaker A: Here's a function defined over zero one by zero one. So there are four inputs to it. Here's its multilinear extension over like some large prime field. So again, the extension is defined. Each variable you can feed in the whole field, but it's an extension polynomial. So if you do feed into it, any of the four inputs at which f is defined spits out the same thing as f. So it's like a domain extension thing, and I denote the multilinear extension with the Twiddle over f.
01:36:14.192 - 01:36:52.220, Speaker A: Okay? There are other extension polynomials that are also low degree but not multilinear. Here's another example here, here's one that's not multilinear, and it has degree two and x one, but it is an extension. All right? So now I'm going to tell you what problem this sum check protocol solves. I won't tell you how it works. So imagine like, there's some Lvariant polynomial g, and all the verifier is going to be able to do is evaluate g at one point. Let's not worry about how it's going to evaluate g at that point. Magically, it can do that.
01:36:52.220 - 01:37:47.942, Speaker A: The verifier's goal is to sum up g's evaluations over the entire what's called the Boolean hypercube, that is L bit inputs. They're two to the L of them. Now, if it were to do this itself, it would have to evaluate g at each of these two to the L inputs and sum them up. So from the verifier's perspective, the sun check protocol will be a reduction in work from evaluating g at two to the L inputs and summing them up to evaluating g at just one input. But that one input will be in the whole field instead of in this special zero one valued subset called the Boolean hypercube. So the costs are there's one round per variable. And if g has degree d in each variable, then in each round the prover actually sends like a degree d univariate polynomial say, just like it's d plus one coefficients, which means the total proof size is d times L field elements.
01:37:47.942 - 01:38:36.940, Speaker A: So like d plus one field elements sent by the prover per round. The verifier just basically reads the messages from the prover kind of does. It winds up evaluating each of the L polynomials, one per round at a random point. But that's just like a constant amount of work per coefficient. So that's order d times L field operations in total, and at the very end it has to evaluate g at one point. Does that make sense? Okay, so in applications, again, L is going to be like logarithmic in m and n d is going to be like two. So the verifier's runtime is like something like two log n field operations and evaluating g at one point.
01:38:37.790 - 01:38:43.990, Speaker B: And you're saying L is log? Exactly. Because it's this sort of grand product kind of thing you're doing it on.
01:38:44.080 - 01:39:46.190, Speaker A: No, it's not coming from a grand product. It's because of okay, it's the multilinear extensions, basically. So the G we apply sum check to it will be derived from certain multilinear extension polynomials. And if you say you have a vector of length, capital n or capital M, you wind up and I'm going to actually cover this in a moment, you're going to interpret that as a function over zero one to the log m because there are M evaluations of such a function. And so that's where log M variables comes from log m. Right? And this is sort of the magic of multivariate polynomials in the context of interactive proofs why they don't use univariate, right? Because you can take a vector size m and sort of interpret it as a log m variate polynomial with degree one in each variable. And then you have the subject protocol whose costs grow with the degree in each variable.
01:39:46.190 - 01:40:36.122, Speaker A: And so with no cryptography, you're getting verifier costs that are logarithmic, whereas in univariate, like you have to have degree M to interpret that vector as a polynomial when what matters is like the cost in each variable. You want to use many variables to keep the sorry, the degree in each variable. You want to use many variables to keep the degree low. Okay? So exactly. Because I was anticipating your question about why are there logarithmically many variables, I wanted to really quickly run through just a random application just to convey that perspective, because once you get this, it's literally like one slide to describe generalized Lasso and then I'll stop taking up all your time. So let's consider this counting triangles problem. So here we're given as input the adjacency matrix of a graph.
01:40:36.122 - 01:41:15.562, Speaker A: So that is, there are n vertices out there and there are edges that connect some of the vertices to each other. And so there's like a row of this matrix for each vertex and a column for each vertex. And the ijth entry is one if and only if they're connected. And our goal is to determine the number of triangles, that is, the number of nodes, node triples i, JK that are each connected to each other. So I is connected to j, j is connected to k, k is connected to I say it's an undirected graph. Okay? Now the fastest known algorithm for this runs in matrix multiplication time, which is super linear. So this matrix si is n squared, and it's not known how to solve this problem faster than n to the 2.37
01:41:15.562 - 01:41:43.310, Speaker A: or something. And it's totally impractical to the algorithm that does achieve this. I'm going to give an interactive proof where the verifier runs in time n squared. So the verifier is saving work by using a prover that it doesn't trust. Okay? And here's this perspective. We're going to view the n by n matrix, not as an n by n matrix, but rather as a function on two log n bits. So that is the first log n bits we interpret as like, the binary representation of a row.
01:41:43.310 - 01:42:12.414, Speaker A: And the second log n bits we interpret as the binary representation of a column. And we view A as the evaluation table of this function. Okay, so here's the example. Here's a matrix, and here's how we interpret it. It's got 16 entries. Here we interpret it as a polynomial on zero one to the four, right? Because there are 16 bit vectors of length four. It's just like we're like, oh, this is the evaluation of A at this valuation of A at one.
01:42:12.414 - 01:42:44.808, Speaker A: This is the evaluation of one one. Okay, so here's the protocol. So once we interpret A as a function whose domain is log n bits, it makes sense to talk about its multilinear extension polynomial, because any function on log n bits has a multilinear extension polynomial. And so we're going to define the poly. You remember I said the G we apply sum check to is not a multilinear extension, but it's derived from them. Here's g. So we take that multilinear extension and we sort of take three copies of it and product them together.
01:42:44.808 - 01:43:24.884, Speaker A: So here x, Y, and z are each log n variables. So you think of x conceptually as like indexing one node, y is indexing another node, and z is indexing another node. So together they index like a triangle, like a triple of nodes that might or might not be a triangle. And this just spits out whether or not they're actually a triangle. So this is like asking, is x connected to Y? This is like asking, is Y connected to z? And this is asking, is x connected to z? So if any of these are zero, the whole product is zero. It's not a triangle because two of the nodes aren't connected to each other. If they're all one, it is a triangle.
01:43:24.884 - 01:44:24.600, Speaker A: X is connected to Y, Y is connected to z, z is connected to x. So the goal is the verifier wants to sum up all evaluations of this polynomial over the Boolean hypercube, because summing over the hypercube is like iterating over all triangles, all triples and nodes, and saying, is this triple A triangle or not? So you just apply sum check to this polynomial. And yeah, from the verifier's perspective, this reduces computing the number of triangles to evaluating g at a random point, which is equivalent to evaluating a Twiddle at like, three points. So if the random point is r one, r two, r three, each is log n field elements. You need to evaluate a Twiddle at r one, comma r two, r two, comma r three, and r one, comma r three, and multiply them together. Okay? And these multilinear extensions are really nice. The verifier can actually get each of these evaluations on its own in time, linear in the number of entries of A.
01:44:24.600 - 01:45:05.784, Speaker A: So that's why the verifier is linear time. And you're just going to have to believe me about that right. I'm not going to cover the algorithm. How easy is it to compute the multilinear extension of a polynomial? Or is that something like the prover also needs to prove that this is the right. Yeah, so the verifier only needs to evaluate the multilinear extension at three points and each evaluation can be had in time, linear in the input that we're taking the extension of. But I mean, like, how does the verifier know what the multilinear extension of the input to the verifier is? This n by n matrix. It needs to evaluate the multilinear extension of that matrix at three points.
01:45:05.784 - 01:45:19.404, Speaker A: How does it get the multilinear extension? Like, is it complicated to compute that or is it simple given the matrix? I'm saying there's a linear time algorithm that given a and given a in a desired evaluation point, like r one, r two will compute a total of r one, r two.
01:45:19.522 - 01:45:22.748, Speaker B: You don't write it out explicitly. Crucially, the multilinear extension, but it's not.
01:45:22.754 - 01:45:54.170, Speaker A: Like you ever output it. Yeah, no one ever writes down the evaluation table of this thing because even one coordinate can have field size many values and there are two log n variables of it. Right. So it's like you can't even iterate over all possible values of one coordinate, much less log n. So the prover and verifier both, and I'm hiding lots of details here, but no one ever they only have to evaluate at Twitter at a small number of points. For the verifier, it's three. For the prover, it's order n cubed or something.
01:45:56.700 - 01:46:01.096, Speaker B: Sorry, maybe it's obvious, but the n squared is coming from just to read.
01:46:01.118 - 01:46:02.744, Speaker A: The input is n squared time.
01:46:02.862 - 01:46:03.752, Speaker B: I see, okay.
01:46:03.806 - 01:46:16.316, Speaker A: And the verifier is just doing a constant factor more work than that to evaluate a twiddle at the three points. Okay. It's the best you can hope for linear time and the size of the input, it has to read the input at least.
01:46:16.498 - 01:46:19.632, Speaker B: Okay, and when you say linear time evaluation, you mean n squared time.
01:46:19.686 - 01:47:11.570, Speaker A: Yeah, because the input s size n squared. Sure. Now, the prover here has time n cubed, which does match like sort of the naive triangle counting algorithm that goes over every triangle. There's actually a way, using my matrix multiplication protocol, to reduce the prover time to like as long as it can compute the squared adjacency matrix, which the fastest known algorithms do anyway, it just does order n squared additive extra work to prove correctness. So even the prover, you know, the one downside here is like prover time, which is not bad in that it matches the simple algorithms runtime, but you can actually really match the fastest known algorithms time up to a low order term. So this is kind of the power of sumcheck, but now, let me tell you, generalized lasso. So this is just two more slides, I think, and then we're done.
01:47:11.570 - 01:47:22.340, Speaker A: Yeah. So this time I'm just going to talk about unindexed lookups. Just makes things a little simpler. So the proverbs committed to a vector A, and claims every entry of A is somewhere in the table.
01:47:24.360 - 01:47:27.648, Speaker B: Can you remind us briefly what you're trying to do relative to Lasso?
01:47:27.824 - 01:47:28.912, Speaker A: Generalized Lasso?
01:47:28.976 - 01:47:32.280, Speaker B: Yes. The payoff is going to be what? Compared to what we already saw?
01:47:32.350 - 01:47:51.196, Speaker A: Yeah. So we're going to be able to handle gigantic tables with prover costs like table size to the one overseas, as long as the multilinear extension of the table is quickly evaluatable as opposed to.
01:47:51.218 - 01:47:53.164, Speaker B: Like a decomposability property.
01:47:53.362 - 01:48:26.584, Speaker A: Right? Yeah. Okay. And so basic Lasso itself had this thing where the prover had to commit to this final passover memory over. All end memory cells involved a bunch of counts. Now, if the memory size is huge, most of them are zero. So it's this weird situation where these very sparse, large vectors, you can commit to them quickly, but then the evaluation proofs might be enormously expensive. So we're not happy with that.
01:48:26.584 - 01:49:01.504, Speaker A: So the generalized Lasso will reduce that cost to table side to the one over C, and it will work as long as the table has a multilinear extension that's quickly evaluatable. Actually, it doesn't even have to be multilinear extension. It's any low degree extension. It's fine. Okay. When the approver commits to A, the vector of values being looked up, it's committing to the multilinear extension of A with the polynomial commitment scheme. So that's what the verifier is handed.
01:49:01.504 - 01:49:32.940, Speaker A: It's handed a commitment to the multiliner extension of A. Now, the natural witness in an index lookup, the witness would be baked in because the indices would already be there. But in an unindex lookup, the witness is the indices, like where each AI resides in the table. That's what the proverbs claiming is for each AI, there's some index where it lives. So I'm going to take this witness and I'm going to represent it in extremely sparse form. Something a little funny. We're going to take each index and write it in unary.
01:49:32.940 - 01:50:06.184, Speaker A: So we're going to turn it into a vector of length N. That's like all zeros except a one in the indexed vector. So the picture is sorry, the picture here is if our table is zero up to seven and our lookups are five, zero, twenty four. Five lives in, I guess it's the 6th cell. But in unary it's zero with a one in the 6th entry. Zero lives in the first cell. It's one in the first.
01:50:06.184 - 01:50:36.208, Speaker A: Okay. So that's M so capital M is little M by capital N. Matrix. It's a very sparse matrix. Okay. I'm going to turn this property that AI lives in index J of I, where again, the I throw of M is J of I in unary, I'm going to turn it into a matrix vector multiplication. That's why we define M this way.
01:50:36.208 - 01:51:36.452, Speaker A: By the way, this is the same sort of witness that appears in the Caulk line of work. So we sort of just take the Caulk approach, but use some check based techniques instead of what all the works in the Caulk line of work are doing. So checking that this property holds each AI lives in the appropriate table entry is equivalent to a matrix vector multiplication. So, for example, checking that entry five, the first lookup lives in the appropriate index of the table. The 6th index is the same as confirming that the first row of the table, the table, equals five, because this is why the table is in unary. This one picks out the appropriate index from the table. All right? So, yeah, if you let B equal M times t, so if the prover is honest, b is the same as A.
01:51:36.452 - 01:52:26.384, Speaker A: If it's dishonest, B is different than A. Now, the multilinear extension of B has a really nice expression that just like, follows directly from matrix vector multiplication. So because multilinear extensions are unique, as long as I can show that this right hand side here is a multilinear polynomial that agrees with B over the Boolean inputs, the zero one valued inputs, it must be the multilinear extension of B. And it's clearly multilinear because M twiddle is by definition, multilinear, and we're feeding R only into M twiddle. So we've got something that's quadratic in J, but then we sum J away. What's left is multilinear. So that's what tells you that this right hand side is multilinear.
01:52:26.384 - 01:52:47.324, Speaker A: Right. Again. We're feeding R only into mtotdle. Mtdle is multilinear. So the right hand side is multilinear in R. And sort of by definition of matrix vector multiplication, like, if R is a bit vector, so thereby indexing an entry of B. This is just doing the inner product of the appropriate row of M and T.
01:52:47.324 - 01:52:52.668, Speaker A: It's computing the R entry of M times t. Right.
01:52:52.834 - 01:52:54.604, Speaker B: You're just picking a random row, right?
01:52:54.642 - 01:53:31.368, Speaker A: Yeah. This equality holds even when R is anything in the whole field. But the way to check that it holds over the whole field is to check that it's multilinear and holds over the Boolean hypercube, right? Yeah. So it's multilinear. And the fact that it holds over R being Boolean zero one valued is like just the definition of matrix vector multiplication. Okay, great. And by the short zip elemma, in order to check that Mt equals A, it's enough for the verifier to pick a random R and confirm that B twiddle of R equals A twiddle VAR.
01:53:31.368 - 01:54:32.668, Speaker A: Right? Because if they're not the same polynomial, then they're going to disagree or not the same vector. Then A Twitter VAR and B total VAR will disagree at R with high probability. Okay, so here's lasso in one slide in a second, and then I'll be done. The prover commits to Mtwittle, so that's like cryptographically committing to like a witness, but like a witness in this weird sparse matrix form. It's like the indices that each lookup value resides in but written in unary row by row. Okay? So it commits this polynomial B, picks a random R, and needs to confirm that a twiddle of R equals this thing. This is an expression for B twiddle of R, right? Okay, so conceptually, we sort of reduced, like checking entry by entry that A and B are the same to evaluating the multilinear extension of A Twitter at just one point and the multilinear extension of B Twitter at just one point.
01:54:32.668 - 01:55:32.176, Speaker A: And that evaluation of B twiddle has this nice expression, okay? So the verifier is going to get a twiddle of R from the commitment to a twiddle like that's. What polynomial commitments do is they let you evaluate the committed polynomial at a point, and it's going to get this expression by applying the sum check protocol to compute this sum. So it applies the sum check protocol to the polynomial within log n variables. Its variables are j to sum up the M total of R comma j times t total j to sum up that polynomial's evaluations over the hypercube. And at the end of the sumcheck protocol, the verifier needs to evaluate the polynomial being summed at a random point. So that entails evaluating M twiddle at one point and T twiddle at one point. Okay? So in summary, what sumcheck has done is reduced checking that A equals M times t to checking that a twiddle of R equals something.
01:55:32.176 - 01:56:00.520, Speaker A: That something via some check in turn gets reduced to evaluating M twiddle and t twiddle at a random point each, one point each. Now, as I said, the verifier gets the evaluation of a twiddle from the commitment to a twiddle and the evaluation of Mtoddle from the commitment to Mtdle. And it gets the evaluation of Ttwiddle if it's a structured table just by evaluating Ttwiddle at one point itself, which think of that as like maybe 100 field operations or something for the verifier.
01:56:01.740 - 01:56:07.000, Speaker B: Okay, t twiddle is sort of like your oracle or like I give you that as a subroutine. That's the assumption.
01:56:07.080 - 01:56:58.860, Speaker A: Yeah. So the polynomial being involves both that you're applying sum check to involves both Mtwiddle and t twiddle. So you need one evaluation of each in order to simulate the oracle for the thing you're applying subject to. Okay, now, there are two major details I've hidden here. I mean, this is, this is generalized Lasso, but the two major details are like the first one is Mtwittle is this huge matrix M by n, where N might be two to the two, five, six, right? But it's very sparse. Okay? So when I just say the prover commits to Mtwittle, you really need the time to produce that commitment and the evaluation proof to be linear in not the total number of entries in the matrix, but just the number of non zeros. That's something called a sparse polynomial commitment scheme.
01:56:58.860 - 01:57:41.944, Speaker A: Okay? And so actually, the Lasso gives a sparse polynomial commitment scheme. You've basically seen it. My presentation basic Lasso is basically how it works. It's the first sparse polynomial commitment scheme with sort of optimal approver costs, like, even asymptotically this is coming from. The key to this is that Spartan also gives one, but these counts have to be committed honestly instead of by an adversary. And so it doesn't actually give you like a true commitment scheme where the committer can be totally adversarial. And the way it works is like the prover commits to just the locations of the non zero entries of M, which is this funny thing.
01:57:41.944 - 01:59:02.704, Speaker A: It's like we're taking the natural witness, which is like the indices that the lookup values reside in turning this very sparse form. But then the way we commit to the sparse thing is just by going back to the dense thing. And then when the verifier says, hey, I need to evaluate the committed sparse thing at a point R, what the proverb does is it just proves it correctly, ran an algorithm, that iterates over all these non zero indices and evaluates Mtwiddle at R kind of one term at a time. And the key point is it's a nontrivial algorithm, but all it really does is lookups into a Table. The Table has size n, the number of columns in this matrix, and it's decomposable. Okay, so basically inside the sparse polynomial commitment scheme used in generalized Lasso is Lasso itself, because basically sparse polynomial evaluation involves lookups into a decomposable table, and that's what Lasso handles. So there's some kind of generic reduction from general lookup tables, at least with kind of fast extension polynomials to lookups into decomposable tables, just because the Snarks use polynomial commitments in their guts.
01:59:02.704 - 02:00:20.440, Speaker A: And polynomial evaluation involves structured decomposable tables. And then the final detail is implementing the prover quickly in this sumcheck protocol. Remember the polynomial that sumcheck got applied to this, like M twiddle of R comma j times t twiddle of j thing, that's a log n variate polynomial, and n might be huge, like two to the two, five six. So that's like basically key twiddle is like the extension of such a big vector that it's not immediately obvious how you can implement the prover in this sum check protocol. It's a sum over two to the 256 terms or something, how to implement that quickly? But the key point is, since M is sparse, no matter what the value of R is, this R was chosen by the verifier. When you fix the first log M coordinates of M total to R, the resulting polynomial in j is like, still sparse. So basically, ultimately, sum check is computing the inner product of two giant vectors, one given by M total of R comma j as j ranges over bit vectors of length, log n and the Table.
02:00:20.440 - 02:00:56.500, Speaker A: So those are two vectors of length n, n might be huge, but one of the two vectors is sparse and also, we're assuming that the table has some nice structure to it. You can evaluate a small two inner extension quickly or something. So we show that this is enough to implement the sum check prover in time proportional to m, and you don't get a factor like N. Really? Yeah. Okay, so that's lasso, I hope that was helpful. And I hope you see where we're sort of minimizing the commit. So the cryptography is only being used, really to commit to these multiplicities.
02:00:56.500 - 02:01:00.470, Speaker A: That's it. Okay, thanks a lot.
