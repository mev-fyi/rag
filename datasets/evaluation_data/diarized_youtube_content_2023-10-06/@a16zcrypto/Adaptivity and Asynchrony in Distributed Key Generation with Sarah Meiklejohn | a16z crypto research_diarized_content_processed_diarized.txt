00:00:07.890 - 00:00:08.440, Speaker A: You.
00:00:10.330 - 00:00:32.790, Speaker B: All right, welcome everyone, to this afternoon's a 16 z crypto research seminar. We're continuing with the reunion theme. Today we've got Sarah Michael John, who was always also a faculty fellow with us last summer. Great to great to have you back. She's a professor at UCL and also a senior researcher at Google. And she'll tell us about adaptivity and asynchrony in distributed key generation.
00:00:33.090 - 00:00:49.220, Speaker C: Thank you, Tim and yeah, thanks. I'm very happy to be back and really excited to be talking about this work today. It's very new. Well, it's not very new work. We worked on it for years, actually, as you'll find out in the talk. But these slides are very new. I finished them at like 545 this morning.
00:00:49.220 - 00:01:22.658, Speaker C: So it's going to be an exciting talk, hopefully, but also kind of like, who knows how it's going to go. So please do interrupt with questions. I don't even know how long it's going to take or anything. So yeah, jump in anytime, please. We'll see how it goes together. So, yeah, the main sort of application that this is all driving towards is distributed key generation. I'll spend a lot of the time talking about verifiable secret sharing, which, as you'll find out, is a very important ingredient in distributed key generation.
00:01:22.658 - 00:02:01.334, Speaker C: But first, what is distributed key generation? So the idea is that we want a set of participants to interact together in some way and in this process generate a public key. And the idea is that some subset of the participants here should be able to do some action that the holder of the kind of secret key corresponding to that public key could do. So like decrypt something, sign something. But the idea is that none of them know the secret key themselves. They all just know shares of the secret key.
00:02:01.452 - 00:02:01.782, Speaker A: Okay?
00:02:01.836 - 00:03:04.780, Speaker C: So the kind of classical use case is what I just described. So we have N parties and we want that some t of those parties have to collaborate, have to work together to again, do the action of someone who knows the secret key, like decrypt something, sign some message, et cetera. So for these kind of really classical use cases, right, and this was kind of how I think DKG was introduced. Like, oh, in a company, you don't want to send out some message unless some multiple employees have seen it or with servers, right? You want them to kind of do things together without any one server learning everything on its own. And so for these kind of classical use cases, we really expect to run the DKG only once, right? We're going to set up our servers and we're going to run the DKG and then they can do all their threshold stuff after the fact. So we don't really care if the DKG is efficient, right? It can be very slow, very clunky. It just doesn't matter because we're only going to do it once anyway.
00:03:04.780 - 00:03:11.790, Speaker C: So in more sort of modern applications of a DKG, this does start to matter.
00:03:11.860 - 00:03:12.094, Speaker A: Right?
00:03:12.132 - 00:03:59.582, Speaker C: So one of the examples is using a DKG for a random beacon, right? So the idea here is we run the DKG, we generate the threshold public key, so that's the public key that t of the participants would have to work together to do something with. We then have t of the parties produce a unique threshold signature, also sometimes called like a verifiable unpredictable function. And then the idea is that you can hash the unique signature and this produces randomness. And this randomness is like all the good things you would want it to be, right? Like unbiasable unpredictable. And so this is a nice source of randomness. And so this is kind of an idea like, I would say, happening in Dfinity. And there's also this Drand kind of random beacon running here.
00:03:59.582 - 00:04:24.210, Speaker C: The DKG is like an important part. It's the first step in the three step process and we have to run that first step every single time we want to generate new randomness. So if we want to be generating randomness continuously, we need to be running the DKG continuously. And so now this does start to become important. We don't want the DKG to just be very slow. So here we're running the DKG a lot. We're running it continuously, so we want it to be efficient.
00:04:24.210 - 00:04:31.622, Speaker C: And then also we're running it in a harsh blockchain type environment. So we want it to operate in asynchronous environments as well.
00:04:31.676 - 00:04:31.894, Speaker A: Right.
00:04:31.932 - 00:04:35.850, Speaker C: We don't want all the participants to have to stay online for the whole process.
00:04:36.000 - 00:04:36.698, Speaker A: Yes.
00:04:36.864 - 00:04:46.586, Speaker D: Why do you have to run the DKG continuously for a randomness beacon? I thought for like, Drand, they really only do it if someone joins, which is pretty rare.
00:04:46.778 - 00:04:47.230, Speaker A: Okay.
00:04:47.300 - 00:05:08.390, Speaker C: Maybe for Drand. Yeah, but I mean, basically in Dfinity they're just like signing zero, so there's no entropy in the message that's being signed. And it's a unique signature scheme, so the only thing you can do to change it is really like the public key.
00:05:08.540 - 00:05:15.522, Speaker D: Right, but how often are they actually changing it in what's?
00:05:15.586 - 00:06:11.106, Speaker C: Yeah, I don't know what they're doing today. So I mean, I know that Jens and Victor put out this paper that's like their own proposal for a DKG and they have this Threshold ECDSA signing service and there they're basically like running the DKG and then producing a signature. And that is kind of like an atomic signing process. So there it's also quite important that the DKG, but yeah, as for their renabekin, I honestly don't know what they're doing now. But for this kind of like threshold signing as a service thing, they're definitely using the DKG. So yeah, you want it to be like as fast as generating a signature or it's never going to be that fast, but that's the idea. Okay, so this sort of, I think, trend from kind of like okay, classical use cases we don't care to kind of more modern cases, I think is reflected in the kind of literature that's been published on DKG.
00:06:11.138 - 00:06:11.334, Speaker A: Right?
00:06:11.372 - 00:06:53.366, Speaker C: So there was kind of this original so this is, I should say, asynchronous DKG protocols. There are many more synchronous DKG protocols, but there was kind of this paper by Kate and Goldberg in 2010 that actually is not fully asynchronous it assumes partial synchrony for proving liveness. But then there's kind of been this real, I think, flurry of activity in very recent years, I think, reflecting this kind of renewed interest in these applications and this renewed interest in making these protocols efficient. By the way, one thing I wanted to say, and I forgot to say is as much as we think of a DKG as a cryptographic protocol, or at least I do, because I'm a cryptographer, it's as much a consensus problem as well.
00:06:53.468 - 00:06:53.782, Speaker A: Right.
00:06:53.836 - 00:07:06.410, Speaker C: You need all these participants to agree on what this public key is. And so that's kind of like a hint of things that I'll talk about later, that this isn't purely a cryptographic problem. This is also an agreement problem.
00:07:06.560 - 00:07:08.810, Speaker E: Do you think Ncube is a lower bound?
00:07:12.510 - 00:07:20.720, Speaker C: We can see at the end of the talk what we both think about that. Yeah, I'll remember to weave in little hints and stuff about.
00:07:23.090 - 00:07:23.598, Speaker A: Yeah.
00:07:23.684 - 00:07:52.578, Speaker C: Again, sort of growing interest in this space. I think it's been pushed forward quite a lot recently. And so, yeah, here's the kind of outline that I want to give for a DKG. Not every DKG is going to kind of follow this outline, but this is the outline I want to present. So most Dkgs are based on secret sharing. So secret sharing is a primitive in which we can kind of have one party that we call the dealer share a secret among all the other parties.
00:07:52.674 - 00:07:53.058, Speaker A: All right?
00:07:53.084 - 00:08:33.122, Speaker C: So this is like deal or share, depending on if you're the dealer or the participants. And then the idea is I'm switching now from T to T plus one. The idea is that then if T plus one parties kind of get together and talk about their shares in some kind of way, they can reconstruct the secret itself. And so the most famous example of secret sharing is the kind of original proposal by Arishamir. And so the way it goes is as follows. So to deal a secret, the dealer will form a random degree T polynomial with the secret as, like, the constant coefficient.
00:08:33.266 - 00:08:33.622, Speaker A: Okay?
00:08:33.676 - 00:08:58.190, Speaker C: So basically they say this is the zero coefficient, and then they pick all the other coefficients uniformly at random. Then they evaluate. I mean, there's multiple ways you could do this, but for our purposes, we'll just say that then. So to party I, they'll send the evaluation of this polynomial at I or at the I root of unity, if you care about things like efficiency.
00:08:59.090 - 00:08:59.550, Speaker A: Okay?
00:08:59.620 - 00:09:27.826, Speaker C: And so now each party has one evaluation of this polynomial and it's a degree T polynomial. So if T plus one of the parties get together, then they have T plus one evaluations of a degree T polynomial. And so using LaGrange interpolation, they can reconstruct the polynomial and evaluate it at zero to get the secret. Okay? So let's sort of see how we can build a DKG from secret sharing.
00:09:27.938 - 00:09:28.406, Speaker A: Okay?
00:09:28.508 - 00:09:47.280, Speaker C: So the idea is that basically here we have one party and it's running deal and so right, like its secret share thing is some polynomial that it's made up and then it's going to, just as we said, send evaluations to all the other parties of their index or whatever.
00:09:47.890 - 00:09:48.350, Speaker A: Okay?
00:09:48.420 - 00:10:11.782, Speaker C: And we have all the parties do this in parallel so everyone acts as the dealer. And so at the end of that process, everyone has evaluations of their assigned index from all the other polynomials. So then the idea is that we basically just perform reconstruction in the exponent instead of in the clear.
00:10:11.916 - 00:10:12.454, Speaker A: Okay?
00:10:12.572 - 00:10:20.726, Speaker C: So we can see that instead of right. So with reconstruct, I said you share the evaluation you were given and now you just share it in the exponent.
00:10:20.838 - 00:10:21.450, Speaker A: Okay?
00:10:21.600 - 00:10:55.394, Speaker C: And then we can interpolation and evaluation of a polynomial. This stuff all can happen in the exponent just as it can happen in the clear. So we can interpolate in the exponent to recover each individual polynomial and then we can sum all the polynomials to define the kind of common polynomial, which is just the sum of all the individual polynomials. And then we define the public key to be that evaluated at zero again in the exponent. And so that's the public key.
00:10:55.592 - 00:10:56.002, Speaker A: Okay?
00:10:56.056 - 00:11:30.778, Speaker C: And then I hope that it's reasonably clear to see how you would then do threshold signing or decryption or something like that using all your kind of individual right, because everyone still knows all their points and stuff like that. Makes sense. I haven't said anything bad yet, right. I've assumed that everyone is great, everyone sends all their shares, everyone's shares are good. And that's not how things work in cryptography. So I've left out all the details. I'm going to basically continue to just leave out all the details.
00:11:30.778 - 00:12:38.466, Speaker C: But obvious things that might go wrong are the Flamingo might not get their share right? And that is true possibly because the dealer is malicious, possibly because our network is asynchronous and things just drop. Or for example, the dealer just sends like complete garbage to the Flamingo instead of nothing, right? And then how do we tell that it's complete garbage? So this is why we often use something called verifiable secret sharing instead of just plain secret sharing, which means that basically the dealer is going to commit to all the shares ahead of time and then you can just check against the commitment if your share was good or not. So yeah, that's kind of one thing that we really need for a DKG. And then the other thing, which I'm not really going to get into because we don't really like it, is again there's a consensus problem at the heart of this DKG, which is so in the previous example we just included the contributions of every dealer in the computation of the final public key.
00:12:38.568 - 00:12:38.834, Speaker A: Right?
00:12:38.872 - 00:12:45.270, Speaker C: We summed all the polynomials in the exponent and here we might not want to do that.
00:12:45.340 - 00:12:45.574, Speaker A: Right?
00:12:45.612 - 00:12:59.626, Speaker C: Because again, someone might have given us junk, someone might have not given out the share and so if we include that dealer's polynomial in the public key then we can't actually do anything with that public key because we don't have the shares to do it.
00:12:59.728 - 00:13:00.186, Speaker A: Okay?
00:13:00.288 - 00:13:33.762, Speaker C: So this is why there's this whole complicated complaints round in a lot of DKG protocols where basically we have to kind of, as the name suggests complain about different dealers and be like, oh, they were bad, they didn't give me this share. And then you have to tell everyone that and then everyone has to be like, okay, I heard you and now I don't like that dealer either. And so there's just a big agreement problem here about which dealers are we going to like, whose polynomials are we going to include?
00:13:33.826 - 00:13:34.440, Speaker A: Yes.
00:13:34.970 - 00:13:37.160, Speaker E: What are rounds if we're in an.
00:13:38.090 - 00:13:56.814, Speaker C: Is it's a great question. I'm not a consensus person. So Kennetti and Robin define this notion of asynchronous rounds and we use that one. But what is that definition? I don't know. Yeah, sorry, I don't remember. But we use a well defined notion at least.
00:13:56.932 - 00:13:57.358, Speaker A: Yeah.
00:13:57.444 - 00:14:06.494, Speaker D: Do these typically assume there's a shared bulletin board that they can post to see what everybody posted or are they sort of running from nothing?
00:14:06.692 - 00:14:34.746, Speaker C: Some protocols assume a bulletin board which in my opinion is a little bit of like a cheat but there's a lot of really cool papers that mean you don't have to do it. So for example, we're going to end up using reliable broadcast and we're going to end up using the Dos et al. Protocol from two years ago that is like the most efficient one but usually you try to start from if you can.
00:14:34.848 - 00:14:35.498, Speaker A: Yeah.
00:14:35.664 - 00:15:12.530, Speaker C: So basically just to summarize this kind of like DKG recipe so each party acts as a VSS dealer and deals some secret. Each party participates in the VSS sharing for all the other parties when they're acting as the dealer. Then the consensus question, all parties agree on some set of dealers using this complaints round or whatever. And then for all those agreed upon dealers they then reconstruct in the exponent the sum of their kind of secrets or really like the sum of their polynomials.
00:15:12.690 - 00:15:17.990, Speaker F: According to your experience, which is the most popular VSS scheme used in practice.
00:15:19.130 - 00:15:51.220, Speaker C: Used in practice. I don't know if any of them are used in mean, you know, like Feldman Peterson. These are kind of like the really classical ones that a lot of VSS schemes are derived from mean, I would say. Yeah, the Feldman VSS is a good choice. It's synchronous though. So AVSS is a more active space, I guess. I don't think there's any clear answer there.
00:15:51.220 - 00:16:43.330, Speaker C: Do people ever use KZG? We do. What a great question. It seems like perfectly suited for yeah, well, thanks for that question and it makes us sound very natural. Yeah, that's exactly what we're going to do. So basically, hopefully I've convinced you that even though this talk is about DKG, I'm going to have to talk a lot about VSS because basically from this recipe you can see that VSS is like an extremely essential ingredient, kind of the only ingredient aside from consensus, which is also very important. And so really it seems like the best way to get a better DKG is just build a better VSS and that's exactly what we're going to do. So yeah, I'm going to now talk about a protocol that we'll have appearing in crypto this year.
00:16:43.330 - 00:17:05.210, Speaker C: This is with Attai, Abram, Philip Yovanovich, Mary Mallor and Gelad Stern. And this is a protocol that we call bingo. It has many adjectives. It's kind of like ridiculous when we it takes up like multiple lines in the paper. It's really embarrassing. So it's an AVSS. So an Asynchronous verifiable secret sharing scheme.
00:17:05.210 - 00:17:31.822, Speaker C: It allows secrets to be packed. That means that we can share F plus one secrets with the same complexity as it would take to share one secret. It has optimal resilience meaning N equals three. F plus one. F is the number of adversarial corrupted parties. It has N squared word complexity and constant round complexity. And it allows this is the most important contribution.
00:17:31.822 - 00:18:17.882, Speaker C: So if you remember nothing else, it allows for adaptive corruptions. So we sort of view adaptive corruptions as equally important in some ways in an asynchronous blockchain style environment as anything else. Right. If you have a set of parties who are sort of controlling an important functionality, whether it's some proof of stake type component, a random beacon or something like that, there's going to be a lot of incentive to corrupt those parties. And if the set of parties is sort of relatively stable over time, then you can actually expect really that corruptions will happen adaptively as the protocol is running. And so having these adaptive corruptions meant we have new definitions VSS termination. Correctness.
00:18:17.882 - 00:18:24.478, Speaker C: And secrecy. I won't talk about those at all, but they're in the paper and we worked a lot on them so I'm happy to talk about them offline.
00:18:24.574 - 00:18:24.882, Speaker A: Yes.
00:18:24.936 - 00:18:27.438, Speaker E: Do you support any thresholds between F and N?
00:18:27.544 - 00:18:42.120, Speaker C: Yes, actually we do, which is really exciting. Yes, we support high threshold reconstruction or arbitrary thresholds between F and two F actually, I think. Yeah, but yeah, so yeah, we're very excited about that part.
00:18:42.430 - 00:18:44.502, Speaker G: What do you mean by word complexity?
00:18:44.646 - 00:19:22.710, Speaker C: Oh, you and Miriam are asking the consensus questions. So yeah, for the camera ready? We were debating this. So the idea is a word is a kind of fundamental object in a message. So it's things like counters or indices and basically any cryptographic object is going to require like o of lambda where lambda is the security parameter words. So the idea is message complexity is a kind of quite broad measure. Messages might be like really large. So word complexity kind of breaks this down.
00:19:22.710 - 00:20:00.770, Speaker C: So if your messages are like n squared and you send n squared messages, then your word complexity will be n to the fourth kind of, I don't know, we spent like a good 20 minutes yesterday trying to pin this down and it wasn't very convincing and my answer is not very convincing as a result. Well, because when you were showing previous work, you were also showing their work. Oh, that was for the DKG. This is just the VSS for now. So yeah, the kind of best known VSS all have n squared word complexity as far as I know. I don't think anyone has slower than that. Yeah, this is just the VSS.
00:20:00.770 - 00:21:04.626, Speaker C: Any other questions? Okay, I will now attempt to tell you how bingo works and we'll find out if I did a good job. Okay, so the idea is how does dealing work? So the idea is that a dealer is going to uniformly sample a bivariate polynomial and embed its secrets in this way. Okay, so we have the bivariate polynomial phi and we're going to embed the secrets as the evaluation of phi at minus k and zero. Okay, and why do we do this? Hopefully it'll become sort of clear, but yeah, that's what we do. And then the idea is that again, this is like a packed VSS, so we can just sort of put in up to like F secrets basically. Yeah, makes sense. And then the idea is we're going to I guess I should have a star next to broadcast as well, right? Because what does broadcast mean? So we broadcast a commitment to this polynomial.
00:21:04.626 - 00:21:52.866, Speaker C: So I'm not going to talk about the commitment at all in this talk except to just refer to it very loosely. But yeah, we had to design a custom commitment for this VSS. We're not just using something off the shelf but it is like a very natural extension of KZG, this commitment scheme to bivariate polynomials. So it's just kind of like if you think, hey, KZG and then you think how would I do KZG for bivariate polynomials? Hopefully you'd roughly come up with what we have in the paper as well. And then what you do is you set these kind of row polynomials and column polynomials. So you have alpha i, which is the evaluation of phi at x and i. So x is like the loose term.
00:21:52.898 - 00:21:53.190, Speaker A: Okay.
00:21:53.260 - 00:22:32.850, Speaker C: And so that's kind of the row here in that thing over on the right. And then you have the beta polynomials, which are like the corresponding column polynomials. Okay, so it's just like fixing one. I guess I shouldn't have called that x. That should be a y, but yeah, you're basically fixing one of the terms and then so, yeah, alpha and beta are kind of the row and column univariate projections of phi. And so really, really importantly, this means that we have this relationship between the alpha and the beta polynomials, right, that alpha I of j is equal to beta j of i.
00:22:32.920 - 00:22:33.154, Speaker A: Right?
00:22:33.192 - 00:22:45.254, Speaker C: And hopefully it's very clear why that's true. And yeah, I apologize, my matrix over there doesn't reflect that the upper triangular part is basically equal to the lower triangular part because of this property.
00:22:45.452 - 00:22:45.862, Speaker A: Okay?
00:22:45.916 - 00:22:51.020, Speaker C: And then what the dealer is supposed to do is send alpha I to i.
00:22:51.790 - 00:22:52.298, Speaker A: Okay?
00:22:52.384 - 00:23:51.946, Speaker C: So the goal basically of the share protocol is for each party I to learn their alpha I polynomial, okay? So there are kind of two paths by which the party can learn their polynomial. So the first one is the obvious one, I guess, the happy path, right? We say at the end of deal that the dealer sends alpha I to party i. So if they just send alpha I to party I and it gets there, then we're done, right? We've achieved our goal, and that's great. Of course, we can't just assume that's going to happen for every party. Again, maybe the dealer is malicious. Maybe the network is definitely the network asynchronous, so maybe the message just gets dropped and it doesn't get there. Okay, this is just the environment we're operating in, but of course we have to assume that some messages get through and are well formed and stuff, right? Otherwise we would just reject this dealer's contribution and it's not interesting.
00:23:51.946 - 00:24:17.060, Speaker C: So let's assume that sort of three of these other birds got their polynomials, okay? So what that means now in particular here for the parrot, is that means that they know all those five evaluations of their row polynomial, right? They know the whole polynomial, so they can evaluate it at 1234 and five and get those points.
00:24:17.430 - 00:24:18.180, Speaker A: Okay?
00:24:18.730 - 00:24:56.062, Speaker C: And so what they can do is send those points to the parties that they kind of correspond to, right? So basically, they send alpha five of j to party j, and they say, here you go, this is like your evaluation of my row polynomial. And now we're going to use our super important relation between alpha and beta. What this means is that party j learns an evaluation of their column polynomial of their beta, okay? So they didn't know beta before. No one sent it to them or anything, but they can actually learn it by being sent these points by the other parties.
00:24:56.206 - 00:24:56.562, Speaker A: Okay?
00:24:56.616 - 00:26:13.958, Speaker C: So if the three parties who got their alpha polynomials send them, then at some point, if they're given this by enough other parties, then the seagull here can actually just interpolate to learn their column polynomial. And so now they know, right? So just like before, if you get your alpha polynomial, you know all the evaluations in your row. Now if you get your beta polynomial, you learn all the evaluations in your column. And so we just do the exact same thing again, right? Now, the Seagull here sends out those points to the relevant parties. They send beta one of J to party J, okay? And then again, party J is learning something about their alpha polynomial, right? Because we're just like going back and forth between the alphas and the betas. And so now, super crucially for the vulture here, they didn't know this evaluation of their alpha polynomial because they hadn't gotten their alpha polynomial yet. But if they get these beta evaluations from enough other parties, then they can interpolate their alpha polynomial and they can learn it bingo.
00:26:13.958 - 00:27:22.960, Speaker C: That's why the protocol is called bingo, right? Yeah, it's whatever, we're academics. This is the other more tortured way that a party can learn their row polynomial is by the help and support of enough other non faulty parties. This is basically, in a nutshell, how bingo works. Of course, there's a lot of complexities that I have hidden from you in this talk, so one of them is pretty hopefully somewhat obvious. Parties don't just send out evaluations. They have to prove that they're correct, right? And it's more work than it sounds for the commitment, right? So KZG already has this notion of evaluation binding, which basically says that if I give you an evaluation, then it has to be the right evaluation, basically against a commitment here, it's a little complicated because what we have is a commitment to the Bivariate polynomial. From that, we can pretty easily derive commitments to the alpha polynomials just by doing like a DfT, basically.
00:27:22.960 - 00:28:15.082, Speaker C: But then when parties are sending out evaluations of their beta polynomials, we don't have commitments to the beta polynomials, so it's all a little awkward, to be honest. And so we ended up defining a new notion of what we call interpolation binding, which basically says that if you get a bunch of evaluations that all passed verification. Evaluations and proofs. There's always proofs that pass verification. Then the polynomial that you get by eventually interpolating those points is the polynomial that was contained in the original commitment. So it's a little more involved referencing the talk from this morning to prove interpolation binding, we end up needing the AGM. I think compared to Fatini, we take a much more loose attitude to the AGM.
00:28:15.082 - 00:28:47.942, Speaker C: I mean, I'm personally completely fine with it. I think I'd rather use the AGM than make up some knowledge of exponent assumption and use that. Yeah, I don't know. I mean, anyway, this is a whole separate conversation, but we use the AGM and we're completely happy about it, and we don't really worry the other complexity, of course, as I said, oh, that dealer just broadcasts that commitment. So there we use, again, this dos. It all reliable broadcast protocol. And then, of course, the other huge complexity is adaptive corruptions.
00:28:48.006 - 00:28:48.186, Speaker A: Right?
00:28:48.208 - 00:29:40.220, Speaker C: And how do we deal with this? Dealing with it in the Dealer is not so complicated because dealing is just like an atomic process. Once you've either dealt or you haven't. Corrupting after the fact is not super interesting. But yeah, for PhD students in the room who maybe bang their heads against the wall with proof, I mean, this took us like two years to prove. We have so many 40 page proofs that just went in the garbage, I'm laughing because I can't do anything else. So, yeah, this was an extremely involved proof for a deceptively simple looking protocol. So, yeah, just to give a little insight into the process here as well.
00:29:40.220 - 00:30:37.150, Speaker C: But, yeah, so those are the kind of things that actually make this all a little difficult. Okay, so that's sort of sharing in bingo. And that's the really hard part. Reconstruction then is hopefully reasonably natural, given how I said, how it works for Shamir and how it kind of works in general. So if we remember how the secrets were embedded in the Bivariate polynomial, the idea is that basically everyone can just right? So no one has this, like, minus case polynomial, but every party can evaluate alpha j at minus k. So, notably, we can perform reconstruction for only one secret at a time, right? So remember, this was all packed, and so we shared k secrets or M secrets at the same time. So here we're reconstructing just the case secret.
00:30:37.150 - 00:30:53.570, Speaker C: And so we share alpha j of minus k. And then same old trick is given enough evaluations like that, we can then interpolate the beta minus k polynomial, and then we can evaluate it at zero to recover the original secret.
00:30:53.910 - 00:31:01.320, Speaker F: Maybe I'm missing something, but doesn't the Dealer need to prove that this matrix is symmetric? If yes, how does the dealer do it?
00:31:02.490 - 00:31:32.014, Speaker C: The matrix is for the slides. Like, there is no matrix. All the dealer polynomial. So all the Dealer does is provide a Bivariate commitment, and then you can deterministically derive the alpha commitments using a DfT, basically. So, yeah, there's no trust in we need a notion of, like, polynomial binding, which says that if you commit to a polynomial, then later when we all.
00:31:32.052 - 00:31:45.330, Speaker F: Like, but don't you need that this vibrated polynomial satisfies this relation like, that it's symmetric. Or alpha j minus k plus beta minus KJ.
00:31:46.230 - 00:31:57.302, Speaker C: No, because again, these are just projections of that polynomial. So it just holds. Like it can't not hold.
00:31:57.356 - 00:31:57.574, Speaker A: Right.
00:31:57.612 - 00:32:17.674, Speaker C: We're like deterministically deriving these things in our minds. They don't exist in any other way. So this just holds inherently, there's no trick they can pull with the Bivariate commitments such that this wouldn't hold. Yeah. Did you have a question?
00:32:17.792 - 00:32:20.782, Speaker E: Sorry, mentioned this is one round or no, maybe.
00:32:20.836 - 00:32:39.140, Speaker C: I'm a constant round because obviously, well, I don't want to count the rounds. I thought I should do this before giving the talk. But yeah, I mean, you need the rounds for everyone to share their alpha evaluations and then share their beta evaluation. So it's three or four rounds or something.
00:32:39.670 - 00:32:45.494, Speaker B: But constant the polynomial does not need to be a symmetric polynomial, is that right? For this to work?
00:32:45.532 - 00:32:50.806, Speaker C: No, the only thing the polynomial, the bivariate polynomial needs to satisfy is this, right?
00:32:50.828 - 00:32:55.434, Speaker B: So the alpha beta relation, that's literally just like two different notations for the same number, right?
00:32:55.472 - 00:32:56.282, Speaker A: Exactly. Okay.
00:32:56.336 - 00:33:23.326, Speaker C: Yeah. We're just sort of making it up in our minds. Okay, so then that's how we can reconstruct just like the KTH secret. Basically. Something really cool, I think, about this protocol is that we can actually do like, advanced reconstructions. So let's sort of consider now reconstructing across multiple sharings. So multiple different dealers.
00:33:23.326 - 00:34:25.938, Speaker C: So let's denote by alpha j i, the alpha polynomial that party j gets with I as the dealer. So let's imagine now we want to reconstruct the secret across all the different dealers. So what the parties can do, basically everything's just like homomorphic, right? So what the parties can do is they can compute the kind of, like across all dealers polynomial alpha j, which is just the sum of the individual alpha j eyes. And then they can just do the exact same thing, right? They can share alpha j of minus k interpolate beta k, and then evaluate beta k at zero. So what this is reconstructing is actually the sum of secrets across all the different dealers. And notably, we can do this with the exact same complexity as we could for reconstructing one secret, right? We're just sharing one point, basically. But that point is like representing all of these different dealers.
00:34:25.938 - 00:34:47.342, Speaker C: Does that make sense? Okay, so basically here we're reconstructing the sum of secrets across different sessions, and then we can also do the same thing for the batch reconstruction of multiple secrets, right? So one dealer is sharing like f secrets at a time. If we want to batch reconstruct all their secrets, we can also do that with the same complexity too.
00:34:47.476 - 00:34:47.726, Speaker A: Okay.
00:34:47.748 - 00:35:28.198, Speaker C: And that's in the paper if you're curious to find more. Okay, so that this is again a nice feature. Again, if it is easier for you to think about it, if I just say like, oh, basically everything's just like linearly homomorphic, then that is true, and that is kind of why all this advanced reconstructions are possible. Okay, so if we now try to think about how do we take bingo and build a DKG out of it, then let's revisit our kind of like recipe that we had, right? So everyone acts as the VSS dealer. Everyone participates in the VSS sharing. Well, this is pretty easy to substitute in. We just use bingo for those parts, right? So we use the bingo dealing protocol.
00:35:28.198 - 00:35:41.918, Speaker C: We use the bingo sharing protocol. Notably, this idea of reconstructing the sum of secrets in the exponent is super easy in bingo because of this advanced reconstruction property that I just talked about.
00:35:42.004 - 00:35:42.350, Speaker A: Right?
00:35:42.420 - 00:35:53.998, Speaker C: So notably, reconstructing the sum of secrets in the exponent looks like just doing what I just showed you, but with G to the stuff instead of just sharing the points directly.
00:35:54.094 - 00:35:54.354, Speaker A: Right?
00:35:54.392 - 00:36:30.880, Speaker C: And so this is basically possible. What this means is that we can do this by sharing one evaluation point rather than having to share N and then everyone figures out how to sum them themselves. Okay, so this is like a really nice thing for the DKG specifically. And then the other kind of big thing that we do is we don't use a complaints round, we don't want to use a complaints round. They're complicated, they're really hard to argue about. So what we use is something called a Vava protocol. I'm not really going to talk about this because I'm one of the cryptographers in the collaboration, not one of the distributed systems people, so I'm not really the right person to tell you about it anyway.
00:36:30.880 - 00:37:18.318, Speaker C: And I've already, I think, kind of like done enough throwing things at you. So I'll just really briefly highlight kind of what Avaba is and how we end up using it in the DKG and kind of the path to building it and then I can try to answer questions about it or just point you at my co authors if you really want to hear more. So what is Avaba? Avaba stands for a validated asynchronous Byzantine agreement protocol. And the idea is that it brings in this notion of external validity into an agreement protocol. Okay, so basically we have the kind of like standard notion of termination and correctness, meaning all non faulty parties eventually complete the protocol and all non faulty parties who complete output the same value.
00:37:18.404 - 00:37:18.702, Speaker A: Okay?
00:37:18.756 - 00:38:28.126, Speaker C: So in that sense, it's an agreement protocol. And then we have this notion of validity though, which basically says we have some external validity function check validity and that the value output by all of these non faulty parties is also going to be valid according to this external validity function. So in the context of the DKG, what we're after with this validity function is we basically so remember what we're using the complaints round or the Vava for. We're using it to agree on a set of dealers whose contributions we're then going to use in the final public key. So what we have is we have the set of dealers and then we have an associated set of signatures. And these signatures kind of come from any parties who say, I have completed the share protocol with that dealer and I think I have the correct shares and I am happy for their contribution to be used. So the idea is that we need at least F plus one dealers in order to be ensured that we have at least one honest dealer, we need then at least F plus one signatures.
00:38:28.126 - 00:38:35.158, Speaker C: And then we need all those signatures to be valid, right? Like we need them to pass verification and that's basically it.
00:38:35.324 - 00:38:35.750, Speaker A: Okay?
00:38:35.820 - 00:39:22.194, Speaker C: So if we have F plus one dealers, then we know that there's at least one honest dealer. If we have F plus one signatures, then we know that at least one honest party completed the dealing sharing with all of these dealers. And that's how we define our validity function for the DKG. Any questions about this? So hopefully this is like because I don't think I'm going to give much more of an outline of how the DKG works. So basically everyone runs share, everyone does deals all at once, runs share with everyone. Once they've completed a sharing session with someone, they'll add that dealer to their set and they'll sign off on their index or whatever. And then all parties enter the Vava with their set of dealers and their set of signatures.
00:39:22.194 - 00:40:02.930, Speaker C: And then the Vava ensures that we all agree on one set of dealers who people have signed. And then with those dealers we do the whole thing I described of reconstructing the sum of their secrets in the exponent. Makes sense. Okay, so in terms of how we build this Vava, we draw very, very heavily on a result that we had at Podc in 2021. The same authors plus Alan Tomescu. And so I'll just again really briefly hint at the outline of how this works. So the first kind of thing we build is what we call a Verifiable gather protocol.
00:40:02.930 - 00:41:04.642, Speaker C: So this kind of says that any set, so parties will have be like this is my set, this is my set. And the idea is that all of these sets, any set that will pass verification and kind of that other parties will like in some sense has to be a superset of some common core, right? So there has to be some common elements in all their sets, like some overlap. So again, it's kind of like vague notion. And so we build this based on reliable broadcast. Then what we do is we combine Verifiable gather with a kind of threshold VRF, like a local notion of threshold VRF, to build a proposal election protocol. So proposal election protocol says that basically if every party comes in with their proposal, then we want a protocol to select one of those proposals uniformly at random. You could also think of this as like a leader election protocol.
00:41:04.642 - 00:42:10.646, Speaker C: And what we actually start with is like a weak leader election protocol, which means that the probabilities around outputting dealers are very low. Like we don't have a very strong guarantee about the leader of the proposal that we're going to output. But anyway, so again, very vaguely, this is kind of what we build on top of Verifiable gather and then we build kind of this Vava protocol on top of this proposal election protocol. And so in this podcast paper, this is called like, no Win hot stuff built on top of the hot stuff, like consensus protocol. And so this was the outline we used again in this paper, and we were using a DKG that basically we were using like a synchronous DKG that was at Eurocrypt 2021 in the kind of proposal election part. And so here what we're doing is really, in a nutshell, we're just sort of substituting in for that synchronous protocol that we used in sorry for that. So in the Eurocrypt paper, we had like a publicly verifiable protocol.
00:42:10.646 - 00:42:40.482, Speaker C: So we just kind of substitute in for that PVSS. Bingo. And that gives us an adaptive proposal election protocol. And then it just magically works out that then we can build an adapt. Well, it's not magic, it's just that the Vava then inherits the adaptive security of the proposal election protocol as well. Okay, so basically by just substituting in bingo. For this old PVSS that we had, we can get an adaptively secure VAPA.
00:42:40.482 - 00:43:15.738, Speaker C: And the VAPA is what's, N cubed words. And I'm not enough of an expert in consensus to say, do I think we can get that lower? It certainly seems really hard to get that lower if you look at how these protocols work. But yeah, that's why I kind of think N cubed is a natural thing to aim for and getting below it would require radically different agreement protocols. And yeah, seems hard to me, that's for sure. Yeah. So basically the VSS, as I said, like, the best word complexity is N squared. For the Vava, the best is N cubed.
00:43:15.738 - 00:43:56.010, Speaker C: And so that's why the DKG is Nqd. Because again, the DKG is really an agreement problem and a cryptographic like VSS problem. So, yeah, that's kind of it. So in terms of our sort of work, so we matched the kind of best known word complexity and round complexity. Yeah, we have these high threshold reconstructions which we're very happy about because earlier versions didn't and it was another tortured path to get there. Very natural in the end. So, yeah, we can support arbitrary reconstruction thresholds and then yeah, I mean, the main downsides, well, apparently some people think that using the AGM is a downside.
00:43:56.010 - 00:43:59.294, Speaker C: So there you go. But obviously you can see that.
00:43:59.332 - 00:43:59.534, Speaker A: Yes.
00:43:59.572 - 00:44:07.886, Speaker C: So we are the first protocol to be proved adaptively secure, but you can see the inverse in that trusted setup column.
00:44:07.918 - 00:44:08.066, Speaker A: Right?
00:44:08.088 - 00:44:53.102, Speaker C: So because we're using KZG, we do use a KZG setup. I will say Mary was incredibly careful to make sure that we could reuse the KZG ceremony. So we can just reuse their trusted setup. We don't need something new for the bivariate polynomial. That was another like, six months of work. But if you think that's a problem, then you can see there's kind of like an open question here of can we get adaptive security without some kind of trusted setup? But at least for us, we can bootstrap off of existing setup ceremonies, which we're pretty happy to leave it at that. So this previous work, we were pretty happy with that too.
00:44:53.102 - 00:45:07.582, Speaker C: But the big caveat there was that the secret key was a group element rather than a field element for the DKG. So for stuff like BLS, it wasn't going to work here. We also get around that and our secret key is a field element. So this is compatible with BLS.
00:45:07.646 - 00:45:08.318, Speaker A: Yep.
00:45:08.494 - 00:45:15.094, Speaker F: Does the AGM I'm sorry, does the AGM comes from the commitments, right? From the polynomial commitments. You needed it. That was the place where you said yeah.
00:45:15.132 - 00:45:24.458, Speaker C: So the only place we actually do end up using the AGM is to prove interpolation binding for our KZG, like bivariate polynomial I don't remember what are.
00:45:24.464 - 00:45:28.262, Speaker F: The actual assumptions that the original commitments themselves for their normal binding?
00:45:28.326 - 00:45:29.290, Speaker C: Qsdh?
00:45:31.870 - 00:45:33.514, Speaker F: There's no way to get to like.
00:45:33.552 - 00:46:50.322, Speaker C: USDH or something from okay, if we're going to talk about assumptions. So the other thing is we end up proving not like a nice it's really annoying that I think we should have done it differently, but rather than proving, like secrecy or a kind of general standard property for the DKG, we tried that and failed. So we proved this notion of oracle aided algebraic simulatability, which Julian and his student Renas have. It's like a CCS paper from last year. I think my suspicion is that this is the same thing as proving secrecy or key expressibility, which is this other secrecy like property for the DKG in the AGM, using the one more discrete log assumption. This is what I kind of wish we had tried to prove ourselves, because I think their definition is basically that proving key expressibility using omdl in the AGM. So their paper is a really cool paper and it kind of shows that it seems like it's impossible to prove this kind of stuff without using omdl.
00:46:50.322 - 00:47:29.566, Speaker C: So forget like Qsdh right. Omdl is a very strong assumption, I think. So they hint at an impossibility around proving this kind of stuff without the omdl. I can certainly tell you from years of trying to prove it that I'm happy to believe them on that. Right. I would be very impressed by a proof that just used any subset of the things I've just said without having to use all of them. I would be surprised or I don't know if I'd be surprised, I'd be impressed for basically, yeah, we're using a lot of tricks.
00:47:29.566 - 00:47:50.294, Speaker C: We're using AGM, the equivalent of using omdl, which is this weirdo notion of oracle aided algebraic simulatability, which is a real mouthful, but we proved it, we have a proof and we're really happy about that. So, yeah, that's actually the end of the talk. Thank you very much. And any other questions? Happy to take them.
00:47:50.492 - 00:48:13.200, Speaker E: Quick question about the setups. You're saying you might be reusing this forced setup strings that Ethereum is constructing. So that setups, like, say one of them is 1000 elements of Tau, powers of Tau. It means that you can support a threshold of 1000 or like no, does it?
00:48:14.690 - 00:48:24.146, Speaker C: Maybe I'd have to go back and look at the commitment because it would.
00:48:24.168 - 00:48:25.550, Speaker E: Be nice to have flexibility.
00:48:25.630 - 00:48:28.802, Speaker C: Yeah, of course. Does it mean that's all?
00:48:28.856 - 00:48:37.746, Speaker E: Yeah, because the degree of the polynomial, I guess, determines how many secrets you need to reconstruct. So natural feeling is that it should translate.
00:48:37.938 - 00:48:48.314, Speaker C: Yeah. I mean, there must be some relationship there, whether it's like, degree Tau or not. I could look later. Yeah.
00:48:48.512 - 00:49:16.994, Speaker G: You said that the biggest motivation for this work was to actually get adaptive security. So I was wondering, is there any notion of semi adaptivity here? So in some of these out of orders, proof of stake protocols, they have this semi adaptivity on corruptions. That person had some time to do corruptions. And I'm wondering whether it's essentially some lighter assumption on I don't feel like.
00:49:17.032 - 00:49:23.926, Speaker C: It would make it any easier to be honest. Right, because you're still going to have to consider I'm basically trying to see.
00:49:23.948 - 00:49:30.806, Speaker G: If this could remove the setup somehow, because this seems to be one of.
00:49:30.828 - 00:50:20.280, Speaker C: The I don't feel that the adaptivity is tied that much to the setup, to be honest. I think the setup is just like we don't really care that much about having the trusted setup. And it's just like the most natural polynomial commitment in a lot of ways. So I don't feel like we could get adaptivity because we used KZG. I don't feel like those two are so closely tied. So yeah, I think if someone else could come up with a setup free, I feel that it's more closely tied to the efficiency, to be honest, that if you don't have KCG, like, things just might not be as compact or you might need more rounds to do things. I feel like that is more of a thing than the adaptive yeah.
00:50:23.450 - 00:50:23.766, Speaker A: I.
00:50:23.788 - 00:50:32.420, Speaker C: Don'T feel like you couldn't get an adaptive thing if you use something other than KCG. I just feel like other things would get worse. Great, thanks a lot.
