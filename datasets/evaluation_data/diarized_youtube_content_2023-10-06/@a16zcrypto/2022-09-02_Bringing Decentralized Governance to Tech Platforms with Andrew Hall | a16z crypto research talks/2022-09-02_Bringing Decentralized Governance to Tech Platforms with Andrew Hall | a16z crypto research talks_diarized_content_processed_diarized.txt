00:00:10.320 - 00:00:25.444, Speaker A: All right, so welcome everyone. Today's agency crypto Research seminar. Very happy that we're being back. Hopefully you saw us talk on Thursday, but he's back again today. Andy hall from Stanford University will talk about bringing decentralized governance to the platform.
00:00:25.642 - 00:00:52.476, Speaker B: Thank you. Long time no see. Cool. So let me give you a sense of what I'm thinking about what I'm talking about today. It's a fairly conceptual talk. Please do interrupt with questions throughout. So I don't know if any of you saw this video or recall it, but this is part of an Epic pun intended conflict between Epic Games, the maker of Fortnite and Apple that is actually still ongoing.
00:00:52.476 - 00:01:56.544, Speaker B: And roughly speaking, the timeline was that Epic intentionally snuck in a way to get around the App Store's payment requirements. Apple then removed them from the App Store, and then they sued Apple. And on the day they sued Apple, they also released this kind of amazing video, which is a parody of a really famous old Apple television commercial about Big Brother in 1984. And they're kind of implying that in its role as the steward of the App Store and the sort of monarch of the App Store, if you will, that Apple has taken on the role of Big Brother. And there's some, I think, remarkable quotes from one of the legal filings that Epic made. Of course, take these with a grain of salt because they're from Epic's point of view that expose, I think, some really important governance issues with modern platforms that I'm super, super interested in, both in terms of my academic research as well as my work in tech. And so just two quotes I wanted to read out quickly, both from this legal filing.
00:01:56.544 - 00:02:59.240, Speaker B: Quote, developers have faced an inefficient and opaque app review process riddled with arbitrary decisions and errors coupled with poor customer service. As Apple has gathered more and more users into its ecosystem and locked them in, the importance of the Apple ecosystem to developers has increased to the point that nearly all developers rely on Apple. But Apple does not need to rely on any single developer. I'm super interested in this really important conflict because I think it exposes some very core governance issues that we've experienced in other parts of society in the past that have to do with when you're subject to a system. Of rules that you didn't play a role in shaping and that if you don't have sufficient outside options, you might have no choice but to live under. And this problem of how these large tech platforms are creating these systems of rules and enforcing them, it's ubiquitous across the space. So I want to give you a few more examples to show you why this is important.
00:02:59.240 - 00:03:52.772, Speaker B: So, of course, a totally different example from Apple. Here's Twitter banning the, at the time sitting President of the United States. And maybe somewhat surprising given the politics involved, quite a few global politicians actually came out and said, we don't think this is a good idea. And so in particular Merkel, as well as Macron and others in the EU, who again, on the basis of their politics, might have thought maybe Trump shouldn't be on social media. They're saying that this is actually very problematic, that a private social media company can just set rules, arguably arbitrarily, and knock someone as important as the President of the United States off their platform. And because so many users might be locked into this platform, that might have a meaningful effect on who's able to see what. This is not obviously limited to Twitter.
00:03:52.772 - 00:04:39.704, Speaker B: We know that the exact same issues play out on Facebook, on Instagram, et cetera. I just want to highlight, though, the issues are also not limited to sort of like binary rules that you write down. They're also about algorithms. And so another long running concern of this nature has to do with Facebook and the way the newsfeed, the ranking algorithm, prioritizes some content over others, which then creates incentives for news producers. And news producers actually arrange their businesses to do well under the newsfeed ranking algorithm. And then when the algorithm changes, it sort of picks winners and losers, sometimes accidentally, sometimes people have argued, maybe intentionally. And so people are again, it's another governance problem that people are quite worried about.
00:04:39.704 - 00:05:25.530, Speaker B: So how do you create these? Ranking algorithms is affecting a bunch of people in unpredictable ways. This also isn't just about content on existing platforms, it's also about platforms of the future. So we're already seeing this play out. I see this in my own work at Meta. Lots of difficult issues about how VR platforms are going to regulate their content or conduct, as the case may be. I also want to highlight it's not just about Web Two platforms, it's also about these newer platforms that cater to Web Three, but that have certain Web Two characteristics. And so you think about, for example, Coinbase, which has faced its own difficult content moderation issues.
00:05:25.530 - 00:06:42.080, Speaker B: Or you think about OpenSea, which in addition to these content moderation issues, is also exploring actively these sort of automated techniques for content moderation at scale, similar to what Meta and others have done. And so this is a super wide ranging set of problems that I would describe as platform governance, which is something that I've spent a lot of time working on and I'm super interested in. And part of what's drawn me to Web Three, I would define it as the system of algorithms, product architecture, rules, rulemaking procedures, appeals, processes, and other institutions that shape how platforms are used. And the thing I want to talk about, which is a continuation of what I talked about last week, is that today, in the way these platforms are governed, they're governed largely like monarchies. And I talked last time about some of the pros and cons of that. But basically this idea that platforms are allowed to sort of unilaterally set all of these rules and to govern their platforms as they see fit. Obviously subject to a variety of kinds of regulation, but with a lot of latitude to decide how they create their rules, how they enforce them, when they enforce them, what kinds of appeals you have to the rules, how they rank content, et cetera, et cetera.
00:06:42.080 - 00:07:45.350, Speaker B: And what I want to kind of focus on today is two potential benefits that I flagged last time to bringing more stakeholders, more users, creators, developers, et cetera, into the process that governs these platforms. And so, as I mentioned last time, I think there's two potential key benefits to exploring this. First, I think there are arguments for why you might actually be able to encourage more growth and innovation on these platforms if the people doing the innovating and selling their products or developing their products are actually involved in the governance of the platform. And also for these big legitimacy issues that we're increasingly facing, especially with social media platforms, but also with other types of platforms. These, I think, are two potential benefits to democratizing web two platforms in different ways. So I'm going to talk about those two in order and please do stop me at any time. Okay, so let's talk first about platforms, economic growth and what's known in economics as the holdup problem.
00:07:45.350 - 00:08:46.104, Speaker B: I'm going to draw here on some work that I do with a frequent collaborator of mine, Ethan Bueno De Mesquita, who's a professor at the Harris School at UChicago and is a political economist and game theorist. Our argument in this piece that we're kind of trying to build out into some new research and so would love your feedback on is this idea that actually the way you govern these platforms, just in the same way that the way you govern a society structures its economy and can lead to good economic outcomes or bad ones. This is like the puzzle of economic development. In just the same way, how you govern these very large online platforms could affect outcomes in similar ways. So platforms actually need to attract all these different people to the platform. Obviously, they have to provide a service that's valuable to users. They then have to create this flywheel where they're also bringing in the developers who make the apps, or the sellers who sell the goods, or the creators who make the content, whatever in a way that makes everyone better off.
00:08:46.104 - 00:10:01.010, Speaker B: And so the more of those contributors you get to your platform, right, the better service you have for your users, the more users you have, the more compelling it is for these contributors to come in. Where this problem occurs, this hold up problem that I want to talk about is after a platform gets very, very large, the exit options for these various contributors to the platform may diminish, right? And that's sort of a big part of the argument that Epic Games is making is that it's just not the case that the developers can say, I don't like your rules, so I'm going to go over to some other app store. They can do that in a very limited way, but they lose a lot of potential business if they do that. And the more locked in they are to this platform, the more severe this holdup problem is. So what is the holdup problem? The holdup problem is that once you're at scale as a platform, it's very hard for you to promise that you're not going to use that power to start taking more for yourself and giving less to the developers and the creators and so forth. And if you go out today, I've been trying to quantify the size of this problem across platforms. It's kind of hard, but certainly at least anecdotally when you talk to people who work on these platforms, you notice this across many platforms, right? Growing sense that they're not getting a fair share.
00:10:01.010 - 00:10:40.060, Speaker B: Growing reluctance to invest in learning how to build for a new platform if this problem hasn't been solved. And so I'm not at all trying to argue that today there aren't lots of apps on the iOS store or anything like that, but on the margin, think about someone just getting out of college or something who might want to make an app. Maybe they're a little bit less excited than they used to be. Like when I graduated college, I went to Stanford in Nine. Everyone wanted to make an app. Of course, some of that is because it was new and there was just more low hanging fruit, obviously. But another part of it is today, I think there really is a strong sense that there are certain parts of the deal that are just not as compelling as they used to be.
00:10:40.060 - 00:11:25.924, Speaker B: And the holdup problem isn't the fact that you are in equilibrium mistreated by the platform. It's that anticipating that you choose not to invest in the first place. And therefore, from the perspective of the platform, they're not maximizing the amount of contributions that they could be if they could somehow promise we're not going to raise fees later, we're not going to have these hard to understand rules and use them in ways that advantage us. We promise you we're going to stick to a fair deal. If they could do that, they could get more people building on their platform. But it's very, very hard to do that. And I think we're in a really interesting phase right now where platforms are just starting to grapple with this because these conflicts are getting so severe.
00:11:25.924 - 00:12:22.284, Speaker B: So obviously you saw the example of Fortnite, which is maybe the highest profile, but there's tons of others. There's third party sellers going to war with Amazon, there's like the Etsy boycott. I don't know if any of you saw that this is happening all over the place. And so I think it's becoming an increasingly large problem and a business opportunity, I think, is my argument, at least for platforms that can innovate in how they're governed, to create a credible promise to their contributors that they'll treat them well. And just to kind of we haven't formalized this yet, though, we're working on it. But if you were to formalize it, I think some of the key parameters that might influence how large this problem is, how serious it is, would be sort of how large is the equilibrium platform in this space. If there are very large network effects, then you're going to get that scale, you're going to get that lock in potentially how much platform specific human capital the creator or the developer needs, that's going to drive lock in.
00:12:22.284 - 00:12:58.568, Speaker B: The bigger the investment in your time and energy is, the more reluctant you're going to be to engage in a badly governed platform. Similarly to that on multi homing. Like if it's very hard for you to multi home, that increases lock in further. And of course, for creators especially. Imagine if you can't port your data so you build this big following on TikTok, but you can't bring it with you anywhere, then that leaves you more locked in, that leaves you more subject to this problem. Ex ante maybe people will become more hesitant to engage in the platform in the first place. Quick question here.
00:12:58.568 - 00:13:38.112, Speaker B: So another thing one could do is just issue formal commitments, right? Commitment devices are harder but not impossible, and smart contracts make them easier. How do you think about is it about inability to commit or is it lack of incentive to commit? Definitely my historical thought is both. Oh, sorry. Yeah. So the question was, are there other ways to solve the commitment problem? And has the problem been driven more by not being able to commit or not having incentives to create the commitment? Scott I think both have definitely been in play. I think if you go back to the early days, no one knew how these things were going to end up. People were just excited to get involved.
00:13:38.112 - 00:14:20.688, Speaker B: And so it wasn't obvious yet that this problem was going to occur. But one of the lines we have in the piece is sort of like fool me once or something like that, is like people are learning now that this issue could be there. And so for the future, both if you're thinking about new platforms or if you're thinking about incentivizing effort on the margin, this could be more important now. And so there could be stronger incentives than there used to be. In terms of ability to commit. We've been thinking about it mostly like if you're in the Web two world, it's very hard to commit to this stuff. I mean, you can certainly try to use the legal system to make the commitments, but we know why that's not a super strong commitment, because the costs of litigating this stuff are super high and the outcomes are super uncertain.
00:14:20.688 - 00:15:29.828, Speaker B: And so our thought was along your lines, that web three or the blockchain might actually open up the space to create those more credible commitments. Now, okay, so we're going to think about this in kind of well, we're political economists, so we're going to think of it in kind of a political economy style way, which hopefully is useful, which is how do you solve commitment problems of this form when you can't rely on just signing a legal contract together? Well, one thing you can do is you could observe that this is very similar to the problem that societies face. So if you believe at least, which I do, these stories about economic development that one of the key things that drives economic development is the ability for the state to credibly commit, not to expropriate you in the future. Not to erode your property rights, which makes you willing to invest, to start a business, whatever. Then you can follow the same kind of logic. So there's some fantastic work in the political economy of development that's about how societies solve this problem. You want people to feel empowered to start businesses, to engage in economic activities.
00:15:29.828 - 00:16:22.264, Speaker B: You need to promise them in some way that you're not going to screw them over after they start to do that, otherwise ex ante they'll be reluctant to do. So how do you do that? Well, of course, you build a strong legal system with property rights. Harder than it sounds, maybe it doesn't sound so easy, but you also combine it with democracy. And that's sort of the really important insight from this literature is that if I tell you today this is what the tax rate is going to be, this is the process for how you own property. If someone wanted to take property from you, here's the process for how you would do that, and I promise you, it's just going to stay that way forever. You probably wouldn't believe that because if you looked at the history of human societies, you would observe that those kind of promises don't tend to stick. In particular, if the promise is made to you by someone super powerful, like a monarch, then you might think to yourself, the monarch can't tie her own hand.
00:16:22.264 - 00:17:11.896, Speaker B: Like later, if she sees you've made a lot of money, she's the lawgiver to the land, she can just change the law. And this is exactly what happened historically, and I mentioned this in the last talk. This was the problem that the Glorious Revolution was intended to try to solve in the UK was that the king was repeatedly rewriting the laws in order to extract more resources from society. We see this all the time. It's by no means limited to the UK. And democracy was part of the way you could make, they thought, dynamic commitments that you could say not only are these going to be the rules today? But I'm going to give you a process by which all rule changes in the future happen and they involve you. And so how does that magic trick create commitment? Because of course, the powerful person could just say later, no, I fooled you.
00:17:11.896 - 00:17:57.192, Speaker B: I'm not going to actually honor these elections or something. It's a complicated process, but the idea is that it creates a coordination point for voters. So in the future, if I violate that promise, it could be very hard for you to punish me. But at least you have this clear focal point, which is that I promised you the election and then I took it away from you. And so the idea in this literature, and Ajimal and Robinson is probably the most famous in it, although it's a pretty deep literature, is this idea that elections, giving the franchise to society is a key part about how you make long term commitments. There's sort of like procedural commitments that undergird commitments to develop the economy. Our idea is that and it's not even just our idea a lot of people are talking about this in web three.
00:17:57.192 - 00:18:41.716, Speaker B: Is that there's actually a really deep parallel here to web three, which is that if you can give these developers, these creators, whatever contributors to this platform, this kind of like permanent commitment to governance, power, then they can really start to believe this is a special platform that I can really build for in the long run. Because if they want to change the rule, maybe I like the rules today they're taking a reasonable cut. I don't want them to take no cut because I want the platform to succeed. I just don't want them to take too big a cut. Maybe the rules on what's allowed and whether my app can get delisted and stuff is very clear. I like these rules. I want to believe that even after this platform gets very, very large, it's going to stick to these rules.
00:18:41.716 - 00:19:32.404, Speaker B: Maybe I believe that more if I've been given some arguably somewhat immutable voting rights over any future changes to those rules. So that's kind of the analogy that we're thinking through. And so I think what's interesting about this isn't just that it could be in the interests of these contributors to have the system, but that it actually could be in a platform's interest too, to the extent it actually increases the total amount of investment that's put into this platform by people. So that's the idea. And the general idea here is just that we've learned through the history of human societies that good governance is actually vital to driving growth. Maybe the same logic applies for these platforms. So that's sort of what I would call the commerce problem or the economic growth problem I mentioned.
00:19:32.404 - 00:20:48.388, Speaker B: There's all this second one, which is legitimacy. So let me talk through this one now, and then I'll talk about some of the tools that might actually be useful for implementing this in practice. Okay, so the legitimacy problem is different from what I just described. The legitimacy problem is similar in that it's when you're at a large scale you start to worry that if the rules being set on this platform are important for society, and if people aren't so easily able to exit the platform because of its scale, that you're now setting rules that are consequential for society, but you're not setting them through legitimate mechanisms. And what I mean by legitimate mechanisms or a legitimate system of rules is that it's a system where people believe that the rules have been set through a fair process. And this is a deep area of political philosophy is like when are rules in the real world legitimate? We tend to think one really important way they get legitimate is if they're set by a government that has some story for why it's been empowered by its society, perhaps through democratic elections. Of course, the rules also need to be not crazy because no one wants to follow crazy rules.
00:20:48.388 - 00:22:06.126, Speaker B: But if the rules are reasonable and they're set through a fair process, then people are more likely to accept them as legitimate, which means they're more likely to want to follow them. People don't like to follow rules that aren't set through fair processes or processes that are at least perceived to be fair. And this in turn makes it super important for the global policy conversation around social media. I would say one of the most fundamental drivers of the desire across the world to rein in social media platforms is precisely the feeling that they are, because of their scale, setting rules that are affecting the flow of information in society and that they're not setting those rules through a legitimated process. And so if you can implement some kind of good governance so that you do create those rules through a fair process, it might actually be very good not just for society, but also for the business. Okay, so then how I guess the question here then is how democratizing can help with this in a different way than it helped or at least I argued it helped with the holdup problem. So I think there's an old and super intuitive idea that a system of rules is more fair or more legitimate when the people who are actually subject to the rules participate in creating them.
00:22:06.126 - 00:23:07.422, Speaker B: That's kind of obvious. And there's a more subtle point here which is, okay, fine, why don't we just have the democratically elected governments write these rules for us? Which is obviously a huge part of kind of the movement in regulation today. And I would just say there's some advantage, I think obviously they could be complementary, but there's some particular advantage to having these user communities themselves write the rules if you can get them to do it. And some of the reasons I think this are first of all, these communities tend to be spread across many countries, so it's not actually obvious how any particular country could write these rules. The people who participate in these communities, whichever one you want to pick, are different from the people who participate in real world government. So if you try to use one to write rules for the other, you might not get very good outcomes and you might not even get very legitimate outcomes. And related to that, these are weird nuanced contexts where writing the rules is very difficult.
00:23:07.422 - 00:24:23.498, Speaker B: And it's actually a little bit hard to believe that someone quite removed from the communities is going to be as informed about what would be a good rule and what would be a bad rule. So those are some reasons why we might want this, I guess I'll call it community governance to be at least one part of how the rules for social media are built. Okay, so those are like just high level conceptual views on two reasons you might want to try to democratize large platforms. And I think the question is, how do you do this in practice? So we talked a little bit last time about some ways you can borrow from the history of democratic governance to kind of innovate. In Web Three, we talked a lot about kind of the need to move beyond direct democracy, and now I just want to kind of lay out some ideas for how we might do this, both for Web Two and Web Three. All right. The first thing I'll say as we apply this in practice is just to note that one thing I see over and over again is that it's hard to discuss decentralized governance for large platforms of people because there's no common shared definition of what the word or the phrase decentralized governance means.
00:24:23.498 - 00:25:09.014, Speaker B: And people use it to mean totally different things. It's actually quite complicated to figure out how to apply it to a large platform. So I want to tell you about this is at least one idea for how to divvy up the different things it can mean. So one thing you can do is you could have a platform. It's centralized in the sense that the rules apply to everyone, the same rules apply to everyone, but you could use decentralized means or democratic means to set those central rules. And that's kind of how, like, the federal government in the US. Works, or the central government in many countries works, right? So it's centralized in the sense that it's a set of laws that apply everywhere, but it's decentralized in the sense that it's not one monarch making the rules.
00:25:09.014 - 00:26:08.982, Speaker B: And so some people might call that decentralized governance, other people might not. It's definitely a super important thing for how platforms operate in practice. Second thing is who enforces the rules? And so you see lots of examples of this too, where maybe the rules are actually set by a central actor, but how they're enforced is decentralized. So you have these communities where maybe there's a set of rules, like Reddit, for example, has a set of central rules and they partially rely on admins to enforce those rules. In certain contexts. You could imagine pairing these two together, you could use democracy to set a set of central rules, and then you could also ask the community to help enforce them, or you could do some other combination of the two. This difference between making the rules and enforcing the rules is super important, and Matt and I talked about it earlier, it's actually kind of hard to figure out which is more important in which context.
00:26:08.982 - 00:26:59.740, Speaker B: In some contexts you might think that making the rules is actually the easy part, but getting it right on enforcement is super hard. And so you see, for example, I mean, just to draw on my own experience at Meta lots of times, where the concern is not about the rule, but about the fact that it was applied to this person and not to that person. And the more you have that kind of sense that the rules are being applied unevenly, the less legitimate the system obviously is. And just from a practical standpoint, the worse the user experience is. Especially, imagine you could have some perfect set of rules, but you massively over enforce on them and it makes for a terrible user experience, something like that. So that's one way it plays out, but it also plays out the exact opposite way. It might be an interesting exercise, if you're interested in this kind of stuff, to go read the actual rules of basically any platform.
00:26:59.740 - 00:27:56.606, Speaker B: I think you'll be hard pressed to say that you think that they're written super well, they're incredibly confusing, they tend to be at an almost bizarre level of detail. A good example. I don't mean to pick on them at all because it's completely not unique to Twitter, but go look at Twitter's rules around manipulated media, which is what they use, for example, to apply to the Hunter Biden piece in 2020. It's an extremely weird it's not just like there's a rule, it's like under these conditions we do this, but under these conditions we do that. And it's like if you turned it into a flowchart, it would be an extremely complex flowchart. And a lot of them have this characteristic that's in large part because it's just a super hard problem to write a coherent set of rules, but it might also point to some pathologies in the way large platforms write down their rules. And so in a case like that, the problem is not just, okay, we got to enforce these rules and we're not very good at it, or it's a hard problem.
00:27:56.606 - 00:28:53.278, Speaker B: It's also like if the rules themselves are super unclear, then that's also very bad for users if they're getting enforced on and they don't understand. Why, even if the enforcement is actually done correctly under the system of rules or something like that. And so these two problems go together. And I think both are amenable to sort of some of the insights from democratic governance that you might be able to use tools from democracy both to write more effective rules and also to get the community to help you enforce them. So I think that's kind of where my interest lies. And if you think about all the examples I put up at the beginning, I think it kind of applies to all them. So if you even think about Openc or Coinbase, these are tools that might be useful to platforms that are kind of like web three adjacent, but which still require some kind of central layer of rules and regulations I'll just mention because it's super important and it comes up a lot when I talk to people.
00:28:53.278 - 00:30:12.514, Speaker B: There is, of course, this other meaning of decentralized governance, which is like, you kind of get rid of the platform. And it's sort of like we don't see any examples of this in practice yet, but it's like a dream some people have that you'll have, like a completely decentralized social media platform where there are no central rules, no central anything, and it's on the blockchain, let's say. And then people can write their own competing kind of like wrappers that sit on top of it. And some can have one way of ranking content, some can have another, some could have one system of rules, some could have another system of rules and then you'd have competition among users on which to use and that could have huge gains for both the economic the holdup problem and the legitimacy problem, potentially, if it worked. But I think there's some very interesting questions as to whether and how it could actually work. And I'll just point out that under a variety of circumstances, like if the returns to scale and the ranking algorithms are really high, or the returns to scale and the moderation programming is really high, you might end up with aggregation and basically have all the voters end up hurting into one service or something. And then that service would have all the same exact problems of the web two platforms because it would be the service that everyone's locked into and it would have to decide what's allowed to be said and so forth.
00:30:12.514 - 00:31:08.320, Speaker B: And so I think it's a super interesting area and it's something I'm interested in, but I think in the short run at least, solving these sort of like how can you democratize current platforms? Is a very helpful first step to think about, okay, all right, so how do we actually do this? The obvious first thing to do is to have the users just vote. And so maybe you write the policy. So this would be under this framework of central rules, but voted on by people and centrally enforced you write the rules and you make two versions of them and then you ask the users to vote on them. I think it's kind of an intuitive, simple idea that brings at least some amount of legitimacy into the process of how, let's say, a social media platform writes its rules. And this isn't just hypothetical. This is something that was actually done. I don't know if anyone here remembers this.
00:31:08.320 - 00:31:31.908, Speaker B: I don't, though I was a Facebook user at the time. This completely passed me by. But Facebook actually tried to do this. It was a super long time ago, 2009. It's like 100 years ago, but that's exactly what they did. There had been a big backlash to some of their new community standards. So they said, you know what, we're going to write two versions.
00:31:31.908 - 00:32:14.336, Speaker B: One that's kind of like what we'd been thinking about, one that's more responsive to the backlash, and we're going to let the users decide. And not only that, but on a going forward basis, that's the mechanism we're going to use to update these terms in the future. And so at the time, this was kind of a big deal. And I think one thing that's kind of remarkable about it is that actually the person who led the revolt against the proposed terms came out and said publicly, I think this is a great idea. This is how rules should be made. And so there was this kind of like brief moment in time where it seemed kind of like one of these tech optimism moments that was like, god, this is how we're going to solve the governance problem for social media. We're going to give power over to the users.
00:32:14.336 - 00:33:03.936, Speaker B: It's kind of similar to some of the rhetoric in Web Three. It fell victim to precisely the same problem we talked about last time in terms of Dow governance, which is that in the end, basically no one voted. No one actually kind of thinks 600,000 votes is sort of remarkably high based on my expectations. But it was a 0.3 percentage point turnout rate for the Facebook user base at the time. And I really like this is a headline from Los Angeles Times, facebook Governance vote is a homework assignment no one did, which I think fits Dow governance pretty well too. And so what happens here? I mean, there's a lot to debate about exactly why the participation rate was so low, but it's not that surprising that if you ask people, they're on this platform for other reasons.
00:33:03.936 - 00:34:01.912, Speaker B: I think a lot of them rightly, feel it's not their job to govern this platform. They're not getting paid to govern it. Why should they take the time to read these super complicated proposals and choose between them? And so we talked last time about the paradox of voting rational inattention. I think it's sort of natural that this is what would happen if you just went straight to kind of like a simple vote of the entire user base of really any platform. And if you do, if you're interested, the Wayback Machine has all this stuff, you can actually go read the proposals and they're like very technical and the differences between them are quite subtle. And so there's actually this third document you had to read that was like the differences between the two documents and it's like no one's going to put in the time to do that. I imagine of these 600,000 voters, how many of them actually read the documents? Who knows? Surely less than 600,000 long.
00:34:01.912 - 00:34:51.050, Speaker B: I want to say it was like 40 pages each or something like that. How did they arrive at this? Did they think this was going to work? Okay, so actually that's a very good question because that's an important part of the history of this. They were quite wise to put into the initial procedural promise that they provided to users that they were only going to implement the vote if at least 30% of the global user base participated. So they clearly knew there could be they were smart enough to know there could definitely be an issue. And they were right. I think that they thought at the time again, there was a lot of optimism around this stuff at the time. I did think they thought they might make it to 30%, but they were at least anticipating that could be an issue.
00:34:51.050 - 00:35:41.220, Speaker B: And the funny part is, in the aftermath of getting the 0.3% turnout rate, they actually decided to implement it anyways, and then they proceeded to have several more votes and the participation rate actually doubled on the next vote to 0.6. But ironically, the vote had to do with like, turning off voting for future proposals and so the system died after that point. But the interest in the idea has never really died and it's still very much around today. And it's actually funny to talk to people about it today because not everyone remembers that it happened. In fact, even at Meta, the vast, vast majority of employees didn't work there in 2009. And so it's actually a really interesting piece of tech history that's not known to all the participants anymore.
00:35:41.380 - 00:35:52.220, Speaker A: There is, of course, a much more cynical interpretation of that whole situation. They knew damn well that they wanted to get 30% of the vote and it sort of was a PR stunt.
00:35:54.560 - 00:36:38.856, Speaker B: That's definitely possible. Certainly in general, I think that companies often face that incentive to make a show of listening to people. The only thing I would say, and maybe this is even more cynical, is I think there are potentially large advantages if you could actually get the user base to make these decisions. So this is something you'll see in the press today at Meta. For example, Mark Still today speaks about the desire to get the community to govern itself. And the way, I think, more critical elements of the media interpret it is not that it's that kind of cynicalness, but that rather of course, someone who has to face all these hard decisions would much rather push them off onto other people. And so it could go either way.
00:36:38.856 - 00:36:51.900, Speaker B: Like if you could get high participation, it might make your job as a platform easier in some way. Yeah, it's hard to say though. Of course the voters could make bad decisions and you could end up having to bail them out. It's hard to say.
00:36:51.970 - 00:37:02.864, Speaker C: Yeah, maybe like follow up question to that. I guess there's my very limited understanding of this example. Probably doesn't affect Facebook bottom line for.
00:37:02.902 - 00:37:03.984, Speaker B: Revenue all that much.
00:37:04.022 - 00:37:38.104, Speaker C: If there's distinction between something maybe same thing for content moderation, as long as it doesn't look legal. But some of the things that I can imagine there being some things that people may be concerned about, like the rate of which they're shown ads or how much personal data where there'd be, like, a huge conflict between facebook and that history of them or other tech companies opening up any kind of governance to something where they could possibly come into conflict.
00:37:38.232 - 00:38:47.796, Speaker B: Okay, I'm going to answer a different question and then I'll answer your question. The first question answers are there other historical analogues to this? And I think you're pointing out exactly the balancing act that we see in the history of democratization, which is there's some in the literature they're called like the elites. Someone has a bunch of power and resources in society and they realize that it's in their interest to extend the franchise because of these dynamic commitment problems I was talking about. And so what do they do? Like in equilibrium? The bargain they offer is like just the epsilon amount of power they have to give up to stay in power. And so they don't want to give up too much because then people will vote to redistribute and whatever. And so they give up just enough power. And you definitely see in all the discussion around these sort of voting systems at large platforms, you see a balancing act where there's kind of like how much could we give up before? Yeah, either we get decisions that are bad for the business, or also, as you pointed out, there's some things we just can't give up legally because we're required AML, whatever law you want to pick.
00:38:47.796 - 00:39:39.172, Speaker B: And so there's very much that same balancing act in terms of analogs to companies doing this. There are lots of examples where companies do something that hurts their immediate bottom line because of some long term commitment value. So you see tons of this. I would describe lots of self regulatory regimes like this where nuclear producing energy companies get together and they agree on a set of standards that are actually costly for them to implement. And so it hurts them in the short run that they have to bind themselves to this organization's rules but in the long run it makes people believe in the nuclear power more and they make more money or something like that. You see this with emissions agreements. There's all these kind of like self enforcing agreements where companies come together.
00:39:39.172 - 00:40:25.072, Speaker B: They're not democratizing, they're typically giving power to some kind of independent commission. But it does have that feature of it's costly to them and the commission does tend to make decisions that are costly for the immediate bottom line. But there's typically a belief that in the long run it's valuable. And so you could think about whether the same thing is true here. And that's actually one of the major issues I want to raise. So just to get to how would you actually implement this? This is super early thinking, I want to stress, but you could imagine a platform issuing some kind of governance token to its contributors, to its developers, to its creators, what have you. And to your point Matt, it could be that it's very carefully specified what these votes are.
00:40:25.072 - 00:41:22.128, Speaker B: So it might not be like you get this token and you vote on everything. It's like once a year there's three take it or leave it proposals you get to vote on and they're only in these issue areas. And there'd have to be some kind of balancing act between if the voting power you give up is utterly symbolic, then you get none of the gains in terms of solving the holdup problem. But if the votes are too important, you might accidentally destroy the platform. You hope that there's some balancing act because if you get the incentives right, the developers who hold the governance token shouldn't want to destroy the platform. But it is a tricky incentives problem because of course they have their own private interests and depending on their discount rates and pivotality and a bunch of other considerations, they might conclude it's in their interest to vote, to take a bunch of money, to imagine the votes on future fees, on the app. They might vote to make the fee zero because in the short run that's very good for them even though in the long run if the platform has no revenue, they're screwed.
00:41:22.128 - 00:42:05.180, Speaker B: So there's very delicate balancing that has to be done there. But I do think that basic idea is kind of interesting. It's like what if you gave out these tokens? What if you put them on the blockchain so that it's especially credible that the company can't mess with your voting power? Could that help to solve the holdup problem? That's like the idea. Of course it could also be used for the social media problem. You could distribute these tokens to users, ask them to vote on stuff. Then the key question becomes well, we know people won't vote, so what do you do to get them to vote? Some set of ideas is you could use the token, right, to build in some kind of direct incentives to vote. You could also use other things like delegation.
00:42:05.180 - 00:42:38.910, Speaker B: So I think delegation we talked about last time might be a way to help mitigate the participation problem. So you could imagine taking everything I just said and layering on delegation. So it's like we give out these governance tokens to the key stakeholders of this platform and they don't just get asked to vote, but actually they have the option to kind of essentially elect a representative who will make decisions on their behalf for some period of time. And if we make the problem very simple and we make it very easy to delegate possible that that could improve the participation problem. Yeah.
00:42:40.480 - 00:42:58.564, Speaker A: At this part of the talk, we're now starting to kind of talk about web two and web three broadly. Yes, but I mean, one really big difference would seem to be the state of the civil problem. So just curious, fundamentally different just for the design space, doing building in one.
00:42:58.602 - 00:43:55.796, Speaker B: Versus the other in the context with developers or stuff like that. I'm envisioning a system with known identities as the easiest use case, where even there, there's a bunch of tricky problems to solve. I think it gets even more interesting if you open it up and you say anyone can come and trade these tokens and also develop and be anonymous. And there I think you immediately get all the same problems we're familiar with from web three governance. So not only sybils but also governance attacks where you worry that someone comes in and buys up a bunch of the tokens to destroy the platform, which I think is super interesting. So far my basic idea has been if you have a platform with a set of known important developers or something, you start by distributing it to them and maybe you make it non transferable again to start. I do think there's some advantages to going all the way to transferable, especially.
00:43:55.796 - 00:44:14.190, Speaker B: One thing Ethan and I explore in the piece is like, it actually has this cool feature that if they're transferable, if they trade, then if the platform has a strong private belief that it knows what to do and that the voters are wrong, it can buy out the voters to implement the idea, which is very similar to corporate governance in a cool way.
00:44:17.040 - 00:44:25.520, Speaker A: I guess one point here is that even in the known voting case at least, it still enables weighted votes.
00:44:26.820 - 00:44:27.232, Speaker B: Yes.
00:44:27.286 - 00:44:29.840, Speaker A: Which I would like to say is more corporate governance.
00:44:30.180 - 00:45:30.310, Speaker B: Yeah. So I think we might have touched on this only just very briefly in the last talk, but I think one of the most low hanging, fruit open questions in decentralized governance is just like what are the conditions under which you prefer token based voting, weighted voting? What are the conditions under which you actually want, like, one person, one vote? Very loosely, we see in society that there seems to be a view that when you're deciding normative or societal issues you prefer one person, one vote. On business issues you like the idea that the person with more at stake gets more say. But I haven't seen anyone lay out like a really clear set of models. It's like this rationalizes why you want this here and that there. My intuition is at least for the holdup problem case, the weighted voting is important. Like you want the bigger developers to get more say, I would think, but I haven't seen that actually super deeply formalized though I suspect out in the corporate governance literature there probably something okay, I want to mention two other mechanisms that I think are super interesting.
00:45:30.310 - 00:46:41.470, Speaker B: So one is sortition you could call it a citizens assembly or in this context, maybe a user's assembly. But when you know participation is going to be really low and that people aren't going to become informed, there's actually a very logical argument for why you might prefer to create a randomly sampled set of people. And the reasons for this are twofold, threefold, maybe one is that by randomly selecting them you break the serious adverse selection problem you have. Where if our participation is very low and it's something most people aren't interested in, then who are the people who choose to participate? They're probably very, very unusual. Anyone who's ever read through any notice and comment comments either from the federal government or actually meta publishes theirs from the oversight board, you'll see that the average comment is not from the average person. It's from a very unusual and very strongly people with strong points of view tend to participate and you might not want that for your system if it's supposed to be representative. And so by actually randomly sampling people you might get a more representative group.
00:46:41.470 - 00:47:23.128, Speaker B: And then the idea of sortition, or at least sortition as I'm proposing is first of all you pay them to participate so that you break. Otherwise you could have this attrition problem where you've randomly sampled at the beginning, but then most of them drop off because they're not interested. And once again you have this unrepresented group. But if you pay them, you can get a representative group and by paying them, you can also convince them to really pay attention and study the actually and countries do this now with some success. Ireland in particular has done this quite successfully. You get basically like a jury. You pay them for their time and you ask them to do this service and to study an issue.
00:47:23.128 - 00:47:55.280, Speaker B: Experts come in and brief them on both sides and then the group makes a decision. I actually think this is a way more realistic for the social media setting. I think this is way more realistic as a way to get democratic input than to try to put it out for a vote. Because it's not just that it's representative, it's that you get some informativeness through the briefings of course, this is expensive to do. You wouldn't want to do this a lot, but if you're a large platform operating at scale and these governance problems are severe, it might actually be worth it to pay for this in limited set of cases.
00:47:55.360 - 00:48:27.630, Speaker C: Yeah, prosecuted trials sounds very similar in some ways, similar to how we use during trial. There are studies on sort of like legal experts look at the decision the jury came through. Legal experts generally will be like some evaluation of quality of the decision to get made. We do it because it's law, or we do it we would still choose to do it if we reset the law.
00:48:28.880 - 00:49:38.272, Speaker B: Let me answer that in a couple of different ways. First, there's a big difference between an actual jury that is adjudicating a dispute that's like one particular thing, and then in sortition the way I'm describing Elise, it's more about creating a policy. And the reason it's very different is that in the jury case, while there certainly are some values you can bring to bear, we tend to think there's like a correct answer and do they get it or not? Within a citizen's assembly, you're trading off different values. And so it's not obvious that there is a correct answer to compare it to. There are, however, like, indicia of did people do this in a well meaning, serious, sincere way? Did they put in effort? Did they participate? And when people have studied that the evidence is pretty good, that actually people are surprisingly willing to hash things out and to put aside their preconceived beliefs and stuff like that, but you can't really correlate it with any particular outcome on the jury side. You can do that. And so you could ask a legal expert, how will you decide this case? And then you can compare it to the jury.
00:49:38.272 - 00:50:45.320, Speaker B: I think the consensus view there, and this is kind of nuanced, is no, the jury is nothing like one of the experts. And litigators I'm married to, a litigator will talk about this a lot, that the jury is just a complete crapshoot. But there's a very subtle thing that goes back to legitimacy that's really important, which is that legitimacy in these kind of collective decisions is not imbued exclusively or even primarily by a view of whether the jury comes to the right answer, but rather by the idea that this is a fair process. Obviously, if it's a complete, made up coin flip, that's not legitimate because it's not even like a story. But if you had an expert who is super good, but they made all these decisions unilaterally, I think a lot of people would prefer the jury to that, even if the jury is a little bit loosey goosey with the law, because it seems more fair. And this is something Ethan and I have been studying. It's this idea of process legitimacy that when you're dealing in situations where you're making very difficult and fraught decisions.
00:50:45.320 - 00:51:30.180, Speaker B: From the perception of people who are subject to the rules, it's often more important to them that the process is fair than the outcome is what they want. I think a good test of whether one of these systems would succeed would be, do people agree to abide by the decision, even if they disagree with it because they thought the process was fair? I don't know. That sortition would get there in a super fraught environment, but it would be worth trying. All right, the last one I'll talk about, which is quite different from the others, but I think complementary to them, is this idea of independent bodies of experts. So it's actually super connected to what I just talked about. So Facebook has done this with the quote unquote oversight board. It's a group of experts.
00:51:30.180 - 00:52:21.556, Speaker B: They're compensated through a unrevocable trust. And so they're in some sense independent from the company, though people certainly debate exactly how independent they are. And they make decisions, binding decisions, on a small number of disputes. So, like, Facebook has decided to remove a piece of content, someone appeals, and then the board decides, should it go back up, should it stay down? Or vice versa? They do it in both directions. And so how does this connect to decentralized governance? I mean, it's almost like a very small but potentially important amount of decentralization, where you're still making central decisions for this whole platform, but instead of the CEO making them or people making them on the CEO's behalf, this other group of people who are paid separately are doing it. And there's some idea that that independence is more fair. And so it brings some amount of legitimacy.
00:52:21.556 - 00:53:22.140, Speaker B: And obviously that legitimacy is kind of a function of, well, how many decisions do they make and how important are the decisions? And people certainly debate today where the board lies on that. And then lots of people have used a similar model. And so Spotify recently announced this, for example, an advisory council. So I would say it's less independent than the oversight board. It's directly funded, I believe, and the decisions aren't binding, but it still has this feature that it's like a different group of people being brought in to make this decision. And so I think just in terms of how these methods all fit together, where it seems like we're heading, I don't know if it's a good thing or not, but basically reinventing all the different branches of government within the governance of these large platforms. So I think it's a really interesting space for Web Three because it suggests a bunch of other ways to use Web Three that we haven't really thought about yet, not only in terms of voting, not only in terms of delegation, but even in terms of sortition or even in terms of like.
00:53:22.140 - 00:53:47.264, Speaker B: You could imagine giving governance tokens to a small number of independent experts, depending on the use case. Okay, so that's all I have to say. I know I'm almost out of time. Just to mention here, like next steps. I'm used to giving talks that are much more like research I've done. This is a new space. I haven't actually done most of the research here yet, but I'm starting a bunch of projects that I would love to get feedback from you all when they're done in this space.
00:53:47.264 - 00:54:49.368, Speaker B: So one of the things I'm most excited about is I'm going to try it could be kind of expensive, but we'll see what we can do to launch a survey of platform participants. So developers, creators, and so forth across all the major platforms to track over time their views about how locked in are they, what are their outside options, do they think the rules that they're under are efficient or fair, et cetera? So that's like one part of this. Ethan and I are exploring a lot of sort of like, how can you build the right theories of platform governance? So what are the incentives, what are the strategies? And so forth. And as I talked about last time with Dan Bonet, we're building this sort of coherent set of data on how these decentralized governance systems are actually working in Web Three. And then I'm also working actively to try to pilot some of these ideas both in Web Three for new platforms. But I'm also super interested in this idea of weaving together Web Three with the existing Web Two platforms that are out there. So thank you very much.
00:54:49.368 - 00:54:51.370, Speaker B: I'll stop there and take any last questions.
00:54:56.060 - 00:55:52.124, Speaker D: So correct me if I'm wrong, but it seems like the function that we're trying to optimize is the number of users that think that the process is legitimate or fair. And if that's the case, then I was wondering about when you put something up for voting your proposal, people who object the proposal are less likely maybe to read it and vote on it because it might not be accepted at the end, so why waste the time? So I was wondering about whether you thought about kind of being proactive in deploying the proposal first and then those people who are strong object to their proposal are much more likely to raise the concern. And then based on kind of you can then survey the people, seeing whether the number of people that think that the system is legit now has decreased and then roll back.
00:55:52.242 - 00:56:25.504, Speaker B: Yeah, that's super interesting. I think that raises at least two thoughts for me. So one is there are interesting processes that kind of look like that. And in fact, in the 2009 referendum, I don't think I mentioned this, but it had a characteristic like that. They didn't do the surveying part, which seems key, but they did do a whole period of notice and comment prior to the vote. So they kind of like they had an initial proposal, then they. Got a bunch of feedback and they iterated, which is something we see in web three a lot with these improvement proposals.
00:56:25.504 - 00:57:35.070, Speaker B: And some of the dows have quite baroque processes for how you go from the initial proposal through the amendment process to the final one. And so layering onto that some kind of real time sense of how people think is very interesting. The second thing I was going to say that I thought you might also be getting at I'm not sure is that you could actually have a very well functioning process but if it doesn't move people's perceptions then it sort of doesn't matter. And I think this is like a big problem and something where if you're not pushing out information and engaging in effective ways you could end up with a great system that actually in practice is democratically justified and is bringing in participation and is informed and is making good decisions. But if most people don't realize that and they might dislike the system even though it's working well and that's a big PR problem for democracy which we also see in the real world too. And so there I think there's some interesting questions for very large platforms who can afford to do this at least how do you get the word out about what you're doing? I think are both super interesting.
00:57:35.520 - 00:57:52.240, Speaker C: Yeah this was interesting is that when you talk about partition and independent oversight board they're paid completely independently what they produce and they're not even paid produce effort.
00:57:55.060 - 00:57:57.704, Speaker B: You're exposing the problems to efficiency.
00:57:57.852 - 00:58:31.356, Speaker C: Okay I was going to say definitely at least coming from a mechanism background counterintuitive although I have been in a jury I did see that everyone pay attention. We weren't paid to do that. But I was wondering if that is kind of commonly accepted conventional wisdom that if you pay people to be there that actually may be better not trying to engineer or if that's still an open problem for like when is it good to engineer, when is it good to just pay people to show up and take things?
00:58:31.538 - 00:59:21.112, Speaker B: So I think you're exposing at least two things there. The first is sortition is the worst possible system you can imagine if what you're worried about is accountability between the decision maker and the voters because there's zero formal accountability. Like there's nothing the voters have no control over it whatsoever. And this is why it's never been regarded as a particularly compelling way to govern a society because you would worry if on an ongoing basis other than Aristotle who seemed to think this is a good idea, most people seem to conclude this is a really bad idea. Not only because you could get people who have no idea what they're doing and didn't even want to become politicians in the first place but you now have given them no reason to fear that they could be fired or anything. So if you think accountability is very important. This is going to be the worst possible system for some of these value based judgments.
00:59:21.112 - 01:00:05.980, Speaker B: It's a one off. If you look at how citizens assemblies work in practice, they're almost always a one off decision made by a relatively small group of people where you can hope that some kind of social glue encourages more responsible behavior. That leads to the second thing about whether compensating them helps or hurts. I don't think that's at all settled. You can think I mean, there's lots of interesting work on other cases where compensating people actually leads to worse outcomes. There's like the Israeli daycare finding, although I think it's actually been questioned whether it's a real finding or not. But it's the example everyone points to where they start to fine parents for being late to pick up their kids and then more people are late because they interpret the fine to be like this is an option, I can pay and be late.
01:00:05.980 - 01:00:28.736, Speaker B: Whereas social pressure is actually more effective at getting them to be on time. It's a stylized story. It's probably made up, but people are super into it. But that's definitely a thing that could happen. Like it could be that paying actually sends a bad signal that this isn't your civic duty. For a platform, there's so little sense of duty that I think it probably is important to pay. But there's definitely trade offs.
01:00:28.736 - 01:00:36.192, Speaker B: And I can imagine in a dow where there is a strong sense of civic kind of duty. Maybe you don't want to.
01:00:36.346 - 01:00:48.490, Speaker C: I guess I think the interesting thing is that you're paying. That kind of like just to be there. You're not paying that later. Vote on your decision. Like it.
01:00:51.340 - 01:01:36.100, Speaker B: So this is another area that I would love to study more is if you go around I almost put this on the list of things for discussion topics, Tim, because it's so popular these days. There's all these ideas around slashing or other techniques that try to build incentives directly into voting into voting a particular way. And I think the promise of these is very interesting that you worry that at a baseline people aren't putting in a lot of effort. Let's give them some incentives. The problem is it's a super dangerous game to play because if you don't get those incentives just right you produce these incredibly strong incentious people to start ignoring their private signals and vote. It becomes a coordination game where the only thing you're trying to do as a voter is figure out how other people are going to vote. So you can vote the same way so you can get the reward or avoid being slashed.
01:01:36.100 - 01:02:07.636, Speaker B: And that's terrible for most voting systems. And so I think it's an open question like is there a way to build in those incentives without creating the really bad hurting tendencies? I had a question. Go for it. I put it in the chat. I guess I can read it out. What do you think about trying to make systems forkable as a way to create legitimacy instead of creating governance rights? L ones don't need to explicitly give voting rights because users can run their own rule set. This doesn't apply to most applications, however.
01:02:07.636 - 01:03:27.964, Speaker B: For example, Uniswap can't fork permissionlessly because the assets contract holds can't be split. Yeah, this is a super deep issue that I've been talking to people here this week about because I think it's one where there's a lot more to study. But basically I think your question is it's at least exactly what I've been thinking about, that the stronger your exit option is, the less important all these other more complex forms of governance become. Like, if it's really easy to exit, then your obligations or your opportunities to use democracy to do better are much more limited, I think. And the interesting thing tends to come around, at least for these Web Two platforms, when there's a sufficient amount of lock in that that's where you start to get these legitimacy problems and it's where you start to get the hold up problems. I will just say in web three, the story might be slightly different only because you're using Democracy there, not only for those goals, but also for these goals around, like keeping the protocol decentralized and securing it from governance attacks and the like, where you might have other reasons to want to use Democracy, even if you can also exit. But I think for these legitimacy or hold up problems, I think although I do want to do some more research on this, that the stronger the exit option is, the less you need that stuff.
01:03:27.964 - 01:04:03.210, Speaker B: And that's definitely been there's some very old sort of foundational political economy work about like, well, why are local governments so much smaller and less developed than national or state governments? And the idea has always been because people can move from one city to another. The cities don't have the same kind of lock in and they have, as a result, much more limited governments. So there is some interesting analogs there too. Yeah, it's a really interesting question. Cool. Thank you all.
