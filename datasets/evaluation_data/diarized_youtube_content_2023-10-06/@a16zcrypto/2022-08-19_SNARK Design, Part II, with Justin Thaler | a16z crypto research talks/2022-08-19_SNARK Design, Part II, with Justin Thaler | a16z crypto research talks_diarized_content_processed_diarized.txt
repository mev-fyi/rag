00:00:06.570 - 00:00:07.120, Speaker A: You.
00:00:09.650 - 00:00:10.670, Speaker B: All right. Morning everyone.
00:00:10.740 - 00:00:25.840, Speaker C: Welcome to today's a 16 Z crypto research seminar. Justin Thaler from Georgetown is back. He's going to be giving part two of this Narcs tutorial. We'll see if there's a part three later or not, but for now, let's hear part two.
00:00:28.050 - 00:01:06.720, Speaker B: Yeah. So I'm going to start today by recapping the first talk in case anyone is joining for the first time today. And then I will kind of continue or complete the overview of roll up projects. So last time I sort of described the idea of a roll up, what roll ups are trying to accomplish. But today I'll actually kind of dive deeper into the details of what specific projects are doing. And if there's time, I will then tell you a little bit about how these scenarios actually work in sort of more detail than I described last time. And that's where we'll see if I might or might not have time for that.
00:01:06.720 - 00:02:01.154, Speaker B: Okay, so here is the abbreviated recap kind of 90 minutes into four. So let me remind you what is a Snark? So the prover claims to know some witness satisfying some property. And just as a concrete example, this could be the prover and Verifier might have agreed on some cryptographic hash function and some output Y of that hash function. And the prover claims to know a pre image W of that hash function output. So the trivial proof that the prover knows W would be to just send W to the Verifier, who can just directly check that W satisfies the claim property. In this case, by evaluating the hash function on W and confirming it spits out y. And basically a Snark is just any way for the prover to prove to the Verifier that it knows W with costs better than that of the trivial proof system.
00:02:01.154 - 00:02:59.380, Speaker B: So succinct means that the proof is shorter than actually sending W explicitly to the Verifier. And ideally, the proof will be faster to check than the Verifier's runtime in the trivial proof system. And in that case we'll call the Snark work saving for the Verifier. In general, Snarks are designed in a two step process, so there's a front end and a back end. And ideally, the front end will take some computer program written in a high level language which takes as input the witness and sort of checks that the witness satisfies the claim property. And the front end will transform that computer program into an equivalent representation, typically something like a circuit satisfiability instance because things like circuits are kind of easier for the Snark proof machinery to be applied to than directly to some high level computer program. And then the Snark for circuit satisfiability or people like to use variants of that in practice or generalizations, that's called the back end of the system.
00:02:59.380 - 00:03:47.460, Speaker B: So last time I described sort of the standard paradigm for designing Snarks. It's a three step process. So first you design a protocol called a polynomial IOP. And I said last time what that is. And if we get to the part of the talk where this is relevant again today, I'll of course remind you the second step is to design a cryptographic primitive called a polynomial commitment scheme. And if you put these two together, you get a succinct interactive argument and then you can just pull the interaction out to get a static proof, a non interactive proof, by applying something called the fiat Chamir transformation. Okay? So with sort of that background on Snarks out of the way here's, like the one slide that tells you how a roll up works.
00:03:47.460 - 00:04:33.226, Speaker B: So the idea is that sort of without a roll up today, blockchains are sort of storing the entire state of the world on the L one. So on layer one, that means on the blockchain. So that means all of the blockchain nodes kind of have to know like, everyone's account balances or something like that. That's what I mean by the state of the world. It's like all the data that lives on the blockchain and the roll up seeks to avoid that by storing only a commitment to that data on the L one, a cryptographic commitment. So think of this as just like one cryptographic hash value, which somehow binds everyone in the world to the underlying data. But the commitment itself is tiny.
00:04:33.226 - 00:05:50.438, Speaker B: It's just like 256 bits or something like that. And The Idea Of The Roll Up is that it's Going To Process a Big Batch Of Transactions and figure out what Is The New State Of The World by Applying That Batch Of Transactions to The Current State Of The World, and Then Compute A commitment To That New State and Post that New Commitment on the l One On The Blockchain. Okay? And the problem with that is we don't want to just trust that the roll up service does all that computation correctly. So we don't want to just trust that the roll up sort of correctly applied all of the transactions to the old state of the world and then correctly computed a commitment to the new state of the world. So the roll ups will pair the new commitments with a Snark proof that the new commitment is correct. So that means the roll up is going to prove that it knows a bunch of transactions such that all the transactions are valid, meaning they come with a digital signature and none of them involve somebody spending more money than they have in their account, things like that. And the roll up will prove that when you apply those transactions to the old state of the world underlying the old commitments, it leads to a new state of the world underlying the new commitment.
00:05:50.438 - 00:06:49.020, Speaker B: And I just want to remind everyone sort of I had a discussion last time about how you measure the performance of these roll ups. And sort of there are two things you might care about latency and throughput Latency is determined by two things. So this is the delay between when a transaction posts to the roll up service, which we call layer two, and when a proof capturing that transaction posts to L One, which is when we consider the transaction to be finalized. So the two contributors to Latency are the time to compute the proof because we kind of have to wait around for that proof to be there before we can post it to l One, obviously. And also we have to wait around for enough transactions to hit the L Two, that the roll up is kind of ready to treat them as a batch and roll them all up by computing the relevant proof for them and the commitment that corresponds to the state following their processing and all of that.
00:06:50.270 - 00:06:56.960, Speaker C: So the main reason you want lower down on batch size is just so that the amortized cost per transaction is low enough.
00:06:57.330 - 00:07:30.310, Speaker B: Yes, exactly. Yeah. This is all about saving gas on the L One. Of course there are gas costs there because blockchain nodes have to do work for every transaction and store state for every transaction. So you make people pay for that because otherwise people are just going to try to get as much transaction data as they can on the chain and the nodes will all fall over. Yeah. So this is all about saving gas.
00:07:30.310 - 00:08:49.250, Speaker B: That's why you want the batch sizes to be big. And there are sort of, I guess you could say there might be economies of scale to bigger batches. So it's not just that more batches are getting kind of cryptographically compressed into a single commitment and proof, but it's also that the proofs involve like if multiple transactions in a batch refer to the same account, you kind of get multiple reads to the same piece of data. So the prover is only having to deal with authentication information for that piece of data, sort of once. It doesn't have to pay every time and often. We're going to talk about data availability again in a minute, but a lot of roll ups are storing the data underlying the commitments on chain and you sort of only have to store sort of the latest account balance on chain in principle. Right? So if in a single batch, like 18 different transactions hit the same account, you can kind of net them all and only pay to store the final result on chain, you don't have to store 18 different values.
00:08:49.250 - 00:08:55.960, Speaker B: So there are kind of multiple ways that big batches help save the costs of a roll up.
00:08:56.890 - 00:09:22.350, Speaker D: I don't know if this is interesting or not, but I noticed that you define Latency as like the moment that you hit L One. You assume that are you basically abstracting like that l One is kind of perfect and instantly final in this case. I wonder if there's interactions between the fact that posting to L One for some types of blockchains doesn't really guarantee that the thing is finalized of reorganization.
00:09:23.490 - 00:10:21.586, Speaker B: Yeah. So, I mean, I guess the simplest thing is to wait long enough if it's proof of work chain, wait for enough more transactions. I guess that also applies to proof of stake that is just considered final on L One. And that's what I mean by post to l one. But there are other subtleties I'm about to raise when we talk about data availability, which is I was seeing that some of these roll ups have ways to revert state if bad things happen. Well, okay, I don't know details about if they've at least discussed the possibility of reverting state even after Proust might have posted to L One. So you might have to be very careful about what you consider finality if something like that might happen.
00:10:21.586 - 00:10:36.840, Speaker B: But yeah, at sort of the crudeest level, at least, people sort of consider the roll up to the transaction to be finalized when a proof that kind of incorporates that transaction hits L One, whatever that means.
00:10:38.170 - 00:10:39.894, Speaker D: Basically it's confirmed on L One.
00:10:39.932 - 00:10:56.114, Speaker B: Yeah. Confirmed on L one. Yeah. I guess if it's possible that L One might sort of drop the block that has the proof in it, then there's no sort of global record of the proof anymore.
00:10:56.262 - 00:11:06.010, Speaker D: I guess the question we should come back to later is, like, is every roll up more tolerance of rework or something being dropped?
00:11:06.170 - 00:11:53.802, Speaker B: Yeah, we should discuss and think about that. It doesn't seem particularly specific to any roll up design. Just the whole notion of a roll up posting approved. Capturing a batch of good transactions seems to be key, but definitely their plans to sort of deal with possible attacks definitely factor into finality. And I'll discuss possible attacks momentarily. Okay, so people obviously care about latency. This also means, by the way, if your transaction that's in a batch is a withdrawal transaction, I think you don't actually get your money back on chain until a proof capturing that withdrawal transaction hits the chain.
00:11:53.802 - 00:12:54.526, Speaker B: So in that situation, even if the roll up service provider or some intermediary, you're an end user and there's some service that's using the roll up provider, even if they're willing to kind of grant their users finality before the proof hits the chain. If you're trying to withdraw from the roll up, I think you just cannot actually get those funds kind of liquid in your wallet until the proof capturing the withdrawal hits the chain. As with optimistic roll ups, I think people can kind of step in and potentially provide liquidity before finality, kind of take on risk, if you will, so that end users don't have to. But I don't know. Yeah, but if you're trying to do, like, a direct withdrawal from a roll up, I think this latency is really going to matter because I think it's.
00:12:54.558 - 00:13:05.686, Speaker D: Usually like two transactions. Right. You need a withdrawal transaction within the roll up, and then you submit your own that comes from the roll up server, and then you submit your own.
00:13:05.708 - 00:13:39.270, Speaker B: Thing to actually move yeah. To pull the funds from the roll ups, from the smart contract of the roll up to your wallet. Yeah, I think that's how it works. Okay, so that's latency. And then the other measure of performance is throughput. So this is the number of L Two transactions that can be processed by the roll up in a given amount of time. And we had a fairly lengthy discussion last time about how, due to something called pipelining, this actually has nothing to do with latency in principle.
00:13:39.270 - 00:14:22.890, Speaker B: So maybe I'll summarize that in a sentence here, just as a reminder for those who weren't here last time. So the idea is, imagine it takes a year to compute a proof, but the proofs are really small. So once you have a proof in hand, you can get it on l one really quickly and sort of in steady state. You can do this thing where you're actually getting a proof onto l one. Like, if it only takes a minute to upload a proof to l one, once the proof is in hand, you can actually get a proof onto l one once per minute in steady state. It's just those proofs are kind of capturing year old data. So you think of, like, as soon as you have a batch of transactions kind of ready to be proved, you hand it off to the prover.
00:14:22.890 - 00:15:14.538, Speaker B: You kind of think of putting it in a pipeline that takes a year to get through the pipeline. Right? But after that initial year is done, then every minute, you're kind of throwing a new batch of transactions off to the prover and a proof is coming out the other end of the pipeline, and you can throw that proof up onto l one. And this just happens. Every minute a new batch goes to the prover, a new proof comes out of the pipeline, you throw it up on l one. So in principle, this has nothing to do with latency, but of course, in practice, people won't tolerate, like, one year latency. So, yeah, to maximize throughput, you kind of want your batch size to just be as big as it can possibly be, because that gets the most kind of savings or compression of the transactions, since these snarks have succinct proofs that grow sublinearly with the statement being proved. But there's limits put in place by latency demands.
00:15:14.538 - 00:16:05.422, Speaker B: But also, some of these snarks, like the provers, will just kind of fall over when the statements get too big. So there's lots of things that sort of can limit throughput in practice. But, yeah, I sort of flagged this last time because a lot of the roll up projects will discuss performance in terms of throughput, like transactions per second. And if you sort of don't know that they're talking about latency. You might think transactions per second is referring to the speed of the prover, like kind of the end to end pipeline, which would be really latency and that's not what they're talking about. And that makes sense because throughput is ultimately what determines blockchain scalability in terms of just like how many transactions the blockchain as a whole can process in a given amount of time. But you have to know what people are talking about.
00:16:05.422 - 00:16:59.054, Speaker B: If you don't have the context, it can be very confusing. Okay, so I guess I will mention this is the last thing I talked about last time. A bunch of roll ups today are doing recursive proof aggregation or composition. And I guess covering this also clarifies the pipelining thing. So the idea is they want to sort of work on smaller blocks than they otherwise would, but they can work on the small blocks in parallel. So the idea is you process like a small block, compute the new commitment, and send the little small batch off to the prover to start chewing on the proof. But as soon as you know this commitment, you can kind of start processing the next small block and send that small block off to the prover to compute the next proof.
00:16:59.054 - 00:17:38.654, Speaker B: And you can do that before the first proof is finished. So you can sort of do everything in parallel. And then this is effectively reducing your batch size. So now you have one proof for each small batch, but you don't want to post all of those proofs to L one because your guess cost savings will be poor. So what you'll do is you'll sort of recursively apply the Snark to aggregate all of these proofs into one small proof. The idea is you'll use the Snark for the prover to prove that it knows the proofs. Pi one, pi two up to pi T.
00:17:38.654 - 00:18:05.478, Speaker B: That would have caused each of which would have caused the verifier to accept. Okay, so you need to take the Snark Verifier for the small batches, like represented as a circuit or something, t times once for each batch and then the prover gets applied to that circuit to prove that it knows these T proofs that would have convinced the Verifier to accept. Okay. Does that make sense?
00:18:05.644 - 00:18:08.310, Speaker C: So the witness at the top level is just the pi.
00:18:10.250 - 00:18:26.038, Speaker B: Yeah, and it's the pi eyes and I guess also the CIS because the on chain Verifier for the non, if you were posting these on chains also needs to know the commitment to verify the proof.
00:18:26.214 - 00:18:41.042, Speaker C: Got you. And then at the bottom level, subblock two, the point is to verify the computation assuming some given initial state at the beginning of subblock two.
00:18:41.096 - 00:18:41.554, Speaker B: Yes, right.
00:18:41.592 - 00:18:46.654, Speaker C: And then the correctness of that will be verified in the first of the sublocks.
00:18:46.702 - 00:19:36.050, Speaker B: Right? Correct. Exactly. So this proof is ultimately going to refer to this commitment because if you imagine you weren't doing this recursion, you were just posting these proofs online, like the orange chain Verifier would have needed to know this commitment to check the statement being proved involves this commitment. So that's why you do have to know C one before you can start dealing with subblock two or at least proving subblock two working on computing Pi two checkpoint. Exactly. And the reason to do this is kind of twofold. So the provers in like most narcs can be parallelized, but it can be a bit hairy.
00:19:36.050 - 00:20:48.490, Speaker B: Many of them require the prover to do a giant FFT and that can be paralyzed, but there are certain limitations on it. So this is unlocking more in sort of trivial parallelism because almost trivially, like as soon as you know C one you can just start in parallel working on computing C two and Pi two. And the other thing is, I had mentioned when these Snarks get applied to really big statements, the prover can often just fall over. Some of them are very space intensive for the prover. So this is ensuring that some of the nastiness of the prover like you don't want run into because it's only getting applied to smaller statements, but then you're still getting sort of the gas savings as if it was applied to a big statement since you're aggregating the proofs rather than posting each proof onto L one. Okay, so I wanted to review that because many like ZK Sync, one of the Royal projects, has been doing this for a long time. Actually, as I mentioned last time, they originally were doing it because they were working with a structured reference string that was too small to handle very big statements.
00:20:48.490 - 00:21:58.110, Speaker B: But then other roll ups that use different Snarks are just now sort of launching this kind of recursively composed situation. So I will be discussing some of that later. And then yeah, the last thing I wanted to discuss before kind of going to new material is data availability. So because on the L one we might only be storing commitments to the underlying data that used to live on the blockchain, you have this issue of well, where is that data going to live now? The data underlying the commitment and the reason we needed to live somewhere and be made available to people is, I guess twofold. So one is like the roll up provider might just disappear one day or be compromised and someone else might want to step in and take over. Or if someone might be getting censored, like the roll up provider is refusing to include their transactions in the batches and then they have to force withdrawal of their funds. And I think you can't do that if you don't know the state underlying the commitment.
00:21:58.110 - 00:22:53.810, Speaker B: So, yeah, it is important to have the data be available. And if it's not just living on chain, then how is it going to be available? And sort of there are two different approaches out there. One is to kind of store it on chain anyway but it can sort of be semi on chain in a much cheaper way. So in Ethereum there's something called call data and that's where this data that sort of used to be subject to the kind of full consensus mechanism of Ethereum and stored by all the nodes now can live in sort of this cheaper call data that's not stored by all of the nodes verifying the blockchain. And then there are approaches that will kind of keep the data off the l one and use economic incentives to try to get other parties to make it available. So I wanted to just have a slightly longer discussion about data availability. Yeah, sure.
00:22:53.960 - 00:23:07.554, Speaker C: So thinking about your examples about why you want this call data I think fundamentally what you need is just the ability to reconstruct the full state of the virtual machine.
00:23:07.682 - 00:23:08.262, Speaker B: Right?
00:23:08.396 - 00:23:11.586, Speaker C: One way you can do that is re execution of all the transactions.
00:23:11.698 - 00:23:11.974, Speaker B: Right.
00:23:12.012 - 00:23:24.918, Speaker C: So I would say like having a list of all transactions is sort of sufficient for state reconstruction not obviously necessary. There's some other way you can communicate it, right? Is that right? Is that right that we only care about the transactions in as much as we care about state reconstruction?
00:23:25.094 - 00:24:18.960, Speaker B: That's correct, yeah. Basically to withdraw your funds assuming that the roll up has provided like a forced withdrawal mechanism which I think they're all doing, otherwise people maybe wouldn't use them because they'd be worried their funds would get censored and then they can't get them out. I think what you would need to know is your own account balance which maybe you can just track yourself but then you also need to know the authentication information to prove that it's consistent with the latest commitment that's on chain and that sort of involves aggregated information potentially from all account balances in the whole world. So you definitely don't need to know the whole list of transactions that led to the latest state of the world. But you do need to know if not the full state of the world enough to get that authentication information which in general in the worst case is like the full state of the world basically.
00:24:19.970 - 00:24:24.080, Speaker A: So the DP proof can only give a commitment to the state of the world. Right?
00:24:27.090 - 00:24:50.226, Speaker B: Yeah so these proofs that are getting stored on chain so there's the commitment and then there's a proof that the commitment was computed by applying valid transactions to the state of the world underneath the old commitment. This data that I'm talking about making available sort of has nothing to do with the proof.
00:24:50.338 - 00:25:02.426, Speaker A: No, it gives a commitment to the new state and is there any way to update the state that suppose the state is stored on chain can you update it with the commitment and prove that?
00:25:02.608 - 00:25:34.758, Speaker B: Yeah. So this is going to get into. So let me jump to the discussion. When you say you, do you mean like you're not the roll up provider but you're someone who knows the state of the world? Can you now update, can you function as the roll up provider? So that is the perfect segue to this little conversation I wanted to have, which is should entities other than the roll up service be able to submit proofs to advance the state of the system? Okay. Does anyone have any thoughts on that? So Joe's, giving a thumbs down if.
00:25:34.764 - 00:25:37.800, Speaker A: You know the old state as a commitment, anyone should can.
00:25:40.250 - 00:25:40.566, Speaker B: No.
00:25:40.588 - 00:25:46.918, Speaker A: What if the roller providers themselves update the proof every time they add a new update the state every time they.
00:25:46.924 - 00:25:50.826, Speaker B: Add a new proof? Like on chain yeah, commitment to it.
00:25:50.848 - 00:25:53.514, Speaker A: And then they can also change the state and then prove that it is.
00:25:53.552 - 00:26:43.820, Speaker B: Consistent with the commitment. Yeah, so that's basically what they're doing now if they are keeping the state of the world on chain and call data, that is what they're doing. So with every time they update the commitment, they also include they have a bunch of call data that they include too and they pay the gas fees to get that stored in the on chain call data. People call this like semi on chain, I don't know because only full nodes are storing it or something. So that's what they are doing now. But there's more and more services that either have or are soon planning to roll out off chain data storage and then they might no longer do that. Yeah, I mean, if you just trust the roll up service in every way, you can obviously just trust them to solve data availability themselves.
00:26:43.820 - 00:26:55.280, Speaker B: But the whole point of being worried about data being available is what if the roll up service gets compromised, what if they're trying to censor you and you have to force withdrawal from them?
00:26:56.370 - 00:27:00.126, Speaker A: Yeah, but if the other case is the roll up service goes offline or.
00:27:00.148 - 00:27:02.906, Speaker B: Something yeah, but then the final state.
00:27:02.948 - 00:27:05.380, Speaker A: Is on the actual blockchain, right?
00:27:06.630 - 00:28:14.970, Speaker B: If a commitment to the state is on the blockchain and if they're using the sort of call database data availability solution, then also the underlying data is on chain and at least living in the full nodes or whatever. But again there's this big push to not do that because the gas savings of roll ups is sort of bounded and what I read is bounded by about 100 x or so if you're sort of going this call data route and maybe some of the upcoming EIPS if they get approved. Sharding making call, data expire after a year, things like that, maybe that I don't know if that moves the 100 x up a little bit, but when people will talk about like 20,000 x improvements and I think you really need data off chain if you hope to do that. Even with if they incorporate all the EIPS that are being proposed. And again, that refers to throughput. Okay. So Joe, did you want to interesting.
00:28:15.040 - 00:28:37.120, Speaker D: I think there's kind of arguments in both ways. I think the philosophically correct answer is that anyone should be able to submit a proof. Like the proof is self validating, but seems like it opens things up to a lot more potential attacks and risks. So it's not obvious. Is it actually the right trademark, the right trade off?
00:28:38.370 - 00:29:40.210, Speaker B: Yeah, so I don't think there is a right answer. But I guess just to clarify, I guess if you did want only the roll up service to be able to submit proofs, you could do it just by requiring them to sign the proofs, right? The smart contract says I look for digital signature of the corresponding to the roll up service's, public key or something. So if the answer is only the roll up service should be able to post proofs, I guess the concerns then are like centralization and censorship, right? Like the government, maybe someone could go to the roll up service and say, do not process transactions from this public key. They're an evil party, don't do it. And then I guess that party should hopefully be able to force a withdrawal, but they can't use the service anyway. And so if the world is run by a handful of roll up services, I don't know, maybe that party has to pay crazy gas fees that only roll up providers that have taken over the world can pay or because they can't use a roll up service. Who knows? So I guess those are the concerns.
00:29:40.210 - 00:29:49.800, Speaker B: How can you say that the blockchain is decentralized if the roll up services are centralized and are running the world? I don't know.
00:29:51.290 - 00:29:55.046, Speaker D: There's kind of a disaster scenario if you make them sign every update that they do.
00:29:55.068 - 00:29:56.346, Speaker B: The best team. Yeah, that too.
00:29:56.448 - 00:30:11.254, Speaker D: There's no progress again forever. But you could probably hedge against that by saying if the roll up server doesn't submit an update for Xbox, then all of a sudden it doesn't need to be signed anymore. You kind of open it up or you have like a backup key or something.
00:30:11.312 - 00:31:03.402, Speaker B: Yeah, there seems a really big design space here, but if the answer is yes and anyone can submit a proof or if the service gets compromised, I did want to discuss the following data availability attack. Because actually this hadn't occurred to me until I was making the slides for today. So I guess these things are nice to raise. And then I found discussion online. The roll up services are thinking about exactly this kind of attack and what to do. So a party, whether the person who sort of now has taken over the roll up service or anyone out there who's allowed to function as the prover, can honestly generate a proof that processes some legitimate transactions, but then they just don't tell anyone. In the world what the transactions are or they put out fake transactions that don't actually correspond to the proof.
00:31:03.402 - 00:31:41.260, Speaker B: If you're not careful and aren't not checking for that, they could try that. And the point now is that the system has been sort of updated on chain. Like there's a commitment now to an updated state of the world, but no one but the attacker knows the new state of the world. So the whole roll up is bricks. No one else can advance the state of the world because to do so you have to know a bunch of authentication information and even like a single transaction is going to affect authentication information for the whole state of the, like any piece of data. Yeah. Does that make sense?
00:31:41.950 - 00:31:55.470, Speaker D: It seems like there's a lot of different ways that these could potentially get ripped. It will be interesting. As far as I know, it's never happened. But it'll be interesting to see in the next year or something if the first roll of server actually somehow manages to get itself stuck.
00:31:56.450 - 00:31:57.200, Speaker B: Yeah.
00:31:58.290 - 00:32:01.646, Speaker D: On this slide there's like three different ways that could kind of get stuck forever.
00:32:01.758 - 00:32:47.520, Speaker B: Right. And obviously I'm just raising this as something that everyone has to think about and I'm sure the services are, I'm going to tell you, a solution to prevent this attack even if anyone in the world can update the proof. But this is not necessarily the solution that the services are using. I actually don't know in detail what they're doing. So here it is. So if the data is being stored on chain, the state of the world is in this Call data, you can demand that the Verifier so like the smart contract that runs the Snark Verifier demands all state changes as Call data. So it will take as part of its input like the latest state of the world written to the Call data.
00:32:47.520 - 00:33:48.698, Speaker B: And it will then have to sort of treat that as what's called public input to the circuit that it's applying a Snark to. Right. Because, as I described how roll ups work previously, the prover is going to prove that it knows a bunch of transactions. And the new state of the world that kind of follows by processing those transactions, applying them to the old state of the world, and then show that the new state of the world kind of underlies the new commitment that it's posting to the chain. And in that situation, the new state of the world is part of the witness that the prover knows but is never revealed to the Verifier. So what I'm saying is you have to get that part of the witness out of the witness and directly into the Verifier's hands. So you could think of it as still part of the witness, but it's like getting sent explicitly to the Verifier.
00:33:48.698 - 00:34:41.774, Speaker B: But it has to come directly from the orange chain storage now. Right. Because the Verifier is basically trying to confirm that the new state of the world is actually on chain, so it's available to everyone. And so, done naively, this is extremely expensive because the way that many of these Snarks work, any public input that's fed to the Snark Verifier, like each each little piece of data that's part of that public input, you have to do cryptographic operations. The Verifier has to do cryptographic operations for you. So this is like substantially increasing the size of the public input to the Snark Verifier, causing the Snark Verifier to do a whole bunch of crypto operations, therefore greatly increasing the gas costs of running like the roll up Verifier on chain. But there's fortunately a technical trick you can do to alleviate this called the hash trick.
00:34:41.774 - 00:35:58.240, Speaker B: So this is cryptography's hash trick, not like machine learning's hash trick. So what you do is rather than having the whole call data be public input to the Verifier, public input means, like, both the Verifier and the prover know it, you only make a hash of the call data available to the Verifier. And the Verifier is just going to check directly and this is really important outside of the Snark proving machinery that this claimed hash actually is the hash of some call data. So outside of the proof machinery, the Verifier is going to take a bunch of call data, compute its hash like sha three of call data and confirm it equals this public input being fed to the Snark Verifier. I'm distinguishing here between the smart contract Verifier and the Snark Verifier, which is probably confusing, but basically the Snark Verifier has to do a whole bunch of crypto operations for any public input that's kind of getting fed through the proof machinery. And here we're making sure that only the hash is getting fed through the proof machinery. Whereas the smart contract that is doing the roll up verification is sort of free to also do other checks that don't involve the Snark at all.
00:35:58.240 - 00:36:10.160, Speaker B: Okay, so it looks like this is not coming across totally well, but maybe a hint of what's going on here is coming across.
00:36:11.250 - 00:36:28.034, Speaker A: Just to make sure I understand, the only advantage of this over, like why do we even need roll ups is because you store these things semi on chain. If it was not called data but just on chain, then there's no advantage to roll up here, right? Why do you do all the roll up is to save putting it on chain?
00:36:28.082 - 00:36:30.120, Speaker B: So now you're doing it semi on chain.
00:36:31.050 - 00:36:46.010, Speaker D: I think there'd still be an advantage though, because you don't have to put all the signatures on chain. You just need to know like a pay b, but the signature or any other data that validates that can all be like witnessed with a Snar.
00:36:46.910 - 00:37:15.490, Speaker B: And in principle, also what needs to be available on chain at any time is kind of the latest state of the world you don't need to know the whole history of transactions that led to that. So I don't know that people are doing this now, but you could sort of forget transactions on chain and just at all times only have the node storing like kind of the latest state of the world. I'm not saying that this is an easy thing to engineer or that anyone's doing this, I'm just saying in principle.
00:37:15.650 - 00:37:21.400, Speaker A: Wait, in this solution you need all state changes stored on the call data.
00:37:21.850 - 00:37:35.786, Speaker B: In the call data. What you need is the current data underlying the current commitment. So you could forget everything else. Joe, do you want to say, hold.
00:37:35.808 - 00:37:43.006, Speaker D: On, I mean you can't represent that as like the full state of the world and call data in each block, right? That would be like way too big. It's only the changes.
00:37:43.108 - 00:38:38.320, Speaker B: Right? Yeah. Well, okay, if I'm understanding what you're saying correctly, you're saying that something that people actually, I guess do do now is they're storing only sort of deltas like differences between each state update. They are remembering kind of the whole history then in a sense. But they can net out multiple transactions within each batch, within each state update. Right. So if like in a single batch there's 1000 transactions that all 500 of them increased in account balance. 500 of them decreased in account balance, they can all sort of net out and you don't need to know the individual ones, which is I guess related to what I was saying.
00:38:38.320 - 00:38:41.440, Speaker B: Basically.
00:38:44.070 - 00:38:59.110, Speaker D: Effectively you have to put every transaction, like a summary of every transaction in the call data. But you save on, I guess netting because you can net transactions and then you save on all the witnesses, like the transaction witnesses signatures which don't have to go on chain.
00:39:00.890 - 00:39:08.182, Speaker A: Worst case, this is as costly as every transaction operates on different states, different parts of the state.
00:39:08.316 - 00:39:09.000, Speaker B: Yeah.
00:39:10.830 - 00:39:17.750, Speaker D: Right, but that's usually much usually the signature is bigger than the effect of the transaction.
00:39:17.910 - 00:40:00.220, Speaker B: And you're also saving like the time of all of the blockchain nodes verifying every signature. So you get to both cut out the storage of the signatures and the time to check their validity. Okay. Yeah. So that's the savings if you're keeping stuff semi on chain but with off chain storage, which is like really where the giant gas savings will come in. If people sort of are comfortable with this, here's something else you could do. So you could have some just other distributed system, say using BFT techniques to achieve consensus or something.
00:40:00.220 - 00:41:37.018, Speaker B: And that other system can threshold sign the hash value that's supposed to be the hash of the new state of the world, basically attesting that they are storing the state of the world. So the idea is like if enough people have said, okay, I'm going to try to make this data available, the L one is going to say, okay, well, I guess I'm convinced that this data is going to be made available because enough of you have said that you downloaded it, that someone should be making it available. So this is basically reducing kind it's a reduction from the liveness of the roll up, from data availability for the roll up to sort of liveness of the BFT system that has now sort of attested with some kind of majority or two thirds or something that that BFT system is going to make this data available. So it's just kind of placing the trust in the BFT system instead of in the costlier solution that was keeping the data sort of semi on chain. Just as a reminder, the nice thing about cryptographic commitments is that nobody can fuss with the data and change it. So you only need really one party to be able to broadcast to the world what the data is that they're supposed to be making available. There's no way that someone can maliciously change that data because they wouldn't be able to authenticate it.
00:41:37.018 - 00:41:42.118, Speaker B: With respect to the commitment that is stored on L One yeah, I don't.
00:41:42.134 - 00:42:03.570, Speaker D: Think you need a BFP system for this. I think it's much simpler. You only need one party to keep the data and they don't really need to reach consensus because if you believe like the L One achieves DSP or achieves consensus, then they can all just check that they're storing things. They can check if they got the right data by checking the commitment.
00:42:06.550 - 00:42:32.140, Speaker B: Yeah. So I guess the concern would be that it'd be expensive if you got a million people saying, I have the data, I have the data, I have the data. And you sort of have to figure out which one is telling the truth. Like you can check, but you have to potentially check a whole bunch of people if there's malicious parties out there. And that's just expensive. I guess you don't think that's something people are concerned about?
00:42:36.190 - 00:42:41.674, Speaker D: I guess you could sort of do it through some smart contract where people claim they have the data and don't.
00:42:41.722 - 00:42:44.080, Speaker B: Then they yeah, they pay something.
00:42:47.570 - 00:42:57.342, Speaker D: You could have some simpler you have x dose and they all say they have the data. As long as one of them is on it, then you're kind of okay.
00:42:57.396 - 00:43:14.146, Speaker B: Right, right. Yeah. So at any rate, basically if enough people say they're going to make the data available, you only have to trust one of them. So it's probably fine. Basically, if 50 people say they're going to make it available, one of them is going to I guess DFC wouldn't.
00:43:14.178 - 00:43:28.542, Speaker D: Actually even help with the attack that you're talking about because if you have a DFC system, you can still have malicious nodes that will lie about having the real data. So you still have to check. Right. Like anybody who gives you the data still have to check.
00:43:28.596 - 00:44:06.570, Speaker B: Yeah. I don't know how big the threshold here would be for the threshold signature. Like if it's above a majority, then you could just like yeah. Anyway, I just mostly wanted people to think about this data availability problem in more detail because I was just thinking about it almost for the first time as I was preparing this talk and realizing the subtleties and thought it was a useful conversation to have. So I did it online in the talk. But yeah. Thanks so much, Joe, for keeping me from saying anything too dumb.
00:44:06.570 - 00:45:09.206, Speaker B: Any questions at this point? I know what some of the projects are planning. Like ZK Sync is planning something called ZK Porter. I believe they're launching a token and using basically proof of stake to try to incentivize people to make data available and also sort of be honest about it so people don't waste their time, I guess, checking it and it turns out not to be correct. It's almost like a second blockchain to make, potentially. And it's just you just have this nice property that no one can fuck with the data. But there is, I guess, potentially a cost if people do want to try, there's a cost to sort of root them out, I guess. I think Starkware is already keeping data off chain, but I don't know exactly how they are dealing with these issues.
00:45:09.206 - 00:45:52.700, Speaker B: I don't know the details. There's a large design space. I mean, people have plans at a minimum if they haven't launched. And I guess it remains to be seen how some of these play out, I suppose. Yeah, it can be hard to follow where some of these projects are with their plans and they've been openly discussing their plans for a while. And so you find an old post and it's hard to figure out like, well, has it been implemented yet or not? Yeah, there's a lot of activity in the space. It's hard to track what was very open discussions about plans and what's actually been done.
00:45:52.700 - 00:46:35.330, Speaker B: Okay, so I guess we'll dive into what the roll up projects are actually doing, especially in terms of their Snarks, because that's what I'm an expert on and there won't be too much discussion about data availability anymore. Okay. So remember, Snarks kind of work in a two step process. You got this front end and that sort of spits out some kind of circuit satisfiability instance and then you apply a Snark for circuit satisfability instance. So let me start with front ends. Okay. So there are sort of two extremes that are very prominent, at least they're maybe not the only approaches, but they're certainly, I think, the most prominent ones being pursued.
00:46:35.330 - 00:47:47.594, Speaker B: So one extreme is kind of inhabited by Starkware, which has exposed a high Ish level language. One bullet point later, I call it a limited assembly like language. So it's higher than just writing down a circuit for sure, but it's still know somewhat low level language, not like Java or something anyway. So a high level language called Cairo to programmers and they have an automatic compiler that turns Cairo programs into of the programmer has to be aware of many snark specific issues and there are limitations to it. So for example, all arithmetic in Cairo is done modulo or large prime and greater than unless that integer inequality tests are not supported. And I believe the programmer has to actually leverage what's called nondeterministic advice, which is kind of taking advantage of the ability that you're dealing with a circuit satisfiability instance and not a circuit evaluation instance. So there are sort of ways to keep the circuit small by sort of adding extra elements to the witness called advice.
00:47:47.594 - 00:49:15.606, Speaker B: And I believe the programmer in Cairo sort of has to explicitly program that advice. In many cases I think that's true just looking through their high level descriptions of Cairo without actually using it myself. Yeah, so the idea here is you're sort of pushing more responsibility to the programmers and giving them limited expressivity. I guess programmers today write smart contracts in Solidity and for a programmer ideal programmer experience you'd like them to not have to change that. So that's the other extreme is just let programmers write in Solidity and there are a whole bunch of projects seeking to do this and they sort of all use the term Zkevm but mean slightly different things by this and ultimately the circuits that they're planning to use might differ. I think some of them are okay, so last time I mentioned so I'll say this again I believe the way Ethereum smart contracts work is they're typically written in solidity and then they're compiled into what's called EVM bytecode, that's Ethereum virtual machine and then the smart contracts actually execute that bytecode. So I believe some of these projects are like literally kind of taking each bytecode instruction and coming up with an optimized sort of circuit that implements that bytecode instruction.
00:49:15.606 - 00:50:27.326, Speaker B: And so they're working sort of directly with representing the bytecode and others are kind of transpiling solidity into other lower level languages and then sort of turning those other so they're not going through EVM bytecode and then turning those other lower level languages into circuits. So just different sort of engineering approaches to go from solidity into the circuits that are equivalent to the solidity program. And as far as I know, I've heard the Starware team publicly podcasts say one reason they chose their approach was basically faster development time. This just takes a ton of engineering to make this reasonable. So they went with the kind of easier compilation problem where the language is sort of designed so that all programs in it are snark friendly and amenable to kind of standard off the shelf techniques. So I believe, as far as I understand, the Zkevm projects have not yet launched full front ends and backends on testnets. So I think ZkSync, they launched a testnet back in February.
00:50:27.326 - 00:51:06.800, Speaker B: I believe that it's generating and executing circuits, but not generating the proofs for the circuits yet. In contrast, so ZK Sync 1.0, which I'll talk more about in a bit, only supports token transfers and NFTs, so much more limited functionality than general smart contracts. But that has been live on main net for a couple of years already. So ZK Sync 2.0 is what's going to have this very exciting support for Solidity where no one will have to know anything about Snarks to use it. But it's a very difficult engineering problem.
00:51:06.800 - 00:51:10.430, Speaker B: Okay, any questions about that?
00:51:10.580 - 00:51:15.090, Speaker A: Does group know the front end circuit when they're checking a proof?
00:51:15.670 - 00:51:17.422, Speaker B: Yeah, they need to know the circuit.
00:51:17.566 - 00:51:19.438, Speaker D: Like the Verifier on chain.
00:51:19.534 - 00:52:09.198, Speaker B: Yeah. So great question. The Verifier needs to know the circuit too, because if it doesn't know the circuit, it doesn't know what the prover is actually proving statement about. And I'll say more about this shortly. Okay, so something I want to point out is that there might be a tension here between achieving performance of these roller projects and security in the sense that especially with the Zkevm projects, because it's just a very difficult problem to turn general Solidity programs into circuits. Like Solidity was not designed with Snark proving machinery in mind at all. So what many of them are doing is they're sort of using very intricate hand coded gadgets, which just really means like optimized circuits for common subroutines or instructions.
00:52:09.198 - 00:52:47.886, Speaker B: Okay, so just as an example, to give you a sense of this ZkSync 1.0 in the ZK Sync repo. So this is like not even dealing with Zkevm, this is just like NFTs and transfers and swaps. They have like 5000 lines of Bellman in the ZK Sync repo. So Bellman is like a language you're almost like hand coding a circuit, like just writing down gate by gate to circuit almost. It was originally designed for Zcash, I think, and now the whole community uses it. But then even outside the ZkSync repo they have something called the Franklin Crypto Gadget Library and that has well over 5000 more lines of Bellman in it.
00:52:47.886 - 00:53:34.906, Speaker B: And something that's particularly complicated just for them is I think I mentioned that they're doing this composition of Snarks and it's kind of nasty to represent a Snark Verifier as a circuit. Exactly. What complications come up depends on what Snark you're using because the verification machinery is different. So they wound up with an annoying situation where Ethereum today supports just one pairing friendly curve. People have different names for it, but BN Two Five Six is one of the names. And for technical reasons this curve is not friendly to recursion to representing the Snark Verifier as a circuit. So they wind up having to implement really big integer arithmetic in a finite field, in a circuit.
00:53:34.906 - 00:54:18.730, Speaker B: And so one of their big achieving this when they did in 2020 is like a huge engineering accomplishment. But it's complicated to make that efficient. Like the circuit is really reasonably small. So that's like 5000 lines of Bellman code, like just in one repo to handle big integer arithmetic. So just to be clear, these efforts are some of the most innovative important engineering efforts in the space. But the tagline of ZK sync is rely on math, not validators. This is sort of to differentiate from optimistic roll ups where you're hoping that people will raise an alarm if they detect any fraud.
00:54:18.730 - 00:55:11.166, Speaker B: They call those validators. But just to be clear, the math might include like 15,000 lines of hand coded circuits. And so if there is some bug in there, the prover winds up proving a statement about a circuit that is not actually equivalent to the statement that we really want to be proven. And maybe that is a security vulnerability, maybe it's just a bug, but people should be aware of that. So I think formal verification of these circuits is an important direction. Like sort of in the long run it's just such intricate hand coded circuits auditing. We're going to audit them as best we can, obviously, but it'd be much nicer to have some kind of automated formal guarantee of equivalence or something like that.
00:55:11.166 - 00:56:06.160, Speaker B: But it seems like a challenging verification problem. Now I guess the main project maybe that's not doing this sort of complicated hand coded circuit specifications is this Cairo compiler. But that's also pushing more responsibility onto programmers. So it's not clear that that's less bug prone. It's just the bugs might be coming from the programmers who aren't necessarily expert on snarks or something like that. And so I do want to mention so there was a nice paper I learned about recently. So their transformation from Chirop programs to circuits, they formally verified it with some proof assistant, but of course they haven't, I shouldn't say of course, at least right now I don't believe there's machinery for the programmer to confirm that the Cairo program they wrote matches what they intended it to mean.
00:56:06.160 - 00:56:44.780, Speaker B: Yeah, so kind of might be a pick your poison situation just to be aware of. Okay, the last thing I want to say about front ends. So Cairo picked an interesting approach to generate their circuit. I was surprised to learn this actually recently. So they generate a universal circuit with a von Neumann architecture. So that means it actually takes as public input the Cairo program to be run and like the circuit itself kind of compiles and runs the program on the witness. So I think this gets to your question.
00:56:44.780 - 00:57:31.338, Speaker B: The smart contract snark verifier needs to know the circuit that you're dealing with. This is one way to deal with that. And to have just one smart contract that supports all circuits is to just have a universal circuit like this. But this is sort of well known to this approach to generally lead to big circuits. It's analogous to like a CPU architecture, in particular a von Neumann one that's kind of compiling the program as part of the computation versus an ASIC. An ASIC is like an application specific circuit and there's sort of two main sources of overhead here. One is the circuit winds up repeating the entire transition function of a CPU for every step of the program.
00:57:31.338 - 00:58:25.270, Speaker B: And so it's like at each step of the program, only one of the possible CPU instructions gets executed. But the whole transition function sort of has to have logic. It doesn't know in advance what that instruction is going to be, has to have logic kind of implementing all of them. And that's another reason I think they probably tried to keep the Cairo language as limited as possible to keep that transition function as simple as possible. So hopefully I'm giving some sense of what this means and ASIC if you somehow know in advance or something what instruction is going to get executed at timestep 50, you can just sort of have logic for that instruction there. But if you have a Vin Neumann architecture, you don't know that because you have to be able to handle any program. So the other issue is potentially like the program kind of has to get compiled inside the circuit.
00:58:25.270 - 00:59:39.874, Speaker B: So I found the blog post Darkware was saying that basically the second source of overhead is not so big. And my guess is that's probably because their programs, they might be fairly simple, but at a minimum they're probably applying them on a very large batch of transactions. So the work to compile the program might be minimal compared to the much longer work to sort of run the program on many, many transactions. But that sort of doesn't address overhead one because the circuits that they use, they use something called they call it air algebraic and demeter representation, kind of have a CPU formalism built in, right? So whether they're sort of taking an ASIC approach or a program specific approach or not, they still have the CPU formalism. I can't say like, oh, I took a Cairo program and I sort of implemented it with a totally different intermediate representation that takes an ASIC approach. And this was the difference in size. But it's sort of well known that there are overheads to this particular approach in terms of the size of the circuits that come out.
00:59:39.874 - 01:00:35.874, Speaker B: I do want to mention that the cost of large circuits in these Snarks is mostly borne by the prover because verification tends to the costs grow very slowly with circuit size. So as far as like, gas costs, this isn't such a concern. As far as latency and prover costs, maybe it is. Anyway. So this is just my general impression of the approach. And there are other ways to achieve kind of one smart contract for all programs. Like the ASIC approach is still consistent with not having a different smart contract to verify every different smart contract or program people want to write, there are various ways to kind of preprocess the circuit so that the verifier kind of just has to deal with what comes out of the preprocessing.
01:00:35.874 - 01:00:52.362, Speaker B: So the preprocessing might be fairly expensive, but it doesn't have to happen. Like on chain essentially. Yeah, that's what I wanted to say about front ends and how people are taking quite different approaches to the problems.
01:00:52.496 - 01:00:54.410, Speaker A: Do you know how big the circuit?
01:00:54.990 - 01:01:00.014, Speaker B: I don't. It's probably something that could be figured out.
01:01:00.052 - 01:01:07.280, Speaker A: Although every instruction in the Cairo language has to be converted to the circuit form.
01:01:07.810 - 01:02:10.434, Speaker B: Yeah, but there's nothing specific to Cairo about that. Like Snarks just need circuits and I'm using circuit in a broad sense. Starkware itself might think of this algebraic intermediate representation as not a circuit, but conceptually it is just a certain kind of low level representation that's amenable to designing a Snark for. But the specific way they're going about it is in designing the programming language called Cairo, it effectively involves programming a very simple processor and they are taking that processor and representing it kind of as something like an arithmetic circuit that operates over a finite field. And then if someone wants to run a Cairo program for a million steps they're sort of repeating that processor, sorry, that circuit implementation of the processor like a million times. Does that make sense?
01:02:10.472 - 01:02:14.180, Speaker A: It depends on the size of the program itself. The runtime of the program.
01:02:14.790 - 01:02:26.742, Speaker B: Yeah. So you would say the circuit they finally generate is like here's the Cairo program. I guess as public input for each.
01:02:26.796 - 01:02:28.870, Speaker A: Step there's a description.
01:02:31.390 - 01:03:24.250, Speaker B: This might be more in the details for me to say definitively, but no, I think it's like, literally, like maybe like, here's the text of the program maybe transpiled in some way and also the number of steps it needs to be run for or something. And then there's going to be a bunch of advice inputs basically plus the witness itself and then the circuit will basically using the advice, help confirm that. So the advice might actually be like a transcript, like a step by step record of running the program for a million steps on the witness and the circuit just checks that at each step of this transcript the transition function or the circuit representation of the transition function of the Cairo program was correctly applied.
01:03:24.990 - 01:03:26.140, Speaker A: That makes sense.
01:03:29.950 - 01:04:17.594, Speaker B: Yeah, exactly. So IVC is a certain kind of proof system that supports applying some kind of simple function to an input and proving you did it correctly and then updating the proof to capture another application of that function, then updating the proof to capture another it's just the function is the process. Yeah, but they're not going through IBC, they're representing the many, many applications of the function as a big circuit and hitting the whole circuit with a monolithic Snark until they start recursively composing Snarks as I sort of had the little two level tree before, which starts to. Get closer and closer to IBC when you do recursive composition snarks question other.
01:04:17.632 - 01:04:19.194, Speaker D: Things that aren't snarkware doing the second.
01:04:19.232 - 01:04:20.300, Speaker A: Thing you said about.
01:04:20.910 - 01:04:59.830, Speaker B: Yeah. So ZK sync uses Planck, and Planck has holography, fancy term for this preprocessing baked into it. So at least at a high level, that's what they're doing. And it's easy, especially for ZkSync 1.0, I think, because there's basically only six programs that might be run, and each is just like swap mint NFT or something. But yeah, for each of those there's a circuit. The circuit is maybe hand coded and very complicated, but it sort of gets summarized.
01:04:59.830 - 01:05:57.820, Speaker B: So what happens is what this really means is there are certain polynomials that capture the wiring of the circuit that's used inside the Snark, and if I give a third talk, you'll see them. And so the preprocessing winds up producing cryptographic sort of commitments to those polynomials. So the verifier only needs to know the commitment to the polynomial. So these polynomials, like encode the circuit wiring, so the verifier only needs to know sort of commitments to the circuit instead of like a full description of the circuit, which would be very complicated and large, potentially. Does that make sense? Yeah. And in that way you can write sort of one verifier that sort of takes as it has to be like a trusted input, the commitment to the circuit and then operate does the same exact thing. It's just the circuit is kind of getting into the verification through the commitment to the circuit instead of a full description of it.
01:05:57.820 - 01:06:16.248, Speaker B: Okay, let's see. Time 20 minutes. So, yeah, this good. I'll get through basically the roll ups, and you still won't know how snarks work in any detail. And then if Tim lets me, I can tell you some more detail next time. Okay, so that's front end. So let's talk about back ends.
01:06:16.248 - 01:07:04.130, Speaker B: And this is really what I know more about to be on, but they're both really important for end to end call, so I need to talk about front ends. So just a very brief summary on a high level of what various projects are doing. So Starquare's backend uses the Fry polynomial commitment scheme combined with some constant Round polynomial IOP, which I won't get into the details of at all. So Fry, if you remember from last time, it's the polynomial commitment scheme that's with the shortest proofs among Plausibly post quantum polynomial commitment schemes. And the proof length is like log squared of the size of the polynomial being committed, like times the security parameter. So they're still bigish. We could hope to do better in the future, maybe.
01:07:04.130 - 01:07:38.280, Speaker B: Certainly compared to KZG polynomial commitments, which is used in Plonk, they're not post quantum and they require trusted setup. So that's what ZkSync is using is just Plonc. So that is a constant Round polynomial IOP combined with KZG polynomial commitments. Instead of Fry, it's a different polynomial IOP underlying it. Too. There's a project called Polygon Zero, which is using Planck, but they want post quantum security and they also like Fry for other technical reasons I think. So they're just kind of swapped out KZG with Fry.
01:07:38.280 - 01:08:14.070, Speaker B: So Zcash is now swapped out KZG with Bulletproofs. None of the roll ups will do that because Bulletproofs Verifier is not work saving. So the proofs are short, but the Verifier has a long runtime. So you wouldn't want that to happen on chain. So I thought I'd mention that as like an alternative. But that's not nice design choice for roll ups. And kind of, as I'll say, a little more about soon, I think that the prover computation in these Snarks is going to increasingly be a problem.
01:08:14.070 - 01:09:03.700, Speaker B: I would speculate that for these Vkevm projects, just the circuits that come out might even after all the engineering be pretty big and the proverb work to prove those circuits will just be challenging. That's pure speculation, but it's just compiling solidity into circuits just seems like a challenging problem. So I know several projects are focusing on hardware acceleration for the prover, like, I guess designing asics like the actual hardware asics for common operations done by these provers. Okay, so now I'll get into some more details of the performance of some of these projects. My main example is going to be Starkware. So they have kind of two products, if you will cut StarkNet and Starkx. I'll say more in a second about them.
01:09:03.700 - 01:10:00.616, Speaker B: My goal is just to give a sense of the current latency, the current gas savings, the current throughput and the current security features of the know that the most prominent projects are using now. So just some background on what are StarkNet and Starkx? So StarkNet is a public smart contract l two. So I guess you can write, I think, like smart contracts via Cairo, like arbitrary Cairo programs and they can interact with any other smart contract deployed on StarkNet, I think. And then they're posting validity proofs of batches of StarkNet transactions to Ethereum. I think it's a pretty recent launch. It appears that there is only several hundred thousand dollars of total value locked in StarkNet right now. Starkx, in contrast, is permissioned and application specific and it's been running much longer.
01:10:00.616 - 01:10:42.020, Speaker B: There's much more value sort of locked up in it right now. The main, I guess, client if you is dYdX, which is decentralized exchange, although apparently they're leaving Ethereum soon. So I guess that will StarkWare's ethereum roll up. So that'll change. But anyway, for what I'm going to describe, actually you can forget dYdX. So there are several other Starkx users and they turn out to be most relevant for the performance results I'm going to share with you. And that's because Starkware actually has StarkNet sharing approver with these Starkx applications.
01:10:42.020 - 01:10:52.404, Speaker B: And they call the shared prover sharp shared prover and the proverbs closed source. But the Verifier runs as a smart contract on Ethereum. So you can see the Verifier.
01:10:52.532 - 01:11:03.612, Speaker D: When you say they share approver, do you mean, like, it's the same software implementation? It's the same actual server that's running it? Are they actually all in one big proof together?
01:11:03.746 - 01:12:17.012, Speaker B: Yeah. So as far as I know, they are in one big proof together. And the reason they can do this well, the way it's architected so they can do this is because everything has to go through a Cairo program and because they generate a universal circuit that can handle any Cairo program. They can just have, I guess, like, one big circuit that I guess takes as input now, like, I don't know. Well, I guess four programs and then the Cairo programs for each of the, I guess, the StarkNet contracts that have transactions in there in the batch, and then the whole batch is proved at once. And I think the reason they're doing this now is just to get the volume up enough to get gas savings, basically. So there's just not that much volume on StarkNet now, so it might not be enough to actually get major gas savings, but if they sort of combine batches with the higher volume Starkx clients, then you get enough to actually get gas savings, basically.
01:12:17.012 - 01:13:11.408, Speaker B: Does that make sense? Okay. So, yeah, I wasn't aware of the shared prover when I sort of started trying to figure out some performance numbers. So I thought I would just be able, like, there was just a single StarkNet prover, and StarkNet has a nice block explorer. So I thought I'd be able to sort of just see everything going into the roll up and just have sort of unequivocal information on performance. But because of the shared prover and the Starkx is not some public blockchain the way StarkNet is, I did email the Starkware folks, and they were very helpful in helping me understand the performance. Here are my findings as far as latency goes. So, proofs appear to be taking 8 hours to generate.
01:13:11.408 - 01:14:10.010, Speaker B: I don't know this for sure. So the minimum delay between a transaction hitting StarkNet and any proof getting verified on L One is definitely 8 hours or more, and it can go up to, like, 16 ish hours. So what seems to be happening is they're waiting 8 hours for a batch to complete, and then they're computing the proof. And, I mean, it's possible that they're computing the proof quickly and just not posting it until the next eight hour window is up. But there's no reason to do that. I didn't ask them directly, how long is your proverb taking? But their answer was not inconsistent with the proofs taking 8 hours to generate. So anyway, what I can say with certainty is the latency for transactions is between eight and 16 hours, and eight of the hours are just waiting around for the batch to fill, but the rest is not waiting for the batch to fill.
01:14:10.010 - 01:14:40.636, Speaker B: Okay, as far as what it costs to verify a proof. It's costing on average about zero point 46 e. So this is just me looking at a Block Explorer for Ethereum proofs take about 38 Ethereum transactions to verify. This is also consistent. I mean, I did this calculation myself, but there's sort of a nice website that just displays daily fees that the roll up services are paying on Ethereum. So this matched up to what they were reporting.
01:14:40.748 - 01:14:43.532, Speaker C: Do you remember what it is? Just in units of gas.
01:14:43.596 - 01:14:44.604, Speaker B: Units of gas.
01:14:44.732 - 01:14:47.492, Speaker C: This sort of depends on the gas price.
01:14:47.546 - 01:15:33.570, Speaker B: Yeah, no, I should do that. So for ZK for the millions, I assume. Yeah, I know Plank proofs are 510,000 gas, and just my own estimates are that by using Fry, the proofs and the parameter settings that they're using, it's going to be about an order magnitude bigger. But what the Verifier does to check the proof is different. It's a bunch of hash evaluations instead of a bunch of elliptic curve operations. So I can't say that the gas costs are exactly proportional to the proof size. Yeah.
01:15:34.100 - 01:15:38.672, Speaker D: Also somewhat of an artifact of what was pre compiled on Ethereum.
01:15:38.736 - 01:16:23.490, Speaker B: Right? Yeah. But Starquare is using KSAC two five, whatever, two five six, which is Ethereum's like built in hash. So everyone obviously uses the primitives that Ethereum makes nicest to use. Yeah. But obviously whatever gas cost Ethereum set were somewhat arbitrary and not done with roll ups in mind, and then people just have to live with them. Okay. So, yeah, like I said, Starkware was nice enough to send me, which I really appreciate transaction counts from like a week of Starkx clients because they said there were APIs, which I never figured out.
01:16:23.490 - 01:17:05.760, Speaker B: Anyway, they made it simple for me. Based on those counts, there appear to be about 90,000 transactions in a batch of which only a really small fraction are StarkNet transactions. So that sort of might explain certainly StarkNet users being interested in sharing approver. And yeah, sort of like Tim said, just talking about ETH per proof verification isn't so helpful. I should have looked at gas, but just sort of very unscientific estimate for the sort of gas savings they're getting. I estimated about 100 x. And this matches something that they said in a blog post.
01:17:05.760 - 01:18:03.868, Speaker B: I don't know how old the blog post was, so seems like I was getting the right ballpark. Something to point out is just looking at gas costs, there's more costs that the roll up provider is sort of paying that are hidden because they're running the prover, which might know killing an Amazon cluster or something. Right. So I don't know what those are. And then for the Starkx clients, I suspect, and they sort of mentioned it in the blog post announcing Cairo, that well, that was my interpretation of their language, that an expert Cairo programmer might know they have to write in Cairo so that they can all share the same smart contract. But you might have an expert Cairo programmer for a Starkx client and a StarkNet person is writing the Cairo program. You know, they talked about optimizations that were being done in the Cairo program and that's probably, I'm guessing, coming from an expert programmer.
01:18:03.868 - 01:18:51.344, Speaker B: So those are sort of the caveats about this 100 x number. And they claim, and I certainly believe them, 1000 x savings for dYdX and 20,000 x for certain mass NFT Minting operations. I don't know the latency of these applications, though. I think they mentioned for this 20,000 x, I think the email said that these were like for 600,000 NFT Minting operations. So that would be a much bigger batch than what I was seeing, 90,000 transactions in the Sharp batch. So 600,000 is at least seven times more than that and it's specific to the NFT Minting anyway. So, yeah, you get big enough batches and you can see like really amazing gas savings for sure.
01:18:51.344 - 01:18:55.120, Speaker B: And the question is just how big can you get those batches?
01:18:55.780 - 01:19:10.212, Speaker A: You're saying the Verifier multiple transactions? Yeah, a very rough estimate is up like 7 million gas for that price right now. It makes sense you'd have to split it.
01:19:10.346 - 01:19:57.716, Speaker B: Yeah, and that would also make sense as be I was just guesstimating the proofs are maybe order magnitude bigger than Plank proofs. And plank is 510,000 guests. So they're sort of matching up. Fortunately there were blog posts and stuff they provided to me in email, and the performance I was sort of guesstimating through extremely unscientific means is fortunately like matching up with them. Even though my methods are not great, there doesn't seem to be any kind of mystery or disagreement here or anything. Let's talk about security. So Sharp claims it's running at Snark at 80 bits of security.
01:19:57.716 - 01:20:40.144, Speaker B: So what I mean is the smart contract won't let it run at lower than 80 bits of security. And I looked on chain at what the parameters are and it's running at 80 bits of security. But one thing I sort of want to clarify here is it's actually running Fry, which is the polynomial commitment it's using at 48 bits of security. And that's under what I consider pretty aggressive conjectures about its soundness. So it's basically asserting a specific, fairly simple attack is optimal. So the proved security like what we can formally analyze of Fry is at most 22 bits. So how are they going to 80 bits of security if they're claiming 48 bits from Fry? So they add a 32 bit proof of work puzzle.
01:20:40.144 - 01:21:31.030, Speaker B: That basically so what's kind of happening is for the prover in its own head to kind of attempt to find a convincing proof of a false statement with probability two to the -48 it has to first solve a 32 bit proof of work puzzle. Okay, so they call this grinding. Okay, what this means though is that sorry, this is a little technical I guess you can just jump to, like, this bulletproof here. So with more or less two to the 64 Hash evaluations, you can have a two to the -16 chance of finding a convincing proof of a false statement. And if you up the number of Hash evaluations to two to the 80, you can get that probability like, quite close to one. So this is like an attack that is there even if the conjectures are true.
01:21:31.800 - 01:21:36.772, Speaker A: Okay, can you say something about these conjectures for 48 versus for 22?
01:21:36.906 - 01:22:30.088, Speaker B: Yeah, shortly. There's a gap between the attacks. We know what a cheating prover can, with what probability, convince the verifier to accept and with what obviously we can analyze. And there are certain technical lemmas that go into the analysis. So one part of the conjecture is that, like, this technical so all of this involves Reed Solomon codes. And the technical lemma is proved up to what's called the Johnson bound, which is some kind of list decoding radius for REIT Solomon codes. And so that turns out to be like, one minus square root, the rate of the code.
01:22:30.088 - 01:23:24.476, Speaker B: So one part of the conjecture is that the same result would apply all the way up to you can get rid of the square root over that rate. And that conjecture alone is not enough, because the way they analyze the combination of the Fry security analysis with the polynomial IOP involves list decoding. And so the Johnson bound, which is the list decoding bound, comes in again. So you then, I guess, need even more like a second part of the conjecture that says not only can the technical lemma analyzing Fry go up past the list decoding bound, but you also have to conjecture there's like another way sort of around needing list decoding or I don't know enough about list decoding. Maybe, I don't know if you could just conjecture that there are better list decoding results out there. I have no idea. Really? Yeah.
01:23:24.476 - 01:24:23.000, Speaker B: But this list decoding bound is coming up in terms of so the prover is claiming to have sent a Merkel Hash. The leaves are evaluations of some polynomial, but the verifier doesn't actually know, like, is this a polynomial? So the question is how far away from a polynomial is the actual thing that's committed? And that's where the Johnson bound is coming in. Is that helpful? That is proved in an information theoretic statistical sense. And the crypto is only coming in in terms of Merkel Hashing with collision resistant Hash function and then applying fiat shamir to cut out the interaction. Yeah. So 22 bits refers to the statistical security of the interactive thing that then gets sort of Merkel Hashed in. Okay, so here's my really half baked analysis, though, of these attacks.
01:24:23.000 - 01:25:14.876, Speaker B: So I think they might eventually make economic sense. I mean, they certainly don't today, but I looked up but apparently several years ago, two to the 64 shaw One evaluations cost about a million dollars. And in principle, the way people think about these roll ups is if you can produce a convincing proof of a false statement, you can potentially steal everything in the roll up. I mean, could you in practice? I don't know, people will. And could you actually spend all those funds? Probably not. But the idea is that the snarks are just going to protect you completely and all bets are off if someone finds a convincing snark proof of a false statement. So the hash function they're using is not Sha One, it's some variant of Sha Three because that's what's built into ethereum.
01:25:14.876 - 01:25:49.610, Speaker B: I think that's only a factor of like, three or something slower. So you could imagine running this attack, you pay a million dollars or something, and if you succeed with probability of two to the -16, you get, I don't know, everything in the L Two, if they actually gain traction, could be $100 billion. Right. In principle, I don't actually think you could spend that money. And the expected winnings are significant. Like, you would come out ahead in expectation, just not a great probability you're going to succeed. But if you do, you win so much money.
01:25:49.610 - 01:26:19.776, Speaker B: And if these roll ups, some of them might launch a token. I mean, Starquare hasn't announced plans for that, but ZkSync is going to for their off chain incentives, data availability and in that definitely an attacker could make money without being able to spend stolen funds. Right? You just ruin the roll up. The token is worthless if you shorted the token. That's how you can make your money. And you could potentially find other ways to short companies. They don't have a token.
01:26:19.776 - 01:26:34.490, Speaker B: So, yeah, right now, attack, it running like the sort of trivial attack on this kind of snark doesn't make sense. But in a world where roll ups are highly successful, it might.
01:26:35.980 - 01:26:43.908, Speaker A: Only the group approvers can make proof, who can add a Stark proof over?
01:26:44.094 - 01:27:15.990, Speaker B: Yeah. So in practice right now, I think the answer is only Starkware, because StarkNet prover is shared with Starkx. And I don't believe that the data underlying Starkx is available publicly. That's my best understanding. But if know it's new, every time I check on it, the total value locked in it is going up. That'll be a public blockchain. All of its data should just be sort of publicly available.
01:27:15.990 - 01:28:18.090, Speaker B: I didn't see anything in these kind of sharp Verifier smart contract that required a digital signature from Starkware. If you look at their way of thinking and their plans and blog posts they described it, actually, it wasn't clear to me. They talked about, well, what if the roll up provider gets so in the context of the data availability discussion we had before, they were worried about the roll up provider getting compromised, which suggested to me that they were imagining only the roll up. You only had to worry about the roll up provider doing this data unavailability attack situation. But if the data underlying the state of the world is public and the smart contract doesn't require a signature, then anyone can submit proofs. So I don't have a complete firm answer to your question. Okay, so let's discuss why they would run at 80 bits of security.
01:28:18.090 - 01:28:57.190, Speaker B: So in a sentence it's to save gas. So what they're doing, they could increase the security without meaningfully affecting the proverb know it would only slow down the verifier that's running on chain. So they're doing this to save gas. Okay, so some takeaways here. So the existing post quantum Snarks, they all have bigger proofs than the shortest non post quantum Snarks. And in particular, those proofs grow linearly with the security parameter. So this is why Starkware is sort of setting that security parameter aggressively and small.
01:28:57.190 - 01:30:06.188, Speaker B: Yeah, and I think I already mentioned that my rough estimate is at the parameter settings they're using their proof should be about ten x bigger, roughly eight to ten, something like that. So there's this notion that post quantum security is better. Future proof kind of part of the branding of the post quantum roll ups. But if your classical security is substantially worse, possibly to the point of making attacks make economic sense now, is it really better? Especially while the L one itself is using non post quantum secure crypto and all this stuff. So that's something to consider. Also, while I don't know that the prover is taking 8 hours to run for sure, it sort of appears to be unless they're just sitting on the proofs. So I think that the latency will continue to be a concern moving forward as batches get bigger and people maybe start doing more than just doing swaps and minting NFTs if that ever happens.
01:30:06.188 - 01:30:56.428, Speaker B: Something to point out is snarks with slightly bigger proofs might be fine if the prover is significantly faster because then you can take the batch sizes to be bigger and like a bigger proof from a bigger batch might actually be better than a smaller proof from a smaller batch. So this is sort of where I'm hoping to make some headway with the composed Snarks I was mentioned last talk, okay? And then I know I'm out of time. So I have an ultra brief case study for ZkSync. And then like one wrap up slide and then that's it. Okay, so just to compare because ZkSync is using Plonk, it's not post quantum, it's not transparent. So just to compare performance, they have a Block Explorer. They actually post timestamps when the commitment to the new state of the world is posted to the L one.
01:30:56.428 - 01:31:51.128, Speaker B: And they also post timestamps when the proof gets verified. So looking at their Block Explorer, the proofs typically capture several thousand transactions, sell much less by least in order magnitude than these sharp StarkNet and Starkx batches. And the latency tends to be several hours, but this appears to mostly just be waiting around for batches to fill because I wasn't able to get a sense on my own of how long the proofs take to compute. But ZK sync documentation says it should be about ten minutes. So I don't have any verification of that. But yeah, this was just an example of what you can see on the Block Explorer. So all of these things have been committed, but not to l one, but the proof hasn't been posted, whereas all of these it says verified here, the proof has been posted and verified.
01:31:51.304 - 01:31:56.716, Speaker C: Would you say the relatively fast proof of time is driven mostly by the limited set of operations, or is the.
01:31:56.738 - 01:32:59.308, Speaker B: Fact that these driven SNARP also yeah, it's very hard to know. I mean, in general, I would expect for a given circuit size, the Starkware approver should be Fry bottlenecked for the prover time. But I would also suspect, because they're using the CPU abstraction, that their circuits are pretty big, but this is just based on just high level they're doing this CPU abstraction here. The circuits are highly hand optimized, and the prover should more or less be KZG bottleneck, but also has to maybe commit to more. I don't know the comparison of how many polynomials are getting committed in each, and that's also going to really affect the prover time because Planck requires the prover to commit to a large number of polynomials, and yes, StarkWare's polynomial IOP probably does too, but I don't know the exact comparison. So, yeah, there's a lot of components to these Snarks. So to actually isolate bottleneck costs, you really have to sit down and look at the circuit, look at the underlying polynomial IOP, look at what the polynomial commitment scheme is.
01:32:59.308 - 01:33:12.180, Speaker B: I think for a given polynomial, like one polynomial of a given size, how long does it take KZG to commit to it? How long does it take Fry to commit to it? I think KZG should be a little faster.
01:33:15.080 - 01:33:19.476, Speaker A: Circuits here are a lot smaller, right? Because they only do one payment operation.
01:33:19.588 - 01:33:58.792, Speaker B: That's all it yeah, yeah. So I would guess the circuits are smaller, the batch sizes are smaller by a lot. And this is getting a little more speculative, but I would think that the prover for a given circuit size is also probably a little faster with Plunk than with anything using Fry. And so they all sort of combine to give apparently about ten minutes, whereas versus possibly 8 hours. Although, again, I can't say that with certainty. Should have just emailed them and said how long we're taking instead. Yeah.
01:33:58.792 - 01:34:51.080, Speaker B: Is that helpful? Okay, so just the last thought for today. With the case studies and this discussion of security and latency and stuff, I hopefully made clear that I do think that these front end and back end like Snark choices have major implications for performance and security of the roll ups. Another thing I realized I was trying to just understand what these roll ups are doing. There's a lot more that goes into them, just designing the whole system and the community. I think the Block Explorer for StarkNet was just contributed by a community member. These smart contracts on StarkNet are probably going to interact with each other. So there's a lot of success of the roll ups are going to depend on a lot more than the thing that I know about.
01:34:51.080 - 01:35:17.024, Speaker B: But when security is important and latency and throughput are important, you should understand these details that I tried to kind of COVID today. But there's a lot more to and so the more I look into these roll ups, just the deeper sort of the rabbit hole goes of everything you have to consider as you design or build them.
01:35:17.142 - 01:35:28.944, Speaker C: I will say that whole list is relevant for almost any project. And so what's really cool here is that in addition, how well you can implement these core primitives through science also.
01:35:28.982 - 01:35:37.296, Speaker B: Really? That I agree. Yeah. So maybe for most projects, it's everything but this. And here's the union, the core engineering.
01:35:37.328 - 01:35:39.450, Speaker C: Challenges are usually hard and interested.
01:35:40.060 - 01:35:40.810, Speaker B: Yeah.
01:35:42.140 - 01:36:02.812, Speaker D: Aside from Starkware, which I guess they have some closed source stuff, I think most of these other projects are open sourcing everything. So I guess I wonder how much even if they develop, like a really fast prover, like it can sort of be adapted by anybody else.
01:36:02.866 - 01:36:28.500, Speaker B: Right? Yeah. I mean, it's not clear. And then with hardware acceleration for the prover, it's a whole nother story because some of these projects actively want to distribute the prover, both for liveness reasons, other reasons, too, I guess. But if you're imagining running the prover on an ASIC because that's what it takes, you need all these people out there to own these asics.
01:36:30.380 - 01:36:48.510, Speaker D: I guess I'm wondering about, from a business perspective, what's the barrier to entry for any of these roll ups? If the market kind of settles on a specific proof back end, front end that everybody likes, then it could be portable for somebody else. Launching their own competing roll up.
01:36:52.080 - 01:36:52.444, Speaker B: Could.
01:36:52.482 - 01:36:54.400, Speaker D: Be like a barrier.
01:36:55.140 - 01:36:58.924, Speaker B: Yeah. So potentially the hardware, it's a centralized.
01:36:58.972 - 01:37:07.680, Speaker A: Bottle, and people have to trust that they yeah.
01:37:07.750 - 01:37:31.924, Speaker B: I don't know. You mean you could imagine a world, I guess, where there's a centralized roll up with the best performance, but no one else can use it, but if they start censoring you, you can go to a less performant roll up. Just speculating. I don't know. Yeah, and there is this difference in open sourcing stuff. I know ZK Sync 1.0, for example, is all open source.
01:37:31.924 - 01:38:00.624, Speaker B: And I don't know if Starker eventually plans to open source some of these provers, but they open sourced, approver for a toy computation, like a hash chain verification, apparently. But that doesn't support General Cairo program. So you certainly can't use it to prove StarkNet roll. Ups, even if it wasn't shared with Starkx. So, yeah, I don't know what the plan is.
01:38:00.662 - 01:38:05.104, Speaker A: There another open source project that tries to optimize the provision.
01:38:05.232 - 01:38:51.010, Speaker B: Yeah, a lot of the projects are open source. Polygon Zero is open source. Yeah. I do wish, having gone through this exercise of trying to figure out what a lot of these projects are doing, I wish there was more than just the source code, because it can be hard sometimes. You have to go to the source code for some of them to know what's going on, and that's not a fun process, but it's definitely appreciated that there's a ground truth to what they're doing because the implementation is there. Okay, so that's a good stopping point for today, and maybe next time we'll actually see what these marks are. Maybe after 3 hours of listening to me.
01:38:51.010 - 01:38:52.610, Speaker B: Yeah. Thanks.
