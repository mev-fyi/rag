00:00:07.410 - 00:00:07.960, Speaker A: You.
00:00:09.850 - 00:00:43.402, Speaker B: So thanks everyone for coming here. So this is going to be a little bit different than your standard seminar series, but we're trying to fit it into the eleven to 12:30 p.m. Slot today and tomorrow. This is going to be a little mini workshop on shared sequencing for roll ups and hosted by a 16 D. Thank you so much. I'm Ben Fish, I'm the CEO and co founder of Espresso Systems, which is working on decentralized and shared sequencing for roll ups. I'm also an assistant professor at Yale University.
00:00:43.402 - 00:01:25.558, Speaker B: So let me first quickly just go over what the agenda is going to be today and tomorrow. So we're going to have guest speakers today just to introduce the topic. I'll start then we're going to have Josh Bowen, who's here from Austria, another company that's also working on shared sequencing for roll ups. We have Quintess here from Flashbots who will be talking about some of the mev aspects, and Tarun Sheetra as well, who will be here, should be here in like five minutes. I think he's just running a bit late, but we'll be giving the last talk. Taruns from Gauntlet. So tomorrow we're going to then convene here at eleven.
00:01:25.558 - 00:02:25.062, Speaker B: If some of you can't be here, that's fine, but you can just come for the talks. But it would be wonderful to have as many people participate in the working group discussions as well. We're going to introduce the three open problems that we'll be discussing tomorrow, today, and then tomorrow it would be great to split into three groups, work for an hour, discussing them, whiteboarding, whatever, and then have someone from each group present for ten minutes at the end. Okay, so great participants, of course, a 16 Z research team. We have some roll up teams who are joining remotely over Zoom. So we have some people here from Zksync, from Optimism, from Aztec, and then additional guests. In addition to the four speakers I mentioned, including myself, we also have Michael and John who just walked in from DBA, and Uma who's in the back at Succinct Labs.
00:02:25.062 - 00:03:24.718, Speaker B: So great. Let me first just start introducing the problem space here. So, roll ups are horizontally scaling the application layer of blockchains. We can think of roll ups as making it such that when you add new transactions for a new application, or even an application that itself hosts many other applications, it doesn't computationally burden the layer one because you can. Have a designated set of servers that are even just run by that application who do all the computation and just prove the results or report the results to the layer one. So the key points of a roll up are that it enables sharding of computation across applications, and two, that it's leveraging heterogeneity in the network. So you can have very powerful nodes that are doing the computation help weaker nodes do less computation.
00:03:24.718 - 00:04:35.766, Speaker B: If every single node participating in the blockchain had exactly the same computational power, then you might get the benefit from Sharding. But roll ups are really taking advantage of this heterogeneity as well, which is a natural feature of the realistic networks that we have today. And so the weaker nodes in the layer one can be very, very decentralized, whereas the powerful nodes which are doing the computation are typically less decentralized. Today roll up servers control a lot. So these roll up servers that are either app specific chains or more general platforms like Optimism ZK Sync, that host on top of them other applications, they centralize the entire process of deciding which transactions to include and in what order. So people are concerned that that undermines some of the point of running on blockchains in the first place because it gives these roll up servers the ability to perhaps censor. It also leads back to monopolistic pricing and mev, unless, of course, these servers are trusted.
00:04:35.766 - 00:05:38.026, Speaker B: A lot of the mechanisms that have been designed, like EIP 1559, that can achieve non monopolistic pricing rely on the myopic behavior of miners or validators in the consensus protocol. And that very much has to do with its decentralization. So we'll talk a bit more about economics. So with the roll up servers controlling this whole process, it's as if the L One is really just auditing the integrity of Web Two applications rather than really leveraging the layer one to actually run Web Three infrastructure. So one solution is to sort of separate this ordering from execution entirely so that the roll up servers are only doing the sharding of computation, which is what roll ups are supposed to do. And we can still use the layer one infrastructure to just make data available and order it. So the L One would only order transactions, make it available to applications.
00:05:38.026 - 00:06:23.100, Speaker B: Roll ups would be reading from this layer one that's ordering the data and then just reporting what the state change is and proving the result. A ZK roll up would prove the result. And an optimistic roll up would of course just report the result and somebody could challenge it through a fraud proof. So this has also been called based roll up architecture. But I sort of see this as just using the L One to do the ordering and availability. And so again, the architecture is that the roll up server would read a transaction stream from the L One and then report to stay root and produce proofs. The users will be submitting the transactions directly to the L One.
00:06:23.100 - 00:07:12.140, Speaker B: There are some challenges, but I'll get into some of the challenges later. I won't dwell on them right here. Something else to note is that we could separate out this ordering layer into a layer, what we might call a layer one and a half, so that it's actually separate from the layer one itself. And why might we want to do that? Well, first of all, just protocol modularity. So it's nice to have a separate logical protocol that's handling the ordering and availability, even if ultimately it's run by the same physical nodes as the layer one like Ethereum. But there's also an opportunity to make different design trade offs from the l one. So you could make design trade offs that optimize for higher throughput, perhaps lower latency, but not the dynamic availability that Ethereum has.
00:07:12.140 - 00:07:32.980, Speaker B: And if you do so, then it gives you the ability to give pre confirmations. A layer one and a half with fast finality can provide pre confirmation that a transaction will be included in the execution trace before it actually gets settled on the l one. So those could be reasons to do a layer one and a half.
00:07:34.150 - 00:07:34.610, Speaker A: Yes.
00:07:34.680 - 00:07:39.746, Speaker C: Is there a reason that you keep grouping together the ordering and the availability? You view those as fundamentally linked.
00:07:39.858 - 00:08:50.394, Speaker B: I do view them as fundamentally linked because a service that provides ordering typically would also provide availability. But you can view them as separate modular components of the same protocol. You can be ordering references to available data in some other data availability layer, but typically a consensus protocol which would be used for creating finality and ordering would also be leveraged for availability because availability is almost a step within consensus. But that's a little in the weeds, but logically they can be separate. Speaking of which, logically separate but same physical node. So through the advent of services like Eigen layer as well, we could even get the l one validators to opt into running these other protocols. And then the only thing that is changing from a user perspective is the properties of the protocol, not the physical set of nodes who they are trusting to run the service.
00:08:50.394 - 00:09:31.654, Speaker B: So the other issue with roll ups and in general like app chains that are now through roll ups, scaling the horizontally scaling the execution layer of Ethereum is that it leads to more isolation between applications. So it compromises on liquidity and interoperability. Applications on different roll ups are isolated from each other. Bridging across roll ups becomes complex, atomicity is limited. You can't easily do a cross roll up, flash, loan, et cetera. So one question that comes out of this is can shared sequencing help interoperability? Well, there are some advantages. Okay.
00:09:31.654 - 00:10:12.120, Speaker B: They can partially simplify cross roll of Bridging and atomicity. This is a whole other discussion topic, so I won't go into this too much. I'm going to get to the open questions that we're going to be focusing on in the workshop. Sharing a single consensus layer as opposed to having many, many independent consensus layer can reduce systemic security risks. That comes from bridging across many different consensus protocols. And the one that I want to focus on the most is that it can support cross roll of building with economic bonding. And this cross roll of building is also what gets us to this cross domain mev problem which leads to some of the main open questions that we're looking at today.
00:10:12.120 - 00:11:02.520, Speaker B: So let me just focus for a bit on what I'm talking about here in terms of cross roll of building. So a builder who tries to build blocks simultaneously for multiple roll ups faces complications. Well, high risk of failure rather than success. You could say it's high risk of success. But if you're trying to simultaneously win auctions for multiple roll ups and you're making some guarantee to a user or some promise to a user that you will only include their transaction on one roll up, if you include it on the other. You're taking on a lot of risk because there's a possibility that you'll win one auction for one roll up and not the other. And so your transaction will end up being included on one roll up but not the other.
00:11:02.520 - 00:11:57.686, Speaker B: So if you're a builder who's trying to provide promises across multiple roll ups at once, it's much simpler if you only have one consensus protocol to talk to that can auction off wholesale a super block for all the roll ups at once to you. So, through proposer builder separation, a consensus leader can accept from a builder simultaneous blocks for one or more roll ups. And now cross roll up builders can promise any form of atomicity, from flash loans to Arbitrage. And the fact that they only need to talk to this one consensus protocol makes that possible. Now, this could either require the user's trust in the builder or economic, even better economic enforcement of promises. So builders could post slashable bonds such that if they violate their promises, then they would get slashed. There are fraud proof mechanisms for doing this.
00:11:57.686 - 00:12:53.130, Speaker B: Uma actually has a very nice post on how you can have a fraud proof mechanism for a special subset of the things that a builder could guarantee, such as Bridging and atomicity or cross roll up messaging. But in general, the shared sequencing layer ensures that an honest builder who is trying to fulfill the promises to these users will not get slashed if it doesn't violate those promises. And that would not be the case without a shared sequencing layer. Here's just a diagram of how this all fits in. This is a diagram from Espresso, but it would look similar for other project architectures as well. Okay, so now I want to quickly introduce the three open problems that we'll be focusing on. Okay, so first is revenue sharing.
00:12:53.130 - 00:14:08.434, Speaker B: How is revenue shared among roll ups or applications even, that are sharing the same sequencing layer? In fact, you could even look at this problem of how roll ups utilizing the same sequencing layer share revenue as a problem that we should already have today on Ethereum. It's just that applications on Ethereum are not demanding a share of mev. But if you view mev as the revenue which is being generated by Ethereum, that's not being shared with the applications that are generating it. So uniswap could say, well, look, I'm contributing a lot of mev, why don't we get a share of the mev? It's all going to the validators roll ups have raised this as more of a concern, perhaps because roll ups are also layers as opposed to just applications. But in any case, I just want to point out that this problem has already existed before the advent of roll ups because all applications share an underlying consensus protocol or sequencing protocol. And we could try to solve that problem for applications on Ethereum as well. So for basic fees, it's straightforward because you can see, okay, all the transactions for this roll up or all the transactions for this app are paying this much.
00:14:08.434 - 00:14:52.430, Speaker B: We can give some of that amount that they're paying back to those users. Where this becomes a lot more complicated is when the revenue that's being generated by an individual transaction or a set of transactions from an application or a rollup are not so transparent. And this is the case for mev. So the marginal contribution of each rollup to the overall mev is not transparent. In fact, mev can be dependent on private information available to various agents in the system. It could be information on some exchange somewhere that they know about. And in general, we only discover the total mev by running an auction.
00:14:52.430 - 00:15:33.450, Speaker B: It's very hard to truthfully simulate an auction for each roll up independently to see what they would get if they were running the sequencing layer themselves. So how do you figure out we were to isolate this roll up, what its overall contribution to the mev is? Right, that's the problem. And that's one of the main open problems we'll be discussing tomorrow. So if this problem appeals to you, you should come join that group. It's connected to this order flow auction problem where the goal is, okay, so we have users. Users are submitting transactions. You can think of roll ups as just special types of users.
00:15:33.450 - 00:16:37.454, Speaker B: So in this order Flow auction problem, the goal is to design a mechanism which will allocate to each user some rebate for some fixed percent of their marginal contribution to the mev. There could be variations of this. So users are submitting their transaction to some trusted party or ideal functionality that we implement somehow that's running the auction. The auction is taking bids from searchers that bundle these user transactions in different way and then ultimately this results in some bundle or block with a payment to the sequencing layer and some rebate to the user. So the goal here of and I'm being intentionally vague here because I personally don't fully understand if there's a single definition of an order Flow Auction. So it's describing it in a very general way here, but in terms of this goal, variations could include ensuring that users get a certain percent of the searcher and sequencer profit. And as I was discussing with tim yesterday.
00:16:37.454 - 00:17:36.370, Speaker B: That could actually be quite hard even for an individual transaction. If you're running, for example, a second price auction where one party is bidding much values the transaction much higher than the other, then we only discover what the second price is. We don't discover from just looking at the blockchain what the total value of the mev is to the searcher. So the rebate would not necessarily capture that overall mev, it would just be capturing the percent of the sequencer profit or only the winning bid, which could be perfectly fine. I'm just pointing out that there could be different variations of this goal here. So what exactly is the goal with roll ups sharing a common sequencing layer? So I would say that one way of looking at it is comparing two different worlds. One in which users or roll ups would directly run the auction with searchers, so they would directly auction off their order flow to searchers.
00:17:36.370 - 00:18:52.470, Speaker B: This corresponds to every roll up running its own sequencer, right? Because then the searchers are going to talk to each roll up individually and every roll up is running its own auction. Compare that with the users or roll ups sending their transactions to a sequencing layer, which then runs the auction with the searchers and gives some rebate to the users. So the question is, can we design a mechanism in this world over here which ends up giving roughly the same rebate to users as the users would get in this first world over here? Other people may have different ways of looking at this problem. So if this isn't helpful, that's fine. The overall problem is just how do we make rollups sharing the same sequencing layer happy that they're getting their fair share of the overall revenue so very quickly, and then I'll pass it off to Josh. So another open question is whether a shared sequencing layer can be agnostic to choices around how different roles want to approach the mev mitigation problem. So there's many different approaches to addressing mev, right, from optimizing it to preventing it entirely.
00:18:52.470 - 00:19:30.782, Speaker B: And there are many examples. There's different types of auctions. There's also first come, first serve ordering policies. There are approaches where we try to keep the transactions hidden until after they're committed, using threshold encryption mechanisms to try to achieve some kind of content oblivious ordering policy. There's various things based on time delayed permutations the design space is very large. Some involve various assumptions, others do not. So some of these ordering policies can only be implemented assuming an honest majority of validators.
00:19:30.782 - 00:20:26.140, Speaker B: So ultimately the question is, if we have one sequencing layer that all these different roll ups are sharing, to what degree can this sequencing layer be agnostic to the mev mechanism favored by each roll up? Could you have one roll up which is actually implementing a first come, first serve ordering policy? Could you have another which is Threshold encrypting intuitively for threshold encryption. It wouldn't be too hard because you could just have all the transactions for one specific roll up, be, Threshold Encrypted, they're all still being processed by the same sequencing layer, but those are Threshold Encrypted. The key could be shared among a set of servers that are designated to that roll up. It wouldn't need to necessarily be part of the common infrastructure and you could do the same for first serve perhaps. So this is an open question. It's one of the open questions we'll be discussing tomorrow. If this appeals to you, please come tomorrow and join that group.
00:20:26.140 - 00:21:29.726, Speaker B: And finally, a sequencing layer also needs to charge something for transactions to control for congestion and spam. But since the sequencing layer is not actually executing each of the roll ups, this statelessness leads to some challenges. So first of all, the sequencing layer, while it can easily charge based on the size of transaction data that it's processing, it can't easily charge for the gas being consumed. Because it's not executing on each of these roll ups, it doesn't exactly know how much gas is being consumed. Perhaps the data size could be a proxy for the number of constraints that you're creating for a ZK proof system, but I don't know exactly. So this is an open question. Can you bound that right? The other issue is how does the sequencing layer even verify the correctness of these fees? If you want the sequencing layer to be stateless, but you're asking it to verify the fees, that isn't exactly compatible, so it requires keeping some minimal state.
00:21:29.726 - 00:22:02.010, Speaker B: Perhaps this could be a separate roll up that's just for the sequencer fees or it's tracked by the sequencer consensus protocol itself. Perhaps it involves a separate fee token and that can also lead to UX challenges because users don't want to make fee payments in two different tokens. Okay, so those are the three open problems and I'm going to pass it off next to Josh from Astria to go a little bit deeper on shared sequencing layers.
00:22:02.590 - 00:22:47.910, Speaker A: Thanks Ben. So I'm Josh, I'm the founder CEO at Astria. We're working on a very similar thing to Espresso shared sequencing layer for roll ups. I'm going to go over, I'm not going to answer any of these questions. I'm going to kind of like restate some of them. And by and large, the goal of this talk is to cover some potential architectures and generally just come to some shared understanding of what is the even design space that we're working within such that we can have more productive kind of workshops tomorrow where we say, okay, we're at least all speaking somewhat of the same language on the assumptions. So, as Ben stated, some of the open problems in shared sequencing, really the top three are going to be these revenue allocation, the mev mechanism, Agnosticism, and the transaction fees.
00:22:47.910 - 00:23:21.358, Speaker A: And so I think I'm going to start at a very low level and kind of work up. So we'll say what is the sequencer when we use this word? I think there's different definitions used by different projects for kind of the point of this discussion. This is the very simple architecture I'm going to use for a sequencer in what I'll say like as a centralized roll up. Today you have a user, they're going to submit transactions. Those transactions are going to go to the roll up sequencer. That roll up sequencer is going to give a soft commitment to the user saying, hey, I saw your transaction, I received your transaction. I give you a promise that I'm going to include it in my next block.
00:23:21.358 - 00:24:22.530, Speaker A: And then the roll up sequencer is responsible for generating a batch of transactions a block. There technically doesn't need to be like a one to one thing there, but they are going to be responsible for posting the data to a data availability layer in the centralized sequencing design. And there's some flexibility here, but we're going to assume there is like a separate data availability layer that is like the mental model like ASTRI uses, rather than necessarily coupling the sequencing layer and the data availability layer together and then this data availability layer will give a firm commitment of inclusion. The reason for a lot of this, if we look at Ethereum today, a lot of this is for speed reasons and UX reasons that an end user prefers, right? We can go look at all of Google's research they built on from stuff from the does a user prefer when using a website, they generally like to click a button and have a response in this one to four second window. Anything longer than that gets our getting concerned. We look at Ethereum and Ethereum is going to have twelve second block times. So that's a little bit slower than a user would like a confirmation.
00:24:22.530 - 00:25:10.446, Speaker A: If we look at what I'll term like the next generation DA layers, specifically Celestia is the one I'm obviously most familiar with. They have 15 2nd block times right now. So again, these base DA layers are giving us block timings that are slightly slower than what an end user might prefer. So one of the benefits of sequencing is giving this faster user experience. What's a shared sequencer? I'm going to use a relatively, again, simplistic model here. We're going to assume that we have a user and we have these roll up RPC nodes. I'm not going to go into too many details on those in this talk, but we're going to assume that a user to submit a transaction has some server that they are using a wallet or like an app that is talking to this server, and that allows them to actually construct what I'll say like a stateful transaction that corresponds to a specific roll up.
00:25:10.446 - 00:26:08.546, Speaker A: Right? So the users are not going straight from themselves to like the shared sequencer, which in our model is kind of taking opaque bytes and it says, hey, look, I'm just ordering blocks and these blocks consist of opaque bytes. And it is the job of a roll up somewhere to choose to interpret these bytes as a transaction within its kind of state machine. But the user is presumably going to interact with his roll up RPC, and then that RPC is somehow going to communicate these transactions in some serialized kind of opaque format to the shared sequencer. And then it is the responsibility of this shared sequencer to then give these soft commitments to the user. And the benefit here, right, is that we have one shared sequencer for many roll ups and therefore we can get, as Ben discussed, consensus over this. We can get different design trade offs for kind of the strengths of these commitments. Something I'm not focusing on here, but I think it's like well known, right, is existing centralized sequencers that are run for a single roll up are generally like one node, and they're not like one node.
00:26:08.546 - 00:27:18.490, Speaker A: In practicality, I'm assuming optimism is running like a High Availability cluster, but from a logical actor perspective, right, there's a single entity responsible for running nodes. In this case, the shared sequencer as kind of represented by the three boxes. It is assumed to be like multiple actors in kind of a decentralized network mechanism similar to like an L one blockchain. And then again, the shared sequencer. This is some nuance here, but there is some mechanism by which batches are being posted, the DA layer. One of the open questions that we can cover tomorrow, if it's interesting, is, is it beneficial for the shared sequencer to be, at a consensus level, responsible for posting these batches? Essentially, is it a slashable offense if a shared sequencer who gave you a soft commitment does not do the additional labor of relaying that block to a DA layer of your choosing and giving you a firm commitment later? That's actually like an open design question, I think. But this is what I'll use for a simplistic model here to go through I'm going to go through what I view as like a transaction lifecycle here, right? So we'll assume that transactions or intents, if we want to go down that rabbit hole, start in this, what I'm saying, like unordered transaction space in a standard L one, this would be a mem pool that we connect to a peer to peer layer.
00:27:18.490 - 00:28:08.338, Speaker A: I specifically don't label this a mem pool because as I think Quintess will go over, there's a pretty broad design space of where transactions come from. I will generically refer to this as order flow wherein transactions start. And there is kind of this moment of a user signs a thing and that says the user is essentially giving authority to say someone is allowed to debit my account on a given state machine. But how that kind of flows from here is like a pretty broad design space, but then the transactions become ordered in a block and then they eventually become executed. And so specifically for this discussion, I guess it's useful to go to the more labeled one. We're going to say sequencing is the act of ordering. It is specifically not the act of executing, wherein executing actually generates an update to a state database as well as like a resultant state root.
00:28:08.338 - 00:28:41.654, Speaker A: We're just going to assume merkel roots. Now we can talk about merkel trees or whatever, but we're going to assume that there's like a deterministic state route from a given state DB. And then there is an additional stage that we're also not going to cover that is like proving where we view proving in this kind of linear fashion as like the actor who is doing execution and proving probably the same person, but strictly you could do execution and not proving. But this is like the linear flow we're going to assume for this talk. You can disagree with this, but this is what the premise I'm going to base the rest of this on the important thing as it relates to kind of yeah, tim so in the last.
00:28:41.692 - 00:28:53.290, Speaker C: Slide you kind of said, oh, it's a shared sequencer, of course it's sort of decentralized. And is that because once you've decoupled the sequencer from the roll up, you're worried about censorship, for example, or why did you make that jump?
00:28:54.190 - 00:29:23.254, Speaker A: Yeah, I'll go that one. Right? So I guess this is a little bit of like a personal premise to some point of what is one of the root problems of kind of the sequencers within roll ups right now. And there is this concern about censorship resistance and centralization in general, right? We can get into mev and we can have however much kind of fear mongering you want. Right. But there's strictly you don't know if you are being censored by a given sequencer. Right. It's not visible because you don't know.
00:29:23.254 - 00:29:53.038, Speaker A: Right. They could reject it. I'm not like a distributed systems research PhD, but there is some general understanding of it's hard to prove whether you're being censored or whether there's just like a latency thing. Like you just get a 500 when you submit a transaction. Like is that malicious or is that just like the system's not working? Right. Decentralization here gives some benefits in that censorship resistance. And generally what we view to the idea of why do we have a sequencer versus posting data to the DA layer directly or to the L one directly, right.
00:29:53.038 - 00:30:46.410, Speaker A: Is generally to get this speed and generally this liveness property, right. You want to have a faster response time, so you want to have some useful commitment. Going to a decentralization probably implies like a consensus algorithm because now you want consensus over that you're in a less trusted system. But yeah, it's an assumption that's the useful thing we actually want out of that one of the things kind. Of shared sequencing from at least the ASTRI perspective kind of evolved out of as. Like a thing that we wanted to create was because in what I'll say, like the celestial realm of sovereign roll ups, there was at least an assumption that the naive design does not necessarily give what I'll call like, an inbox model or like, a forced transaction inclusion from a base layer. And if you assume you don't have that, there are obviously designs that could have that, but if you assume you don't have that, you've dramatically increased the importance of the censorship resistance of your sequencing layer.
00:30:46.410 - 00:30:55.186, Speaker A: And so for us, it was kind of necessary to say that must be decentralized because if you don't have forced transaction inclusion, then you're just done if you get censored there.
00:30:55.208 - 00:31:12.002, Speaker C: I see, so you're envisioning. So if you just think about a single roll up running its own sequencer, you kind of also have in mind, like, today's designs, where you do have the forced inclusion. And so part of your vision here of multiple roll ups is like they actually are a little more they don't directly ever interact with the L one, perhaps.
00:31:12.066 - 00:31:12.822, Speaker A: Yes, exactly.
00:31:12.956 - 00:31:18.102, Speaker C: Without that escape hatch, you need to hold the sequencer accountable. Is that a fair summary?
00:31:18.166 - 00:31:44.802, Speaker A: Yeah, that's kind of the path dependency of how we got to our origin of like, oh, we need this decentralization as a top priority. We obviously see all the existing roll ups having roadmap items for decentralizing their sequencers, but it's strictly like lower priority for them because they can say, look, if it crashes or if you think you're being censored, you do have today a mechanism by which you can get around that. Whereas our initial design said, well, we don't have that. So that bumps the priority of that.
00:31:44.936 - 00:31:57.734, Speaker B: I think Tim's point is that decentralization of the sequencer and sharing of a common sequencer are still two orthogonal things. It's hard to imagine all role of sharing a centralized sequencer but they are.
00:31:57.772 - 00:32:33.300, Speaker A: Still yeah and I think one of the things about we use the word like shared sequencer like astrinspresso I think we use in a very similar way, but the kind of implied actual like all the adjectives are it is a shared, decentralized, lazy sequencer is kind of the set of adjectives that we would define the whole thing. But we generally assume that's like a shared sequencer, whereas Uma has had like a post on I think you call it a validating sequencer or like a validity sequencing. Right. So there's other designs, but we're to this assumption, right, where the sequencer is doing this step. It is not doing this step. It is not doing this step. Right.
00:32:33.300 - 00:32:46.454, Speaker A: We could proceed. All right. Yeah. So going back right, order flow here, we're going to say that's kind of generic. I'm not going to go into too much depth in that, but it's very relevant. Right. Where are the transactions coming from.
00:32:46.454 - 00:33:37.094, Speaker A: I'll have like a final slide that'll post some kind of chaotic options, routes can happen and how it might have implications on mev and various revenue sharing. Sequencing is the one we're really focusing on going from a set of unordered transactions to an ordered block and then we're going to assume execution improving is done kind of beyond the scope of this. Right? And the important point is that mev is occurring at the ordering of the block. Right. We are also assuming again, this is like again, this is a very broad design space, but we are going to assume that the execution layer is not reordering the block based off of the sequencer's order because, yes, that is within the realm of possibility. But there becomes a question of why are you using a shared sequencer that you are presumably paying some network fee to do ordering for you if you then choose to also do ordering yourself? It's a little bit of a question of what are you getting here? Not that there's nothing you can get like liveness, you can get availability. Right.
00:33:37.094 - 00:34:21.958, Speaker A: But generally we're going to simplify the problem and assume that we are not reordering and thus mev occurs at the sequencing step. Right? So this is like a slightly modified form of the kind of standard mev supply chain flow. And we're going to need to explain dissimilar to the transaction lifecycle. We're showing who the actors in the system are. And so for this diagram, I've used like a shared sequencer mem pool that can be abstracted. And I think I have other slides that abstract this to make it a slightly simpler picture. Right? But this is I think I'm assuming we're generally familiar with the mev supply chain space, right? You're going to have a user, they're going to go to an RPC, they're going to sign a transaction that's going to go to a mem pool in like a normal sequencing roll up or even an L one that's going to go to the specific mem pool of that L one.
00:34:21.958 - 00:35:02.446, Speaker A: And then searchers are going to look at that mem pool. We can expand this to say where are searchers acquiring their order flow? More generically, right? It could be from something like Swab, it could be from off chain, whether those are auctions, whether it's a payment for order flow style system, whether it's through any of these kind of like mev shielding, like RPC nodes. But searchers are somehow acquiring that. Those searchers are building bundles that then builders will build in the blocks. And the shared sequencer proposer is the thing that is actually receiving a block on ethereum like L one. Today, I think we're sitting at, I don't know, Quintess, you have more recent numbers, but what it was like 90% or something of blocks are still going through like mev boost. And we're looking at about four builders.
00:35:02.446 - 00:35:21.314, Speaker A: And as I understand it, searchers and builders are not as separate of entities as they may often be modeled. There's a bit of kind of consolidation, but this is useful context to have. Here's a more simple model. We're going to say a user sends an intent. An intent could be a transaction execution, trace, whatever. But you have order flow. That order flow is going to go to searchers and builders.
00:35:21.314 - 00:36:14.698, Speaker A: We're just going to assume these are kind of mev actors off chain within the system. Again, the shared sequencer proposer is going to receive. That the kind of key thing here, right? And tying back to where does mev happen? What are the things if we assume that the shared sequencer is the thing responsible for doing the ordering and the roll up node is not reordering the transaction after that, then this shared sequencer proposer is the entity in the system that has a monopoly on the ordering of a block. And as I'm sure everyone is aware from like an economic principle, right? The actor that has a monopoly in the system is going to presumably be able to extract the most rent out of the system. I believe on ETH, we're looking at somewhere in the 90% range of mev profit is going to proposers right now that's like back of the envelope that I've seen on Twitter. So this is going to be a similar situation. But we've pushed this to shared sequencer proposer, right? And obviously the implications from this to all this is, well, okay, I'm a roll up, right? And I'm using this.
00:36:14.698 - 00:37:08.598, Speaker A: Why do I want to inject a third party that has monopoly on ordering to extract essentially the transaction revenue of which mev is essentially like you can think of it broadly as the real market rate of transaction inclusion. So this is kind of a useful framing thing. I want to touch on briefly to kind of get to like a definitional thing about think about this. The word and kind of the history of like Astria is originally ruined what we were calling a settlement layer and we've referred to L ones as settlement layers before this terms go around. Settlement is like probably a relatively poorly defined or at least agreed upon term within crypto. I actually like Nick Carter's definition he gave from his post on like it's the settlement assurance is stupid from 2019. I think it says that settlement assurances refer to a system's ability to grant recipients confidence that an inbound transaction will not be reversed.
00:37:08.598 - 00:38:17.040, Speaker A: And I think this is useful when we say a soft commitment. This is kind of the actual guarantee that an end user is expecting to get right. They're saying, you told me that you accepted my transaction and I'm going to not worry about that transaction anymore. We generally have a kind of two phase thing of this soft commitment and what we call a firm or a hard commitment wherein you get a soft commitment from the shared sequencer. You get a firm commitment from DA layer, but it's all kind of these settlement assurances and then we can go into fraud proof and fraud proof windows and ZK proofs and whatever. But that's what I'm going to use as the definition of settlement here. So then if we ask at a higher level, like what is a shared sequencer? We can read that a shared sequencer is a single domain where multiple roll ups are being settled, right? And again, we can get into definitions of tiers of finality here, right? Where like, well, what happens if the shared sequencer gives you a commitment and then that data never shows up on a DA layer or a proof is never generated for that data, right? But fundamentally the expectation that roll ups using a shared sequencer are getting that is giving some sense of settlement in that transactions which go are paid to be included in that and get put into a block by the shared sequencer are settled for some definition of settled here.
00:38:17.040 - 00:40:29.334, Speaker A: So now I'm just going to cover kind of the open problems and restate them a little bit. Again, we have revenue allocation, mev mechanism, agnosticism and transaction fees. Again, going back to this, this is the useful kind of set of assumptions we're making is that the shared sequencer has a monopoly on ordering. And if we think about this one again, right, mev occurs here, so ordered block, mev monopoly on ordering in this mev supply chain thing, if we think about revenue allocation, I'm just going to state kind of the problem and some potential kind of like sub questions here, right? The top level question is how is revenue shared between multiple roll ups using a single shared sequencer? And there's kind of two components of that there's what is the fair or deserved portion of the total revenue that a given single roll up should receive? There's also what I'll say like an architectural, like an engineering level question, like what is the method by which this revenue is shared? So let's say we have some magic mechanism where we say we know a priori which percent of all the sum revenue should be given to which roll ups, where are we sending that revenue? Right? Who is the roll up in this case? Right? Like what entity is responsible for receiving that? What state machine would a rebate even exist on? Right? This ties into the transaction fee question, are we collecting the revenue on the shared sequencer which might have a minimal state machine for transaction inclusion and then we're paying the roll up on the shared sequencer? How is the shared sequencer aware of who the receiving party for a given roll up is? Is there some registration phase to say I am this roll up? Do we have some kind of governance mechanism by which you say, well, there is a set of namespaces that may represent individual roll ups and you have to have some mechanism of claiming a namespace ahead of time prior to using the shared sequencer. So I think beyond the kind of question that is beyond kind of my specialty of how do you kind of economically calculate like a deserved amount? It's literally even if you knew that, how are you choosing to pay? What is an entity that is a roll up using the shared sequencer? Again, I don't have an answer. I'm just adding questions here. We think about mev mechanism agnosticism.
00:40:29.334 - 00:40:41.546, Speaker A: I'm not going to go too in depth here, obviously, but it's fundamentally can a shared sequencer be agnostic to a given specific mev mechanism favored by a specific roll up? Right? So Ben covered, right? We can think about like, first come, first serve.
00:40:41.578 - 00:40:41.806, Speaker B: We see.
00:40:41.828 - 00:41:12.040, Speaker A: Like, Arbitrum has a very strong kind of desire for that. Threshold encryption has been proposed by the shutter guys as well as I think the Fairblock guys and the radius guys are doing some kind of threshold encryption thing. I think as well, that makes a little bit more sense because you can again, just say, well, the transactions that are going into the shared sequencer are just opaque bytes and they are encrypted. And then the roll up is the entity that knows how to decrypt those. Right? It can kind of work through this. We have unordered transactions, we get an ordered block and then their execution. And at the execution phase is where the threshold decryption could happen.
00:41:12.040 - 00:42:17.886, Speaker A: But then fundamentally, the kind of driving question we're thinking about here is to be like a useful thing. What are the constraints that a shared sequencer must inherently place on a roll up to just exist? Right? If anyone's used like in a Web Two world, right, you can use an API and they can often be very generic. But fundamentally, to define an interface, it can't be perfectly generic. You have to pick some level of kind of definitions and say you have to satisfy this interface and then touching on transaction fees. Right? So this is kind of I'll generalize today. Like, what inputs does the shared sequencer proposer accept, right? And so if we go back to that kind of like mev supply chain style where we could have searchers or builders or off chain actors in the system, right? What is that shared sequencer actually accepting? Is it accepting a full block where this block may be what we've called like a mega super meta block of one block that represents many, many potential roll up things. Is it accepting bundles that may be bundles for one roll up set of transactions? It could be a block that a roll up would prefer to be the entire block for that roll up in one go.
00:42:17.886 - 00:43:01.302, Speaker A: And so we could have a layering of this searcher builder, MAV supply chain, a lot of options. I have a slide that kind of shows the kind of chaotic level of optionality in this space there's who pays fees to include transactions on shared sequencer? As like Ben mentioned, there's like a UX question. I'm a user. I use my EVM or my Zora roll up, right? I want to pay transactions or pay a fee for a transaction on Zora. Okay, but Zora says, yeah, but someone has to put this in. My shared sequencer. Is the Zora RPC node responsible for submitting that? Is the user responsible for signing a transaction for the Zora specific state machine, then wrapping that transaction into a shared sequencer transaction, signing that? And now you have to account for balances on multiple chains.
00:43:01.302 - 00:43:43.886, Speaker A: That's obviously a problematic user experience. And then expanding from that, what currency are these fees paid in? So that ties into, again, the question of is it a roll up specific currency? Is it a shared Sequencer currency? Or I'll positive even what I've argued in a couple of talks before is there is this Cosmos problem of when you have many, many tokens and to use a given chain, you must have the native token of any given chain, you start to run into kind of like fragmented liquidity issues. I think the example I always use was like and I think I'm pulling this from the skip guys, is like Stargaze. You want to go buy an NFT on Stargaze, you have to go get the Star token to do a transaction. Well, the Star token only trades on Osmosis. It has about $50,000 of annual volume. So if I'm an end user, I can't buy Osmosis.
00:43:43.886 - 00:44:06.480, Speaker A: Actually, if I'm a US. End user. I can't buy Osmosis Tokens on coinbase. I have to go to Coinbase, buy some Atom, bridge some Atom to Osmosis to then on Osmosis, swap that Atom for Star tokens to then bridge those Star tokens to Star. Now I can buy an NFT. That's just not going to happen for any I'm not going to do that. And I work in this space.
00:44:06.480 - 00:44:53.914, Speaker A: Someone is not going to be texted by their friend and be like, oh my God, there's a cool NFT. Go buy this. And then be like, here is a seven step process by which you have to pay fees to get tokens to literally make the purchase, right? So there's a question of what currency are these fees paid in? Right? And this expands not just to the inclusion on the shared sequencer, but also from the roll up sense. I think we've punted on this, or at least not had it kind of come up in the ethereum roll up land because all the ethereum roll ups use east for gas, right? We've seen at least Mantle propose that they will use the Mantle, which is like, I think like the shifting of the bitdao token. Say the fees would be paid on that roll up. And like the Mantle token, well, that's going to have a UX impact on what is the possibility of an end user acquiring the mantle token that is adding friction to your ability to use this application for app specific roll ups. They might say, I like to have my own currency.
00:44:53.914 - 00:45:27.786, Speaker A: There's benefits from that in like a monetization sense. But okay, now you have to solve all the liquidity problems, right? So that's another thing I'll kind of add to the complication here. So yeah, these would be kind of like the closing things. What are the fundamental limitations imposed by a shared sequencer right, with this design and the monopoly and ordering? Right? I'm going to assume that the shared sequencer proposer defines the canonical block ordering that is going to be like the bare minimum thing we're assuming here. And that the shared sequencer must have some civil resistance mechanism, like a transaction inclusion cost, which has implications on the fee structure. Right. And then this is the last kind of chaotic thing.
00:45:27.786 - 00:45:49.860, Speaker A: I'm not going to go into this, but this is roughly all the different ways in which a user could get a payload and submit that payload over to a shared sequencer proposer down here through a blocks. There's a lot of pathways here. Obviously it's a complicated design space. Do you support all of these? Do you support some of them? Presumably we're not going to have all of these with like an equivalent distribution, but this is the broad space. So sorry for going over.
00:45:50.390 - 00:46:34.350, Speaker D: Hello everyone. My name is Quintus and I am from Flashpots Research. I focus mostly on incentive related problems, although I'm sort of all over the place sometimes and I guess today's presentation will be evidence of that. I did craft this presentation with sort of a high context group in mind, so I'll try and give sort of the background when necessary, but just feel free to interrupt me. I left time for questions and I'll leave time at the end as well, at least I hope to. And so I'll be doing two things today. I'll be addressing the first question that Josh and Ben highlighted, which is sort of this auction revenue sharing problem between different roll ups or domains participating in a shared sequencer.
00:46:34.350 - 00:47:23.600, Speaker D: And at first I'll sort of set the scene, explain my assumptions, establish why the problem is hard and what the problems are, and then maybe spitball some possible solutions. And so I'm using Espresso as an example here. The high level sort of like entities I'm thinking of here is this amorphous cloud of blockbuilding. And for those familiar, it's like Suave and all sorts of other activities happening in here. I don't think it's necessary to go into too much depth, but we just sort of assume they're sophisticated actors who are receiving order flow and transactions and turning these into blocks. And then we have the shared sequencer which is like one unit but consists of many smaller units. And I'm treating this as a proposer network, right? So for those familiar with PBS, that's the basic idea.
00:47:23.600 - 00:48:15.534, Speaker D: So it's not that Espresso is necessarily, or whichever shared sequence is necessarily producing blocks itself, but it's sort of receiving blocks from an outside market. And this interface between the outside market and the proposer network is complicated. There are many questions you can ask whether you want what kind of interface you want exactly. Yeah, a bunch of questions there and I sort of leave those for a different talk. I'll focus specifically today on this question of so there's supposed to be some robots over here, but it doesn't matter. You can imagine there are some robots. And the question is there's some value flowing into this proposal network from the auction? How do we divide between these different domains? And this is quite a hard problem, and part of my explanation for why it's a hard problem is going to be hard to see without the robots.
00:48:15.534 - 00:49:11.178, Speaker D: But anyway, before I continue, I'll just make one assumption clear, which is that I'm assuming the active governance of the sequencing of the sequenced domains. Right. So you can imagine, I don't know, optimism using Espresso. One would imagine that if optimism is willing to use Espresso, they sort of believe that they'll be able to fork away or change the way that their blocks are formed if ever something happens and they disagree with how Espresso is evolving, it's not a super important not a super important assumption, but it might come up later. So I just thought I'd make that clear and maybe that's an assumption I shouldn't be making and we can talk about that afterwards. And so one of the main points I want to communicate is that there's no obvious implementation for how this auction should happen and how the revenue should be shared. A paper just like one week ago came out and apart from that, it's not a super well studied problem.
00:49:11.178 - 00:50:14.920, Speaker D: Most of the results you can find, or many of the results you can find in the setting, and I'll go into the setting in more detail in a second, are quite negative. Right. Crypto poses a lot of challenges because we have these guarantees, we try to provide permissionlessness, et cetera, et cetera. And because of this, a lot of the mechanisms we might want to rely on are not actually that feasible or may not be feasible in the way we understand them. And so apart from not being able to enforce or use the techniques which we might have traditionally liked to use, we have the additional problem that dollar signs isn't the only way in which you want to divide some things. You can imagine two domains opting into shared sequencing, where one is some lending protocol and another is a sequencing like a Dex. It's not clear that just by revenue gained from mev auction, that you can gauge exactly how much one domain is gaining versus another.
00:50:14.920 - 00:50:58.180, Speaker D: Lending protocol doesn't maybe generate a lot of mev until one day it generates a huge amount because there's some big price movement and a lot of series of liquidations. They're gaining also from maybe reduced risk and these kinds of things. And so it's not exactly clear how you want to chop that up to give a bit more color as to why without my robots, why it's difficult to solve this problem. We have basically like two sides of a problem here. One is like a combinatorial auction and the other is maybe you can think of as a coalitional game theory problem. We have to split things up. So the first problem is civil bidders and now there are multiple clones of the invisible robots over here, which makes things hard.
00:50:58.180 - 00:51:52.310, Speaker D: The second is that there might be collusion. Right? So you could imagine that I use the example of optimism. Again, optimism is participating in some auction and by some way the auction is defined. Some behavior of the bidders changes how much revenue goes to optimism. So they might submit bids in the auction themselves to try bump up how much value they're getting out of the auction or their share of the auction. Similarly, you could have one of the bidders just sort of having their own domain sort of inserted on the shared sequencer if the shared sequencing is permissionless which would allow them to bid higher on basically like a null item, a dud item and channel some of that revenue from the auction back to themselves allowing them to bid unnaturally high. I thought I would sort of lay out some possible solutions.
00:51:52.310 - 00:52:40.470, Speaker D: The kind of directions I outline here depend on the underlying shared sequencer and how that's defined. But yeah, I'll lay these out. So the sort of philosophy for my approach to this, and I think a lot of people at Flashpots have also been influenced by this, comes from a paper in 2003 written by David Parks and some of his co authors, which was I think, remarkably prescient. And what it argues is basically know we're in this world with lots of infrastructure interacting, lots of decentralized infrastructure operated by different interested parties, and all has to interact in some way. And so we really should think about incentives. We should be designing mechanisms that have all the guarantees we like. And mechanisms design is very important for this.
00:52:40.470 - 00:53:59.178, Speaker D: But at the same time, mechanism design is very brittle in that its assumptions, it makes many assumptions and it's sensitive to those assumptions holding. So if you make some assumptions and those turn out to not be true or not be true in a year's time, your mechanism might just do the opposite of what you'd like it to do. Similarly, it's not always easy to agree on what kind of guarantees you want the system to provide. Do you optimize for efficiency or revenue or these kind of things? And so what David Parks and his co authors argue for is a market for mechanisms which says we want to be able to choose which kind users should be able to choose, whoever should be able to choose which mechanism works best for them. And if we have this market satisfy properties which we like, it's open, it's decentralized, it's competitive, then the best mechanism should rise to the fore. And so, embracing this kind of philosophy, I'll make the assumption that the shared sequencer has a smart contract layer. And the reason this is necessary or interesting in this case is because we're moving this sort of opinionated decision of what should the auction look like into something which is in the control of the domains which are being sequenced and can be modified.
00:53:59.178 - 00:54:59.218, Speaker D: And so I'll give a couple of examples of what that could look like. So the most basic example of what this could look like is you can imagine just a single meta block with a fixed split. So the proposer at every round is receiving is running an auction basically for one block of blocks, for all the possible, for all the domains that are being sequenced. And presumably in that cloud of block building I was showing earlier, there's some mechanism for the different economic actors to sort of aggregate their activities into one meta block, which kind of already happens on Ethereum today, and it should be able to work. And a very basic way of dividing value is just according to a fixed split. And so your contract could imagine, you can imagine your contract looks something like this, which is nice in that it's explicit and if something needs to change, you only have to update the contract. But still, a fixed split is not very interesting.
00:54:59.218 - 00:55:08.500, Speaker D: Conditions change how you divide the value might not you might don't want to keep it the same, you might want to vary with market conditions and these kinds of things.
00:55:09.510 - 00:55:14.742, Speaker C: So I should think of value as like the winning bid in builder or something like this.
00:55:14.796 - 00:56:01.030, Speaker D: So it depends on how you define the auction. But let's say we can imagine running like a first price auction or something and that's the highest bid or whatever, maybe higher, like revenue, right? Auction revenue. A more interesting example is like the same meta block setting but with variable split. So you can imagine that the contracts are offered more inputs about the outside world, right? So in this example, I'm running three different auctions, one for domain A and B together, one for domain A and one for domain B. You can imagine again like three first price auctions or whatever. And according to the input from these auctions, we decide how the revenue should be split between the different domains. And maybe even according to this, you can decide which block should be the sequence block.
00:56:01.030 - 00:56:55.346, Speaker D: But here what we've done. Or maybe to explain how I'm thinking about this in my head. We say, okay, we have a problem that we don't know what auction mechanism we want. Then we say, okay. It's fine, we'll create some programmable auction layer so that the best auction mechanism for the occasion can rise to the fore and people can sort of choose between that. But now we have a second problem which is that we don't know how to get any sort of notion of ground truth for these mechanisms, right? We need to get credible signal and to come back to this broken picture from earlier, one sort of challenge that I didn't list earlier is that this proposal could these slides are completely messed up. But anyway, this proposal also offers some challenges in that they could be either dumb or dishonest.
00:56:55.346 - 00:58:00.842, Speaker D: The dumb proposal case is a bit easier to deal with and that's just the case where the shared sequencing layer doesn't want to maintain state for all of the different domains. So if you wanted to do something like divide revenue based on volume on different domains, that might be a bit hard to do for the shared sequencing validator, but I think we can get around that. The harder question is a dishonest proposal, right? So the basic example is auction revenue is three dollar signs, they lie, they say it's $2 signs, they take a dollar sign for themselves. And this to me is where the most interesting problem or interesting questions come from. And here's where I'll sort of like spitball some solutions and then leave everyone else to think about it a bit more deeply. One example is you could just leverage your active management from domains. So if optimism and Arbitrum and whoever are being sequenced by these shared sequences, then they could also be monitoring the public relays and cross check whether the auction revenue makes sense.
00:58:00.842 - 00:59:08.770, Speaker D: Right? So if you think about maybe I should say what relays are in the proposal builder separation design that we see in Ethereum today. There's bids are public, that's the most important part. And so since these bids are public, or at least the majority of them are, you can sort of keep track and see if there's some mumbo jumbo happening in the auction. A more interesting example is looking at tees, which I will not explain now, but there are mechanism for implementing private relays among other things and I think a lot can be done here. So for example, you can check a Tee proof which is just basically a signature that the bid it's producing is the highest bid it's seen. Or that because presumably the relay has maintains a state of the underlying domain, you know that it can sign something about the volume of the block, the volume of the trades on the block and these kinds of things. The relay has a lot of information and if you can rely on the te proof then you should have a lot of external signal.
00:59:08.770 - 01:00:06.930, Speaker D: This is mostly a response to the dump proposal problem so you don't have to calculate rebates as the auction happens. You can do it maybe later when the sequence of transactions the block is being executed and information like the volume or whatever is available either in some L one contract or somewhere else. This is probably the solution I've thought about the least. So I'd welcome more advanced people to think about this. Then another idea is to make shared sequence of validators stake tokens according to whatever reward ratio you want. And the idea here is that you can't make it might be hard to have a proposer or anyone tell you exactly how much the auction earned, how much revenue it brought in. But knowing how much it'll bring them, they should be willing to stake at least the appropriate amount to get the risk adjusted returns.
01:00:06.930 - 01:01:10.150, Speaker D: And so this is probably a way of getting a bit more credible signal and also to gather value for the appropriate tokens. An easy way of doing this would be having the shared sequence to just stake a token which corresponds to an underlying treasury which is rebalanced according to whatever properties you want. And then the last point and then I'm done is you can use a committee to determine the signals. Right. So an example of this would be what they're trying to do with mevburn and Ethereum, which is to establish what the highest bid in the auction is by having a committee of validators or a testers. Agree roughly agree at least a lower bound on what the highest bid is. And of course, you still have problems with this in that you can have collusion and as soon as the committee colludes, you don't know what the it's very hard to trust what that signal is, but it's much better than having an individual be able to deviate unilaterally.
01:01:10.150 - 01:01:15.660, Speaker D: So that's basically all I have. Yeah. Any questions?
01:01:16.670 - 01:01:25.218, Speaker C: So for that last bit, was the focus more on estimating mev in a single roll up or estimating it kind of at the shared sequencer?
01:01:25.334 - 01:01:27.210, Speaker D: Are you talking about the lost point or the lost slide?
01:01:27.290 - 01:01:29.498, Speaker C: Last slide or is it a mix?
01:01:29.594 - 01:02:11.020, Speaker D: Yeah, the question is sort of like if we want to enable these sort of smart contracts which define the auction logic we need to give these smart contracts some input which they can work off to compute what the reasonable output should be and so the question is what kind of input can we provide that's reasonable? It's not that easy to manipulate and that's sort of an exploration of this. The third point is both a way of achieving signal but you can see how much people are staking but at the same time it directly does the redistribution as well.
01:02:12.190 - 01:02:15.318, Speaker B: Is this for a single roll up or for multiple?
01:02:15.414 - 01:02:16.654, Speaker D: Yeah, you can do it.
01:02:16.852 - 01:02:48.840, Speaker B: I guess what wasn't entirely clear to me here is that part of the issue is that if you're running an auction for one roll up, then there could still be challenges in figuring out, because of gaming, what the actual mev is. But it seems like the main challenge is that if there's multiple roll ups who are participating in an auction, figuring out what is the actual marginal contribution of an individual, how do you simulate the auction for an individual roll up?
01:02:49.210 - 01:03:11.258, Speaker D: This is the point I was trying to make with the market for mechanisms point, in that there's no obvious way to do this, there's no shape value approximation which works under all conditions. Not that I'm aware of. Right, but you can leave it up to the domains to sort of figure out, we'll try something, see if it works, doesn't move on to the next thing, but also maybe leave it up to the domains.
01:03:11.274 - 01:03:13.120, Speaker B: You mean leave it up to the roll ups themselves?
01:03:13.810 - 01:03:17.394, Speaker D: Yeah. When I say domains roll ups in this case, really?
01:03:17.432 - 01:03:26.382, Speaker B: Right, so how would you be leading it up to a domain or a roll up to estimate its own mev.
01:03:26.446 - 01:03:53.914, Speaker D: And report not to estimate its own mev, but to come to agreement with the other domains which are being sequenced with it on what the mechanism is for establishing its contribution. Right. So in this case, it's like a relatively simple thing, but if you know what the highest bid was for domain A, the highest bid was for domain B and the highest bid was for both. Together, you can maybe use that information to find some allocation that's in the core, or in this case, I guess you can just find the shapley value.
01:03:54.032 - 01:03:58.186, Speaker B: Presuming that searchers are going to be biding separately and then also together they'll.
01:03:58.218 - 01:04:06.382, Speaker D: Be doing all yeah. If you run the auctions and give them incentive to, then you could right, yeah.
01:04:06.436 - 01:04:19.794, Speaker A: Umar but what if two different roll ups never agree? They have to come to consensus and agree on the auction mechanism for both of them. So what if they're just fundamentally not agreeing the resolution to that?
01:04:19.832 - 01:04:50.158, Speaker D: Yeah, I agree with that point, but I think that that's an issue you have no matter what you do. Right? If you enshrine, you say, this is like we are espresso or Astria or whatever, and we're going to have this specific allocation method. It might still be that the roll ups go, no, that doesn't work for me. So here you have a bit more freedom and hopefully that means that people domains can find something that actually does work for them and is a bit more reasonable than as opposed to having one specific thing and then having no choice outside of that.
01:04:50.244 - 01:04:51.966, Speaker A: And how do you allocate even the.
01:04:51.988 - 01:04:54.110, Speaker B: Voting power between the domains?
01:04:54.690 - 01:04:56.560, Speaker D: Yeah, great question.
01:04:57.810 - 01:05:20.870, Speaker A: This is just like a generic, like, we're going to pre negotiate a contract and say there are five roll ups and enshrined into the auction mechanism of the shared sequencer state machine. Right? It says, I'm going to pay out my block rewards in this distribution, and if you don't like that, then you don't have to submit transactions for the shared like, you don't have to use it. Right, but fundamentally, we're just saying we're pre committing to something, essentially.
01:05:21.850 - 01:05:36.190, Speaker D: So I think there's a separation here between the logic of the underlying shared sequencer and the smart contract layer implemented on top, which is what I'm suggesting. Right. So I'm saying you have a layer on top which implements the auction logic.
01:05:37.410 - 01:05:38.206, Speaker B: Okay.
01:05:38.388 - 01:05:42.026, Speaker D: Does that make sense? Which would require the shared sequencer to have its own VM?
01:05:42.058 - 01:05:46.566, Speaker A: Yeah, I would assume it would be within the logic within the state machine of the shared sequence.
01:05:46.618 - 01:05:50.820, Speaker D: Yeah, I guess it's like semantics how you chop it up. Yeah.
01:05:51.990 - 01:05:55.522, Speaker B: I think we'll switch to Turin is here, right?
01:05:55.576 - 01:05:55.806, Speaker A: Yep.
01:05:55.838 - 01:06:28.750, Speaker B: Turin's in the back. Okay. Turin will be up in a second. But I think that the source of disagreement is due to the fact that would only be the case if there isn't an optimal solution. Like an optimal solution would give to each roll up exactly what their marginal contribution is. So you're saying supposing that we don't have one, we're just going to have to come up with different heuristics or approximations. Then it becomes difficult to convince everyone to agree on what the heuristic or approximation should be.
01:06:28.820 - 01:07:08.586, Speaker D: Yeah. And I would contend that even if you had some notion of optimal, that notion of optimal would be restricted to okay. This is how, in the example, we have one domain, which is a lending protocol where lots of liquidations happen. In the other, where you have a dex, it might be the case that according to just, like, pure mev revenue, the Shapley value says that this one should receive one third of the revenue and that one should receive two thirds. But that might not be how the rollup teams think about it, because one is gaining a lot more because they have reduced risk or whatever, because liquidations are more efficient. And so even if you could find, like, a numerical mathematical optimal solution, I don't know if that's going to cut it.
01:07:08.688 - 01:07:15.018, Speaker B: Can you define it, though, in terms of what they would get if they were running it on their own? You can't simulate that.
01:07:15.184 - 01:07:19.920, Speaker D: Yeah. If you ran multiple auctions and you assume the well, the problem is because.
01:07:21.010 - 01:07:29.710, Speaker B: You can't simulate it because the auction won't be run for real. Right. There's only one auction that's actually being run in the end that has payouts.
01:07:29.790 - 01:07:30.894, Speaker D: Well, you could run multiple.
01:07:30.942 - 01:07:31.298, Speaker E: Right?
01:07:31.384 - 01:07:45.446, Speaker B: Well, but you have to define what the outcome would be. So if you're running multiple I mean, how do you run every roll up runs its own auction, and then you run an auction involving all of them. You still have to define what the outcome will be.
01:07:45.628 - 01:07:47.954, Speaker D: Yeah, but that would be the contract.
01:07:48.002 - 01:08:03.980, Speaker B: It's not the same as simulating each of those auctions independently. Right, but there could be a theoretical definition of what the definition of what a roll up would get on its own is perhaps well defined, but the problem is we can't come up with a single mechanism that discovers what that value would be.
01:08:05.150 - 01:08:06.966, Speaker D: Yeah, cool.
01:08:07.008 - 01:08:09.390, Speaker B: Well, thank you. Let's go to Tarun.
01:08:10.530 - 01:09:17.458, Speaker E: Hey, so I'm going to talk a little bit about sort of some of these issues, somewhat more theoretically, but nothing more than high school math, so maybe middle school, depending on what your background was. But the main point that all of these things have sort of got at is all of this type of mechanism design has very different trade offs than single chain mechanism design. A lot of things don't make obvious sense. And so we'll talk a little bit more about the uncertainty that you maybe have to lean into quantifying here. So Mev is both an attractive and unattractive research topic because it has such poor definitions. I think most of the papers have proved any theoretical results about Mev acknowledge that their particular definition only encapsulates some aspects of it and not everything. I think Tim's recent paper has a whole paragraph dedicated to saying that, so.
01:09:17.544 - 01:09:19.874, Speaker C: I was just trying to preempt all the Twitter response.
01:09:19.922 - 01:10:14.310, Speaker E: Yeah. And so I think a better way to think about it, at least from a statistical point of view versus sort of the optimal algorithm point of view, is there's a lot of uncertainties in Uridon, particular users, usually unsophisticated users by more sophisticated users. So I like to think of these uncertainties as ordered in terms of certain types of payoff uncertainties. So what does a payoff uncertainty mean? So we have some notion of a risk measure. In this example I give in the bottom right corner, it's a variance conditional on some event sequence. And we have a set of payoffs sort of a function class think, like statistical learning theory, basic type of stuff. We take a supremum over that and we say this is sort of a minimax loss rate.
01:10:14.310 - 01:11:30.590, Speaker E: And my claim, and this is Heuristic, is that as you go to the right, you're actually increasing those payoff uncertainties for these different classes of uncertainty of issues that come from Mev and crosschain systems. So ordering, I actually put it the lowest end, which is pure ordering, not censorship. And this is reordering transactions to increase particular user payoffs, whether it's moving your sandwich attack to the back of a block because it gets higher revenue because the other trades in front of you, or moving around Oracle updates so you can do a liquidation. Inclusion is censorship of Aliding transactions to have certain things work, or adding extra transactions to change payoffs for other people. I put that as higher uncertainty. The next one, which I think is the one that shared sequencing in particular, is kind of meant trying to deal with, is atomicity, where a lot of developers, a lot of users are used to expecting some types of atomicity guarantees in blockchains. And those atomicity guarantees are stronger than their expected guarantees in normal financial systems.
01:11:30.590 - 01:12:13.366, Speaker E: But obviously the lack of Adamic guarantees in yours passive participants a lot more. And so there's a sense in which the payoff uncertainty goes up. Now, again, I gave an example by just saying it's a variance of a payoff, but choose any coherent risk measure, whatever you want. I think defining it is the hard part to compare across these. So this is again meant more as a qualitative comparison. The next thing is the relay, which is network layer attacks of where and how crosschain execution is transactions are executed. You could argue that this allocation problem for how auction revenue gets distributed falls in this category.
01:12:13.366 - 01:12:50.070, Speaker E: And I put the highest uncertainty at the data availability layer. So you might say why would the highest uncertainty be there? Isn't it supposed to be reliable? We have data availability sampling, we have all these statistical guarantees. The problem is, imagine a fraud proof is executed, how much is the user state worth after that? And how much do the auction behaviors change? How much do the valuations change for the end users? That could be arbitrarily high. And so that's sort of where I consider that as like extreme loss of stationarity in the expected valuations of users. And so I put that the highest.
01:12:51.530 - 01:13:02.538, Speaker D: Is there a particular reason that variance is more important than the first moment? In these cases, if you have to worry about data availability, isn't the first order issue not the variance, just the.
01:13:02.544 - 01:13:05.614, Speaker B: Fact that your payoff is probably lower.
01:13:05.732 - 01:13:08.240, Speaker D: Than if you don't have to worry about that particular thing.
01:13:08.850 - 01:14:13.358, Speaker E: Yeah, I think the main reason I hesitate to use sort of just pure expectations is more that for a lot of these things they're extremely discontinuous functions. Also they tend to have this property where when they lose stationarity, they have extreme jumps to zero. And that's why I said don't think of variance as this fixed functional here. I'm saying choose some functional think about any statistical learning theory type thing, right? You have a minimax loss, but then you have a certain per instance loss metric. I chose variance as an illustrative one, but this is just what I would say. Again, heuristic the distance between these things is not linear in the spacing on the slide. Okay, so cross chain mechanism design, what's the real goal? It's to construct mechanisms that reduce this uncertainty for users given a payoff.
01:14:13.358 - 01:15:16.006, Speaker E: You know, to the point that Quintus made earlier about a dex roll up generating regular revenue stream but have small payoffs versus liquidations, which are very irregular and have high payoffs. You have this uncertainty partially because in some ways these payoffs, you can't really optimize them point wise. I can't say at a given time here's exactly how you should allocate at all times, right? You can only kind of say over some long time interval. Over some learned interval I learned some approximate allocation rule. And this is kind of where I think a lot of the classical mechanism design kind of does fall over, is that other than sort of some of the recent learning and auctions literature is that it kind of is not flexible in this way. And one interesting thing about the cross chain world is there's much more complex trade offs than a single chain. So for instance, reducing one type of uncertainty may increase another.
01:15:16.006 - 01:16:27.070, Speaker E: So depending on your view on PBS, you could argue that it reduces inclusion uncertainty. Now, for the average user that might be true, for the searcher that might not be true. So again, it's a little bit duplicitous, but it does increase relay uncertainty. You rely much more on the relays than you did before and optimal guarantees that might be possible in a single chain might actually have impossibility results in MultiChain. And again, this means that your problem is a lot more like statistical learning, understanding, online optimization, and less like classical, here's an optimal allocation statically. Okay, so I thought I would go through the list of different types of mechanisms that people have and if this is not a comprehensive list, includes a bunch of them and then categorize them by the level of rigor of understanding of them and with a natural reference to Donald Rumsfeld via this little tree. And the idea is that a lot of the things that people have proposed or talk about have no actual analysis of their economic uncertainty.
01:16:27.070 - 01:17:48.870, Speaker E: And I think a lot of things that claim to get around that, like first come, first serve things are usually quite duplicitous in how they represent it. But if you look at this, we were talking about things like decentralized sequencers, data availability, sampling, like kind bridges, intents, I put it in quotes. But they all have very different levels of understanding of where they fall on this uncertainty diagram. So you might say, oh well, auctions are great, we understand them, must be awesome, why don't we just use them for everything? Well, I'm going to go through some examples that show you that there's feedback in these auctions in cross chain land that mean that a lot of the classical results don't work in a black box manner than in the way you might think. So one thing maybe more from a historical standpoint is to talk about why auctions might be harder than you expect. Well, historically, centralized entities have enjoyed abusing multi venue auctions a lot. So this is from the Justice Department's case against Google in which Google actually manipulated auctions that weren't their own, but also other auctions where there were people sort of bidding on remnant supply and they added sort of fake bids on alternative auctions to make theirs more attractive.
01:17:48.870 - 01:19:01.594, Speaker E: So historically this kind of abuse exists and is a bit difficult to manage. Second, I think these properties that you really like in auction theory, like seller intensive compatibility or credibility, buyer intensive compatibility, communication complexity. They become a lot more difficult in the open world of permissionlessness and having trouble with civil resistance. But so you have to end up using more complex mechanisms or you find some impossibility theorems that are not fun. Okay, so now we can go through what I would call the simplest nontrivial example of why cross chain auctions are actually sort of significantly more difficult to reason about and don't have the same sort of classical guarantees. So the gadankan question is, is it better to sort of bid in two cross chain auctions? So I have two chains, A and B. The idea the setup is going to be these two chains have a Dex that's trading the same pair of assets.
01:19:01.594 - 01:20:02.558, Speaker E: So you could think of it as like X has X versus synthetic Y and Y has Y versus synthetic X and assume there's a one to one redemption of the synthetic underlying. So as I go across chain, I have a way of redeeming and consider an auction where you have an Mev auction where you bid on each leg separately. You bid on chain X separately from chain Y versus one, where you bid atomically. The atomically is the version of the shared sequencer, right? I go to the shared sequencer, they run one auction and they give me some not full, but partial guarantees that they execute the bundle as specified. Okay, so we have the two chains, we have the two prices, and we're going to assume, without loss of generality, that chain X has a higher price. Chain Y, there's some type of arbitrage. So basically, buy low, sell high, you kind of do the execution.
01:20:02.558 - 01:20:58.434, Speaker E: And what happens is you change the price on chain X by the current price minus some slippage, you change the price on chain Y by the current price plus some slippage, and you get a profit if the units under that loop are positive. So now if we try to write out what that looks like, we simply look at the no arbitrage condition. Again, I'm not telling you what the slippage function is, just treat it as some arbitrary thing for now. And you can write out what the profit is, where you also include the gas costs denominated in X terms. So this is as if you paid atomically in units of X to buy the whole bundle. And then, of course, you can always convert it to Y based on the no arbitrage price. And you can also compare that to biding separately.
01:20:58.434 - 01:21:44.238, Speaker E: So bidding in X and bidding in Y and you split this into two legs. And so one question and one important thing to note is that in the process of doing this arbitrage, you are changing the gas price on the remote chain. So you're changing the biding price that you're actually paying because the Dex is actually changing the price of the thing you're bidding in. So now you're not stationary, there's feedback between the actual biding in the auction and the item you're buying, which is the Dex trade. And so now you might say, okay, let's compare the PNL of the atomic. So I'm bidding to the share sequencer for a single bundle versus the separate. I'm bidding on the two chains separately.
01:21:44.238 - 01:22:36.142, Speaker E: And you'll find a condition that says, hey, actually, depending on the price and the slippage functions and the bids, you'll find that in some cases the separate auction is better and in some cases the atomic auction is better. And what this sort of says is like your optimal mechanism in a long running process where there's stochastically arriving trades, where the auctions are interlaced by the fact that bidding in the auction changes the price. On the other side is sort of this stochastic optimization problem. It's no more the stationary. I have a set of valuations, I choose a Myerson reserve price, dot, dot, dot. Whatever you learn in basic algorithmic game theory, it becomes a lot more like this stochastic control type problem. And so the answer is there's not really a sort of best unique auction format.
01:22:36.142 - 01:23:28.098, Speaker E: It's actually better in these cross chain environments to be usually good for most payoffs, which is slightly different than, hey, I'm optimal in some sense. And again, I think in the auction theory world, there's actually a lot of recent efforts on studying this type of problem where you're sort of learning what the optimal revenue should be, learning what parameters should be optimally. But the weird part in the Dex example is this feedback, the idea that the item, the purchase of the item changes the actual thing you're bidding in. You have to incorporate that feedback process in the worst case. Right. Because there's obviously things of like cross chain NFT bidding where the NFT doesn't impact the price on the other chain. Right? But this Dex case is actually quite important because it's the largest source of mev in general.
01:23:28.098 - 01:24:01.182, Speaker E: So understanding it is actually key to understanding what these things are. Okay, so now we have uncertainty. We hopefully can agree that it exists. It's kind of something we are going to have to grapple with. So how do we quantify it? And this is where we just, again, all of this is coming from work. We're working on kind of getting some types of bounds on these types of things. But the key thing is that users have a variety of non stationary payoffs changing in time.
01:24:01.182 - 01:25:16.040, Speaker E: And I think a really great example of why you have to assume non stationarity is this active versus passive proposer paper that Tim put out recently, where effectively, if there is non stationary payoff, you get a very different auction behavior amongst users. And you could think of these payoffs as taking in sets of transactions, sets of blockchain history, and some notion of metadata. The metadata is something that is kind of different than the single chain environment the metadata includes things like what routes are you taking across chains? How did you choose those routes? It also could be mechanism parameters like do I bid separately or do I bid atomically? Assuming you could do both and we have some sort of notion of welfare, which is the payoffs for the users minus some notion of maintenance costs. When I say maintenance costs, I mean things like paying the bridge, things like sequencer fees, things like excess fees that are not included in the single payoff world. And then you just try to optimize your med data for some regret measure. Right. Again, very simple classical type of stuff.
01:25:16.040 - 01:25:59.110, Speaker E: And so just as a walkthrough example of this kind of simple Dex ARB example, you might say, okay, let's look at the best in hindsight, regret for this atomic versus split bidding. So go backward. So say I see a sequence of trades, I see how they would have been executed and I compare the was it better to do atomic versus split? And then try to learn a policy of like how do you choose one or the other over time? And kind of these learned mechanisms can be more robust in some ways, at least for a large class of payoffs, than a very fixed brittle.
01:26:00.890 - 01:26:01.640, Speaker A: So.
01:26:03.690 - 01:27:05.260, Speaker E: Hopefully that was relatively straightforward. I think the main ideas are cross chain mechanisms add a lot more uncertainties and those uncertainties interact with each other and that changes the economic payoffs to users. Mechanisms that are well understood in the classical setting, whether it's due to stationary valuations, due to sort of some notion of how you should think about the seller, are very different when you have to think about DeFi, where the actual action being executed changes the valuation behavior of the users and it turns these static problems into these dynamic problems. And kind of a lot of the places and future soon work will kind of show there's a lot of advantages to looking at the adversarial optimization literature for thinking about this. And I think the open problems are really understanding how to quantify these uncertainties and their interactions with each other. And with that questions?
01:27:10.270 - 01:27:18.382, Speaker B: Yeah. So in the previous slide, are you modeling this as like a banded problem with the goal of minimizing regret over time?
01:27:18.516 - 01:28:02.122, Speaker E: Yeah, exactly. It's basically like a relatively simple abandoned problem. But the key thing is that you have to keep track of this feedback loop, right. The idea that the trade interacts with your bid distribution means your bid distribution and valuation distribution implicitly is changing as a function of the Dex trades on both legs. So you have to be somewhat more careful where a lot of times these kind of even in context aware bandit problems, the numerator or the value function units are stationary. But keeping track of that actually makes us a lot more annoying and changes the problem quite a bit. And I was thinking a lot about.
01:28:02.122 - 01:28:36.262, Speaker E: What are, like, traditional finance things or online auctions things that look like this. And there's not that many that I could think of. The number one thing I could think of is the euro dollar auction opening on CME does impact the US euro trade when it opens. And so, okay, yeah, there is kind of this FX thing coming from a large it's very it's not as fundamental as it is here because the Dex trades are just constantly changing your numerator. Andre, that was kind of my question.
01:28:36.316 - 01:29:02.986, Speaker C: Which is, in traditional markets, we have this interactivity where you have latency differences between a leading, lively pricing of the same equity. And so we use ISO orders to have that conditional between two venues. And so is there some type of pretrade metadata or conditional that you could attach and say, like an ISO order to say, I have context that this is going to execute in two domains and I don't want interactivity?
01:29:03.178 - 01:29:32.800, Speaker E: I do think that's what all of the people who spoke before me are working on is, like, how to give some expressivity to those covenants. The interesting thing, right, is that the order types are programmable. The user can choose their own order type versus the traditional finance version of, like, here is a set of order types. You can only use those. Does that make sense? Cool.
01:29:37.110 - 01:30:12.350, Speaker B: Okay, so that concludes the talks for today. So tomorrow, for those who are interested, we'll be having the working group discussions on the three open questions that I spoke about, that Josh spoke about, that Quintess and Tarun maybe gave some direction or insight on. They're all in the Google Doc that are attached to the invitation for this event. So I do suggest taking a look at the Google Doc since it explains the three problems. Okay, so conclude today. Thank you so much to all our speakers.
