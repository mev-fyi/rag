00:00:09.720 - 00:00:28.072, Speaker A: So welcome to this afternoon's Decrypto research seminar. Very happy that we'll be having Joe Bonot who just joined as a research partner research team last week and he's going to be telling us about a very important cryptographic primitive which he co invented verifiable delay functions. So Joe, it's all yours.
00:00:28.216 - 00:01:29.940, Speaker B: This talk is going to be, as Tim said, about VDF and slightly more broad theme of time based crypto. So just to position what's kind of interesting about this whole field, traditionally in crypto, there's things that we want to be as fast as possible. Everything that honest parties do, like encrypt things, decrypt things, sign things, and then there are a bunch of problems that we hope are intractable as slow as possible. So brute forcing a key, computing a discrete log. And with time based crypto, we actually want to design specific operations so that they take a specific amount of time. So not fast, not slow, but some specific amount of time that hopefully we can parameterize and get as close to a specific amount of real world time as possible. And I'll get into the fact that of course we can't design an algorithm that always takes exactly 10 minutes.
00:01:29.940 - 00:02:48.670, Speaker B: It would be good if we could. The real trick we have is just requiring a specific number of sequential steps that algorithm is going to have to take, but we use that to sort of approximate real time. So VDFS I'll talk the most about, but there are a lot of other tools that fall into this category of being designed to take some specified amount of time rather than just being really fast or really slow. So the idea is pretty old, or at least people have been thinking about things along these lines for a long time, about inherently sequential puzzles and their use in crypto. Before, I guess, 2018, when we got into the VDF work, there were a couple of different schemes known and it's kind of interesting, I guess, retrospectively looking at it, that they all sort of had three or four potential properties, but none of them had all the properties at once. So the properties being like kind of a fully public setup without needing a trap door. So for example, time commitments and time lock encryption, somebody would have to basically set up the slow function with a trap door that they knew that would allow them to solve it quickly.
00:02:48.670 - 00:03:29.564, Speaker B: There are things that didn't emit an efficient verification routine, things that weren't inherently sequential, proof of sequential work, maybe I'll talk a little bit later about. But it was kind of interesting. It sort of seemed like it wasn't clear you could get all these properties at once. It turns out VDFS are the thing that has all these properties at once. So I'll show you how and why we actually want to do that. But I should say that we did not get into this research by drawing that table and asking a theoretical question. It actually started from a really practical question.
00:03:29.564 - 00:04:18.670, Speaker B: I guess for those of you who know me, I'm not really certainly not a theoretical cryptographer. I'm maybe barely an applied cryptographer. I've always been much more interested in applications and security problems and I've gotten more into crypto than I have on any other problem with VDFS because that's just sort of where solving the problem took me. And the problem was generating public randomness or having a verifiable lottery. So obviously a really old problem by the fact that there's tile mosaics of people using dice to try to run a lottery still very much a real thing. We still run fairly high stakes lotteries trying to use physical randomness. And sometimes I give a whole talk that just talks about kind of the history and the ways that this goes wrong.
00:04:18.670 - 00:05:18.380, Speaker B: If you don't know anything about soccer or association football and international tournaments, they have to put all the teams into groups and that's a really big deal. People often think that this is not done on the up and up. And in fact, Sep Latter, who's some would say disgraced former head of FIFA, he allegedly said that the way that they would manipulate this process so it looked random on TV, even though it really wasn't. The process consisted of having these miniature soccer balls with different teams'names written on them and then a celebrity would draw them out in a random order supposedly. And Setbladder said that they would put some of the balls in the freezer before going on TV so you could reach around and feel for the cold or the warm balls. Of course on TV you're not going to notice anything. So it's just a really simple way that they would try to manipulate this process, allegedly.
00:05:18.380 - 00:06:21.536, Speaker B: But there have been a lot of other examples, especially from state lottery drawings on TV and it shouldn't really be that surprising if you've seen a magic show. Just professionals who are very good at making it look like they chose something randomly when obviously they didn't. So this is thinking about this problem and how we could try to generate public randomness in a more verifiable way is what got me to VDFS. And I guess in lieu of telling the whole story, I'll just give you a really simple example of a really old cryptographic protocol that probably a lot of you have seen this before, but it's a way for a group of parties to generate a random string together. So we could call this whole thing a distributed randomness beacon. That's kind of a more modern terminology for it. Cryptographers have thought about this since the distributed coin flipping or a couple of other names, but in the blockchain world people think about distributed randomness beacon.
00:06:21.536 - 00:07:22.660, Speaker B: So the goal is just that these three parties can cooperate to produce some random output such that none of them can control what it's going to be so classic way you do this, everybody commits to their own random contribution. So they just use really any cryptographic commitment function. So everybody has to commit by some specific deadline and then of course, once everybody's committed, everybody can reveal, they can only reveal the number that they committed to. Of course that's what the commitment function is supposed to get you. And then somehow you combine everybody's contribution and there's a lot of ways to do that. You could run them all through a hash function or XOR them all together so the details aren't that important there. And it's a really simple protocol but it's actually really powerful because it guarantees you that as long as one party is honest and contributes something truly random, the output should be truly random.
00:07:22.660 - 00:08:23.080, Speaker B: So every participant in the protocol, kind of like Sarah said yesterday, as long as you trust yourself, you should trust the outcome of this since you were able to participate and contribute good randomness. But there's a classic flaw. If one of the parties is malicious, they wait until everybody has revealed. So they want to be the last party to reveal and notice that if everybody else is revealed, obviously this malicious party who hasn't revealed yet, they can compute what the final output is going to be because they have seen everybody else's random contribution and of course they know their own. So they can predict at this point what the final result is going to be. Nobody else knows yet because they don't know what the last participant's contribution was. And if the last revealer doesn't like this, they can just refuse to reveal so they can abort or drop out of the protocol.
00:08:23.080 - 00:09:18.830, Speaker B: And if you're designing this on a blockchain platform, the answer might be that you punish them, they forfeit some bond or you kick them out and don't let them participate again. And there have been a lot of examples of this being built on top of blockchains where that happens, where you try to punish the party that aborts, but it does give them some ability to they get at least one bit of manipulation of what the random output is going to be. So it's a bit of a tricky problem. There's a whole stack of papers I have with an NYU student. I'm working on a survey of this whole space right now. But there have been like at least 30 papers in the last three years that have extensions to this problem. So distributed randomness beacon is a pretty hot topic for the research community at least.
00:09:18.830 - 00:10:36.564, Speaker B: So obviously can't tell you what all these different protocols do. But I can give you like the high level flavor is that they involve the different participants in the protocol publishing enough extra information so the PVSS is publicly verifiable secret sharing. So before you publish your contribution, you secret share it to the rest of the participants so that if you drop out some quorum of the other participants can recover and they can reconstruct what your contribution would have been if you didn't drop out. And there are a lot of other variations of this, but they all sort of have the generic problem that if if the protocol can recover from F parties aborting, then that necessarily means that N minus F parties can simulate what the outcome would have been if those parties didn't abort. So you're sort of left with like a trade off either. If you want the N minus one trust that the commit reveal gives you, then you can't really recover from many aborts. And if you want to recover from lots of parties dropping out, then a smaller and smaller quorum is able to collude and predict the outcome early.
00:10:36.564 - 00:11:34.852, Speaker B: This was the problem we were working on that led to VDFS, but since then we've looked at auctions and I'll show a couple of other examples. But yeah, this general theme of anything with a commit reveal where people are dropping out, VDFS are potentially a solution. So it's a pretty general, like we've discovered more generality since doing the work. Okay, so a much, much simpler protocol that fixes the problem adds a delay function. So we'll get to a verifiable delay function in a second, but for now it only needs to be a delay function. And I'll just say informally that's some function that nobody can compute in less than some minimum amount of time. So how does the protocol look if you have a delay function? It's amazingly simple now, there's no commit reveal, it's really just reveal.
00:11:34.852 - 00:12:56.960, Speaker B: So everybody publishes their contribution before a deadline. There's only one deadline now because it's only a one phase protocol. And the secret is that you combine everybody's input and then you pass it through this delay function and that's what the final distributed randomness beacon output is. So why is that delay function so helpful? Well, it's really only helpful if you ensure that the minimum time that anyone can compute the delay function is longer than the window that everybody has to contribute randomness to the protocol. And the reason that that kind of fixes the problem. If you have a malicious participant who's trying to wait till the very last minute to choose their contribution and even assume that they can see everybody else's contribution and they can choose whatever they want based on that, they can think about different potential choices for their contribution and they can try and compute what the final result is going to be, but they won't have time to actually finish. So the delay function, the one property it's supposed to give you, is just that they can't compute it within the time window that they have to make a contribution.
00:12:56.960 - 00:13:48.988, Speaker B: So they have no idea what the impact of any contribution that they choose to the protocol is going to be or not. And that basically fixes the problem. I mean, you can give the attacker the ability to publish last in response to what everybody else has done and they still can't evaluate what's a good or a bad choice. So it's quite a powerful idea. You kind of get back to the trust model of only needing one honest participant. You don't need some mechanism to punish aborts or some other kind of blockchain based infrastructure. The only kind of extra assumption you need over standard crypto assumptions is that these delay functions exist and that we can build them, it turns out, for other types of randomness generation.
00:13:48.988 - 00:14:32.012, Speaker B: It's also a really powerful idea to add delay functions. So I'll show you two other examples which maybe you've seen or heard of before. So there's another idea which is also quite old actually. I mean this idea is kind of cool actually because this is how organized crime did illegal lotteries in like the 1910s and the old timey like pre prohibition gangster era. But it's actually a serious idea that people still think about that. You generate a randomness beacon by taking a bunch of asset prices. So say you pick a basket of stocks on New York Stock Exchange or some other large stock exchange.
00:14:32.012 - 00:15:31.440, Speaker B: These are large cap stocks that are very difficult to manipulate the prices of. And you say we're going to take the closing price of all of these stocks and hash it or run it through an extractor function at the end of the day and that will be our random output. The attack on that, which is conceptually very similar to the last Revealer attack, is that some attacker looks at what the market is immediately before the market is set to close and then they can kind of test a lot of scenarios for very small market manipulations that they could make, which are maybe cheap enough to do. Like they could change the price of a specific stock by one penny. Maybe that's feasible. You just need to push a few extra trades through before trading ends. I know this is like an incredible oversimplification of how stock markets work and how closing prices are quoted, but just abstracting a little bit.
00:15:31.440 - 00:16:39.400, Speaker B: Imagine that the attacker has the ability to push stock prices by a small amount so the attacker can simulate like well, there's a lot of different small trades I could make and those would all lead to different randomness outputs. So I can sort of pick the one that I like the most and then I can try and force the market into that closing configuration. So that's what an attacker could do. If you did kind of the simple choice here, you'd be vulnerable to a high frequency trader trying to do this, but you can probably predict solution. If you hash all of those stocks and run them through a delay function, you get the exact same security property that the attacker is not able to simulate the effects of any of these manipulations. So they can't determine what strategy they should take, hopefully before the market has closed for the day and everybody's gone home and we're stuck with whatever outcome we got. So that's another beacon idea that also benefits from delay functions.
00:16:39.400 - 00:17:28.516, Speaker B: And the third one, which is also kind of a popular idea that a lot of applications that try to do lotteries in the blockchain space have done, is to just hash blocks in the blockchain. Same idea. Difficult to predict future blocks in the chain. If it's a proof of work chain especially, you obviously at least can't predict the solution to the proof of work puzzle. But that's also sort of vulnerable to manipulation by miners. So imagine that you're just going to hash the block that occurs at this slot or this depth in the chain and generate a lottery outcome as a result. An attacker can look at that if there's a block collision and they can try and choose one of these two branches to promote.
00:17:28.516 - 00:18:16.852, Speaker B: Right, so it could be that this attacker has a lot of mining power and they can switch from one chain or another to try and make that chain be the eventual longest chain. They might not have any mining power, they might just be bribing the people who do and say, I'd really rather you promote this one chain or another. Could be that most of the miners don't have any preference between the two chains, so any small amount of bribe will be enough to push mining power onto one chain or another. And of course, the way to prevent this manipulation, add a delay function and then hopefully by the time the attacker has figured out which branch they would even want to promote. Again, the very consistent theme here, it's like too late for them to act by the time they figured out what they even want to do.
00:18:16.986 - 00:18:23.930, Speaker A: Can you say a little bit more about what you have in mind about why the attacker would have a preference between the two and what information they're waiting for?
00:18:25.580 - 00:18:40.508, Speaker B: Yeah, the idea is you'd be just hashing this block in the chain to pick the winner of some lottery. So it could be like that this block means they win the lottery and this block means they lose the lottery or whatever application you built on top.
00:18:40.594 - 00:18:45.420, Speaker A: I often think of those in like a proof of state context when you're actually the lottery is actually picking broth producers.
00:18:45.500 - 00:18:46.320, Speaker B: Yeah, exactly.
00:18:46.470 - 00:18:53.288, Speaker A: I was mixing up a little bit. What I was thinking is like a proof of stake application.
00:18:53.454 - 00:20:02.524, Speaker B: Yeah, it's just lotteries in general. I was kind of describing it in terms of like building an application layer lottery on top of what's happening on the consensus layer, which people have done. But yeah, with proof of stake it's like purely within the consensus layer, you might want to manipulate who's on the next committee or who's chosen as a block producer. And same idea applies. Okay, so thinking about all these problems made us realize these delay functions are potentially really useful, and we wanted to add the property that they're efficient to verify, which just makes it much more practical in all these cases that somebody has to compute a slow function, then everybody else can verify that it's been computed correctly. So without getting into the full crypto formalism, a Verifiable delay function is supposed to have three properties, two of which are pretty straightforward. The verifiability just means that it's efficient to verify solutions.
00:20:02.524 - 00:21:03.696, Speaker B: You don't have to repeat the slow computation to check that the result is correct, which means hopefully only one party has to actually do it, and everyone else can just do a verification, which is cheap at a technical level. By efficient, we mean that it's like either constant time or polylogarithmic in the delayed parameter. So if it's not taking more and more time, the longer the delay is to actually verify it. The delay part, the D property, is that we don't know any way to actually solve or to run this evaluation algorithm without taking at least T sequential steps. And t is a parameter. So when you set this whole thing up, you can specify T how many steps you want it to require to solve. And then the security property, the VDF, is supposed to ensure that we don't know of any algorithm that can solve it in fewer than T steps.
00:21:03.696 - 00:22:14.264, Speaker B: And of course, there's a slightly more detailed crypto definition, which has some epsilons and gives a slight percentage that the attacker can go a tiny bit faster. But that's kind of all we'll tolerate, like cryptographically. The only thing that we can really guarantee is that it takes T steps how long those individual steps take in the real world, on real hardware, that's not a crypto problem, that's like a security problem. And I'll talk about how we would actually reason about that, but that is like a very tricky thing, like kind of translating between the two. But yeah, obviously we wish that instead of saying like, T sequential, we could say it takes T seconds to solve, but we definitely can't do that. Okay, so that's the delay property and then the F property. The fact that it's a function just means that there's only one output, which may seem kind of obvious, but there's proofs of sequential work, which I think are on the next slide, which don't have that property and don't work.
00:22:14.264 - 00:22:55.432, Speaker B: Obviously, if we're using it in some kind of lottery application and there's multiple ways to solve it, that would be different possible lottery outcomes. So that wouldn't be good. We want to ensure that there's only one output to this thing. Okay, so if that makes sense, I can sort of show how we build these things. Cryptographically. This is just kind of the API for a VDF as a cryptographic object. So there's some setup where you pass in a security parameter and a delay parameter t and then you get some public parameters out and I'll talk about later.
00:22:55.432 - 00:23:45.064, Speaker B: But that might be a trusted setup for some constructions, which obviously is a potentially big concern. Might be a public setup, which is great. And then there's this evaluation function that takes an input and produces an output and a proof so that pi is just the proof and the proof kind of drives this efficient verification algorithm that checks if your input output pair XY is correct. It just gives you kind of a yes no answer. Some schemes potentially don't require an extra proof, but I think most do. So the proof maybe could theoretically be null, but in practice we need it. One comment actually is the attacker may not care about the proof, they may just want to know what the result is and they don't care if they can prove it to anyone else.
00:23:45.064 - 00:24:34.920, Speaker B: So there might be like an eval prime that just produces the output y and not the proof, which is useful to an attacker. They can figure out what to manipulate, whereas the good guys, the honest solvers, have to actually produce a proof so that they can prove to everybody else. Fortunately for the constructions we know of, there aren't like huge shortcuts. If you don't produce it is extra work to produce the proof. I'll talk about that a little bit, but it's not too much extra work. But yeah, I guess to drive the point home, ideally this eval is kind of run by one party and they don't have to be trusted since they're producing a proof. You just want somebody to do it and then verify is the thing that everybody will run to actually verify that the lottery was computed correctly.
00:24:34.920 - 00:25:24.410, Speaker B: Okay, so like I said earlier, if you take away any of these three properties, it's kind of a known construction. If you just want to build a delay function that's not efficiently verifiable, that's really easy. You just take kind of any one way function iterate it a bunch of times and you iterate it t times and it will take T steps to evaluate and you're sort of done. But that doesn't necessarily give you an efficient way to verify that this was done correctly. If you don't care about a delay, a sequential delay, there's all sorts of problems that have this property but can be parallelized. Like simple example is just solving a discrete log. It's efficient to verify and it's a function at least modular the group order.
00:25:24.410 - 00:26:44.690, Speaker B: But there's no kind of minimum number of steps to compute that because you can parallelize computing discrete logs. And then the trickier one that's not as well known is a proof of sequential work which is verifiable and requires a fixed number of steps but it's not a function. It doesn't guarantee a unique output, probably not worth I guess I could explain that kind of offline if anyone's interested in how it works. It uses depth, robust graphs, and it's actually a very interesting construction, but I'm not sure it's really used anywhere because it turns out like, the modern VDF constructions are more efficient than this anyways, so it doesn't seem like there's too much advantage to this construction, even though it's really interesting. Okay, so before I show the VDF constructions, I'll make a point that it's not actually clear that you always need an efficient verification algorithm. So Solana uses what they call a proof of history. They've been using the term VDF more now, but arguably they kind of, like, came upon the idea independently, but they just use an iterated chain of Shaw two five six, and they call it proof of history.
00:26:44.690 - 00:27:28.560, Speaker B: Somewhere there's a forum with them arguing with Benedict about whether or not this qualifies as a VDF. And they said, we think it's a VDF. And Benedict said, well, you can kind of call it whatever you want, but it's not Verifiable. But the way they verify it. So they compute this long chain of Shaw two, five sixes, and then they verify it kind of in parallel by breaking the chain up into pieces. And then you can verify it a lot faster than computing it by if you have parallelism for the Verifier, because you can just have different threads verify different parts of the chain. So they've basically said that that's sufficient for their purposes and they're not really interested in a fancier VDF construction.
00:27:28.560 - 00:28:07.230, Speaker B: So just kind of interesting that this idea has popped up there and they seems like they're fine living with it. I guess one more before I get to actual VDF. So this will be useful in explaining our VDF construction. This is an old idea, I guess, that's hidden there, but that should say 92. But this is it's kind of like a proto VDF, or we call it a weak VDF in the paper. But the function is really simple. You're just computing square roots mod P.
00:28:07.230 - 00:28:50.872, Speaker B: So this doesn't even have to be a really big prime number. It doesn't have to be like a 1024 bit diffie hellman size prime. It can really be anything. And if you want to compute the square root of some element in the group mod P, it's just one modular exponentiation. So it's like roughly log of P steps to do this. So the larger P is, the more steps it is to actually compute this square root. Whereas to verify it, it's always just one squaring, right? Because verifying a square root is easy.
00:28:50.872 - 00:29:36.264, Speaker B: You just square and see if you get the result that you want. So this is kind of like almost good enough to be a VDF. The problem you can say it gives you this asymmetric delay. So why is this not enough? The problem is it doesn't really allow you to scale the number of sequential steps to solve this to be as large as you want. Like if you want it to take more steps to solve, you have to choose a bigger and bigger prime. And that kind of eventually gets unwieldy. So if you want it to take like two to the 40 steps to solve, which is like a practical parameter that people think about now, you end up using some enormous prime number that obviously the result gets really big and it's not very convenient to work with.
00:29:36.264 - 00:30:23.604, Speaker B: So that's kind of the limitation. But I'll show you how we use this to build something that is kind of scalable. Okay, so the construction in our 2018 paper. Well, first we sort of made a simple observation that you can sort of just take any delay function and use a snark and then it's a verifiable delay function. So that's kind of obvious. This might not actually be very efficient for the prover, but kind of generically, you can just take that chain of one way functions, just run it through any system that gives you succinct proofs, and then it's verifiable. Again, probably not very practical because the proving time is probably going to be relatively large in concrete terms relative to the time to evaluate.
00:30:23.604 - 00:31:16.920, Speaker B: But this kind of shows that it's possible. And the trick we introduce that makes this actually practical, we compute a chain of modular square roots. So that's kind of the iterated one way function. It's not just kind of any one way function like a hash, but it's a modular square root. And then instead of verifying the forward direction, we verify the reverse direction, which is much faster. So what we actually verify using our succinct proof system is the reverse chain that if you take Y and square it a bunch of times, you get back to X. So obviously this is a much simpler it's much faster to go in this direction than this direction.
00:31:16.920 - 00:32:23.630, Speaker B: But the really kind of powerful observation is that you can just sort of run two threads in parallel. One thread is evaluating in the forward direction, so it's iteratively taking modular square roots and then you have another thread that's chasing it, that's computing bits of proof that's kind of doing the reverse direction. So every time the evaluation thread does one more modular square root, the proving thread computes a little bit more of the proof circuit that just proves the squaring in the reverse direction. And it turns out that you can achieve this goal of having the proving thread keep up with the evaluation thread so there's almost no extra time to compute the final proof. And of course, one of the tricks to get this to work is that you do this all in the base field for the proof system. So that just proving one squaring is like one gate in the circuit ends up being really cheap. Yeah.
00:32:23.630 - 00:33:08.750, Speaker B: How does this solve the problem? You said a couple of slides ago with like, if you. Wanted to take, like, if you want the parameter to be T oh, so T here is like the number of steps in this chain. Yeah. So we're only requiring the prime to be big enough, basically, so that there's a big enough gap between these two that the proving thread can keep up with the eval thread, but the T comes from the number of steps in this chain. So you can make that as long as you want. Same modulus. Yeah, I'm leaving out a detail which is like, you kind of re randomize between each of them.
00:33:08.750 - 00:34:12.850, Speaker B: But yeah, you're working on same modulus each time, and it's a very specific modulus that's like the really efficient one. It's the field that the proof system is operating over the rebrand innovation that makes this inherently sequential. Well, I mean, yeah, you could sort of collapse the chain if you didn't do anything between them. Our construction in the 2018 paper, and this has been built out, so Starquare had a project to implement this using Starks. Obviously, they were running it. I think that they've stopped running it, but I mean, the code and everything is still available, but they were actually running this and verifying the VDFS on ethereum. So it's practical to do it works with Starks, so it's even like post quantum, which is great.
00:34:12.850 - 00:35:07.010, Speaker B: And this is one way to get a VDF. There's some other ideas in our 2018 paper, too, like using incrementally verifiable computations. So instead of like one big proof that you're kind of computing pieces of, you have proofs that verify a proof from the last step and you iterate. So if you're doing T steps, you actually have T proofs and each proof verifies the previous one. And there's some kind of related ideas with proof aggregation and things like that. I'll move on and talk about the RSA based ones, but I'll just say that there's a couple of different flavors of these proof based VDFS that we talked about in the paper. The other kind of main family of VDFS that was developed right around the same time as our paper came out.
00:35:07.010 - 00:35:45.276, Speaker B: I guess 2018, like summer of 2018 was a hot time for VDFS because our paper came out and then two different papers from two different researchers independently coming up with ways to build a VDF in a totally different way with squaring mod N. So this delay function is really, really simple. You take your input, you work modulo an RSA number N. So this is a large number. It's a product of two primes. And nobody should know these two primes. And the delay is just that.
00:35:45.276 - 00:36:23.560, Speaker B: You take your input and you square it T times. Really, really simple. Just t straight Squarings. Or you could think of it as raising it to one exponent, two to the T. And the reason that this only works mod N, you need it to be a group of unknown order. If you know, the group order, then you can reduce this large exponent, mod the group order, and you can always do it in maximum, like log N multiplications. The unknown order part is really important for making this actually inherently sequential.
00:36:23.560 - 00:37:19.420, Speaker B: So we knew that this was a delay function for a long time. This idea was originally in this RSW paper in the mid 90s for time lock encryption. So we knew that this was a good candidate for an inherently sequential problem. The question that we didn't know how to solve until 2018 is how you actually verify that somebody's computed that correctly. And it turns out, like I said, two different papers showing two very different proofs was really, really interesting. They work in kind of completely different ways, which I'll show you, and they came out around the same time. Okay, so how do we prove that you've actually done this large squaring in a group of unknown order? So this is wezelowski's proof, and I'll show it to you as an interactive proof, even though of course, most people do it non interactively with fiat shamir.
00:37:19.420 - 00:37:56.676, Speaker B: So the prover claims that they've computed this correctly. The output is Y. The challenger sends them a random prime. So this is kind of important actually, because this is like the big performance bottleneck in verifying these proofs on the blockchain and other places, is that if you're doing this with fiat shamir, you have to have a hash function whose output is a prime, which is pretty ugly to actually verify. Like, you can do it, but it's kind of expensive. So that's like a limitation of this proof. You need hash to prime.
00:37:56.676 - 00:38:37.190, Speaker B: But anyway, the challenger picks a random prime number and then the prover, this is done over the integers. The prover represents this large exponent as they basically do long division. So they get some quotient. If you divide the large exponent by this prime L, and then there's a small remainder and they send this value back to the verifier, which is kind of almost X to the two to the T. It's this very large number, but there's still this small kind of remainder left. So this is a very, very large number. It's not as big as two to the T, but it's like almost all the way there.
00:38:37.190 - 00:39:36.116, Speaker B: And then the challenger just does like one multi exponentiation, I guess, where they take this value sent by the prover, they raise it to the power L and then they add in the remainder kind of manually. So they're not doing the huge exponentiation themselves, they're just doing like a very small exponentiation, which is like at most the size of the prime L. And how big does that prime have to be? It just has to be so big that the challenger has no chance of predicting it in advance. So like 128 bit prime number. There's a lot of details about how this works and the security assumption even though it's very, very elegant if you've never seen this before and it doesn't make sense. It took me a long time staring at this to feel like I had an understanding of what was going on. But just to show you, this is like a really great discovery.
00:39:36.116 - 00:39:54.996, Speaker B: It's really elegant. Really elegant way to do this proof. Really simple. And notice that the proof is constant size. You always just send this one value no matter how big the exponentiation was. So it's a constant size proof. It's very cheap to verify, so it has some really nice properties.
00:39:54.996 - 00:40:41.790, Speaker B: There is a question of if the prover has to compute this value and then they know this almost as big value, they have to compute how do they actually compute that efficiently? The naive solution is like they basically start again and compute another big exponentiation and it takes almost twice as long. Obviously that's not very good. So there's some optimizations where you store points along the way, sends R. No, I think the Verifier recomputes R. I think the challenger picks L. And then once you've picked L, R is uniquely specified. Once you've picked L, two to the T is public.
00:40:41.790 - 00:41:18.920, Speaker B: So once you've picked L, like Q and R are uniquely specified. Yeah, so that doesn't have to be sent. One point to make here is that we have a way of making it a zero knowledge proof. But by default it's a bit strange. It's not a zero knowledge proof and it's something that the challenger could compute themselves if they spend enough time right, there's no secret that they don't know. Okay, so that's wezelowski's proof then the totally different way of doing it, which I actually think is more intuitive. Maybe this is Peters X proof.
00:41:18.920 - 00:42:12.876, Speaker B: So this I think it's more intuitive. You basically take this long exponentiation and you break it in two. So you pick kind of the midpoint like think if you're doing T Squarings to get from X to Y or X to the two to the T, you pick this value in the middle, which is when you've done T over two Squarings. So you pick this halfway point in the chain. So he calls the halfway point V halfway between U and W. And then if V is actually supplied correctly so basically the proverb supplies U and V and W and claims that it's the halfway point on the chain. And T prime is this big exponent here.
00:42:12.876 - 00:43:29.890, Speaker B: So the prover makes this claim that these are picked in this special way. So then this kind of relationship will hold where U times V raised to the power two to the T over two should equal V times W. And that's just like guaranteed to hold if this chain was set up correctly. So the challenger can actually just pick any values A and B and this relation will still hold even if you raise, if you add these random exponents. So you sort of like randomize it and check that. So this is just kind of a way of checking that you've actually supplied the correct midpoint of this chain. So why is that useful? You've just proven what the midpoint is and the proof strategy is that you do that and then you sort of recursively do it now for so if you only did one step, the Verifier would have to actually manually verify that V is really u raised to the power two to the T over two.
00:43:29.890 - 00:44:40.724, Speaker B: But the observation is that you can just prove that it's the midpoint and then you can recursively do the proof from the beginning to the midpoint and you can do this as many steps as you want and then eventually you cut it off and you manually verify the rest. Probably, at least to me, this is a much more intuitive way to do it, but you can maybe sort of see the downside is that if you're doing this interactively, you have to do like a logarithmic number of steps to keep cutting the distance down. So instead of having a constant size proof, you have like a logarithmic size proof just show you a couple of interesting properties. Comparing the two. I think the main thing to remember the Wezeloski proof is kind of optimal for the size of the proof, which is constant size. So if you have to post this proof to a blockchain or anywhere else, wezelowski proofs are kind of the best, but they're actually more expensive to compute, so it's much cheaper and takes less space to compute the Petersac proof and it has some other nice properties like it relies on a strictly weaker assumption. It's still a sound proof even if the group order is known.
00:44:40.724 - 00:45:36.604, Speaker B: This is actually kind of like a subtle distinction that I think gets lost a lot. The Wezelowski proof is totally unsound if the group order is known and the Petersac proof is unconditional. So even if the group order is known, it's actually still a valid proof, which is kind of very interesting that these two things emerged and kind of both of them are used in different applications just to give you like a small flavor of all the work that's happened the last four years, as cryptographers like to do. Once came up with the idea of VDFS. People have proposed all sorts of extra properties that VDFS might have and that's led to a lot of follow up papers. So all of these I'd be happy to talk to you about. The watermarking one is kind of interesting because it's very useful and there's a construction that there's no security proof, so we don't know if it's actually like a secure watermarking scheme or not.
00:45:36.604 - 00:46:15.530, Speaker B: So that's kind of an interesting area to work on. The decodable VDFS thing is more of like a dream where we wrote about it in the original paper. You could build really useful stuff if you had it, but we sort of don't really know where to start. And the rest of these have all been worked out. And then in addition to the randomness application that originally got us interested in this, people have proposed VDFS for all sorts of other applications in the blockchain space, out of the blockchain space. Depending on I might be like, up against time. But I can talk about any of these depending on what I'll show the front running prevention one because this comes up in blockchains a lot.
00:46:15.530 - 00:47:18.200, Speaker B: So there's kind of front running probably everybody's seen it. If you have some name registry thing and someone publishes a message to the network saying like I want to register this new name for my new brand that I'm launching. The concern is that some attacker will see that attempt to register a new name and then they'll try to sneak their own claim in for the name right before it can get confirmed on chain. So the idea with VDFS is that when you broadcast something like this and again there's kind of a relation to Commit Reveal because people use Commit Reveal for front running prevention sometimes. But you'd basically publish your claim to the new name and your key and then some VDF that you've evaluated on top of that information. So that kind of proves like I've wanted to register this name for at least 10 minutes or whatever. And then if an attacker tries to front run they don't have enough time to basically compute a VDF on top of a claim to the name for their identity.
00:47:18.200 - 00:47:59.688, Speaker B: So hopefully if you parameterize this correctly, they're just not able to front run because they can't compute it in time. So it's kind of an interesting application. There's some startup called Slowswap that's trying to do this for kind of like a decentralized exchange and there have been a couple of other papers discussing it. The flaw in this well, not in this example, but the flaw that can come up. Obviously, if an attacker can pre compute VDFS on tons of possible orders then this is no longer secure. So the assumption is like the attacker doesn't know what front running transaction they want to make until after they've seen yours. Otherwise if they can just sort of pre compute every possible order and if it's a small finite set.
00:47:59.688 - 00:48:41.320, Speaker B: So if you go back to this example, the attacker could just use a different 1048 bits of randomness. Right? I guess depending on what it's actually like. The point is the attacker shouldn't be able to predict what the thing is going to be. Right? Yeah. Decodable VDFS maybe is a hard problem but I do think that we could make some work on the Watermarking scheme. I didn't talk about class groups but it's an alternative way to get a group of unknown order besides RSA groups that we believe can be done with a public setup with no trusted setup. No one's really done figured out in practice how to choose those yet.
00:48:41.320 - 00:49:50.716, Speaker B: So these are kind of like the theory questions. And then on the practical side, the biggest question is really how efficient you can make solvers and how fast these things go in practice, right? What the relationship is between the number of sequential steps that you pick as a parameter and actual clock speed in the real world. So there remains like a lot of questions about how fast you can go on FPGAs on Asics, whatever hardware you have available. And I think this is really good kind of hands on implementation work. And it'd also be great to have kind of like standard software that evaluates these things and Verifies them with all the different variants that we know verifiers that work in smart contracts. So we have done some VDF Verifiers that run in solidity run on Ethereum, obviously like other platforms we haven't done yet and also verifying these things within circuits for proofs is another hands on area. So, yeah, I can take questions and these are all things I'd be interested in working with on theory side or the application side as well.
00:49:50.716 - 00:50:01.250, Speaker B: As I should say, there's probably a lot of other applications of VDFS and questions about the incentives and who computes them and how they interact with the economics, which would also be good to work on.
00:50:01.860 - 00:50:17.750, Speaker C: There was this paper at Europe last year on delay encryption and that uses an isogeny based mean. This is a boring question. I'm trying to remember is that construction, like, does it require an isogeny based PDF or would any media work?
00:50:18.760 - 00:50:30.920, Speaker B: It's a good question. I remember seeing that paper when it came out, but I never understood the details of it. So I don't know if they fundamentally need the isogeny based construction.
00:50:32.140 - 00:50:53.650, Speaker C: The last decade or so, there's been a lot of work on cryptographic watermarking. What balance you with this technique here? Do you want to be like what is the question here? Do you want to tailor this watermarking VPs or have you thought of whether this monster and watermarking scheme can be helpful here?
00:50:55.220 - 00:51:33.196, Speaker B: It's a good question. This might mean something like slightly different than traditional watermarking. What you're actually watermarking is the proof. So someone evaluates the VDF and then the proof actually embeds a watermark like the prover's identity. So I think it's sort of different than like general cryptographic watermarking because it's very specific to in this case, you're watermarking a proof. So you're like entangling the prover's identity into the proof itself, hopefully in a way that you can't take that and produce another valid proof. I should show you the watermarking seam because it's pretty simple and I'm not sure it's secure, but it's very simple.
00:51:33.196 - 00:52:10.356, Speaker B: And if there is a connection to something else that we already know more about, that'd be great. But I'm not sure. I think it might be kind of different. But I also probably don't know the classic watermarking stuff well enough. Basically, on like, a good laptop, if you're doing like a 2048 bit RSA modulus, which is kind of standard, like, a fast laptop can probably do about a million Squarings per second. And there's a paper with an FPGA implementation that did like two to the 24 Squarings per second. So you get kind of like an order of magnitude on FPGA.
00:52:10.356 - 00:52:57.828, Speaker B: And that paper, which was written by some researchers from intel, they estimated that you would do about two to the 28 on Asics with reasonable kind of feature sizes. So maybe like another order magnitude speed up from Asics. So roughly you probably go like ten times faster with FPGA and another ten times faster with ASIC. But obviously the details might matter a lot. Well, the assumption was, like, that the good guys will have Asics too. So there was this project, like Ethereum put some money in to try to develop Asics that would be publicly available. And then there's kind of a theory that the attacker can't go too much faster.
00:52:57.828 - 00:53:46.250, Speaker B: I had a slide somewhere. Yeah, there's like this big conjecture which we can't really prove, that basically you can always go faster if you have a bigger budget, but hopefully there's pretty strongly diminishing returns that if you want to go 10% faster, you have to spend way more money. But obviously this curve is just like scribbled by me. This is not based on any data, so we have no idea in practice what the shape of this curve is like or be interesting to see. But I'm not a hardware person, so I'm a little bit at a loss to give you a better answer than that. Yeah, it's a good question. Like, T definitely could change over time.
00:53:46.250 - 00:54:22.420, Speaker B: So for all of the unknown order things, technically we call them incremental VDFS, but you don't have to specify T at parameter setup time, so they're really easy. You can change T to be whatever you want. You can even change it on the fly while you're in the middle of solving it. For some of the proof based ones. You do have to bake t in a little bit more, but then you just do like, well, we had a whole talk yesterday about structured reference strings, so maybe you have a universal one, or maybe you have to redo the setup if you change t. Is it.
00:54:22.570 - 00:54:36.612, Speaker C: Incorrect to model hardware differences as one entity faster in evaluating one step of the thing? And then the advantage compounds as T grows? Or is that it contradicts?
00:54:36.676 - 00:54:43.290, Speaker B: Well, it shouldn't compound, it should be linear, right? Like if you go 10% faster on one step, you should go 10% faster on yeah.
00:54:45.740 - 00:54:52.024, Speaker C: But this chart is saying for one step. This is a marginal return on investment.
00:54:52.152 - 00:55:11.570, Speaker B: No, this chart is very abstract. This is supposed to be conceptual, but this is the evaluation speed for one step or for the whole computation. Right. I mean, it's just a chain of things. So if you speed up, each one should be like a proportional speed up.
00:55:13.140 - 00:55:43.310, Speaker D: There was a lot of talk about ground stacks in 2018, and it seems like the kind of theory around that time was that, like you were saying, an asics in the hands of everyone. And I'm curious if there's ever a construction that would eliminate the possibility of grammar fact or if we had previous variance, that with hardware, with the constraints necessary, that it's always going to be something that is outside of the construction itself.
00:55:45.600 - 00:55:49.200, Speaker B: I'm not sure I understand your question. You're asking about grinding attacks.
00:55:49.540 - 00:56:39.500, Speaker D: Yeah. So for example, in DDFS, if someone has a lot more powerful harm than anybody else, they can. Specifically in the construction of ethereum, you could increase the difficulties required to compute some DF by making T increase over time and then go offline such that it completely slows down the system. Or you could pretend that you have less power than you do and then actually have more and affect the last reveal. I'm not sure, but I imagine there are ways in which you can manipulate the amount of compute.
00:56:40.820 - 00:57:10.884, Speaker B: Yeah, I guess with VDFS, it's like the amount of computing, it's very different because it's not like you can just take with Asics, you can take some of them offline. It's like it's a highly parallel thing. So you can reduce the amount of parallelism and change your performance. With Asics for VDFS, it's probably like you have one ASIC computing it at a constant speed and it's either online or it's not. Right. You can't take part of it to do some other problem. Yeah.
00:57:10.884 - 00:57:27.050, Speaker B: So I guess it's true that if you have some really fast who then goes offline, maybe you'd get into trouble. But I guess the assumption is like, that once you built an ASIC, that solves it fast, you probably have it forever. All right.
