00:00:10.160 - 00:00:40.236, Speaker A: So I'm really excited to have Ron Rothblum here today. He's a professor at the Technion and he works at the intersection of complexity theory and cryptography. He's especially focused on proof systems, which is a topic near and dear to my own heart. He's done some really fundamental work both on their security and efficiency. And he's going to tell us a little bit about some of his more recent work today focusing on proving efficiency. Thanks Justin. And thank you all for coming and the invitation to come here for this week.
00:00:40.236 - 00:01:24.196, Speaker A: It's a pleasure to be here. Look forward to meeting many of you later this week or during this talk, hopefully. So the title of this talk is Proving as Fast as Computing succinct arguments with Constant Proverbial overhead. And it's based on a sequence of joint works with the other Justin, Justin Holmgren and with Noga Ronsvy from University of Haifa. Okay? So the picture that I want you to have in mind throughout this talk is the following. So we're interested in deciding whether some statement is true or not. I'm going to capture that, as usual, sort of in complexity theory by asking does a particular input belongs to language L? Does a particular input have a certain property? So we have a Verifier which is interested in this statement.
00:01:24.196 - 00:02:11.412, Speaker A: It's given X and it's allowed to interact with a prover which has the same input but also has some sort of NP witness proving that indeed X has this property. Okay? And we want our prover to convince the Verifier that indeed X has this property, that X belongs to the language L. One thing the prover could do is just send over the witness the Verifier checks and we're convinced that's sort of a traditional NP proof. But what we're interested in doing here is doing something that is much shorter. So we want the prover to send much less information, but nevertheless for the Verifier to be convinced. Ken will define things more formally in a few slides, but that's sort of the picture that you should have in mind. So before being a little bit more formal, why are we interested in this? And I want to give two different perspectives.
00:02:11.412 - 00:03:00.340, Speaker A: The first one, which is sort of where I am coming from, is theory perspective. And really the study of proof systems in general has been an amazing catalyst for the development of theory of computation. So if you think of the beginning of the field notions like P versus NP, NP completeness. If you think about it, these are all talking about the existence of proof systems. And as our field has evolved, we've looked at more various different notions of proof systems and as we've looked at those and studied them, we've had sort of seminal results the discovery. So on one hand, MP completeness zero knowledge proofs for those who are familiar with the PCP theorem. So a lot of the development and most fundamental results in the theory of computation have evolved from the study and entire subfields also have evolved from looking at proof systems.
00:03:00.340 - 00:03:40.180, Speaker A: So it's a very exciting field from a theory perspective. Something that I think is more rare is that simultaneously it is also extremely exciting from a practical perspective. And that's something that's been happening in the recent years. So the fact that I'm standing here and giving this talk is an indication of that. So I believe, and you can all probably convince me better than I can convince you, that there is a lot of interest in succinct arguments, especially in the domain of blockchains and cryptocurrencies. So as you might suspect from my introduction so far, I am coming from sort of the theory perspective. So I'm not really qualified to tell you of how important this is.
00:03:40.180 - 00:04:11.228, Speaker A: But here is a guy who probably is qualified. The title is from Time magazine. The Prince of Crypto. Perhaps the most general cryptographic, most powerful cryptographic technologies to come out of the last decade is General purpose succinct as your knowledge Proofs. Okay? So definitely a huge amount of interest in industry as we see here today. Good. So that is sort of the area of succinct arguments once people started becoming really interested in implementing succinct arguments.
00:04:11.228 - 00:05:07.840, Speaker A: So people like Justin and others quickly discovered that one of the key bottlenecks, if not the key bottleneck, is the amount of work that the prover has to invest in order to prove the correctness of the computation. And it turns out that even today with amazing work by Justin and others, it is still orders of magnitude slower than just performing the computation by yourself. So it's really a practically motivated problem. But it leads to a fundamental question, I think both in theory and practice, can proving be as fast as computing? So that would be the Holy Grail. How close can we get? So that is sort of what this talk is going to be about. And before telling you what the results are, let me start with a disclaimer. So throughout this talk I'm going to be taking and the results that I state, I'm going to be taking a theoretical perspective.
00:05:07.840 - 00:05:53.984, Speaker A: So I'm going to be talking about the asymptotic behavior of proving correctness and achieving in some cases optimal, in some cases close to optimal asymptotic behavior. I am not coming and claiming concrete efficiency. Okay? And I'll say in a second why you should not leave the talk in this minute. Just a second before that. One benefit of doing this is that I'm going to be focusing on very precise, well defined metrics. So I'm not going to be telling you that this MacBook Pro, if I run the prover on some particular implementation, runs in whatever number of seconds. No, it is super precise metric which talks about the sympaty behavior, which is in my mind very clean.
00:05:53.984 - 00:06:28.632, Speaker A: We'll see in a second and then we can argue about that. So things are going to be very precise. So again going to this question of why you should care. So for the people here who are interested in theory, I think this going to be introducing some new ideas which I think are really interesting and we get in particular the best known asymptotic behavior for the prover. So we'll see exactly. If you are interested in practice, I would still encourage you to not abandon the talk five minutes in. And the reason is I think that we are introducing here some new and different paradigms.
00:06:28.632 - 00:07:05.290, Speaker A: And I would say that there is nothing this term of galactic algorithms that have some of N time, but really there's a two to the thousand overhead going on. It doesn't seem to be that there's anything of that nature going on here. So I do think these things have the potential to influence also practice. They certainly would need have to be baked a little bit further to be practical. But once you have these new ideas and we see throughout this field how very theoretical ideas, even from the 90s have tremendous impacts also on practice. So I think there is potential for that here too.
00:07:05.980 - 00:07:25.356, Speaker B: Yeah, you sort of touched on this, but in TCS there's kind of two different ways we abuse asymptotics. One is looking at the limit and seeing stuff that is true there and is not true for reasonable values. The other is just sort of hiding constant factors or even log terms under sort of big O notation. Should I be on the more on the lookout for one of those than the other?
00:07:25.538 - 00:07:52.432, Speaker A: I don't think things start kicking in at huge values. There's nothing of that sort going on here. It's just a question of I'm not claiming concrete behavior a because I'm not an expert in that and I have not implemented it, so I have no idea. B, there are extremely efficient implementations of things with worse asymptotic behavior that are out there and I would have a hard time competing with, but it does not mean that this would outperform them at humongous implement.
00:07:52.576 - 00:07:59.736, Speaker B: Should I interpret it as you're not going to make assertions about the hidden constant factors but as far as you can tell there's no reason they should be ridiculous, right?
00:07:59.758 - 00:08:38.096, Speaker A: Is that kind of yeah, in particular. But one thing to keep bear in mind is that maybe I'm like winning in some log factor which in practice the constant would be larger. Log factors versus constant are hard sometimes to compare. Okay. So I said that I'm going to be precise about sort of the precise metric that I'm going to be looking at today. So when you're looking at such tiny factors like log factors, you have to be in particular really precise about what model of computation you are looking at. So in the talk today, I'm going to be focusing on Boolean circuits.
00:08:38.096 - 00:09:16.960, Speaker A: This is in contrast, I think that in Justin's talk a couple of weeks ago, I imagine you were talking about arithmetic circuits, right over pretty large finite fields and thinking of the prover as doing both field operations and other sort of ram operations. Here I'm going to be looking exclusively on Boolean circuits. So I'm thinking of the computation that I care about being expressed as a Boolean circuit and implementing the prover as a Boolean circuit. Okay? So it's kind of apples to apples comparison. So I'm comparing the original computation, which is expressed as a Boolean circuit versus a prover that is implemented as sort of a similar Boolean circuit. So first of all, what is a Boolean circuit? I hope that everyone is familiar with that. If not, please let me know.
00:09:16.960 - 00:09:44.628, Speaker A: But it's sort of basically graph directed graph of this form. We have input nodes and we have gates. Gates compute simple functions like and or soar stuff like that throughout this talk. Think of the circuit contains arbitrary gates of constant fan in. But for simplicity, let's say NAND gate would be a complete set of gates. So we can just think of NAND gates. Okay? So that's the model of Boolean circuits.
00:09:44.628 - 00:10:29.720, Speaker A: Why, look at boolean circuits. And the study of cryptography implemented by very efficient or measuring the efficiency of cryptography by Boolean circuits is not something new. This was started by work of Ishai, Koshirovitz, Ostrovsky and Sahai 15 years ago. So I do this. First of all, I think it's hard to argue that against the fact that it's a very basic, natural, simple model of computation. So in contrast to, say, Aram, where we need to start having fights about what kind of instructions your ham does and does not have here, it's very clean, very simple. And because of that, it is my feeling at least, and this is more of an arguable point, that it really captures the intrinsic complexity of a problem because of its it is so clean and basic.
00:10:29.720 - 00:11:25.464, Speaker A: Another thing, if you compare, say, to an arithmetic circuit so arithmetic circuits are great, but they are sometimes not inconvenient to express non arithmetic computations, right? So if you want to express evaluating Sha 256, for example, I think it would be quite inconvenient to do it as an arithmetic circuit. You can and there works on such adjustions on trying to optimize that, but it is more natural to express it as a Boolean circuit. And lastly, which I think is an interesting point, once you're able to express the prover as a Boolean circuit and you treat that as a black box, you can do things using that. So, for example, if you're trying to prove correctness of a bunch of different computations now you know that in each one you are running the same prover circuit that is doing the exact same operations. Now you can start thinking about pipelining all these operations. So you're doing the exact same thing. Okay? So the focus today will be on Boolean circuits.
00:11:25.464 - 00:12:08.520, Speaker A: So with that introduction in mind, now we can sort of prove formally what we mean by a succinct argument. So I'm thinking of like the picture before. We have our verifier that has an input x, we have the prover that has x and the witness w we have some circuit that we fix. They interact. Ideally, the interaction is not a lot of rounds like in this picture, but it could be potentially. We want the communication to be short that's this bloat over here, and we want these standard, completeness and soundness conditions, right? So if the statement is true and the prover is well behaved, then we want our verifier to accept. On the other hand, if the statement is false, if there does not exist any witness showing that if x is not in the language right, there is no such witness.
00:12:08.520 - 00:13:05.484, Speaker A: And even if the prover is not well behaved, no matter what the proverb does, as long as it doesn't break crypto, say it's run this polynomial time, then we want our verifier to accept only with small probability. Okay? So the small probability, we're going to call the soundness error and small. Let me just briefly mention that the restriction here to efficient provers for this kind of statement is inherent. Okay? So if you wanted soundness even against unbound provers, you'd have a problem. But I think pragmatically, it's good enough to think of efficient provers. As we said before, we're looking for communication that's much shorter than the witness length, ideally polylogarithmic or so another property that would be great to have, but it's not necessary for this definition to be highly nontrivial and interesting is zero knowledge. I won't define it formally, but it basically means that when the statement is true, you learn nothing beyond the fact that the statement is true.
00:13:05.484 - 00:13:26.636, Speaker A: So it's a very useful property. You don't always need it and entirely nontruvar without it. As a matter of fact, there are at least in terms of theory, you can sort of even tack it on for free. Once you have a succinct argument. Making it zero knowledge is usually easy and in fact, almost without loss of generality.
00:13:26.828 - 00:13:40.550, Speaker B: Okay, yeah, you may be planning to do this already, but in the blockchain world it's mostly non interactive proofs, as you know. So just how these parameters translate for non interactive proofs would be useful to review.
00:13:40.920 - 00:13:46.136, Speaker A: Right, so the comparison between having this interactive picture versus a non interactive thing.
00:13:46.158 - 00:14:00.376, Speaker B: Yeah, you do talk about communication, right, but the point is the non interactive proof will probably include like a transcript or the verifier will have to simulate that communication itself. So the communication will translate to like proof length or verifier time or something like that, I would assume.
00:14:00.408 - 00:14:32.960, Speaker A: Okay, so first of all, I think in the blockchain world, a picture like this that is interactive would be really bad because you really want to just post a proof on a blockchain and not prove to anyone coming along that you've done things properly. It is nevertheless an untrivial. And luckily, we have this amazing magical heuristic called the future mirroristic, which lets you convert interactive protocols to become non interactive. So the way I will express all the results today will be interactive, but they can't be made non interactive using fiat mirror.
00:14:33.120 - 00:14:38.356, Speaker B: And is it correct to say that when you try to minimize communication, I should be thinking about minimizing proof length and verification?
00:14:38.468 - 00:14:44.808, Speaker A: So communication for me is number of bits sent overall, right? Not number of rounds or sure, sure.
00:14:44.894 - 00:14:48.124, Speaker B: No, but in a non interactive proof, there's no notion of the communication length, right?
00:14:48.162 - 00:14:49.384, Speaker A: There is. It's a proof length.
00:14:49.512 - 00:14:50.332, Speaker B: That was my question.
00:14:50.386 - 00:15:17.616, Speaker A: Yeah. Okay. In particular, when you apply fiat Chamir here, the communication that I have here will translate into one static proof. Exactly, yes. So what about the verification time? Right? So here I've not stated anything about verification time. For the moment, I'm happy with the verifier being polynomial time, let's say, not caring so much, I have gained something really nontrivial happened here. I managed to compress an NP witness.
00:15:17.616 - 00:15:37.070, Speaker A: It's a miracle, right? So something highly nontrivial happened here. So that's why I defined it this way. When I give the formal theorem statements, we'll see what the verified time is. Okay, it's a good point, but here I'm trying to sort of say, what is the interesting notion, basic definition, and then we'll see what the exact parameters that we achieve are. Okay.
00:15:38.800 - 00:15:46.800, Speaker B: So again, to connect the dots to the blockchain world, I mean, verification time is very expensive, right, in sort of like the current generations of blockchain.
00:15:51.060 - 00:16:41.170, Speaker A: But somehow so, from my perspective, communication is something that once you sent it's there, you've sent it, it's part of the proof. It's not something that it's going to be hard to compress, it computation. On the other hand, in particular, verifier computation, you could think of doing sort of another step, another composition to reduce that later on. So sort of once you have a protocol that has short communication, in my mind, it is easier to extend it and get fast verification. On the other hand, if the verifier is fast, then in particular, it means the communication is short. But again, I think all these are fantastic questions, but maybe I'll show the formal theorem statement and then we can sort of discuss verification time and so on. Okay.
00:16:41.170 - 00:17:11.690, Speaker A: So before I tell you theorem statement, let me mention some prior work. So first of all, and I think this is less relevant to this audience, if you don't care about small factors and you're happy with approver running polynomial time, this problem has been solved for 30 years. We've known how to do this with a polynomial time prover. If you're happy with polynomial time meaning polynomial overhead. So computation takes time. T, if you're happy with a polyt time prover. It has been well known how to do this, less interesting.
00:17:11.690 - 00:17:55.668, Speaker A: So it would be, let's say, a cubic or more overhead, not relevant for practice and less interesting. But it is sort of, I think, the reason why, because in theory, we are sort of in love with the notion of polynomial time, why it took time and relevance to practice to start looking at smaller factors. Smaller factors. And indeed, in recent years, people are looking at smaller factors. And there have been recent dramatic improvements in prover efficiency, including both in practice and asymptotics. I'm going to focus on asymptotics. So, first of all, for Boolean circuits, the model that I'm currently describing, prior to our work, it was known how to do for a circuit of size S, how to achieve prover circuit with polylogarithmic overhead.
00:17:55.668 - 00:18:39.648, Speaker A: So this was already known and know pretty amazing, right? With only polylogarithmic overhead, you can prove correctness of a computation. So that's for Boolean circuits, if you look at arithmetic circuits, as in Justin's Talk, it was known prior to our work how to achieve strictly linear overhead for arithmetic circuits. And this was sort of a sequence of works. It's related to the techniques that I'll describe here today. So we'll see this, okay, so if you're looking at arithmetic circuits over large fields, we've known how to achieve or we know how to achieve constant prover overhead. Lastly, let me mention this very briefly. This question becomes interesting if you insist on zero knowledge.
00:18:39.648 - 00:19:17.040, Speaker A: This question is interesting even if you don't require the communication to be succinct. So communication can be as big as the circuit, but you require zero knowledge, so it's nontrivial. And even there we have some results I'll mention maybe later on. There are still open questions also there. Okay? So the main result, as the title might lead you to think, is that we construct succinct arguments for Boolean circuits with a strictly linear size prover. Okay? We'll see the more precise definition in a second. But before doing that, there is sort of an issue with this theorem statement that I want to clarify.
00:19:17.040 - 00:19:56.430, Speaker A: So the issue is the following. If you think of a circuit, a Boolean circuit, the number of bits required to just describe the circuit of size S is s log s, because for every gate I need to specify what are the inputs coming in, right? So this means that just a description of the circuit has super linear size. And I want my prover to be really circuit size, size. So it seems problematic. The prover can't even read the entire circuit. The way we get around this is what we basically show is the following. So for every circuit that you give me, there exists a corresponding prover circuit specifically tailored for the circuit that you gave me.
00:19:56.430 - 00:20:26.564, Speaker A: So you gave me a circuit of size S. I can construct a prover circuit for that specific circuit with size o of S. The transformation is pretty efficient, but it has to do to have this logarithmic overhead because just the description of the circuit is that much. But is this point clear? So for every circuit we have a prover circuit. Good. So the main result that I want to state today is the following. So in this theorem, we're going to assume crypto.
00:20:26.564 - 00:20:52.880, Speaker A: So I'm going to be making some cryptographic assumption. The cryptographic assumption is quite mild, or relatively mild, I would say. So we're assuming collision resistant Hash functions that can be evaluated by linear size circuits. And we have candidate constructions for this. Okay, so we're assuming but think of it basically as collision resistant Hash functions. Everyone know what collision resistant Hash functions are? Hash functions in which it's hard to find collisions. They're compressing, a lot of collisions exist, but it's hard to find them.
00:20:52.880 - 00:21:34.412, Speaker A: I want it to be very efficient to evaluate the linear time there is the size of the data you're hashing. So throughout these entire slides, whenever I say linear time, I mean linear size. So I'm thinking of a boolean circuit. So I have data that I want to hash and I want the size of the circuit to be linear in that size and the size of the data that I want to hash. Okay, to clarify that's shaw in every hash function in practice, right? Sure. But sort of Sha is defined for sort of it has a fixed output length. So you'd need to think of a generalization if you're looking at it asymptotically but yeah, absolutely.
00:21:34.412 - 00:22:06.488, Speaker A: Okay, so the theorem says the following. So for every Boolean circuit there exists an argument system, a succinct argument system for this kind of statement. So we accept all X's sort of that for which there exists a corresponding witness that makes the circuit happy, makes the circuit accept with the following properties. So, the prover size, as promised, is strictly linear in the size of the input circuit. Again. This is a boolean. I'm implementing the proverbs as a Boolean circuit strictly linear in the size of the original circuit, the Verifier that you were interested in before.
00:22:06.488 - 00:23:00.010, Speaker A: And justifiably so, it requires a linear amount of preprocessing. And that is inherent, sort of the Verifier has to look at every bit of the circuit at some point, otherwise the prover could in its head change the circuit functionality without the Verifier ever noticing. But we can push that to preprocessing. And after this preprocessing, the online time for the Verifier is basically just reading the input, which is again necessary. And on top of that, it's polylogarithmic in the circuit size and polynomial and lambda, where lambda states is a security parameter. So if you want to be sound against provers of size, polynomial and lambda, or even maybe sub exponential and lambda, then you have a polynomial dependence on the security parameter. So think of 128 or so in terms of the amount of communication, it's going to be polylogarithmic in the circuit size.
00:23:00.010 - 00:23:33.876, Speaker A: So instead of big witness, which the natural witness is maybe the entire circuit, we're drastically reducing it to polylogarithmic and a polynomial dependence on the security parameter. It is an interactive protocol. The number of rounds in what I'll describe is logarithmic. Actually, we can get the exact same results with log log rounds, but maybe I'll describe it. So I should have said in the beginning, this talk is a two. There is going to be two parts to this talk. There's the first part, the other part is on Thursday, maybe on Thursday I'll mention that as well.
00:23:33.876 - 00:24:15.760, Speaker A: So logarithmic number of rounds and can be reduced to log log. And lastly, in terms of the soundless error, remember, the soundless error is the probability of accepting a false statement. So it can be an arbitrarily small constant, which sounds pretty good. Arbitrarily small is pretty good, but actually not so great. So really what I would like here would to be something that shrinks with a security parameter. So something maybe negligible in the security parameter or ideally exponentially small in the security parameter. In the next slide, we'll discuss an improvement that improves on that aspect.
00:24:15.760 - 00:24:21.136, Speaker A: But that is the basic theorem statement. Questions? There was a question on the zoom I didn't see.
00:24:21.158 - 00:24:23.072, Speaker B: All of it was about the preprocessing costs.
00:24:23.136 - 00:24:58.530, Speaker A: Okay. The question is how come the verifier preprocessing time is not omega of C log C? Because of reading the full circuit description. So the verifier's preprocessing is also fixed once the circuit is fixed. So given a circuit, I spit out a prover circuit and a verifier preprocessing circuit which are linear and C. Yeah. What does arbitrarily small but not negligible in lambda mean? It means is there a constant that blows up as I make. So I think of asymptotics where lambda can grow.
00:24:58.530 - 00:25:45.164, Speaker A: For example, you could take this basic protocol and just repeat it polylamba times, okay? The soundness air would grow exponentially with lambda, but in terms of the prover size, it will grow multiplicatively with lambda. So in contrast to the norm, where factors that depend on the security parameter are maybe hidden here, I'm fully expressing them. So, okay, I am hiding one thing, which there is an additive dependence on lambda, but I view that as being dominated by the circuit size. But I'm not hiding any multiplicative factors on the security parameter here. Okay? Yeah. If the circuit has like repeated structure, can you skip the preprocessing? Yes, we'll get to that in the next theorem. So actually in this result with nogo, we couldn't.
00:25:45.164 - 00:26:38.028, Speaker A: In the following thing, I'll show you that we can. Any more questions? Okay, so what I like about this theorem is we're really getting to this strictly linear size prover, which was sort of out of reach beforehand. The part that I don't like so much is that the sound error is pretty good arbitrarily small, but still a constant, a fixed constant. Yeah. So given the Boolean circuit C, what is the complexity of the circuit that spits out the prover circuit? Okay, so the input is of size c log c, right? And it should be linear in that so there's no extra blow up there. No, not a lot going on. Okay, so I'll show you the next result with Justin Holmgren, which we really focus on improving the soundness error, getting a similar result with better soundness error.
00:26:38.028 - 00:27:10.100, Speaker A: Okay. So the improvement, I would say, with Justin Holmgren is the following. So we are able to achieve this exponential dependence, exponentially small soundness error, like we wanted the overhead for the prover. There is going to be overhead for the prover, though. So naively. If you just as I said before, if you wanted to get from the previous theorem, get two to the minus lambda soundness error, you would have to repeat the protocol lambda times. So you'd have prover time SC like size of c times lambda.
00:27:10.100 - 00:27:36.770, Speaker A: Here we're getting only a polylogarithmic dependence on lambda. Just to remind you, if we think of lambda as being 128 log of that is seven. So poly seven overhead. Okay. So that's in terms of the prover time, I would love to get rid of this, but it's still sort of very nice improvement over what you get by naive repetition. So that's the green stuff. The red stuff is the caveats in this theorem, which did not exist before.
00:27:36.770 - 00:28:03.720, Speaker A: So here we are restricted to sort of nice Boolean circuits. Basically. Think of circuits that have a lot of repeated substructure, which happen a lot, especially, I think in blockchains where you take, say, evaluate some small circuit again and again and again and again, if that sounds familiar. So that would be a repeated substructure. Or if you're evaluating the same circuit on a ton of different inputs, that again would be a repeated substructure. So we need some substructure of that form. I won't define it formally.
00:28:03.720 - 00:28:29.504, Speaker A: So theorem only holds for those kind of circuits. And here you see, there is no preprocessing. So what we have is for the verifier is read the input polynomial dependence on lambda, the security parameter, and the size of the small circuit that's floating around in there. Okay. Does this make sense? Yeah. So here, description of c is clock c. No.
00:28:29.504 - 00:28:56.316, Speaker A: So description of c, it's a regular circuit. So if I view this circuit C, think of it maybe as just you have some c zero, which is applied again and again and again and again and again. So this would be the size of c zero, the basic underlying circuit that's being repeated. Good. So that is a theorem that I main theorem that I wanted to show you throughout these two talks. I'll tell you what the plan is for this talk and sort of in combination with a future talk on Thursday. So what I.
00:28:56.316 - 00:29:36.308, Speaker A: Want to start by doing is to review sort of the classical approach to constructing succinct arguments. But the perspective that I'll have so for those of you who've read Justin's book, he covers these things. But the perspective that I'm going to have is going to be very focused on error correcting codes. And I find this perspective so it's kind of a higher level abstraction. I find this perspective very useful and it will sort of lead us also to the solutions that we'll have to getting a better efficiency. So it'll be viewed sort of, I think from a little bit of a different perspective than what people are used to. In particular, usually people describe everything via polynomials.
00:29:36.308 - 00:30:31.652, Speaker A: We'll see that really what the polynomials are giving us are error correcting codes with some very nice properties and try to isolate what these properties are. Then what I want to do is introduce a technique called code switching which really underlies our result. And what they allow us to do is to replace the use of polynomials in some key places with other, in some sense better error correcting codes that are good enough and allow us to have a big benefit over polynomials. And this technique really underlies all the results that achieve linear time succinct arguments in the literature, including Justin's work breakdown and others. So this technique is really a facilitator for that. So also the works achieving sort of linear size provers for arithmetic circuits also rely on this code switching technique. So really that's what I'm going to focus on in the talk today and then in the talk on Thursday.
00:30:31.652 - 00:31:27.500, Speaker A: My plan is to show how to use code switching to derive the main results that I just showed you for boolean circuits either with constant soundness error or with exponentially small soundness error and the caveats that I mentioned. So let's start by sort of a review of how to build succinct arguments in general, but from the perspective of error correcting codes. Okay, so how are babies born? Or how are succinct arguments born? So like babies, succinct arguments have two parents typically. So the two parents are an object that's called an interactive Oracle proof, which I will define in a moment, and another which is sort of an information theoretic object. It generalizes a PCP for those who are familiar with that. And then the other object is something called the vector commitment, which is a cryptographic object. And the simplest way to think of it is sort of what merkel trees give you.
00:31:27.500 - 00:32:04.200, Speaker A: So with a merkel tree you can commit to a long vector so that later on you can decommit to particular bits. So that would be an implementation of a vector commitment. And when you have these two things together, you combine them, you get a succinct argument. So we'll see that in a few slides. But let me first define what is an interactive Oracle proof. So this is a notion introduced concurrently in the work of Benson Chiesa and Nick over here and in a work with Omarian Golden Guy Rothblum. So it's kind of so interactive Oracle proof and someone not heard of the term.
00:32:04.200 - 00:32:29.420, Speaker A: Has someone heard of the term? Okay, so everyone's familiar. Okay, cool. So the way to think of it is a hybrid between an interactive proof and the PCP. The picture that we have is, as before, we have a Verifier and approver. The verifier is interested. It does X belong to the language l here notice that we have a powerful wizard because here soundness is going to be unconditional. So statistical soundness no crypto.
00:32:29.420 - 00:33:12.360, Speaker A: And the model is that the prover is allowed to send very long messages to our Verifier. Let's say that the Verifier, for the moment, sort of ignores it. Verifier responds with a random coin toss. Prover sends again another huge message to the Verifier, verifier responds with a coin toss, and so on and so forth. At the very end of the interaction, after the prover has sent all these long messages, the Verifier can read a few bits overall of what was sent and decide whether to accept or reject. So that's a model of an IOP or interactive Oracle proof. It seems like a bit of an artificial or weird model, which is, I think, part of the reason why it took the community some time to look at it, but it turns out to be incredibly useful.
00:33:12.360 - 00:33:48.052, Speaker A: So its main use, and the one that I'll talk about today, is about constructing succinct arguments. So this started with the work of Nick and others. It's also been incredibly useful for constructing efficient interactive proofs with statistical soundness. That's why we introduced it in our work, getting proofs that are approaching the length of the witness. I won't go into details about this, so maybe take my word on it that they have even additional applications beyond just constructing succinct arguments. It's, I think, a very exciting notion. So that was IOPS vector commitments, I guess I already described.
00:33:48.052 - 00:34:14.960, Speaker A: You can take a long vector, produce a short hash, short commitment, so that later on you can decommit to individual bits of this long vector. And the canonical example would be a merkel tree. Okay, so why do these objects go hand in hand together? Well, the basic idea is you have your IOP. You're supposed to send a really long message. What do you do? You commit to it using the vector commitment. Right. So you send a short commitment at the very end.
00:34:14.960 - 00:34:41.080, Speaker A: The Verifier is interested in reading the few places you decommit. So that's the basic idea of how succinct arguments are born. We're going to get into much more detail about how the IOPS are born as well. But for the moment, if we have an IOP and we have merkel hash clear how we got a succinct argument. Right, okay. How about linear time? Getting a linear time prover a succinct argument with the linear time prover. Well, we need two things for this to work.
00:34:41.080 - 00:35:28.810, Speaker A: We need our vector commitment to be efficient enough, and we need the IOP to be efficient enough in terms of the vector commitment. Certainly from a theoretical perspective, it's sort of a piece of cake. Basically, if you take a merkel tree and you implement the underlying collision resistant hash function as one that is computable and then your size, you automatically get that the vector commitment you can commit and decommit in linear time. The hard part, and the one that I want to focus on today is how to construct an IOP in which the prover can be implemented in linear size. So that will be our focus. Let me just say that this regime of strictly linear size introduces a lot of annoying challenges. But that's on the downside, on the plus side, it produces more papers, so we get something back.
00:35:28.810 - 00:35:40.910, Speaker A: Cool. So from now on, I'm going to focus on how our IOP is built and later via this perspective of error correcting codes. Okay.
00:35:43.040 - 00:35:49.250, Speaker B: Think about the parameters. How long should I think about each message being and how many cells should I think about being read?
00:35:49.860 - 00:36:17.784, Speaker A: So often, what happens? So we'll see in the beginning, in the first constructions we'll have, you'll have a very long first message and the rest of the message will be very short, so long as like circuit size and then the additional message are very short. That is something that is pretty typical. Later on, we'll see that in the eventual construction that we have, the message sizes are going to be gradually shrinking. So you'll start off circuit size, then circuit size over ten, circuit size over 100, and so on.
00:36:17.822 - 00:36:19.444, Speaker B: And it's going to be a constant number of queries.
00:36:19.492 - 00:36:53.812, Speaker A: Or what should I so it will be either a log or log log. Many queries, basically. So we need to read some queries of every round. And if we have the version that I stated with log rounds would be log queries, the version with log log would have polylog log queries. Okay, cool. Do people know about error correcting codes? Okay, so let's do it. So what are error correcting codes? So air correcting codes are an entire huge field in computer science or electroengineering also.
00:36:53.812 - 00:37:32.924, Speaker A: And the goal is know Alice is here on Earth, bob is over there on Mars and Alice wants to send messages to Bob and errors may happen, cosmic radiation and so on. Errors may happen. And so the communication channel is noisy. We want Alice to encode her message so that even if cosmic radiation happens, some of the bits are corrupted. Nevertheless, you can recover the message. That is the goal of the field of error correcting codes. Why is it at all related to the field of arguments? First of all, because essentially all arguments that we have use error correcting codes.
00:37:32.924 - 00:38:18.908, Speaker A: Usually it's hidden via the use of polynomials. But one of the fundamental properties of polynomials is that they form a good error correcting code. But I want to say beyond that, you can think of codes as ways to protect communication from adversarial corruption, whereas succinct arguments, PCPs IOPS proof systems are ways to protect computation against adversarial corruption. So I think it's very natural that these things come up thinking of computation as something sort of more sophisticated than just communication. But anyway, that's sort of more philosophical. To be a bit more concrete, what is an error correcting code? So I view it as sort of a mapping, an injective mapping from this small ball domain, which I think of as the message space. So let's assume we're using an alphabet called sigma.
00:38:18.908 - 00:39:06.604, Speaker A: So we're taking a message that is k symbols from sigma and producing a longer message, which is N symbols from the same alphabet sigma. Okay, why is it important that it be at the very least injective? So that poor Bob can recover the message that Alice sent. So we want it to be at the very least injective. We actually want more than that. So if we look, there are two kind of key parameters that we care about encoding. One is the rate and the other is the distance. So the rate is basically just saying measuring what is the overhead involved in doing this encoding? How much larger is N compared to k? How much redundancy are we introducing when we're encoding? And ideally, because communication to Mars is expensive, we want the rate to be as close as possible to one.
00:39:06.604 - 00:39:51.416, Speaker A: We're looking for high rate codes, the rate closest possible to one. So that's one important measure. So we could just use the identity mapping, but that's not so great because whenever you have noise, you're going to fail in decoding. So the other important property that we want from code is what's called as minimal distance, which basically says that if you look at any two code words, they have a large hamming distance between them. So this is saying that if you look at the minimal hamming distance between any two distinct code words, it has to be sort of, say, a constant fraction of the code or whatever that fraction is. That is called the minimal distance of the code. And here we really want code words to be as far as possible from one another so that we are able to decode.
00:39:51.416 - 00:40:19.536, Speaker A: So we'd like also our distance to be as close as possible, or our relative distance, this fractional distance, to be as close as possible to one. Does that make sense? Okay, the slight unfortunate thing is that we can't have both. So there's a singleton bound which basically says that their sum is at most one. So you can't have your cake and eat it too. You can't have the rate be close to one and the distance close to one. Good. So some extra properties that we would like to have and typically have of codes.
00:40:19.536 - 00:41:07.860, Speaker A: One is that our codes are linear, which just means that the sigma here is a field and you should view the transformation as a linear transformation over this field. Okay. The other is codes that are systematic, which is just a fancy way of saying that if you look at the first case symbols of the output, you see exactly the message. So when you encode a message, you first see the message itself and then some extra redundancy bits or redundancy symbols. And all of the codes that we'll talk about throughout this work, throughout this talk, are going to be linear and systematic. So an example of a code, everyone's favorite code, the Reed Solomon code, basically looking at univariate polynomials, has very nice parameters. One downside is that it requires a large alphabet, but it has other very nice properties.
00:41:07.860 - 00:41:35.388, Speaker A: The reason that it has distance is that if you take any two univariate polynomials, they can only agree on a small number of points, so they have distance. Great. So these are error correcting codes. We're going to be also looking at two specific families or types of error correcting codes. One, which is really fundamental and important in construction of proof system, is what's called multiplication code. So these are codes that have the following serious property. So we're going to be looking at a linear code E.
00:41:35.388 - 00:42:18.460, Speaker A: So mapping vectors of length k to vectors of length n, and I want the property I switched between E and C for this code. So all the C's should be E. Sorry about that. But the property says that if you take any two code words and you multiply them pointwise, so you take two vectors and do pointwise multiplication, the result is another vector of length M. And what I want is that this vector live in some related code word sorry, in some related code. Okay. So anyone have an idea of an example for such a code hint we have seen so far? One code.
00:42:18.460 - 00:42:49.406, Speaker A: Okay, so read Solomon code, right? So if you look at two code words in the read Solomon code, they're going to be univaried polynomials. If you take their pointwise product, the result is itself also going to be somewhat larger degree polynomial, but still polynomial. As long as the degrees are small enough you have space. So it's going to be yet again polynomial. So it belongs to some other related Reed Solomon code of somewhat larger degree. Yeah. So the canonical example would be Reed Solomon.
00:42:49.406 - 00:43:29.840, Speaker A: It's also true for Reed Mueller multivariate polynomials, true for AG codes. These are more or less all the examples. More or less. This is an extremely important property. And I would say in a nutshell, the reason that it's important in the construction of proof systems is this is what lets us handle multiplications that are going on in our circuit. Without this property, we would not be able to do that. The last family of codes that I want to introduce are what are called tensor codes, which is really a way of taking codes, taking a code and producing from it a new code and getting some really amazing properties along the way.
00:43:29.840 - 00:44:21.534, Speaker A: So the way I describe it here is going to be a tensor of two different codes. So I have code e one mapping k one, vectors of a length k one to n one, and code two mapping vectors of a length k two to n two. And this tensor operation is going to produce a new code which maps vectors of length k one times k two into vectors of length n one times n two. Okay, how does this operation work? So I'm going to be viewing, notice that the messages here are vectors of length k one times k two. I will view them as matrices of dimensions k one by k two. And the way that you do the encoding is that you first go along and encode each one of the columns using the code e one. And then you go and encode each one of the rows, both original rows and the new rows using the code e two.
00:44:21.534 - 00:45:02.470, Speaker A: What you get is indeed a matrix of dimension n one times n two, which I view as a vector of length n one times n two. So that is the tensor of two codes. Questions about this. So you might wonder, why did I first encode the columns and then the rows, not vice versa? The answer is that it doesn't matter. But it doesn't matter because actually I would get the exact same code word sort of from basically in your algebra. So it doesn't matter in which way you do the encoding. Okay, so let's see some properties that we get when doing this tensor encoding, which is also incredibly useful.
00:45:02.470 - 00:45:36.990, Speaker A: So first of all, in terms of the rate of the code, notice that the rate of this new code, the message is of size k one times k two. The size of the code word is n one times n two. So basically means that the rate of this new code is the product of the two rates of the original codes. So we are losing compared to each one of the original codes, but potentially not by too much. So that's in terms of the rate of this tensor code, the second property is what happens to the distance. Not a hard exercise to see that the same behavior also happens to the minimal distance. And these are all relative distances.
00:45:36.990 - 00:46:22.882, Speaker A: So if we have distance, this distance for the first code, this distance for the second code, the new distance will be the product of the two distances. Lastly, an important and very useful fact, the technical term is that tensor codes are locally testable and locally correctable. Basically, I won't define what these notions mean. What it basically means that when I'm designing an IOP and I instruct a prover to send over a code word that is supposed to belong to the tensor code, an honest prover will do that. A cheating prover can do whatever it wants. These properties say that if it's going to send something that is not a proper code word, I will catch it. So there is no point in sending when I instruct approver to send a tensor code word, there is no point in trying to send something else.
00:46:22.882 - 00:46:32.520, Speaker A: If it will, I will catch it. Makes sense. So that is not a homework exercise. That's more complicated. But tensor codes have this property.
00:46:33.930 - 00:46:36.840, Speaker B: There's not any simple intuition for fact three.
00:46:37.210 - 00:46:59.790, Speaker A: I can tell you what yeah. Let me say briefly about this. So suppose that you take proofer, is instructed to send over a tensor code word, and it doesn't want to do it because it wants to violate what I'm promising. It has two options. Either send something that is very close to a tensor code word or something that is very far. Let's say for a second that it sends something very far. So it sends some total junk.
00:46:59.790 - 00:47:42.140, Speaker A: Now, you could think of the code that I described here. If you look at the rows, they all belong should all belong to the code e two. And the columns, because I said you can reverse the order of stuff. All the columns should belong to E one. So a natural test would be if the proverb made a lot of corruptions, choose a random row or column and check that they belong to the corresponding row or the corresponding code. And that way you could catch a prover that makes a lot of corruptions. What about prover that makes a small number of corruptions? That you have a mechanism, then it's really very close to a tensor code word, and you have a mechanism to sort of read off symbols from the underlying tensor code word that is there.
00:47:43.150 - 00:47:52.842, Speaker B: Close versus far is just like can you actually would your decoding algorithm work to get you back to is that kind of it like you're within the distance.
00:47:52.986 - 00:48:09.334, Speaker A: Yeah. So there is some threshold where from there on, if you chose a random row or column, you would catch it with good probability and at the same threshold. And below you would just be able to read off points from the actual you have a mechanism for just reading off from the correct code word.
00:48:09.372 - 00:48:12.898, Speaker B: Like, in particular, you're within the distance of you're within that distance.
00:48:13.074 - 00:48:35.034, Speaker A: Yes. So that it's unique or half the distance even. So that's unique. Good. So that is what I needed from in terms of describing error correcting code. The last component that I'm going to need in this talk is something called the Sumcheck protocol. Has someone not seen it? Okay, so Sumcheck is typically described as a protocol involving polynomials.
00:48:35.034 - 00:49:15.420, Speaker A: I want to describe it more abstractly as a protocol for tensor codes. But let me first say, what is the goal of sumcheck? So the goal in sumchek is the following. Think of it as a protocol in which you have a verifier which is given Oracle access to some code word of e of this code e. And its goal is to compute the sum of the entries of the message symbols or bits inside this code word. Of course, it can just do this by reading them off and counting, adding stuff up. But the goal is to do it much more efficiently than K. So we want something that is maybe polylogarithmic in K.
00:49:15.420 - 00:49:45.118, Speaker A: So just doing the sum. So sum check, just want to be able to compute the sum of the message bits much more efficiently than just reading them off such sum check protocols. And we'll show this, I think, in the next slide, are known for all tensor codes. So the one that you're probably familiar with is a specific instantiation for multivariate polynomials, which are a special case of tensor codes. And really, the tensor structure is what underlies sumchek there. It's also known for read Solomon codes in the IOP setting. SS UNIC.
00:49:45.118 - 00:50:30.660, Speaker A: Okay, so Nick and others and, you know, I cannot say enough about this protocol. It's really at the heart of very fundamental results, I would say, both in theory and in practice. In theory, IP equals P space, MIP equals next, some of the most important results in complexity theory. And this new work of Justin's, I think, also heavily relies on sumchek, and a lot of his previous works relied on sumcheck. So a huge fan, I think, Justin as well, very important protocol. And sort of the reason I would say is that it allows you an exponential saving on a very basic task. So all you need to do is sort of reduce other tasks that you care about to this pretty rich class of computing sums, and that's enough.
00:50:30.660 - 00:51:14.014, Speaker A: Okay? So again, the goal and sum check is very simple. You have a code word, it's in the sky. You can read off individual bits, and you want to compute the entire sum of the message bits. So let's see how this is done in general for tensor code. Okay, so this is a picture that I have in mind. I have a tensor code, tensor code word, right? And what I care about is computing the blue part of it, right? So in terms of because I've gone from messages that were vectors or code words that were vectors to matrices in matrix notation, it's a sum over all IJ of CIJ. So this blue part, the sum of the blue part in terms of notation here, the blue part is K by K.
00:51:14.014 - 00:51:41.578, Speaker A: The overall thing is N by N. So the naive protocol would have complexity K squared. Just to read off the blue part, I'm going to show you how to do it with complexity k, give or take. So this is how it works. So the prover computes. It takes the sum, takes all of these, the first three columns and adds them up. Okay? So each CJ here is the column C one, C, two C, three adds them up.
00:51:41.578 - 00:52:14.080, Speaker A: Let's call that the resulting vector. Let's call it Call. Notice that this vector, I claim that it has to belong to our column code. Do you see why? So, because this is a tensor code word, I know that each one of these individual vectors belongs to the column code. And I've added them up and we said that it's a linear code. So when you add stuff up, it should still reside within the linear code. So this vector call has to reside within the linear code.
00:52:14.080 - 00:52:34.740, Speaker A: So we send this over. Notice that that is only like N communication versus K squared. So we're pretty happy. What does the verifier do? First of all, it checks. So I put a tilde here because maybe the prover cheated. It was supposed to send call, but it actually sent call tilde. So first thing that you can check is that indeed, call tilde is a valid code word.
00:52:34.740 - 00:53:16.932, Speaker A: You read it off entirely. Is it a valid code word. Second, you can check if I look at call, which was the sum of all of these things, if I add up the three first coordinates of this vector call, what should I see? So the first coordinate of call is the sum of these three guys. The second coordinate of call is the sum of these three guys, and the third is the sum of these three guys. If I add all these up together, what should I see? The overall sum of the blue part. Right. So I don't where do I have it? Okay, so I don't have it here.
00:53:16.932 - 00:54:00.224, Speaker A: But before the additional test that I'm going to do shortly, if I look at the sum of the first three entries of call, I'm supposed to see sort of the eventual result, right? What am I worried about at this point? That the proverb cheated. It sent call tilde, which was a valid code word. So it passed my test. But the values that are there are just junk nonsense. So I want the way to check consistency that this call tilde was indeed computed correctly from C. So here's what I'm going to do. Maybe the main observation is that if call tilde is different from call, then we have two code words of this code, e one, I guess the column code.
00:54:00.224 - 00:54:41.296, Speaker A: There are different code words, and we know that different code words have to disagree on a lot of places because there's distance. So if I choose a random location, I should see this difference occurring. So that's what I'm going to do. So the Verifier chooses a random index I star, and sort of it compares call tilde. In position is star the value that I should see there is the sum of the istarth row. So I just read off the istarth row from C and see that it's consistent with what I saw here. So again, the prover sent over this sort of partial sum of the overall sum that I care about.
00:54:41.296 - 00:55:02.096, Speaker A: I check that it's a valid code word, and I can check that it's consistent with a real deal. It suffices to check on a random location because the codes have distance. Okay. So that is why, in general, tensor codes support sum check. Good. So that's all the background that we need, and we can go ahead and construct IOPS. So from here on, it's going to be pretty simple, I think.
00:55:02.096 - 00:55:26.680, Speaker A: So we've had all this background. Let's come back again. We're trying to construct IOPS. For the moment, we're not caring too much about efficiency, just how are IOPS boring. So we care about Boolean circuits. So for simplicity, let's think of our Boolean circuits as fan into NAND gates. So all the gates are fan and two NAND gates, and I'm going to assume some topological ordering of the wires.
00:55:26.680 - 00:56:31.750, Speaker A: If we look at some input X for the circuit or input X and the corresponding witness W, I want to define capital W to be the values that all of the wires in the circuit obtain, sorted according to the topological ordering. I'll define WL as the same thing, where instead of when I hit index I or gate i, instead of wire i, gate i, instead of taking the value of the gate, I take the value of its left input, and similarly, WR will be the value of the right input. So once you have a circuit and you fix an input, you can define all of these things. Just something to notice. Is that really WL? I think of it as W, as sort of a permutation of W based on for every gate who is its left or right input. It doesn't necessarily have to be permutations, but for simplicity, let's think of it as though they are permutations. Okay? Any questions so far? Feel like I may have lost some of you.
00:56:31.750 - 00:57:02.684, Speaker A: Yeah. Back to the previous slides, please. So in a sum check, do you need the Verifier to have an Oracle access to the yes, yes. So I'm thinking of this matrix as living in the sky. I need Oracle access to the matrix the way I described it. Here, I'm going to read off endpoints from this matrix the entire istarth row. With a little bit of more work, you could make sure that actually all the Verifier needs to do is read one point from this code word.
00:57:02.684 - 00:57:24.852, Speaker A: Really? Instead of reading the entire blue part, one point is going to be enough. Turned out to be very useful. Okay, so we're back to Boolean circuits. We have our WL and WR. We are trying to construct an IOP. So our IOP is. Going to use all of the guns that were presented in act two are going to appear in act three.
00:57:24.852 - 00:58:11.308, Speaker A: So we're going to use code E. Again, linear and systematic mapping going from K to N. I want to assume that it's a multiplication code. I want to assume that the product of the multiplication code they call this estar, supports sumcheck. And I want to assume that I have this sort of local testing and self correction procedures that we set tensor codes have, so that I can assume that whenever I tell the prover to send over code word, it actually does good. So I have this code, here's how I'm going to construct the IOP. So given an input x and so I guess input x and a witness w the prover sends it constructs W, which is the value of all the wires, encodes them under E, call the resulting thing w hat sending it over to the Verifier.
00:58:11.308 - 00:58:34.884, Speaker A: Remember, we're in the IOP model, it's okay to send long messages. So we sent basically all the values that the wires obtained encoded under a net correcting code. Same thing for WL, same thing for WR. So far so good. So that's what the Verifier does. That's the long message that I was alluding to in my answer to Tim before. So one long message followed by shorter interaction.
00:58:34.884 - 00:58:54.290, Speaker A: So what does our Verifier need to check now? First thing, I need to check these things that were sent. I only have Oracle access to them. I can only query a few bits. I want to check that they are indeed valid code words. How do I do this? Well, I've kind of assumed it away. This is what exactly the local testing and checking procedures allow me to do. So this is kind of for free once I have this property.
00:58:54.290 - 00:59:31.432, Speaker A: So again, it means that even a cheating prover, when it sends stuff, it has to send valid code words. Second thing that I want to check is that WL was computed sort of correctly according to the correct permutation from W and similarly for WR. So I call that a permutation test. Second thing I want to test third thing is that the NAND gates were computed correctly. So if I look at every index I wi is actually equal to the NAND of WL in position I and WR in position I. So that's what this expression is about. So this is the NAND relation.
00:59:31.432 - 01:00:26.444, Speaker A: And lastly, I need to care to check pretty good idea to check that the output of the circuit is one, meaning that it was accepting, and to check that the input is indeed x. Let's ignore those tests for today, okay? Questions? Is it clear that if I'm able to check all of these things, then I should be pretty happy? PL and VR comes from the preprocessing, so so far I've only defined them. I haven't used them. But once the circuit is defined that determines pi l and pi R. You're asking about pi in step two, these two permutations PL and yeah, so I want to check them. Once the circuit is fixed, they are determined. I will have to do stuff in the preprocessing that will facilitate this test, but not like the way that this is implemented.
01:00:26.444 - 01:00:49.076, Speaker A: Will use preprocessing. Right. Because these are things that fully these are what determine the circuit. So once you have a circuit, they are fully defined and vice versa. Makes sense. Okay, yeah, maybe I misunderstood something, but.
01:00:49.178 - 01:00:53.510, Speaker B: Shouldn'T WLN WR they're like strict subsets of W. Right.
01:00:54.460 - 01:01:07.596, Speaker A: So it's true that input gates don't have a left. Is that why you're thinking of, like, input gates don't have an incoming stuff and so on? Well, I'm thinking W is all the gates in the circuit. Think of W as being sure all.
01:01:07.618 - 01:01:11.176, Speaker B: The gates, not the wires. Yeah, that's what's confusing.
01:01:11.288 - 01:01:57.420, Speaker A: Yeah. So step one, we've sort of assumed a way I want to explain how we do two and three using the properties that we have of our code. So first of all, in terms of the permutation test, so the situation that we have is that we have two different or not different code words, and we want to check that they're sort of the way that they are computed is a permutation on their index sets. So W in position pi I is exactly equal to W prime in position I for every i. And the way we'll do this is as follows. Let's think of choosing a random vector, R. And I want to claim that if this property holds, it certainly must be the fact that if I take sort of an inner product of the W composed with pi vector with R, it should be the same thing as the inner product of R with the W prime vector.
01:01:57.420 - 01:02:43.128, Speaker A: If point wise, all these values agree, then I do this sum. It should still be true. But sort of a fundamental property I think most of you probably familiar with is that if these disagree on at least one point, then the probability that this thing happens is at most one half. If we're talking about bits, if you're talking about a field, will be one over the field size. So if the thing that I'm trying to check is false, with at least constant probability here, this reduction will produce something in which these two values are not the same. Let's assume that event happened. Now what do I do? Well, now I will do just a reordering of indices on the left hand side.
01:02:43.128 - 01:03:16.390, Speaker A: So whenever I saw pi i, I will just call it J. Okay, here it's convenient that pi is a permutation, so I can just do it this way. So I'm now summing over. I've shifted over this pi to pi inverse of R, and I basically reduced my permutation test to checking that this equality holds. Okay. And now I'm pretty happy because both of these, both left and right hand terms are involving a sum, and we have a way to check sums via the sum check protocol. Right.
01:03:16.390 - 01:04:08.916, Speaker A: The fundamental thing though is, and this is really one place in which multiplication codes appear, is that W? You can think of it as we have W Prime is a vector, is a code word in our code e. R is under our control, so I can think of encoding it. It would also have it be a vector under my code e. So overall, I have access to these two vectors, two code words, I can multiply them, I get a code word from the product code and I can run some check on that that will let me compute this value. Similarly, here, if W belongs to my code word, to my code e, I compute then coding of this permuted value of R under the code e, multiply them. This is sort of in my head and I'm not actually doing it because I don't want to read stuff off. I run sumchuk on the resulting thing.
01:04:08.916 - 01:04:51.990, Speaker A: I can compute this sum over here. If they disagree, I will catch it. I didn't understand it very well, but are the W and W Prime also code words? And then are you saying that I choose Ri to be a code word as well? So w and w prime are code words, right? For us, from the previous slide, they were the encoding of the values. So I changed notation a bit, which is a bad idea, but W was think of it as W hat, and the W Prime is like W hat l. Okay, what I care is about what happens in their message bits, whether the message is encoded within or permutations of one another. So this is what's being described over here. So I have access.
01:04:51.990 - 01:05:25.292, Speaker A: I'm thinking of Oracle access. It's like the proverb already sent in its first message, both of these code words, so I can query individual bits of them. So I already have them for R. I chose R myself, so I can in preprocessing compute an encoding of R. Okay? And similarly, I can compute an encoding of R permuted according to the permutation, which means that I have access to these two code words. I can imagine multiplying them and running some check on that. Okay.
01:05:25.292 - 01:06:19.472, Speaker A: My worry is like, if R is a code word, how do we ensure that it's like sufficiently random? Sorry, so R is a random vector in k, totally random vector in k, and I encode that vector. Okay, I see, so the message is random. You can also choose it to be pseudorandom, but let's think of fully random and I encode that and same vector R, I permute and encode that. So I can apply sum check on these two sides of the equation and check that the equation actually holds. So that's a permutation test and as you stated, computing these encodings, especially of this guy, which fully depends on the circuit I can do in a preprocessing step. It doesn't depend on the input to the circuit, only on the topology of the circuit, where wires go to. Okay.
01:06:19.472 - 01:06:23.532, Speaker A: Yeah. Can you reuse the randomness? If you cannot reuse it, it's meaningless.
01:06:23.596 - 01:06:24.016, Speaker B: Right.
01:06:24.118 - 01:06:50.648, Speaker A: How do you have to do this for first circuit input? Okay, so it's true. You cannot reuse this randomness. So this preprocessing, it's a good point, which I should have mentioned. The preprocessing in the first result is not reusable. You can do it once, you cannot reuse it again. So it's a one time thing, because you use this randomness once you do it offline, then you do it online. You should not reuse it.
01:06:50.648 - 01:07:13.404, Speaker A: So that is, you could say, a caveat of the first result. In the second result, there is no preprocessing, so there's nothing to worry about. But a good point that I forgot to mention. Good. So we took care of the permutation test. The second test that we had was to check this NAND relation. So situation that we have is that we have three code words w, W prime and W prime.
01:07:13.404 - 01:07:39.144, Speaker A: And we want to check that this pointwise relation holds between the message bits. Right. So we're going to do the exact same trick. So we choose a random vector R. We compare. If this should hold point wise, then these sums should hold. And again, we're in the exact same situation where we can run sum check on the left hand side and separately on the right hand side and see that we got the same values here.
01:07:39.144 - 01:08:07.980, Speaker A: There's no need to do any permutation. There's no permutation. Just like I just need an encoding of the vector R. What have we seen? So if we have a code E, that's a multiplication code, so that the product code supports some check and that it has these local testing and self correction procedures, we can run all these tests and we have ourselves an IOP. In particular, if I take E to be a tensor product of a Read Solomon code. So read. Solomon is multiplicative.
01:08:07.980 - 01:09:09.652, Speaker A: When you do tensoring, it maintains a multiplicative property and it magically gives us some check and local testing. So a tensor product of Read Solomon code would have all the three properties that we wanted. If you do that, you'd get yourself in very nice IOP that would have been the state of the art in the early 90s. Okay, but that's all you need sort of, sort of really isolates what are the properties that you need from error correcting codes in order to get IOPS? Actually, if you took E, the code E to be a Reed Solomon code univire polynomials, then it would have the multiplication property, and due to work of Nick and others, it would also have a different sum check protocol and a separate low degree test. Something called Fry, and that is really the technology underlying stark's IOP or business. So that sort of concludes the review of how typically succinct arguments are constructed. I want to show you sort of took a while.
01:09:09.652 - 01:09:57.700, Speaker A: I want to show you sort of some of the very quickly the new idea, this code switching idea that I hinted on, and we'll do the rest on Thursday. So how do we do construct fast IOPS, not just succinct. So the key bottleneck in what I've described so far is that we had to send this encoding of the wire values. And the way I described it so far, we're sending the encoding under multiplication code. And the unfortunate thing is that the best multiplication codes that we have are essentially read Solomon code for which encoding it's via an FFT and it incurs a logarithmic overhead. So if you want to follow the template that I described above, you're encoding all of the wire values using a multiplication code, you're kind of stuck. I mean, if you had a way to speed up FFT, wow, but we don't.
01:09:57.700 - 01:10:38.652, Speaker A: So not clear what to do. And this is sort of the main bottleneck in everything I described so far because, for example, some check we know how to do with a linear size, linear time or linear size prover. So really the fundamental bottleneck is just doing the encoding. And the new idea that I want to describe is a technique that we call code switching, which it sounds weird, but I'll describe in a second how it works. The idea would be to encode under a code which is not a multiplication code. It's very efficient, but nevertheless be able to later on switch to a multiplication code, but without actually doing the encoding. Okay, so what do I mean by that? So the idea is the following.
01:10:38.652 - 01:11:23.664, Speaker A: We're going to have the proverb send the encoding under, so we have our code E, which is the multiplication code, and I'm not going to send the encoding of the wire values under that code, I'm going to encode it under a different code which I call C. So bring me your fastest code that you have. I want to use that code. So fine, I sent that encoding. The Verifier is somewhat sad because it liked getting encodings under E and it got encoding under C. It's going to entirely ignore that and pretend as though the encoding that was sent was encoding under the multiplication code E and follow the protocol. At some point in the protocol, sadly enough, the Verifier needs to access bits of E, which it doesn't have.
01:11:23.664 - 01:11:48.008, Speaker A: It only has bits of C. The challenge that we have is that we want to query a few points of, say, E of W. But what we have in our hand or in the sky that was sent as an IOP message is CFW. So that's the challenge. That seems weird. How do we overcome this and we're going to do this by via additional interaction with Approver. Okay.
01:11:48.008 - 01:12:42.436, Speaker A: So we're going to design a special code c, so that using additional interaction, even though I have in the sky, I have Oracle access to CFW, I am able, using that, an additional interaction to check claims about E of W. So I've managed to use one code to do then coding. I've pretended that I actually use a different code, but later on using this, I can check consistency. So what we want is a code e, so that on one hand, it's fastest code. So linear time encodable, good, constants, and so on. I want it to still be locally testable and correctable so that I still have the properties when the thing is sent, I can check it. And I want that given Oracle access to e of x or e of W in the previous slide, there is an additional IOP that lets you check values.
01:12:42.436 - 01:13:15.776, Speaker A: Like, is c of x in position I equal to some value. So far, so good. So I need to design the code E like this. And the solution is going to be any tensor of a linear time encodable code is going to work. And I'll show you this. Okay, so why does a tensor of a linear time encodable code work? So think of a square tensor, square root n by square root n. So, first of all, I claim that if I take a linear time encodable code and I tensor it, the resulting code is still going to be linear time encodable.
01:13:15.776 - 01:13:52.610, Speaker A: Do you see why you just go over the columns encode each one in linear time? And because it's linear time, it's also going to be constant rate. Sorry, you went over all the rows and coded them. Now you go over all the columns. Overall, it's going to be linear time. So this preserves tensoring preserves, then coding time. At least here we said already before we had these results showing that in general, tensor codes are locally testable and correctable, which again, is this thing about when I instruct the proverb to send the tensor code word, it will send the tensor code word. So all that remains is this tricky property.
01:13:52.610 - 01:14:05.350, Speaker A: Right. So given Oracle access to encoding of x, I want to be able to compute C of x in some location i. So how do we do that?
01:14:08.360 - 01:14:17.224, Speaker B: Going back to the what was it you wanted? Multiplicative but also linear time encodable. Right. Like if you had that, you wouldn't have to go through these hoops, right?
01:14:17.262 - 01:14:17.464, Speaker A: Yes.
01:14:17.502 - 01:14:22.136, Speaker B: Do you think there's a fundamental barrier there, or do you actually need like a trick like the one you're doing?
01:14:22.238 - 01:15:08.010, Speaker A: It's very unclear because you would think that if there was a fundamental barrier, it would have to do with the rate of the code. But we do have constant rate multiplication codes, so if there is a bottleneck, it has to be about efficient computation, which we are very bad at reasoning about. So I really don't know what the answer is. It would be, I think, a huge breakthrough in various different fields to construct multiplication codes that are better than FFT, and it will solve all of these problems in particular. So what we want is to be able what we have in the sky is an encoding of E of W. But we want to be able to sort of compute this. And the first observation that we have is that this nice thing is just a linear function of W.
01:15:08.010 - 01:15:44.372, Speaker A: Why? Because C is a linear code. So it means that if I look at C of W in position I, it's some linear combination with some corresponding coefficients of the values of W. Right? Because it's a linear code. This claim that I have is a linear claim that's pretty nice. So in particular, as a warm up, let's hide these coefficients. So if I hide the coefficients, my goal now is to compute the sum of WJ. Given oracle access to E of the or rather the tensor of W time encodable code.
01:15:44.372 - 01:16:21.124, Speaker A: It's the code that I'm using. And my goal is to compute the sum of the message inside. Any idea of how to do that? So what we have is oracle access to a code that happens to be a tensor code word, and we're interested in computing the sum of the message inside. So half an hour ago, we saw that tensor codes have the sumcheck procedure, sumcheck protocol. So sumchek is exactly doing this right. It says any tensor code word, not just polynomials. Also, if you take a tensor of linear time encodable, your favorite linear time encodable code, it still supports sumcheck.
01:16:21.124 - 01:17:01.608, Speaker A: So we are able to compute this thing, the sum of all the message parts. So we can do this right. The thing is that this hand is not really there. So for the warm up, just using the fact that E is a tensor code is enough. But in reality, we do have these annoying coefficients. So the question, can you do sort of a lin check, a check of when there are coefficients around for any tensor code? And when you first look at this, you start getting feeling a little queasy because the way that this is sort of typically handled is you just say, oh, I have my vector w that I care about. I have the vector of coefficients.
01:17:01.608 - 01:17:34.980, Speaker A: Let's just encode these coefficients and use the fact that we have a multiplication code to multiply the two code words together. So once you have a multiplication code, then dealing with coefficients is sort of automatic. But our entire point is that we don't have multiplication codes. So at this point, when we developed this, I was very worried, okay? And in particular in general, I don't know how to do this. What I will show you is how to deal with a specific structural coefficients. So if coefficients are nice enough, then I'll show you that we can do this. And luckily enough, that suffices.
01:17:34.980 - 01:18:12.432, Speaker A: Okay, so what do I mean by the coefficients are nice enough? So recall, this is the structure that I have and talk about the tensor code word. And now I have coefficients, I'm not just doing the sum. So suppose that miraculously, the coefficients have the following structure. So if I look at the coefficient for coordinates I j, it's actually a product of something that depends only on I and something that depends only on j. Okay? So there is sort of a fixed vector lambda so that each lambda I j is sort of the product of lambda I times lambda j. Suppose we had this nice property. Then we're in business, I claim.
01:18:12.432 - 01:18:47.592, Speaker A: Why? Basically because the same sumchuk protocol that I showed you before as is almost basically works. All we'll need to do is adjust. When we're doing this sum over the columns, I'll just adjust according to sort of the column coefficients, lambda j. So I sent over that. So I've just taken into account the coefficients corresponding to each column. And now when I do this test is a test checking that at a random locations call I agrees with the IThrow. So when I compute the IThrow, I should adjust with the corresponding coefficients.
01:18:47.592 - 01:19:50.630, Speaker A: And when I compute the sum of the entries of call tilde, I should also adjust with the corresponding coefficients. So as long as my coefficients have this nice structure, then I'm in business. So if I think of this lambda I j as a matrix of coefficients, do you remember what this kind of matrix is called if it has this property? So it's a rank one matrix, right? It's an outer product of two vectors. So it has to be a rank one matrix. And what this is it's on the title. So what this is telling you that actually if the coefficients, the coefficient matrix happens to be rank one, then we can do this for any tensor code. The next observation is that our code c, which was the multiplication code, the code c that we were using throughout the protocol, the one that we pretended had been sent, if we choose that code to be tensor code, turns out that for tensor codes, their coefficient structure has rank one.
01:19:50.630 - 01:20:48.432, Speaker A: It has to do with tensors of vectors leading to rank one tensor codes for the same reason. So indeed, the coefficients that we have will have the right property, will have the right structure. So as long as we choose our code c, under which we are doing all of the machinery, all the sum checks and so on, to be a tensor of a multiplication code, and we choose the code e to be a tensor of a linear time encodable code, then we're golden. So somehow the point is that the multiplicative property is something that we only need for the code C. And whereas the claim that we have sort of of going from the claim about C to E is a linear claim, which is why E doesn't have to be a multiplication code. Okay? So sort of bird's eye view of what the protocol looks like. So the IOP, so the prover sends over an encoding under the fast code of the wire values and their shifts.
01:20:48.432 - 01:21:57.512, Speaker A: You can actually get rid of the shift, but let's ignore that. So basically sends over the encoding of the wire values. Verifier chooses the random vector R, and now that reduces to you sort of pretend that you have what was sent here was C. You run all the machinery that we saw before relating to C, you get claims about CFW, CWR, CWL, and now you run sumcheck on E to verify these claims relative to these values. So there are two sort of phases to this or three phases to this protocol. Okay? One thing I'll briefly mention nearly out of time is that if we were to replace the linear time encodable code with a high rate code, meaning that a code that has rate approaching one, we would actually get an IOP in which the communication is nearly, nearly, nearly equal to the size of the circuit. And actually using for nice enough NP relations, which include probably any NP relation that you have in mind, we can even get the communication in the IOP to be just 1.1
01:21:57.512 - 01:22:31.600, Speaker A: times the witness length. So if you compare to the naive proof of sending over the witness to a proof in which you need to read only a small number of bits, you only need to increase it by a 1.1 factor. Okay? And using this recently in last crypto, together with my student Shafik, we construct the shortest known zero knowledge proofs. So not arguments, proofs of statistical soundness based on one way functions. The point of the slide is that this machinery is very flexible. It lets you sort of distill different properties that you need from your codes in different places.
01:22:31.600 - 01:23:09.440, Speaker A: So you can use a high rate or fast code to do then coding and then use a separate code for which you don't need to pay for encoding to do sort of the proving. Okay? So just to summarize, this code set switching techniques, as I said, lets you sort of distill different aspects of the code. So we don't need one code that has everything which turns out to be problematic. In fact, you can separate it is very beneficial. It's a very simple technique, right? All we did is sum check, but it easily yields, as I described, linear size proving for arithmetic circuits over large fields. If you're talking about small fields, I'll talk about that on Thursday. Then sort of sum check starts breaking down.
01:23:09.440 - 01:23:33.850, Speaker A: So we'll need some mechanism to compensate for that that I'll do on Thursday. So the mechanism, because we're using sumcheck, this stuff works over large fields. On Thursday, I'll describe what we do over small fields, both with the constant soundness and with the small exponentially vanishing soundness and the small overhead for the prover. Okay, so I think I'll stop here and happy to take any more questions.
01:23:34.700 - 01:23:48.770, Speaker B: Maybe a question for Justin as much as it is for you. But the arithmetic versus Boolean distinction, like to what extent does working sort of optimizing for Boolean circuits interfere with trying to get say, practical snarks or something like that?
01:23:49.220 - 01:24:32.430, Speaker A: Okay, so Justin will have a better answer for this than me, but let me start before letting him complete there's. I do think it's a problem, but it's a problem that people are working on using different approaches which don't necessarily focus on asymptotics, but focus on concrete efficiency. So things like Justin's lookup arguments and so on are dealing with this kind of thing. So I think it is a problem, but there is other machinery as well to deal with it which maybe has different asymptotics or different behavior. So certainly a concern, but concern that people are interested in. But I'll hand it off to Justin. Yeah, I'd say it creates more issues that aren't necessarily present.
01:24:32.430 - 01:25:20.556, Speaker A: If you are happy to work over large fields and you're happy for your Prover to run on like a random access machine instead of as a Boolean circuit, you might be dealing with things that are maybe unnecessary in practice to deal with, but it's also a good way to force you to develop new ideas. And a lot of these ideas have already gone into things like breakdown and a bunch of follow up work. Let me maybe follow up on your question or ask. So there are two things going on. There is the fact that the input circuit is Boolean and the fact that I'm trying to implement the Prover as a Boolean circuit. And I was sort of addressing the fact that whether it's relevant in practice that the input is expressed so that the computation you care about is expressed as a Boolean circuit. So that is one aspect and I think that is interesting.
01:25:20.556 - 01:26:09.788, Speaker A: People are trying to use to invent new hash functions that are Arithmetization friendly and so on because they want to work with these big fields and so on. If they were just treating their input as a Boolean circuit and had good machinery for doing so, they would just use Sha. So that is why I think it is useful to be able to construct protocols that handle Boolean circuits. I think that's important. Then comes the question that I think Justin answered, which is why is it important for the Prover to be implemented as a Boolean? Then I think Justin's answer is the right one. Just as a context. Is this a precursor to some of these so called practical linear time prover snarks like Orion, et cetera? Yes, they all rely on.
01:26:09.788 - 01:26:35.728, Speaker A: Code switching. Can you name the codes that you use? Those are not here. Where was it? So here, I just need C to be a tensor of a multiplication code. So I take Reed Solomon and I Tensor. It So basically be Reed Mueller. So multivariate low degree polynomials here. I want a tensor of a linear time encodable code.
01:26:35.728 - 01:27:11.912, Speaker A: So there is a code by Dan Spielman from the 90s, which is linear time encodable. If you tensor that, E would be good enough. And that's the code that underlies Breakdown and Orion as well, I believe. But I think there is so it's very flexible. So give me if someone develops a faster code and I think there are good candidates, you can use those. So the bottleneck becomes it goes now we can go to the coding theorists and ask them, give us faster code. And I think coding theorists, you would think this is like super basic questions for coding theorists, but I don't think people encoding were thinking about encoding messages of size two to the 32 to the 40.
01:27:11.912 - 01:27:38.610, Speaker A: So it was never they had nice asymptotics I don't think it was a concern for coding theorists. Now I think it is a huge one, or should be. Are there any places where this code switching is done with more than two codes? So some like, chain of yes, on Thursday, it was not planted. Question okay, cool.
