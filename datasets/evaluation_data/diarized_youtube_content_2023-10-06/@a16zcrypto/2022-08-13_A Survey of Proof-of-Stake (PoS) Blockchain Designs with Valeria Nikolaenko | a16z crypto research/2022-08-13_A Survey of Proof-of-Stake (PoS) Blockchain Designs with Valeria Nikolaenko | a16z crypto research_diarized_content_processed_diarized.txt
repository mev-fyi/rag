00:00:06.330 - 00:00:06.880, Speaker A: You.
00:00:08.850 - 00:00:30.870, Speaker B: All right, everyone, welcome to today's research seminar here at a 16 Z Crypto Research. Very excited for today's talk that's by our very own Lara Nicolenko. She's going to give a two part survey about proof of stake blockchain. So the first one is now, and then the other one will be Thursday at the same time. So at 11:00 a.m. Eastern. So definitely check that out as well.
00:00:30.870 - 00:00:39.350, Speaker B: And very excited to hear about this. On the one hand, very central topic in web three. On the other hand, I think still fairly poorly understood.
00:00:40.650 - 00:01:17.550, Speaker A: Fantastic. Thank you for this introduction. So this talk here will be a survey of proof of stake blockchains. The main purpose of the stock will be to map the landscape, to kind of get the view of the land so that you know how it all looks like and we'll be able to map new consensus protocols into this landscape. This is a very vast topic and can easily substitute a semester course. So I will be touching the tips here, but hopefully you will get a general sense. So I was trying to balance the talk between academic papers, white papers, talk, blog posts, to make it a little bit more lively and interesting and less academic.
00:01:17.550 - 00:02:09.282, Speaker A: But of course, if you're interested in specifics of any protocols that I'm going to describe, please look at the papers for more rigorous analysis of why this works. So this tutorial will also help understand somewhat a little bit a deep dive that I will give on Thursday about mitigating the long range attacks for proof of stake blockchains. So this is the outline. The talk will be subdivided into a few topics that are somewhat independent. So if you lose me on one, you can always catch up on the other. Roughly, I'll start with some pathway into the proof of stake by comparing it to proof of work and sort of motivating why we want to move to proof of stake and what are the new properties that these blockchain systems give us. Then I'm going to describe two most popular approaches to designing proof of stake blockchains, which is Nakamoto protocols, very similar to proof of work.
00:02:09.282 - 00:02:47.866, Speaker A: They're sometimes called longest chain consensus protocols or fork choice rule consensus and also BFT protocol is another popular approach. I'll then focus on leader election strategies or committee election strategies and blockchain randomness. This will be somewhat a little bit more technical. And then finally, I will overlook at different possible attacks that the proof of stake design enables that were not possible with proof of work blockchains. Okay, so let's start with proof of work versus proof of stake. Hopefully this will be easy proof of work blockchain. You all probably know it starts with the Genesis block and the blocks are built on top of other blocks forming a tree.
00:02:47.866 - 00:03:27.434, Speaker A: And so each miner, it needs to pick one path through this tree to extend this path with another. Block. So it exercises the fork choice rule to select a path in the tree. And to get a right to extend the chain with a new block, it needs to solve a computational puzzle that's hard to solve but easy to verify. And solving the puzzle, what allows them to extend the chain by one more block? So we notice in this consensus, we get total ordering of all of the transactions. So transactions ordered within a block, and then the blocks are ordered so everything has total order at the end. It doesn't have to be this way.
00:03:27.434 - 00:04:24.618, Speaker A: Some transactions actually independent, so potentially can be run in parallel. And we'll see some modern consensus protocols are trying to do partial ordering, but not total ordering, but proof of work is doing total ordering. And another interesting thing to note about the proof of work system is that the miner who joins it needs to figure out what's the longest chain. It needs to verify all the cryptographic puzzles along the way to figure out what's the latest block to build on top of, unless it's willing to trust a centralized checkpoint. So with proof of work, miner spends a lot of money on the infrastructure, paying electricity bills and also buying special mining hardware. So you can envision as if the money are kind of leaving the protocol into the physical world. And this infrastructure allows the miner to participate in building the blockchain.
00:04:24.618 - 00:05:18.250, Speaker A: And the reward that the miner gets is proportional to its computational power. So if it doubles on its infrastructure, then his reward will double. So the more money he spends on the infrastructure, the larger is his reward. And the whole idea of the proof of stake blockchain is, why don't we eliminate this physical component altogether and kind of circle this money just directly back into the protocol by just locking the same amount of money as stake. And then the reward that we give to good miners that honestly participate will be proportional to the stake that they had locked to participate. Okay, so that's a high level idea. So to compare proof of work to proof of stake, roughly.
00:05:18.250 - 00:06:09.054, Speaker A: So, a bit of terminology we call miners in proof of work, the nodes who help build the blockchain and validators in proof of stake, the reward is proportional to computational power and proof of work, but proportional to stake in proof of stake. And then what the miners are doing in proof of stake is they're validating and executing transactions same as in proof of work, except they are not solving these expensive computational puzzles. So that's why proof of stake blockchains are very energy efficient. The Byzantine threshold that the blockchains can handle is roughly the same. So to attack the proof of work, you need to compromise some significant portion of computational power. And for proof of stake, you need to compromise some significant portion of stake. But the world is still, despite all the benefits of proof of stake is still mostly proof of work.
00:06:09.054 - 00:06:47.546, Speaker A: So if you look at the cryptocurrencies by market cap, about 57% of them are still doing proof of work. So Bitcoin is the largest and Ethereum is the second largest. But Ethereum plans to move to proof of stakes somewhere this year, hopefully. So this will be the largest proof of stake cryptocurrency after they make the merge. And we'll be focusing on this, right rectangle of protocols, including the Ethereum consensus layer. So just a little bit more about electricity consumption and how bad it is. So some of the latest numbers, the bitcoin is consuming 85 terawatts an hour and Ethereum 112.
00:06:47.546 - 00:07:30.502, Speaker A: That's comparable to the size of the Argentina, which consumes 130. But to be fair, it's still less combined than mining gold, which is 240 terawatts an hour. And also, if you look at how the hash rate climbs up of the bitcoin network and hash rate is directly proportional to electricity consumption is it climbs up very, very rapidly, especially in the last couple of years. So if it continues climbing up this way, you can start worrying about sustainability of this whole blockchain. If it doesn't worry you now, maybe it will start to worry you sometime soon. Less electricity loss with proof of stake is, I guess, the major selling point, but not the only one. Also, mining becomes cheaper.
00:07:30.502 - 00:08:11.794, Speaker A: So if you look at the numbers, currently ethereum is paying 2% of the total supply of coins to its miners annually and bitcoin is paying 1.5%. So that's a lot of stakers, a lot of rewards going to miners. With mining becoming cheaper, these rewards can probably be made lower. Another good thing about proof of stake blockchain is some protocols have much faster finality and larger throughput. So, as you all know, bitcoin and Ethereum are not so good in terms of finality and throughput. You need to wait. Like, for example, this is a coin based number, so you need to wait three blocks on top of your transaction in Bitcoin and 35 blocks in Ethereum.
00:08:11.794 - 00:08:45.506, Speaker A: So that makes the finality time about 30 minutes for Ethereum, 30 minutes for Bitcoin and nine minutes for Ethereum. And then the transaction throughput is also really low. With proof of stake. Potentially you can have finality in seconds and be able to do thousands of transactions per second. Then we'll look which protocols are achieving that performance. It's also arguably easier to launch new projects with proof of stake consensus because you don't need to acquire a lot of miners to support your chain. So it's still challenging, but it's somewhat easier than proof of work.
00:08:45.688 - 00:08:49.374, Speaker C: Yes, I guess one could just increase.
00:08:49.422 - 00:08:52.530, Speaker B: The block size of the proof of work chain.
00:08:53.270 - 00:09:36.802, Speaker A: Absolutely. I had a slide on that, but I removed it for the sake of time. But you are right, there are research directions that are trying to improve the proof of work blockchain by increasing the block size or decreasing the frequency at which the blocks are allowed to be produced, like decreasing the difficulty puzzles. The problem with that is that when you start growing your block size, it takes longer to propagate through the network. And so it's more likely that other miners will find blocks at the same height and the chain will fork more as a result. So I guess the main worry of the people who are opposing increasing the block size is the fact that the chain will be less reliable. But also there are other research directions trying to improve the proof of work.
00:09:36.802 - 00:10:20.800, Speaker A: For example, instead of solving this useless computational puzzle of finding pre images of the hash function, you can do maybe something useful. And there are some ideas around help fold proteins or help figure out transportation and such. So some interesting research direction like substituting the puzzle for something useful. Because if you think about it, the infrastructure is already built and we have all this kind of hardware, people figured out the logistics with electricity, it's kind of a wasted there having all that there kind of wasting it and solving stupid kind of hash puzzles instead. Why don't we maybe repurpose them to solving something useful? Would be kind of nice. So that's another approach of maybe improving the proof of work.
00:10:21.250 - 00:10:44.370, Speaker B: On the topic of block size that Justin mentioned, I feel like that's almost like an orthogonal dimension compared to proof of work versus proof of stake. I feel like because proof of stake blockchains you also have to worry about is the block size bigger or smaller? How do we feel about propagation delay? So a different thing is just kind of like what is the minimal computational requirements just to validate the chain?
00:10:44.450 - 00:10:45.030, Speaker A: Right?
00:10:45.180 - 00:11:05.582, Speaker B: Something that comes up, I guess one thing that's maybe so bitcoin and ethereum. I feel like both of them want to make sure that it's very easy to be able to just sort of follow along so that you don't need like a crazy computer. And I feel like that's a completely orthogonal design dimension to choice of proof of work. Is there any connection between those two.
00:11:05.636 - 00:11:36.294, Speaker A: Or does that seem yeah, especially those proof of stake protocols that mimic proof of work, they will have the same problem. Yeah, good point. Let me mention the desired consensus properties or blockchain properties that we want before we kind of see the designs of those blockchains. So of course we want safety. So miners need to agree on history, on some common prefix of the chain. You want to have liveness. So every valid transaction should eventually get added to the chain and you should have no censorship.
00:11:36.294 - 00:12:26.250, Speaker A: Ideally you should be able to tolerate some percentage of Byzantine miners, adversarial miners who are trying to disrupt the protocol. And finally, you would want to have some minimal network assumption for safety and liveness to hold. And Andy was giving great seminar talks about all of that in more detail, but I want to mention it here still just so that we are able to map the consensus protocols I'm going to be describing into those properties, see what properties they have. So in terms of network assumptions, people usually look at three different models. First is synchronous. It's the best for academics because it's easy to design protocols. In this model there is a known bound delta network delay and a shared global clock.
00:12:26.250 - 00:13:09.938, Speaker A: So if you think for a bit from first glance it doesn't seem like this model of the internet very well because sometimes we have very large delays between the nodes or there are network partitions and such. But since the protocols are typically designed to tolerate some number of Byzantine miners, you can hide the network delay that's larger than delta inside your Byzantine assumption. So maybe in practice, sometimes synchronous network is still a good model because if two nodes have delay larger than delta, you can just assume this at Byzantine. As long as you don't violate the threshold of Byzantine nodes, you're good with a synchronous assumption.
00:13:10.034 - 00:13:16.798, Speaker B: Just like for example, you're saying suppose 10% of the nodes really were Byzantine and 20% of the nodes just had a crappy connection.
00:13:16.834 - 00:13:17.130, Speaker A: Exactly.
00:13:17.200 - 00:13:17.898, Speaker B: That would be fine.
00:13:17.984 - 00:13:57.394, Speaker A: Yeah, right. Another better model is partial synchronous where we know there exists a bound delta, we just don't know what it is. And also shared global clock. Again, so nice thing about this property is protocols designed in this model they cannot depend on delta, so they cannot have any forking based on delta. They cannot do any action based on how much time has passed compared to delta. But the properties we can reason still about, for example, finality in terms of delta. So we can prove properties about this consensus protocol and partially synchronous model based on delta.
00:13:57.394 - 00:14:49.666, Speaker A: So the better the delta, the better are the consensus protocols. And finally, Asynchronous protocols, these are the best because they make no assumptions about the network and they also subsume network partitions. And actually it's very active area of research right now building consensus protocols in a synchronous network. I'll explain why this is possible in just a second. So I wanted also to very briefly mention possibility and possibility results because we start feeling the bounds of what is possible to do with consensus in different network models. So in synchrony it's possible to achieve both liveness and safety with any Byzantine threshold. Even if everyone except one node is Byzantine, you can still achieve consensus.
00:14:49.666 - 00:15:57.194, Speaker A: But the consensus becomes particularly efficient linear time when the number of Byzantine threshold is less than half impartial synchrony. We also know how to build great protocols with Byzantine threshold under a third and in Asynchronous, unfortunately there was an impossibility FLP impossibility result showing that you cannot achieve neither safety nor liveness even if you have a single Byzantine node. But there is actually a way around this impossibility result if you build a common randomness source, if you build a common coin for the nodes, just involves some cryptography and this is being done actually, maybe this year or last year, some protocols start being practical for building common randomness. We will touch a little bit on how they look like. But Asynchronous protocol synchronous consensus kind of becomes possible. So typically we're fine with synchrony or partial synchrony models and most of the protocols will be using one of them. But typically people worry about network partitions.
00:15:57.194 - 00:16:40.006, Speaker A: So what if there's some disastrous event and two countries get disconnected? Maybe transatlantic cable gets broken or something. Or maybe a large country builds a firewall around it, around itself. Then you have a network partition and large portion of nodes start being disconnected from another portion. That's kind of a variant of a synchronous network. But turns out you can have in this particular restricted asynchronous network, you can still preserve one of the two, either liveness or safety. And we'll see which protocols prefer which one. So our goal will be to design protocols for this network for partial synchrony that will prefer one of this safety or liveness.
00:16:40.006 - 00:17:13.800, Speaker A: But work have both when the network doesn't have a partition. Cool. So the proof of work that I was describing fits into the synchronous model at least. It was only proven to have to be life and safe in synchrony and can tolerate up to 50% of business nodes. And in the event of a network partition, it prefers liveness. So that's how proof of work fits into the picture. Okay, so now we will describe two popular approaches to proof of stake consensus.
00:17:13.800 - 00:17:53.036, Speaker A: First one is Nakamoto Consensus. It was pioneered by Pier Coin and sort of extensively studied from academic standpoint in family of auroboros papers. It is very similar to proof of work in that it has a fork choice rule similar to proof of work. So finality is eventual or probabilistic. You need to wait some K number of blocks to be built on top of your chain to be sure that your transaction has been finalized. And the larger the scale, the higher is your assurance that the block will not be rewarded.
00:17:53.228 - 00:17:56.512, Speaker B: So just to clear borrows. So that's in cardano.
00:17:56.656 - 00:18:25.064, Speaker A: Yeah. Communication though in this consensus protocol is really low, usually linear, but even better than linear. I'll explain just a second. The number of nodes that you can handle is limitless. This consensus can work when the number of validators is over 1000 easily. And in the event of network partition, it favors liveness service safety. So it will continue finalizing transactions.
00:18:25.064 - 00:18:30.280, Speaker A: It's just that two partitions will diverge and be building two conflicting forks.
00:18:30.440 - 00:18:33.420, Speaker C: So what do you mean when you say the number of nodes we listed?
00:18:36.500 - 00:19:20.270, Speaker A: Well, let me explain the other and then we will see what it means. Maybe it will be more clear. So the other approach is BFT consensus built on PBFT, although PBFT was not a blockchain protocol. It was adapted to be a blockchain protocol by tendermint first and then Hotstaff sort of conceptualized tendermint and also built a responsive protocol from Tendermint. So these are the most notable protocols, but there are also others that are being published now. So finality with BFT, consensus is immediate. Sometimes it will take longer to get finality if your network conditions are bad, if you start seeing delays or if the network is under attack.
00:19:20.270 - 00:20:03.932, Speaker A: But if the conditions are good, your finality will be extremely fast. And once you get assurance that your transaction is final, there is no way it's going to get reverted. So finality is deterministic. Communication is very high because each validator needs to be connected to every other validator. So they need to form a click to answer your question. So the higher the number of validators, the heavier the protocol becomes. So typically the protocol limit the number of validators by about 100, like 20 to 40 is a good number for performance system.
00:20:03.932 - 00:20:50.024, Speaker A: So the more nodes you have, the slower the consensus. And in the event of network partitions, this protocol favor safety overlift. So if there's network partition, the protocol will just halt and not produce any new blocks until the network is restored, at least it will not be finalized in completing transactions. So safety is preserved. And on the communication side, in the Commodore Consensus, you can have flexible network topology. So one node can be connected to some constant number of other nodes and they can be connected to constant number of other nodes. So typically when a node announces something to the network, it sends it to its peers and the peers send to the peers, et cetera.
00:20:50.024 - 00:21:44.328, Speaker A: So although it takes longer to propagate through this whole network, you have kind of longer network delays this way from reaching the periphery. Still the burden on each node is lower because you're only sending something to a constant number of peers and you're listening to a constant number of peers. Whereas in BFT Consensus where everybody's connected to everybody, first of all, you need to open channels, so there's a lot of channels to maintain. But the good thing is that these channels don't have to be authenticated just by the nature of this BFT protocol, all the messages assigned. So you don't need to additionally open TLS connections, you don't need to additionally Mac or do message authentication codes on these messages. So in that way, maybe you can build a more efficient network layer if you don't need authenticated channels.
00:21:44.504 - 00:21:47.944, Speaker B: So with BAC you could use version.
00:21:47.992 - 00:21:49.564, Speaker C: Some of there's like an honest path.
00:21:49.612 - 00:21:51.360, Speaker D: Between each pair of honest nodes.
00:21:53.540 - 00:22:04.896, Speaker A: Yeah, you're right, maybe as long as you need to be sure it's well connected. But definitely you can do the kind of the gossip style as well.
00:22:05.078 - 00:22:15.130, Speaker B: Sort of like logically that's what the network should look like and maybe each of those edges maps down to a physical path of multiple hops, I guess, right?
00:22:15.660 - 00:22:42.576, Speaker A: Yeah. So BFT can be ported to this topology. I guess that most frequently people do this kind of communication. But yeah, you're right, the other way is not true. Nakamota Consensus keeping work also with this network topology, but it doesn't do this. I think none of the protocols do this topology for Nakamoto Consensus, but indeed, some of the protocols, BFT protocols can adopt this flexible is maybe one way.
00:22:42.598 - 00:23:05.636, Speaker B: To think about these two pictures. So, like, longest chain, basically just whoever created the block has to tell everybody else, and everybody else doesn't have to talk to each other at all, basically about that block. Whereas, like BFT, everybody's like voting and everybody has to know whose votes to make progress. And so the communication pattern looks kind of more all to all. Is that reasonable?
00:23:05.828 - 00:23:16.724, Speaker A: Yeah, that's a reasonable assumption. Although Nakatenji was saying, if you can tolerate larger network delays, you can wait for the consensus decision to reach you through multiple hops.
00:23:16.852 - 00:23:23.480, Speaker B: Right, but I can interpret those graphs actually as just sort of like the communication patterns, I guess, in some sense.
00:23:23.550 - 00:23:24.152, Speaker A: Okay, right.
00:23:24.206 - 00:23:32.288, Speaker B: I can interpret the sort of complete graph as kind of everybody needs to to keep track of what everybody else is doing because they're voting and cash flows and stuff.
00:23:32.374 - 00:24:12.076, Speaker A: Yeah, that's a very good way to phrase it. So I thought it will be fun to kind of look at the blockchains by Market Cap and see what they are. So I took 30 blockchain projects ordered by Market Cap and let's remove the, ERC, 20 tokens that don't have their own consensus and see what the rest are. So this is my best effort. I really apologize if I messed something up here. Sometimes the projects, they incorporate the ideas both from Nakamoto Consensus and BFT Consensus. So it's a little hard to say what exactly they are.
00:24:12.076 - 00:24:40.644, Speaker A: Some of them also don't document their consensus that well. But this is my best effort. Correct me if you see any mistakes. So you can color code them as well. Also, the yellow ones are the proof of work blockchain, so bitcoin ethereum, dogecoin litecoin ethereum, classic monero, bitcoin cash. The green ones are BFT protocols or BFT style. So.
00:24:40.644 - 00:25:33.220, Speaker A: Binance XRP although Ripple is not quite BFT style. It's a little bit different, but has the BFT characteristics. Polygon kronos cosmos are BFT. Algrant is sort of BFT, but it's so different from the rest that I'm going to focus on Algrant a little bit more later on. And the purple ones are Nakamoto Consensus, so cardano, Solana, polkadot, tron, near protocol. You notice there are three that are not colored, and those are, as I said, different consensus protocols that are harder to map to this paradigm avalanche, Stellar, and Algorand. But roughly speaking, most of the time you can classify the consensus protocol in one of these three categories.
00:25:34.440 - 00:25:37.380, Speaker B: When you say BFT you also mean proof of stake?
00:25:38.380 - 00:25:40.788, Speaker A: Sorry, yes. BFT is proof of stake and Nakamota.
00:25:40.804 - 00:25:43.556, Speaker B: Is proof of proof of stake.
00:25:43.588 - 00:25:57.900, Speaker A: BFT proof of stake also proof of stake. Yeah, sorry, it's a little confusing because proof of work is also Nakamota style consensus. But yeah, the green and purple ones are all proof of stake.
00:25:58.560 - 00:26:01.880, Speaker B: So Solana and I have to say I think of the BFD more.
00:26:02.050 - 00:26:30.970, Speaker A: Yeah. So I spent quite some time trying to figure out and they actually have some fork choice rules that they define in their protocol. So it definitely has some Nakamoto style component to it. Maybe some of the other parts are BFT style or maybe they combine the two. Yeah, would be nice to figure out what exactly they do. I guess they're in the process of documenting what they do somewhat better.
00:26:32.220 - 00:26:34.520, Speaker B: Polkadot is also sort of the hybrid.
00:26:37.420 - 00:27:07.380, Speaker A: You'Ll probably talk about finality gadgets. Yeah. Thank you. Yeah, we like the purpose of the next talk probably on Thursday, but yeah, some of the protocols combine the BFT and Nakamoto together and run BFT on top of Nakamoto. All right, so the first approach is Nakamoto Consensus. I'm going to describe it in a little bit more depth. So typically in all of these protocols, the time is divided into epochs and epochs are divided into slots.
00:27:07.380 - 00:27:56.800, Speaker A: So some fixed number of slots constitute an epoch and validators stake for an epoch, they indicate their willingness to run consensus nodes. So I focus on two approaches, aroboros and Ethereum 2.0. There are other protocols that are doing something else, but I think it's important to understand at least this too. So here I will explain the Roborous design. So the leader is selected per slot from the validators who have staked for this epoch. And the leader is selected with probability proportional to its stake. So leader at a slot, it exercises this fork choice rule to figure out which chain to extend and creates a block on top of it and proposes it to the network.
00:27:56.800 - 00:28:30.812, Speaker A: So quite similar to proof of work. But an honest leader should not be proposing two conflicting blocks for the same slot. So it should only pick one chain and propose on top of that chain. So for example, here in slot three, the leader was probably just faulty or failed to announce the block correctly. Or maybe there were network delays. Network partitions. Leader at slot four was clearly Byzantine because it created two blocks at the same height and then it causes the chain to fork.
00:28:30.812 - 00:29:12.792, Speaker A: So leader at slot five, if it was honest, it probably heard about the top block first. So it's built on top of that leader. Slot six, if it was honest, heard about the bottom block first and built on top of that. So then you have to split ties or introduce figure out which chain to choose to build on top of. But eventually one of the chains will win. So, Robert's, proof of fork choice rule is pick the longest chain, the chain that has more blocks in it and does not fork deeper than K. Blocks from the local chain with k is the parameter of the protocol.
00:29:12.792 - 00:29:23.360, Speaker A: But it introduces some stickiness property in that if you already know a good chain and you've been following along, you are less likely to switch to a fork to a deep fork or deep rework.
00:29:23.780 - 00:29:28.770, Speaker B: Interesting. There's a notion this assumes like a global shared block, is that right?
00:29:29.140 - 00:29:30.080, Speaker A: Yes. Okay.
00:29:30.150 - 00:29:33.990, Speaker B: Which like in proof of work, you don't really need to assume so much. Right.
00:29:34.760 - 00:29:58.180, Speaker A: Like bitcoin doesn't really yeah, great question. Actually the proof of work, I think the difficulty kind of depends on how frequently the blocks are generated. So difficulty is adjusted if the blocks start to appear more frequently. So in a way it kind of judges by the time steps and the blocks, how frequently they arrive. So there is still some notion of time that's required.
00:29:58.260 - 00:30:08.780, Speaker B: You still need to keep track of like a two week period roughly, whereas here you need to have a notion of time on a sort of much shorter time scale, I guess the timescale of a single slot, is that right?
00:30:08.850 - 00:30:24.628, Speaker A: Right, you're right. But Arobros genesis like a follow up to the Cerobros papers, it was getting around this assumption that you need global clock and they were building a protocol that doesn't need that to follow up on that.
00:30:24.794 - 00:30:49.148, Speaker D: Aside from defining what is slot one, what is slot two? Do they actually need synchronized local flocks or do they use it they don't use it in any of the tiebreaking or chain selection. So do they just need some definition of what do they actually use the Fox for? Other than deciding what is time slot one, what is time slot two? What is time three?
00:30:49.314 - 00:31:04.512, Speaker A: Yeah, I think just deciding what the slot number it is. Let's see what else they need the clock for. Yeah, I don't know. I don't think being needed for anything else.
00:31:04.566 - 00:31:10.480, Speaker B: I'm just corresponding number of blocks on the chain then because there's one block produced.
00:31:10.900 - 00:31:35.064, Speaker A: Well, roughly speaking, you know, how quickly the chain grows. You should not be synchronizing to like an old state. So roughly speaking, you should understand that what slot is the full system right now and how many blocks have been created roughly at least, so that you know if you're really synchronizing to an old chain or if it's more up to date.
00:31:35.262 - 00:31:37.080, Speaker C: Was this local chain?
00:31:37.440 - 00:31:41.980, Speaker A: Oh, local chain is the one that the leader built on top of previously.
00:31:44.240 - 00:31:50.588, Speaker D: Maybe following up that, do you have a sense of what K is in practice? Is it like single digit or hundreds?
00:31:50.684 - 00:31:54.464, Speaker A: Oh, great question. I didn't look into that. That's a great question.
00:31:54.582 - 00:32:10.896, Speaker D: And then I was also going to ask the way that they choose proportional to stake, is that still just like they have a pseudorandum. Number generator. And it's just like everyone knows right now who's going to mine at slot eight. Everyone knows who's going to mine at slot.
00:32:10.928 - 00:32:47.840, Speaker A: Oh, that's fantastic question. So I have a whole section on leader election process. We will discuss it there and then when tools explain the Ethereum fork choice rules. Ethereum Consensus, it's different in that not only the leader is selected per slot, but also a committee is selected per slot and the committee is supposed to attest to what the leader is proposing. So the leaders and this validators from the committee, they do fork selection according to a different fork choice rule to select the chain to build on top of. And this fork choice rule is called LMD Ghost. It was originally proposed to improve bitcoin.
00:32:47.840 - 00:33:41.840, Speaker A: The idea there is that at a slot you look what were the latest votes that the validators in the committee casted and these are the circles what blocks they voted on. And so the whole tree that you know about from which you need to select one chain gets weighted. So all of the blocks gets weighted by the amount of validator votes that are in their subtrees. So eight votes total, five votes in this subtree, three votes in this subtree, and then there is a greedy selection algorithm of a fork. So you select nodes with highest weight. So anyway, there is an interesting design choice that along the leader you're also selecting a committee. Cool.
00:33:41.840 - 00:34:41.940, Speaker A: So enormous amount of academic work as well as white papers on Akamoda consensus, arobara's papers, I guess is the most prominent from the academic side, especially since those protocols have been published in the best crypto and security conferences, crypto, Eurocrypt, CCS, SOP and have been of course reviewed by great academics. On the other hand, there are great ideas also in white papers. And I think the white papers also deserve to be looked at and maybe analyzed from more academic perspectives, trying to prove actual consensus properties about them. So to fit into the table that I showed proof of stake, Nakamoto Consensus is same as proof of work. Nakamoto Consensus is that you can prove properties in synchrony. It can tolerate up to 50% Byzantine nodes and prefers liveness over safety in case of network partitions. So the other approach is BFT consensus.
00:34:41.940 - 00:35:00.572, Speaker A: I'm going to look at next. BFT Consensus symbolized the time is divided into epochs and fixed number of epochs constitute a slot. You have a fixed number of validators per epoch and two thirds of validators are assumed. To be honest, you said four to 100.
00:35:00.626 - 00:35:02.520, Speaker B: The 100 is just kind of empirically.
00:35:02.680 - 00:35:25.908, Speaker A: Empirically, yeah, right. I think that what people think in practice is roughly a good number. You can always grow this to be larger, but then your consensus will be somewhat slower because you need to collect more votes. Right. So that's just empirical. It doesn't have to be, it can be larger easily in each sort. You also have a leader election process.
00:35:25.908 - 00:36:04.610, Speaker A: So you elect a leader from the validator set and the leader proposes a new block. And validators then work collectively to try to finalize that block. And they either arrive at a consensus decision to finalize the block or they drop the block on the floor. And the slot is unoccupied, but the finality is deterministic. So you will know exactly when it should occur, at which slot, if the network conditions are good, and everybody's honest. So, different protocols take different routes. But roughly you can either finalize in a single slot, within a single slot, or within one to four slots depending on the protocol that you use.
00:36:04.610 - 00:36:55.250, Speaker A: So, this is my attempt to describe tendermint in one slide. You should definitely I recommend checking out tim's lectures on tendermint with a lot more details. But roughly speaking, the tendermint consensus was the first blockchain protocol that was adapting the BFT ideas to the blockchain world. And the BFT protocols have been studied for decades, so they're based on very long line of academic work. So in tendermed consensus, you have three phases pre, prepare, prepare, commit. So when the node is selected as a leader, it creates a block of transactions and broadcasts it to the rest of the network. And then the nodes vote on this block by signing it and send it to each other.
00:36:55.250 - 00:37:26.648, Speaker A: Then each of the nodes after prepare forms a quorum certificate. A quorum certificate is a collection of two thirds of signatures from other validators. Then they signed this quorum certificate and broadcast it to the network. So that's roughly the communication pattern in original tendermint. Then everybody independently kind of arrives to the consensus decision. It's guaranteed that a good fraction will arrive at the consensus decision. Then they proceed to the next leader broadcasting a block.
00:37:26.648 - 00:38:24.220, Speaker A: So as you see, the communication is quadratic because everybody is sending something to everybody else. And then Hotstaff was kind of an improvement to tendermint in two ways. So first of all, they observed that you can use aggregatable signatures and you can squash these signatures collected after the end of prepare step into one signature. Or you can use threshold signatures and you also can reroute the communication through the leader. So instead of everybody sending to everybody else, you can achieve linear communication using aggregate small signatures and rerouting through the leader. So the leader will send to everybody, everybody will send to the leader and the leader will sort of be this rebroadcasting point of rebroadcasting the QC. But this QC quorum certificate is now constant size because of aggregable signatures.
00:38:24.220 - 00:38:56.856, Speaker A: So the second great idea of Hot stuff is to pipeline block production. And you can make the next leader proposing block in parallel to the first leader proposing a block at a certain stage. So you can make the network kind of send messages in parallel for parallel kind of consensus processes. It's not that simple. Just the picture to have in mind. It's not as simple as that. That's a rough idea of how this works.
00:38:56.856 - 00:40:06.812, Speaker A: So the leader will collect the quorum certificate and after broadcasting it to the rest of the network, the next block created will include this quorum certificate inside of it and will already kind of link to the work that the first validator has done. And with this approach of linking, you can actually get rid of the rounds after the QC is formed and broadcasted. So you can kind of have this chaining of QCs at the end and that's roughly the idea of Hot Stuff and your block will be finalized after some number of QCs has been built on top of it, or some number of blocks with QCs have been absorbed on top. So Hot Stuff, you can think of it having a three chain rule. So the block B one will get committed after there are three blocks with QCs built on top of it. So that's how Hotstaff kind of conceptualized this block creation process on BFT for.
00:40:06.866 - 00:40:18.960, Speaker C: General BFT, when you picked and teach Epoch, if you have more than two thirds of the values in epoch being like Byzantine, does everything still work even outside of that epoch?
00:40:21.240 - 00:40:50.060, Speaker A: No, you need to compromise more than one third, actually, so it's a little less. You need two thirds honest and then at most one third can be Byzantine. So if you compromise one third of Byzantine, they can disrupt liveness of the protocol. So the protocol will stay safe and will not be finalizing. Wait, I'm not saying it wrong, right? Yeah, exactly. Sorry. They will be able to screw up the system.
00:40:50.060 - 00:40:56.264, Speaker A: Yeah, sorry. Serpent. Yeah. So if you compromise more than one third, the system is compromised.
00:40:56.392 - 00:41:22.820, Speaker C: So, you know, you have kind of guaranteed like in each epoch you have less than one third Byzantine, not just like okay, so as you have concentration bound of like even if maybe like two thirds of all the validators, being honest isn't enough. You need something like a bit stronger, I guess. Even if you have less than one number of notes in total, there's some probability in each epoch they all get selected, right, to be leaders for that.
00:41:22.970 - 00:41:25.716, Speaker B: So you're thinking that there's some random sampling step.
00:41:25.818 - 00:41:33.320, Speaker C: Yeah. In the random sampling step, if you get very unlucky and that just you get tons of distancing people in Epoch, that could be bad.
00:41:33.470 - 00:41:34.600, Speaker D: That's why it's.
00:41:37.740 - 00:41:55.680, Speaker B: I think for what Larry says, she just goes like a stream of almost 100 people, period. Obviously you could have another wrapper around it randomly sampled down. But I think for what Larry's talking about, it's just like there's 100 nodes. Hopefully 67 is doing it correctly. And if it's not, who knows?
00:41:57.060 - 00:42:13.664, Speaker A: Yeah, that's right. Yeah. If you're doing subsampling of committees, then you need to make sure with high probability you will not have more than a third of Byzantine. There in your subsample, so I guess that will require a somewhat stronger assumption on the overall number of Byzantine nodes.
00:42:13.792 - 00:42:29.692, Speaker B: Quick question. First of all, these pictures are super helpful for visualizing concept, I think. Really good, yeah. For the aggregatable signatures, what's the most sort of practical instantiation? Like, what would you actually use if you wanted to do that?
00:42:29.746 - 00:42:57.990, Speaker A: Oh, fantastic question. I would use BLS. But there is a caveat we're exploring with Miranda is that BLS signatures are not actually aggregatable to constant size signatures. You still need to provide the bit vector who have signed in order to be able to verify. And that bit vector, although in practice it's super small, maybe you can assume it's constant and insignificant. Still it's a linear size vector. So technically speaking, BLS is not aggregatable to constant size.
00:42:57.990 - 00:43:40.720, Speaker A: Hotstaff paper is proposed, but it's good in practice. I think it's for all purposes that's fine to use BLS and that's what Ethereum, by the way, is using BLS for signature aggregation. Ethereum consensus layer. Hotstaff is proposing using threshold signatures. And for threshold signatures to create a signature, you can do it in one round, you can do it with EdDSA signatures, which are relatively popular, but you need to have some pre processing round of generating the distributed key jointly. So that complicates the protocol quite a bit in practice and I don't think people kind of like it for their consensus layer.
00:43:41.380 - 00:43:44.912, Speaker B: Are there any deployed projects which actually use threshold signatures?
00:43:44.976 - 00:44:17.340, Speaker A: No, I haven't seen the BFT projects employ that, by the way. Still, research is very active in the area of threshold signatures and especially in distributed kit generation of how to do that efficiently. The simplest protocols also involves somewhat heavy machinery of musics, non interactive, zero knowledge proofs. So to make it efficient, you need to introduce another assumption which comes from Palia crypto system and starts being a little more scary.
00:44:17.760 - 00:44:19.528, Speaker B: There's hope that we'll get more practical.
00:44:19.624 - 00:44:59.496, Speaker A: Yeah, absolutely, yeah. Many projects, by the way, build the threshold signatures for other purposes, not just to underline the consensus protocol, but on top of that have, for example, for custody, if you want to split your key between parties and make them jointly create a signature for you such that you need large participation to sign your transaction. So I think that's what drive. These are the projects that are now driving this research. International signatures. Yeah, great point. Academic works on BFT, so these are the four that I want to highlight.
00:44:59.496 - 00:45:41.780, Speaker A: So PBFT is from 99, it has nothing to do with blockchains, but it was surprisingly very easy to adapt to the blockchain setting. The tendermint was an adaptation of PBFT. So as I said, the protocol is quadratic. We haven't covered the case of a malicious leader, it's somewhat more involved to explain on the slide. But in leader failure, there is something else you need to add to the protocol to handle that. So tendermint was doing switching of the leaders in also quadratic time. The tendermint consensus unfortunately is not responsive.
00:45:41.780 - 00:46:32.440, Speaker A: And that's why Hot Stuff was built as criticizing tendermint for not having responsiveness, which means that the protocol cannot proceed with network speeds and Hot Stuff can. So if your network is fine, everybody is honest, you can finalize faster if your network becomes faster. But with tendermint you have some lock steps that don't allow you to move to the next round as quickly. This PBFT protocol, PBFT, tendermint and Hot Stuff, they work in partial synchrony. And interestingly research in synchronous protocols also continues. So I was saying maybe synchronous assumption is not that bad in practice and synchronous PBFT is just three years old. Very nice protocol with linear communication that's also responsive.
00:46:33.580 - 00:46:44.844, Speaker B: It looks like Kafka background dominates strictly synchronous BFT, but they're like the same year. Are there any benefits of synchronous BFP over?
00:46:45.042 - 00:47:30.460, Speaker A: Ah, great question. I think this protocol is simpler, that's the main thing for just for implementation purposes. But yeah, you can go well with Hot Stuff too. All right, so to map it back to our table, this is the kind of protocols typically work in partial synchrony, can tolerate up to 33% of Byzantine nodes, and in the event of network partition, they choose safety over liveness. So they completely halt and are not finalizing in the blocks. Great. So going back just to the picture, just to reiterate that finality is probabilistic in a Kamota consensus and immediate in BFT.
00:47:30.460 - 00:48:54.384, Speaker A: And yeah, depending on which properties you would prefer, liveness or safety, you would choose one of them. More of the recent protocols they prefer, I guess at least the trend that I've observed, tend to prefer the BFT style consensus just because the finality is immediate and you can also scale kind of the protocols are better understood. So in these protocols it's possible to have really fast finality and really large throughput, especially if you don't have too many validators, right? And also BFT protocols are somewhat more accountable. So if there is a safety violation or if there is some problem with the protocol, it's kind of easy to see why. If you observe the history of how everybody voted in the BFT system, rather than kind of analyze this more permissionless systems where a lot depends on network delays, especially in case of network partitions, you don't have safety. I wanted to dedicate one slide to staking design choices, just maybe to have something for later discussions. So validators in all of these protocols, they stake coins as somehow indicate that they're willing to run the nodes and the required staking amount can be either fixed that's the ethereum approach.
00:48:54.384 - 00:49:38.548, Speaker A: You have to stake 32 east to be a validator or it can be variable. And the more you stake, the more likely you are to be elected as leader and the more rewards you get. Then I also wanted to highlight the delegated proof of stake. It allows stakeholders to delegate their stake to validators. And you imagine in practice there will be participants in the system who have a lot of stake but just don't have the technical capability to run nodes. So it's likely they want to lend their stake to validators who can run nodes and then they can sweep the reward. So this kind of delegation would probably happen naturally in any system.
00:49:38.548 - 00:49:59.950, Speaker A: It just is more accountable and more safe if it's baked into the protocol somehow so that you don't purely trust your validator to use your stake correctly and to give you back the reward. You actually can revoke this, right, et cetera. So many systems have this delegation mechanism in place.
00:50:00.480 - 00:50:03.468, Speaker B: Isn't Ethereum sort of unique in not having it?
00:50:03.634 - 00:51:02.640, Speaker A: Yeah, but in a way they do. They separate kind of the consensus key and the withdrawal key. So if you're a validator, you can have withdrawal key that allows you to withdraw from the validator set and get back your stake with the withdrawal key and you can keep it to yourself and then the consensus key, you can embed the nodes key as a consensus key and it will be signing on your behalf. So in a way they kind of allow this separation. Foundry but yeah, indeed, the fact that they have fixed stake doesn't quite naturally kind of fit into the delegated proof of stake model. Some protocols, as you were mentioning, want to limit the number of validators and then they will need to do some random selection of a subcommittee and make sure there are enough good notes in the subcommittee. Or they can also take a more centralized approach like the current committee can decide who joins and who leaves.
00:51:02.640 - 00:51:46.030, Speaker A: And I wanted also to show this table of how much is staked in different cryptocurrencies right now. And this is ordered by the market cap of the staked amount. So just 10% of Ethereum is so big that it dominates the rest. But it was surprising to me that these amounts are super high. So Ethereum has 10% of its total supply staked. Salana has 75%, cardano 71, avalanche 58, et cetera. So I don't know, I leave it to the economist to kind of figure if this is a good thing to have or not because the stake is typically idle, not transacting, just sitting there.
00:51:46.030 - 00:52:34.888, Speaker A: Great. So that's on staking. And now I will move to more technical part leader election strategies and also the tools assume committee election strategies. So what are the goals of the leader election? So, yeah, again we have this epochs on slots and the leader is elected per slot to propose a block and the leader is rewarded for proposing a good block. And if typically in the protocols, if the leader violates the protocol and it is absorbed by someone, this can be presented as a proof to the protocol and the leader will get slashed. So the leader election properties. Leader election can either be public or private.
00:52:34.888 - 00:53:23.260, Speaker A: And public means that the outcome of the leader election process is visible to all participants at the same time, including the leader. So the leader learns that he become a leader at the same time everybody else learns. And private leader election is the leader checks herself whether she was elected or not and she can then prove to the network that she was elected. So she has this a little bit of a delay and she can take this time to prepare a block and then announce herself as a leader together with a block. And it's nice for dido's protection, as we will see, because once the adversary will not know whom to attack who the leader is until the leader reveals itself. But once the leader reveals itself, it also has already broadcasted a block, so there is no purpose of attacking. It's already done its job.
00:53:23.260 - 00:54:11.020, Speaker A: And the leader election properties that we want is typically fairness. So the leader election probability should be proportional to their stake. Although different protocols define fairness in different other ways. Some favor leaders who were good leaders in the past, like the Mbft for example, has the reputation system. Or you can maybe not be willing to elect the leader that has the most take too often and make it a little bit more decentralized. So fairness, it's typically easy to achieve if you define it well, unbiaseability is somewhat harder to achieve, as we will see. So unbiased ability means that nobody should be able to influence the leader election in its favor.
00:54:11.020 - 00:55:09.608, Speaker A: So as in the current leader, I should not be able to make myself a leader more frequently in a subsequent slot. And unpredictability basically means DDoS resistance. So for an attacker observing the leader election process should not be able to predict who the leader will be at some future slot so that it cannot prepare DDoS attack on it. It's surprising that this leader election can be generalized to solve more general, more interesting problems. So if we design a leader election process that's public, unbiased and unpredictable, these three properties will give a randomness beacon. Randomness beacon is this common coin that the synchronous protocols need in order to be built. And this can also provide randomness API for smart contracts, not just in asynchronous in any consensus.
00:55:09.608 - 00:55:32.340, Speaker A: So smart contracts, if you have a randomness beacon in your protocol, they can have an access to random bits, they can have get random API and for example, that's useful for lotteries. If you want to figure out who the leader is, randomness beacon will allow you to do that. So the problem, although we will be focusing on leader election here, the problem is more generic.
00:55:34.600 - 00:55:50.408, Speaker B: How should I interpret these quality signs, sweetie? So you would assert that if you can actually have all these properties, then you may as well use it as a randomness beacon.
00:55:50.504 - 00:55:51.150, Speaker A: Interesting.
00:55:52.960 - 00:56:07.024, Speaker B: Should I term that as a positive statement or a negative statement. Right. Like it's like a reduction. Right. So randomness beacons reduce to good public leader election. Does that mean randomness beacons are easier than we thought? That the public leader election is like.
00:56:07.062 - 00:56:40.990, Speaker A: Part of the harder? Yeah, well, we'll see protocols for randomness beacons so they have been built so there are positive results on that side. But also you'll see those protocols are not simple, so maybe you can interpret that as a negative result. Cool. So we'll start with the more straightforward leader election process, which is round robin. So the leaders are chosen one after the other in some lexicographical order or equivalently. This is deterministic leader election. So the leaders are chosen as a function of the slot number.
00:56:40.990 - 00:57:18.676, Speaker A: This leader election process is public. Of course, everybody knows who the leader is together with the leader. It's easy to make fair. It's also unbiaseable because there is no way you can bias round robin. It's predetermined, but unfortunately it is predictable. So if the Validators don't have good DDoS protection, you can figure out in which schedule to take them down in order for the consensus to halt and not be able to make any progress. But before we kind of discard this design, it's actually good for leaders with good DDoS protection.
00:57:18.676 - 00:57:45.280, Speaker A: And for DM blockchain, for example, we reasoned that this is good enough because there the Validators were known entities with very good infrastructure and they could afford being DDoS protected. So that was not a problem. This laser election was fitting there perfectly fine. But for more permissionless systems, this of course needs to be enhanced.
00:57:45.700 - 00:58:06.010, Speaker B: Because another thing which if you were doing longest chain consensus like Matt's written papers saying how things like selfish mining get worse if you have predictability, but if you're doing, I think at DMU or doing a BFT type protocol, so it may not have mattered. Kids prolong is chaining or other issues.
00:58:06.620 - 00:58:07.370, Speaker A: Interesting.
00:58:08.140 - 00:58:09.128, Speaker D: Yeah, that's correct.
00:58:09.214 - 00:58:10.090, Speaker A: I see.
00:58:11.020 - 00:58:20.490, Speaker D: Can I also ask you have public versus private. You're saying private is better if the leader learns that they're a leader in advance of the public?
00:58:21.280 - 00:58:45.380, Speaker A: Yes, because it has this time to prepare a block because it typically takes a few seconds for it to pack transactions together to make sure you execute them. They're all executing correctly. So typically there is some delay you want before you announce yourself as a leader to be able to prepare this block.
00:58:45.720 - 00:59:00.804, Speaker D: That doesn't sound like a public versus private issue though. But here it's public. But I know now that I'm going to be the leader like 15 years from now and I can it sounds like I can prepare my block very well in advance if it's round broadband.
00:59:00.932 - 00:59:10.520, Speaker A: Oh, yeah. It's not solving the problem of giving you more time to prepare a block, it's solving the problem of shrinking the Dos window.
00:59:10.600 - 00:59:11.390, Speaker D: Got it.
00:59:15.920 - 01:00:03.608, Speaker A: Then I was thinking it was fun to look at leader election approach in Ethereum consensus layer that's running right now. It's called so this is the picture from their specification. Leader of the previous epoch contribute verifiable deterministic randomness. If you know how BLS signatures work, think of BLS signing epoch numbers. So they contribute this verifiable deterministic randomness to the next epoch. So Randall Reveal kind of mixes their randomness into Randall Mix and then Randall Mix is used to elect a leader, elect all the leaders in the next epoch. So the epoch is 32 slots.
01:00:03.608 - 01:00:39.192, Speaker A: So you see here that the rundown mix from slot N plus 96 is influencing leader election of slot N plus 128. So that's exactly 32 slots in advance. One epoch in advance. Leader schedule gets known one epoch in advance. It's also public. So the leader learns they're going to be the leader at the same time the public learns and there is unpredictability is one epoch in advance. So the adversary will have one epoch of riddle's window to try to prepare the attack to take this leader down.
01:00:39.192 - 01:01:08.000, Speaker A: This protocol is biasable. The last leader to reveal to add to Randall Mix can choose just to drop out of the protocol instead of revealing his randomness, contributing his randomness and that way he gets one bit of bias for the resulting value either dropping out or contributing. It cannot contribute different values because the process is deterministic. It's deterministic randomness.
01:01:08.500 - 01:01:12.712, Speaker B: This is what I mentioned. Contribute to the signature.
01:01:12.796 - 01:01:51.676, Speaker A: Yeah. And there is only one signature possible because BLS signatures have this nice property of uniqueness. You only can generate one signature on a question. Maybe I didn't hear you well. So if one attacker has many different accounts and can choose to reveal any subset of them, does that mean they have more than one bit or is this somehow prevented? Absolutely sorry, great question. Yeah. If you have some K number of Byzantine leaders in succession you can influence the output by K bits.
01:01:51.676 - 01:02:03.990, Speaker A: You're right, absolutely. So yeah, it becomes larger than one bit if you have more Byzantine leaders in a row. But they still argue that small enough to work.
01:02:05.880 - 01:02:06.864, Speaker C: Can you clarify?
01:02:06.992 - 01:02:20.170, Speaker D: You mentioned like the last leader that this DLS signing is a sequential protocol that orders people or everyone signs at the same time and last just refers to whoever happens to speak last.
01:02:21.820 - 01:03:04.424, Speaker A: No. So the leader schedule is predetermined so you know who will be signing for each slot. So last means is that just the last leader of the epoch has a little bit of bias on the randomness in the future. And maybe you're asking about how the leader is chosen. So the leader is chosen from the validator set that also gets fixed a little bit in advance. So when you seed your randomness and your validator set through some random selection as an output, you get your committees, your schedule of leaders. Yeah, shard chain proposals they don't do right now.
01:03:04.424 - 01:03:07.130, Speaker A: But does that answer your question?
01:03:07.500 - 01:03:22.728, Speaker D: I think you answered the question. I'm so confused. This is the protocol they use to select the leader for the next block or the next epic. Or they select a sequence of leaders for the next epic.
01:03:22.824 - 01:03:28.224, Speaker A: Yeah, they select a sequence of leaders for the epoch next to the next. Okay.
01:03:28.342 - 01:03:43.760, Speaker D: And then in addition to being a leader who proposes blocks, the leader also has the option of contributing to the Randao. Or Randau is just completely parallel, like completely disjoint.
01:03:43.920 - 01:03:48.112, Speaker A: No, it's exactly the same leaders that are contributing to Randall and proposing blocks.
01:03:48.176 - 01:04:01.560, Speaker D: Okay. And then you said that when you said basically I can propose a block. My options are either don't propose a block at all or propose a block and contribute my BLS signature.
01:04:01.980 - 01:04:02.536, Speaker A: Exactly.
01:04:02.638 - 01:04:21.736, Speaker D: If I propose a block and contribute my BLS signature, then that will mix in with the randomness. And then at the end, there's some deterministic function that takes this input everyone's BLS signatures and says, this is the seed. This determines the leaders for the entire next epic.
01:04:21.848 - 01:04:32.800, Speaker A: Yes. Yeah. So the leader can either propose the block and his randomness reveal or propose nothing. You know, the block, no randomness. So it's either both or none.
01:04:32.960 - 01:04:41.072, Speaker D: And then the leaders for the next epic are fully known the moment that epic begins.
01:04:41.216 - 01:04:43.648, Speaker A: Yes. Even one epoch in advance.
01:04:43.744 - 01:04:49.224, Speaker D: Got it. And then how? Like, roughly how many oh, you already said never mind.
01:04:49.342 - 01:04:52.152, Speaker A: So an epic is like 6.4 minutes.
01:04:52.206 - 01:04:55.444, Speaker D: And that corresponds to like 132 slots.
01:04:55.492 - 01:05:12.784, Speaker A: Each slot is 12 seconds. Got it. Okay, thanks. The fact that this is biasable doesn't qualify this leader election as a randomness beacon, so you cannot use it in Lotteries, for example. But as we'll see, they're still using.
01:05:12.982 - 01:05:18.640, Speaker D: Longest Chain to determine the blocks. Or they have some consensus protocol.
01:05:19.380 - 01:05:33.270, Speaker A: Yeah, great question. They use Longest Chain and BFT on top of Longest Chain, so they have somewhat complicated fork selection rule that takes into account the fork choice rule as well as what the BFT Layer was saying.
01:05:35.320 - 01:05:35.792, Speaker B: Earlier.
01:05:35.856 - 01:05:44.612, Speaker A: Right. You talked about yeah. LMD go. That's the fork choice rule and I didn't explain the BFT Layer. That will I explain on Thursday.
01:05:44.676 - 01:05:45.632, Speaker B: But on Thursday?
01:05:45.796 - 01:05:46.510, Speaker A: Yeah.
01:05:47.360 - 01:05:55.710, Speaker D: Is the BFT Layer happening within an epoch or the BFT Layer? The BFT Layer is like multiple epics long.
01:05:57.620 - 01:06:06.816, Speaker A: Yeah, it's multiple epochs long. So still your time to finality is about two epochs, 1213 seconds, I think.
01:06:06.838 - 01:06:19.060, Speaker D: Like roughly within an epoch or not exactly. Longest chain within it's the fork selection rule and then every epic or two epochs they do BFT.
01:06:19.880 - 01:06:26.730, Speaker A: Yeah, that's a good way to think about it. Cool. Great.
01:06:29.260 - 01:06:46.744, Speaker D: Within an epoch, they're still using a fork selection rule and the leaders are fully known in advance. So they would still be vulnerable to these predictable selfish mining attacks because within an epoch, all the miners are known and they're using something like longest.
01:06:46.792 - 01:06:51.630, Speaker A: Chain. Well, I'm not sure about selfish. Maybe you should discuss the soft line.
01:06:52.260 - 01:07:13.968, Speaker B: The basic issue is that proof of work, selfish mining. Whenever you try to do something clever, there's always some risk that you're going to actually lose a block you would have gotten otherwise. Do some sort of like private mining. Then if honest miners sort of succeed before you extend your private chain, then you lose the non trigger like cost benefit analysis.
01:07:14.064 - 01:07:14.484, Speaker A: I see.
01:07:14.522 - 01:07:24.680, Speaker B: Whereas if you knew in advance exactly which rounds were going to be the leader, you just do all of that in your head. It makes it sort of riskless in a sense. Is that right?
01:07:24.750 - 01:07:24.984, Speaker A: Yeah.
01:07:25.022 - 01:07:36.348, Speaker D: Or basically like anytime you know that you are going to mine two blocks in a row, that is an opportunity to cancel someone else's block and get more revenue for yourself.
01:07:36.514 - 01:07:51.190, Speaker A: Yeah, exactly. If you can compromise leaders in succession but hopefully at some point there will be an honest leader. So it's not like you only have few bits of bias here to make yourself more likely to be a leader for next.
01:07:51.960 - 01:07:57.940, Speaker B: Yes, I agree with you. Liveness and safety should be fine, I would think.
01:07:58.010 - 01:07:58.452, Speaker D: I agree.
01:07:58.506 - 01:08:00.272, Speaker B: But the fairness of the reward distribution.
01:08:00.336 - 01:08:45.296, Speaker A: Is what I see. Okay, so these are the designs that are both unbiaseable and unpredictable but are public. And these are protocols for randomness beacon construction. And as we were saying, this is also good for leader election, for public leader election. But it's more generic in that it can also power random lotteries, give randomness API to smart contracts and also help bootstrap asynchronous consensus. The star here means that your underlying random beacon protocol should be asynchronous as well in order to bootstrap asynchronous consensus and not all of them are asynchronous. So this is good for leaders that are hard to dedose within the time it takes to create and broadcast the proposal.
01:08:45.296 - 01:09:28.992, Speaker A: Usually a few seconds. So I will briefly highlight you these two approaches, the VDF one and the threshold signature one. This will be as technical as it will get. Sorry, but the VDF based leader election and I encourage you to watch Joe Stoke on VDFS to understand that better. VDF is a verifiable delay function. The idea is to pass your randomness through a VDF to make it unbiaseable. So verifiable delay function is roughly speaking, the setup will output a single public key and the evaluation will take this public key and an input X output the result of the evaluation Y together with the proof that this result is correct.
01:09:28.992 - 01:10:29.520, Speaker A: Evaluation is taking very long but it is very fast to verify that the result is correct. If somebody has spent a lot of time computing why and the proof, it will be both to show you the Y and the proof and you will be able to quickly say whether that's correct or not. So that's why delay long time to verify long time to evaluate. And some VDFS require trusted setups, some do not. There are different constructions for VDFS and to make your beacon unbiaseable, you just apply the VDF on top of it. So, roughly speaking, when a party, the party can have some bias window in which it can pick randomness from a set and hoping that figuring out which one of them to pick will influence will help them become a leader more frequently later on. So there is a little bit of window between the party knowing the set already from which it can choose and revealing the randomness.
01:10:29.520 - 01:11:19.424, Speaker A: So delta bias is that interval. If you pass your randomness through a VDF and VDF, where VDF takes longer to evaluate than your bias interval, then you make it impossible for the party to predict the randomness before revealing it. Right? It will have to reveal the randomness before it knows what it will be. So it can definitely try to run the VDFS on every member of the set in parallel, but it will need to reveal the randomness before the evaluation is complete. So yeah, that way you can get an unbiaseable lead selection from biasable one if you apply VDF. Great. Another approach.
01:11:19.424 - 01:12:05.540, Speaker A: I will go fast here because I think it may be harder to understand if you haven't seen Threshold signatures before. So the setup threshold signature based lead selection collectively generates a secret key. A secret key has this property that it can be reconstructed from any Threshold subset of shares. So each party holds a share of the secret key. Any two third of them can get together to reconstruct the original key and it will be the same key irregardless of which subset gets together. Parties will collectively sign a slot number by submitting signature shares. So there is a signing process that allows you to sign with a share of the secret key, not the full secret key.
01:12:05.540 - 01:12:50.120, Speaker A: So your message will be the slot number and your secret key will be a share. You'll produce a share of the signature and then they will be able to reconstruct full signature from the shares. And the hash of the signature is your randomness beacon. So of course, communication doesn't have to be that big that you can always pipeline your communication through a leader. There is, I think, only one scheme. We know that you require to have unique signatures for this to work. There's only one unique signature that's friendly through Thresholdization, which is BLS.
01:12:50.120 - 01:13:29.328, Speaker A: And there is a notable project. Right? So this Threshold signature randomness beacon powers Drant project. It's a very interesting project. It's all open source and basically League of Entropy is a collection of companies that get together to run this distributed protocol in generating randomness beacons. And you go to their website, you have a hexadecimal string that gets updated every few seconds with fresh randomness. And as long as Threshold of them stays honest, you get verifiable unpredictable, unbiasable random numbers as a service.
01:13:29.494 - 01:13:31.440, Speaker B: So they're not using the BDF, right?
01:13:31.510 - 01:13:33.308, Speaker A: No, they're using BLS.
01:13:33.484 - 01:13:36.368, Speaker B: Do they have this last person problem?
01:13:36.454 - 01:13:39.760, Speaker A: Oh, sorry, you last person problem where.
01:13:39.910 - 01:13:44.100, Speaker B: The last person that can decide whether or not to contribute a signature?
01:13:44.680 - 01:14:13.948, Speaker A: Well, that kind of protocol is totally like in parallel to your consensus protocol. So it doesn't quite fit into the strandao mix paradigm where you have a leader contribution. But definitely you're saying maybe well, there are issues like that inside the construction of this protocol itself, but the protocol has a way to deal with that.
01:14:14.034 - 01:14:15.436, Speaker B: Which is not a VDF, some other.
01:14:15.458 - 01:14:53.400, Speaker A: Way of dealing with it. Yeah. Is it the same signature that's reconstructed regardless of whether it's like two thirds N plus one or like two thirds N plus two people signing? Because then in that case it seems like the last person to sign doesn't have influence beyond deciding whether it fails or deciding whether it passes. Yeah, fantastic question. Yeah, that's independent of who have signed or how many of them have signed. And that's because the signatures are unique. So there can only ever be one signature, full signature for any message.
01:14:53.400 - 01:15:02.090, Speaker A: So if you were able to reconstruct a signature, it's guaranteed to only depend on the secret key that's independent of the subset and the Flot number.
01:15:03.500 - 01:15:08.150, Speaker B: If for some reason you were pivotal, if you knew there were exactly 200. And.
01:15:29.240 - 01:15:37.210, Speaker D: So why does ethereum not use this and make themselves susceptible to the last, whatever problem?
01:15:39.020 - 01:16:27.080, Speaker A: Yeah, great question. I think the argument that they make is the leader needs to prepare, which I'm not sure why, but they think that it's a desirable property actually to have a little bit of window between the leader knowing it's going to be the leader and it becoming a leader. So for that reason they see it as a benefit to have this window. Although they're of course making the leader subject to DDoS probably attack and on other handle the rundown mix. It also uses BLS signatures, just not threshold version. And this is very involved protocol, especially the distributed key generation part. So maybe they will eventually do that.
01:16:27.080 - 01:16:48.864, Speaker A: It's just a lot of stuff to implement very quickly. I want to also highlight the VRF based leader election that makes the leader election private. Solving the last desirable property. VRF. Don't confuse with VDF. These are two very different cryptographic schemes protocols. Although they're named roughly the same.
01:16:48.864 - 01:17:21.960, Speaker A: VRF stands for Verifiable Random Function. It also has three algorithms. The setup outputs the secret public key pair. The evaluation takes the secret key. The VDF that I showed you didn't take the secret key, but only the owner of the secret key can evaluate the VRF, get the output and the proof. But anybody can verify the output and the proof that they match x. So the resulting Y should be indistinguishable from random, hence Verifiable random function.
01:17:21.960 - 01:18:03.350, Speaker A: And they can be instantiated from unique signatures like BLS or RSA. So each leader will use this primitive to determine whether he is a leader. He will take his secret key, assuming that his public key is known to everybody else, and it will evaluate basically X being a slot number with his secret key and will get Y and approved. Pi and if his Y is less than some Threshold, the leader, he is elected as a leader. So the leader creates a block and broadcasts it together with the proof. Pi there is no GDOs window. As I was explaining, the leader has already fulfilled its responsibility, also revealing itself.
01:18:03.350 - 01:18:55.012, Speaker A: But there is a kind of major problem where this approach is difficult to implement, just as a building block into your consensus protocols that multiple leaders can get elected, or sometimes none of the leaders can get elected. So you can imagine several leaders will have their value less than Threshold or maybe none. And so your consultants protocol needs to be able to handle that, needs to be able to handle multiple leaders or non leaders. Algorand, I guess, was the first protocol to be designed to deal with that, just to go back to the properties that all of these protocols achieve. So round robin dermistic, it's unbiasedable, but unfortunately predictable. So DDoS window is really high. The rundown of Ethereum is a little bit biasable, a couple of bits of bias.
01:18:55.012 - 01:19:27.730, Speaker A: It's somewhat unpredictable. You can predict the leader like six minutes in advance. So your DDoS window is six minutes, the random beacons ones are better, and the Jdos window is a couple of seconds that it takes for the leader to propose a block. But they're not private. And if you really don't want any GDOs windows there, then you should use the VRF based approach and satisfies all those properties. Great. So in the last couple of minutes, I just want to highlight a few interesting consensus protocols that I didn't color code.
01:19:27.730 - 01:20:13.760, Speaker A: This is Algrand, where you can have multiple leaders per flot. So they also make other interesting design choices, particularly they don't make nodes lock their tokens, so there's no slashing. So it's really safe to be a leader. You only get rewarded. User still needs to indicate his willingness to run consensus node, so he need to register the participation key. Each node runs a VRF twice, similar to Ethereum consensus layer, once to figure out if he was elected as a leader, and the other time to figure out if he was elected as a member of a committee. And the committee will be voting on leader proposals.
01:20:13.760 - 01:20:47.720, Speaker A: So there could be multiple leaders per round who are proposing block, and the blocks start propagating through the network. And so there's this fork choice component in that you need to choose one of the blocks. And the way they choose one of them is just the lowest block. By hash you choose that block and you get that block propagated. And then in BFT style consensus, the committee votes twice on a block to get it finalized. The other interesting choice they make is making their keys forward secure. So we will discuss this more on Thursday.
01:20:47.720 - 01:21:21.776, Speaker A: But the leaders and validators, they rotate their keys, kind of discarding the old keys to invalidate the possibility of long range attacks. And yeah, Algorand is also network partition resilient, but it resumes when the network is restored quite efficiently. The other protocol I want to highlight is Avalanche. It doesn't have leader election process altogether. It's a leaderless consensus protocol. It's a very nice idea. There are consensus protocols called Snowball or Snowmen.
01:21:21.776 - 01:22:27.470, Speaker A: It was originally described in the UTXO model. Thus transactions form a Dag directed asyclic graph. And this consensus establishes partial order of the transactions. So roughly, it works as follows which node accepts a valid transaction TX one, when it hears about it and sends it to other nodes, like rebradcasts to peers until it sees a conflicting transaction TX two, in which case it needs to figure out which one to choose, either TX one or TX two. And then it asks its peers what do they think is the transaction that they're willing to include? If TX two gets more votes, then it switches to TX two. If TX one gets more votes, it switches to TX two and rebroadcasts it also if didn't get too much many votes, you stick to TX one. So the finality is probabilistic and eventually kind of the full chain collapses to believing either transaction one or transaction two.
01:22:27.470 - 01:23:03.848, Speaker A: Okay, so that's avalanche. And finally, this is my last slide. I want to show different attacks on proof of stake consensus. And this is rather a glossary. Like I'm sure you can figure out those attacks yourself because they all are a consequence of costless simulation. Basically, you don't need to solve proof of work mining puzzles which are very expensive. It's very easy to create alternative forks just for the glossary, kind of because the projects have settled on these terms.
01:23:03.848 - 01:23:54.760, Speaker A: So nothing at stake is relevant to Nakamoto Consensus, when a rational validator will work on all possible forks, trying to maximize him being elected as leader on all of these forks and eventually kind of hoping to collect more reward. This is typically mitigating by slashing for double voting. So voting for two conflicting forks at the same height constitutes a slashing condition that can be presented to the protocol. And this validator will get his stake revoked or some substantial amount will be deducted. Stake grinding attack is exactly bias in leader election. By manipulating the randomness. It's also mitigated as worst thing by good and biasable leader election strategies.
01:23:54.760 - 01:25:08.848, Speaker A: There is also risk of centralization. So if the code base is cracked, everybody is behaving honestly, then kind of wealthy individuals will become even more wealthy because since the reward is proportional to stake, they will grow even more stake from broke rewards. So there is this centralization that will be happening in a good Behaving system and I guess people are proposing mitigating that with some proper incentivization or taxation of wealthy nodes. And then Thursday we will discuss long range attacks. So what if you compromise historical consensus keys or you compromise nodes who were very wealthy in the past, and then you get the ability to completely influence the fork that gets built from that point onwards? Because you will be controlling the majority of the stake of some necessary fraction of it. And we will see the ways to mitigate that on Thursday. And stake bleeding attack is also a form of long range attack where the attacker can collect minting rewards in his alternative chain that he doesn't reveal to anybody else.
01:25:08.848 - 01:25:49.110, Speaker A: But he gets wealthier and wealthier there until at some point he gets enough stake so that it violates the Byzantine threshold and then he has full control of the fork. There is a very nice survey. I recommend and kind of reading more about these attacks and also different consensus approaches. Not just the ones that I describe, but there are also proof of authority, for example, proof of elapsed time, proof of history, proof of what else? Proof of storage, et cetera. So it's very interesting also to check out this other protocols that recommend the surveys from IEEE. Okay? That's all I have.
