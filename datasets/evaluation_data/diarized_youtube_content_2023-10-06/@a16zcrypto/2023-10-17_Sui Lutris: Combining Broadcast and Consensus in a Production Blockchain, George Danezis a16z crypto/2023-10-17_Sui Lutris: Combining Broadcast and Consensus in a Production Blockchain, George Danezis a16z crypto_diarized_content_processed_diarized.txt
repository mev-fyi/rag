00:00:07.500 - 00:00:08.050, Speaker A: You.
00:00:10.500 - 00:00:28.860, Speaker B: Welcome everyone, to this morning's a 16 Z crypto research seminar. Super happy to welcome George Zinesis. He's a really well known computer scientist, professor at UCL, and also chief scientist at Miston Labs. And he'll be giving us an overview of Sui. George, all yours.
00:00:29.020 - 00:01:11.588, Speaker C: Thank you, Tim, and it's a great pleasure to be here. I'm going to present Sweelutris, which is the core distributed systems primitive underpinning the Sui blockchain that just launched a bit more than a month ago. And this is the culmination of many years of research of people both who are working at Miston Labs as well as wider collaborators that we have worked with before. I'm indeed. George Denezzis. I'm a professor at University College London and chief scientist at Miston Labs. And to some extent the underlying design is part of my work for over five to ten years.
00:01:11.588 - 00:01:53.680, Speaker C: So let's jump right in. What makes a blockchain is the first question, right? To have a blockchain you really need a lot of components. You first of all need a kind of strategy, a protocol for doing distributed, replicated transaction processing. So this is actually the kind of boring end of blockchains if you want. Like transaction processing is as old as computer science. It's the kind of underpins most things that companies and organizations do, databases, et cetera, right? And it also kind of underpins to some extent blockchains, right? I mean a blockchain that has smart contracts to some extent does distributor transaction processing. That's what it does.
00:01:53.680 - 00:01:56.464, Speaker C: Now, of course, blockchains are much more than this.
00:01:56.582 - 00:01:57.008, Speaker A: Okay?
00:01:57.094 - 00:02:30.392, Speaker C: So to have a blockchain, you also need to have some strategy to make sure that you have civil resistance. To create a permissionless system, you also need to have tokenomics incentives so that you don't get flooded, you don't have denial of service attacks. Everybody has incentives to act honestly, et cetera. You also need to do a lot of crypto. You need to have high integrity data structures, merkel tree, zero knowledge proofs, et cetera. And if you want to have a blockchain that really can be used for all applications, you need to have some strategy about privacy because blockchains by themselves are public ledgers. Effectively.
00:02:30.392 - 00:03:09.924, Speaker C: I'm not going to be talking about any of these later parts. I'm just going to focus in this talk about how do we ensure that we can do safely and reliably this distributed, replicated transaction processing quite efficiently at high throughputs and at low latencies. This is what the topic of this talk is going to be today. Sui as a blockchain, of course, does quite a few of the other things, but these we will talk about another time. So let's first examine what is the kind of most reliable and I would say the dominant paradigm even in blockchains for doing replicated transaction processing. And this is called state machine replication.
00:03:10.052 - 00:03:10.776, Speaker A: Okay?
00:03:10.958 - 00:04:16.648, Speaker C: And I would really love to talk to the folks in the really coined this kind of pattern for doing it because I suspect that before this pattern was really established there was a mess of mechanisms that led to all sorts of unsafeties and lack of liveness. So this work on state machine replication from the early 90s, late 80s really coined a pattern for how can we do safely replicated transaction processing. And largely speaking, most blockchains today kind of follow it, which is a testament to how timeless this is. So the flow goes a little bit as follows and is illustrated in the slide transactions come into the broader distributed system, okay? And because transactions usually are not sent to all participants of the distributed systems, they go into a mechanism that we usually call a mempool in the domain of blockchains that kind of gossips and distributors transactions around the miners or the validators of the blockchain.
00:04:16.744 - 00:04:17.390, Speaker A: Okay?
00:04:18.100 - 00:04:26.844, Speaker C: This is usually a peer to peer network and then there is a distributed protocol that broadly speaking sequences those transactions.
00:04:26.972 - 00:04:27.552, Speaker A: Okay?
00:04:27.686 - 00:05:47.256, Speaker C: So now we go from this world of a mess of transactions that is just gossiped around to a world where all the participants in the blockchain, the miners, the validators, whatever, actually have a common sequence of transactions, okay? And then once basically they have a common sequence of transactions, we go to the next stage, which is execution of those transactions. So now all the components of the blockchain, the miners or the validators deterministically and in the same sequence execute those transactions using usually some kind of isolated execution engine. Beat the Ethereum virtual machine. Sui uses the Move virtual machine but you can use other things like the Berkeley packet filter or whatnot. There are many options there. And then once this execution happens and concludes there is a resulting state and many blockchains actually summarize that state and also authenticate it cryptographically and basically put both the transactions and maybe some summary of that state into blocks. And kind of send the blocks around for the purpose of archival and the purpose of dissemination, the purpose of basically updating everybody about what happened at the end in the blockchain.
00:05:47.256 - 00:06:20.164, Speaker C: Now, some blockchains do these things in slightly different orders. In particular, the longest chain designs kind of start with blocks seemingly and then build everything else. But broadly speaking you would recognize most of these steps. And I myself have designed quite a few blockchains that follow this pattern. I was involved in the design of, for example, the Vega protocol. It uses tendermid for doing sequencing and broadly speaking follows this pattern. I was also involved in the design of DM that basically follows pretty much that recipe and they work fine.
00:06:20.164 - 00:07:13.396, Speaker C: There is no fundamental problem. But despite the fact that there is no fundamental problem, there are all sorts of problems that still need to be solved, right? And this is really where building distributed systems is truly one of the most difficult tasks that as a computer scientist I have faced because there are all these details that are quite critical to both correctness and performance that one has to actually take care of. So on the front end of a blockchain in the mempool, really it's a world of chaos at that point. Transactions have come through, nothing really is authenticated. There aren't really very solid stories for doing denial of service protection. So it's a world of heuristics where there is actually very little research on how to build these things in a way that is correct and scales. It's largely a set of ad hoc designs and kind of engineering heuristics there.
00:07:13.396 - 00:08:10.776, Speaker C: And there are fundamental problems because a lot of resources and time is spent in the dissemination of these transactions in the MEMP pool. So if you get that wrong and for example, tendermint in one of its variants did get it wrong, the whole system just doesn't work right. And sadly, there isn't actually a lot of research on how to do this right. Then if we move to the next phase, the sequencing is where basically most of the research has gone into in this famous consensus part of blockchains, right, in order to make it more performant and more reliable as well. Still today I cannot say that sequencing is totally out of the woods. For example, in terms of performance, it takes many seconds to many minutes for longest chains. And traditional sequencing mechanism consensus mechanisms have kind of low throughput and are quite delicate and hard to build as well.
00:08:10.776 - 00:08:57.252, Speaker C: So it's quite a difficult thing to build. And then even though this is difficult to actually build, the sequencing is difficult to build and get to be performant. Traditionally, the execution is still a bottleneck, even though sequencing is slow. So virtual machines that execute Ethereum for example, can do a few hundred transactions per second in a sequential fashion. The execution is not just slow because of the virtual machine, but it is also the interplay of executing over a huge state that you have to keep accessing. So there is an interplay between getting the transactions in in a sequence, executing in sequence pretty much on one core, and reading and writing from a database in a kind of like sequential way. So all of this is very expensive and not super well optimized.
00:08:57.252 - 00:09:53.572, Speaker C: And finally, many blockchains, when they are about to form their blocks, need to maintain high integrity data structures such as merkel trees. Now, a merkel tree, when you implement it in your crypto one on one course over like maybe ten items or 20 items is a trivial data structure. It's just a tree with like hashes at every node that you summarize in a root. When you actually have to build it over a database that spans terabytes, it is very difficult to build in an efficient way, right, because you have to keep doing these random reads across your storage and you can do tricks like keeping the top of it in memory, et cetera. But still, it is quite an expensive data structure to maintain. So you lose a lot of performance if you basically don't do that, right? So, as you can see, every single step of that process is delicate, is critical for security and reliability, and potentially is a performance bottleneck. And this is the challenge that basically we're facing, where we're building a blockchain.
00:09:53.572 - 00:11:08.656, Speaker C: How do we solve these problems? The pattern is well understood, but how do we solve these problems? And in my own research over the years, I have provided answers to some of those problems. So as part of the DM project, actually, we looked at how do we formalize mempools and how do we actually make sure that the data dissemination there is both secure and extremely efficient. And then we leverage those kind of designs in Narwhal, as we call that system, to build consensus on top of it, which we call Tusk. And there is an asynchronous variant that is called bullshark. So to some extent, we have ways of doing high throughput now sequencing and correct kind of mempool designs. And then we looked at the next bottleneck, we looked at execution, and we thought, well, why not use all the cores of a computer in order to do execution? And we designed the block STM algorithm for doing that. But there are still some important questions that were remaining unanswered that we tried to answer when we designed suite, all right? And the most important one is how do we make sure that when the capacity that is needed from a blockchain exceeds if you want a single machine, we can still scale up the system.
00:11:08.656 - 00:12:31.180, Speaker C: All right? And how can we actually ensure in all cases that the latency is even lower than the latency we can do for processing transactions if we use consensus? As I said, despite our great advances in consensus, and even in bullshark or other consensus mechanisms like hot stuff, the latency is a few seconds. This is not very satisfying for a lot of applications. Can we actually lower it even further for common classes of transactions, at least? And this is to some extent, one of the main motivations of why we build Swee, the way we build it. Now, as I mentioned, state, machine replication is a very reliable paradigm for building blockchains. But there are also other paradigms, okay? And we are going to lean on some of these paradigms to actually build low latency processing paths in sweep. And one of the most interesting findings, if you want, of the last few years is that you actually, strictly speaking, do not need to have consensus to build the cryptocurrency. This was mind blowing when I actually started thinking about it, and I started reading other people, like Rashid Guru's work that pioneered this approach.
00:12:31.180 - 00:13:04.216, Speaker C: Because if you think about it, we usually use consensus in a cryptocurrency in order to avoid double spending, right? So if someone sends a transaction that spends a coin. Conceptually, the first transaction goes through, the second transaction that uses the same coin, is void and just gets annulled. But in order to actually guarantee this property, you don't really need to sequence everything with respect to everything else. You only need to sequence transactions touching a particular coin with respect to each other.
00:13:04.318 - 00:13:04.970, Speaker A: Okay?
00:13:05.420 - 00:13:49.904, Speaker C: And this is really the insight. So since a coin is owned by a particular party, by a particular owner that owns the address of that coin, the insight of that line of work is that the entity that is the only entity that can spend that coin effectively can suggest a sequence of operations and the system just checks that that sequence is correct rather than actually coming up with a sequence by itself. And it turns out that in order to do that, you only need a much weaker primitive than consensus, because someone already suggests a sequence and the system just checks it rather than the system having to decide what the sequence is. This primitive is known as consistent or reliable broadcast.
00:13:50.032 - 00:13:50.564, Speaker A: Okay?
00:13:50.682 - 00:14:48.180, Speaker C: And consistent or reliable broadcast, broadly speaking, has one initiator of the broadcast sending and otherwise has Replicas, out of which at most, a third are corrupt. And the initiator sends a message. And correct Replicas eventually deliver a message that is broadcast informally. This consistent broadcast or reliable broadcast, they are slightly different, but they offer informally at least two properties. The first property relates to safety. If any correct Replica delivers a message and another correct Replica also delivers a message, the message is guaranteed to be the same. Okay, so that solves our double spending problem, right? If basically on each coin, you can basically broadcast who I'm going to spend it with, and one Replica hears that I'm spending it with Alice, the other one will also hear that I spend it with Alice.
00:14:48.180 - 00:15:27.300, Speaker C: So that's the kind of safety property. And the second property relates to liveness, namely a correct initiator is always able to actually make the protocol progress from initiating the broadcast to all the correct Replicas actually delivering this message. So that gives us liveness. So what is the catch? What is the catch? Right? And the catch is really a subtlety that I mentioned in the second property, namely a correct initiator can guarantee to make progress if the initiator is incorrect or Byzantine and they equivocate, for example, the protocol is not really guaranteed to finish.
00:15:27.450 - 00:15:28.150, Speaker A: Okay?
00:15:28.600 - 00:16:37.564, Speaker C: So that is something that we will have if we use this primitive to design around to some extent, right, to somehow see how can we expose this, so that this lack of liveness in this byzantine case does not restrict us too much in what we can express. And in fact, when we started looking at this work back when I was at Novi already, we designed a cryptocurrency that uses consistent broadcast in order to do payments effectively. I mean, Novi at the time, for those who don't know the history, I guess, was the Facebook division that was building this famous cryptocurrency Libra, or DM. And the vision was, and some of us know it already, that one day it's going to be used by 2 billion people to do payments, et cetera, right. So it had to scale, right? And this was one of the ways we thought it had to scale and be low latency. So we started thinking about these consensusless agreement protocols in order to scale it up. And we designed Fast Pay.
00:16:37.564 - 00:16:45.070, Speaker C: Fast Pay, I think, was actually published at Aft 2020 that Tim, you probably were chairing at the time, I believe.
00:16:45.840 - 00:16:48.800, Speaker B: Was that the Zurich one? That was one of the COVID ones.
00:16:48.950 - 00:17:48.720, Speaker C: I don't know, because I didn't go I send my students to these things these days. So FastPay was actually a great system, if I may say so myself, right. We used multiple machines and multiple cores because it turns out that consistent broadcast is highly parallelizable and like consensus. And we were getting transaction processing for simple payments from 40,000 to 160,000 payments per second. It had extremely low finality because these protocols are literally kind of two round trip protocols, right? So we were getting 200 to 300 milliseconds finality. So it was not bad at all as a protocol. It was a very good protocol, but it had some limitations, which is that it was just limited to payments, right? So the only operation you could do is the kind of, like simple bitcoin operation of take an account.
00:17:48.720 - 00:18:07.944, Speaker C: An account has an address and a sequence number and a certain amount of coins in it. And the only thing you can basically do is transfer. You say, okay, from that account, this address and that version number, take a bunch of coins and send it to another account. That's it.
00:18:08.062 - 00:18:08.730, Speaker A: Right?
00:18:10.220 - 00:19:37.270, Speaker C: So the question that we had when we started working on Sui was, can we generalize such a Fast path, 200 milliseconds, such a scalable path to general smart contract processing, not just payments? And if we do, how do we solve the problem of just having a system like FastPay, which is extremely asynchronous since we're not using Consensus? Things happen in parallel. Things happen in parallel on different cores. Things happen in parallel across different cores. How do we actually then make all the other functions of a blockchain work, such as, for example, the need for people to have an auditable kind of history that they know is complete eventually, right? Or how do we actually allow operations on accounts, objects, something that are owned by multiple people that are not cooperating with each other in order to suggest sequences of actions as in consistent broadcast? And how do we combine maybe fast paths with slow paths that are needed in order to do that? And also, how do we do committee reconfiguration? The Fast Pay mechanism did not really tell you how to change committees ever. In fact, it was a side chain of another system that was doing that. And I mean the question of privacy also came to our mind. But we are not going to talk about this today.
00:19:37.270 - 00:20:08.270, Speaker C: By the way, since I'm talking here, I would like to point out that one of our co authors, Mateu Bode, is actually going to market with variants more powerful actually and more general variants of Fast pay Linera. And he also has some answers to those questions, which are very interesting by the way. They're slightly different than our answers, of course. So I guess you will have a talk from him to find out what he does extending this work.
00:20:08.720 - 00:20:19.740, Speaker B: Quick question. So you mentioned sort of the weaker liveness guarantee of reliable broadcast. So like in the payment system, that just results that you might find your coin frozen basically like unspendable.
00:20:19.820 - 00:20:55.230, Speaker C: Is that kind of in the payment system? It's a straightforward world, right? Each account is owned by one address. One address corresponds to one signature key and one entity has it. So to some extent that entity has to sequence correctly. If ever there is an equivocation, the coin is locked, right? Forever. So this was also a problem that we wanted to solve. This is not very satisfying to say that, okay, all your funds now are forever inaccessible. So that was an additional problem that we are solving here.
00:20:55.230 - 00:21:53.900, Speaker C: All good. All right. So FastPay offers fast paths but is restricted. Traditional state machine replication offers a generic way of doing any transactions but is slow. This is really the equation we have here, right? And now what we're going to ask ourselves is how do we combine the two? Okay, how do we combine the two to build a generic smart contract platform to make sure that we can do some things fast, something slow, everything is safe, that we can combine all of this in ways that are consistent with each other, that we can execute in parallel. We can get early finality as soon as we can, as soon as it makes sense. And that we can also have functions in the blockchain that you need in a kind of like production blockchain such as checkpointing and reconfiguration so that we can change our committees.
00:21:53.900 - 00:22:25.576, Speaker C: This is really what we're going to try to do. And also down the line, how do we build an architecture so that we can scale each validator to be effectively multiple machines so we can support any kind of demand that comes our way. And all of this has to happen while maintaining safety in the system. The more we talk about things happening in parallel, in terms of execution, in terms of distributed systems, in terms of many machines, the more there is room for inconsistency, right? So we have to make sure that all of this is done safely.
00:22:25.688 - 00:22:26.156, Speaker A: Okay?
00:22:26.258 - 00:23:38.912, Speaker C: And this is effectively the recipe or maybe the problem statement I would say that led to the design of the suite blockchain. So let's look at how does a validator in Sweelutris, which is basically the underlying mechanisms of sui actually operate. And it is quite different, I mean quite different. It is an extension I would say, of the state machine replication in combination with the fastpath. So what happens is that users submit transactions at the far left of the slide and the first step is that a user sends the transaction to all validators and all validators check that transaction and I will talk about what check the transaction means and send the transaction back to the user. This arrow that goes from the transaction to the consensus is a mistake and I will talk about it then the user gathers signatures that validators have sent back after they check the transaction and gathers them into a certificate. What is a certificate is effectively signatures from two thirds of the validators by stake in the system.
00:23:39.046 - 00:23:39.392, Speaker A: Okay?
00:23:39.446 - 00:23:49.844, Speaker C: So now we have a certified transaction which we call a certificate and that certificate is again sent to all the validators in the system.
00:23:49.962 - 00:23:50.532, Speaker A: Okay?
00:23:50.666 - 00:25:11.040, Speaker C: And now what they do is they check that certificate and we will talk a little bit more about what check that certificate means. And like the line that is actually on the slide, the certificate when it is checked is always sent to consensus. But there is a fast path under some conditions where we can just go and execute straight away and this is really the key path here that allows us in many cases to have very low latency after the certificate is checked. We also acknowledge the fact that the certificate has been received and is valid and will eventually be sequenced, executed, et cetera, to the user. And at that point we get what we call transaction finality and I will talk again about what is that guarantee that we get immediately basically after two round trips here in all cases now all the certificates go through the consensus path. The arrow should actually be that way. Validators talk to each other through the consensus algorithm we're using which happens to be a narwald bullshark and then eventually we assign versions to them, we will talk what that means and we execute the ones we couldn't execute already and we store all the data in any case.
00:25:11.040 - 00:26:46.776, Speaker C: And then when basically certificates are both sequenced and their results are known, the results of the executions are known, we are ready to form checkpoints and send them to all other validators and archival nodes, et cetera. And this offers the kind of like reference sequence of the system. However, as you can see, the checkpoint kind of happens asynchronously already when we execute and we store either because we have followed a fast path or because we're following the consensus path. We reply to the clients that send the transactions with the effects of their transactions that are signed and there we get settlement finality, which is a stronger kind of like guarantee, not a finality, but allows you to do more things than just transaction finality. So this is the flow of transactions within a validator in Sui Lutris. And the thing to note is that this kind of red box that encompasses checking the transaction, checking the certificate, executing and storing effectively is the fast pay path, right? It is a consensusless path. Whereas the path where we receive transactions, if you want, we put them in a consensus, we assign version, then we execute and then we checkpoint is effectively the state machine replication path.
00:26:46.776 - 00:27:35.080, Speaker C: So Sweet Lutherous is this kind of Frankenstein system effectively that just takes these two paradigms, glues them together. But of course, because some things may be executed through the fast path and then have also other things that are executed through the consensus path, we have to make sure that they're consistent with each other. Now, let's not be abstract about that. Let's actually be quite concrete. What if I have some piece of state that is referenced both by an operation that goes through the fast path and an operation that goes through the consensus path and it uses, let's say, the same coin or the same object or whatever, something inconsistent. How do we ensure that only one of the two things happen, right? Because we cannot have both things happen and then have double spending, for example, just because we use different paths.
00:27:35.240 - 00:27:35.996, Speaker A: Okay?
00:27:36.178 - 00:28:19.180, Speaker C: So that is the difficulty that we will have to deal with. And similarly, as you can see here, we have a very fast path, right? And we also have some checkpointing that really relies on the consensus path. How can we ensure that, for example, when we close our epochs, when we do reconfiguration, whatever, when we get finality through the fast paths, we can guarantee that eventually things will happen, will be represented in checkpoints and will exist across epochs. Again, there is a potential here for inconsistency because of this fast path versus the consensus path that is leveraged in order to build all our checkpoints, et cetera.
00:28:20.160 - 00:28:35.280, Speaker B: Question to what extent is it helpful? Should I think of this as almost like a black box composition between a reliable broadcast protocol and a PBFT style protocol? Or does the correctness kind of rely on details of how those components are built?
00:28:35.430 - 00:29:24.144, Speaker C: So it is absolutely a black box composition. So in here lives a consensus protocol just in the consensus and sequencing of certificates box. And on purpose we have chosen to expose nothing about this protocol to anyone but validators. And this is very important to us because ultimately we expect to make fundamental advances in that space and like most blockchains that really try to, in effect, structure everything they do around their consensus, we're actually trying to hide. As much as possible about our consensus, so that, literally, we can evolve the consensus from one day to the other without anyone in the ecosystem being bothered by this except the validators whose job it is to be bothered by this.
00:29:24.262 - 00:29:24.736, Speaker A: Okay?
00:29:24.838 - 00:30:55.628, Speaker C: So it is absolutely black box, okay? So let me now in the second half of the talk, dive deeper into effectively the protocols that make the Fastpath and then why we need the consensus path and how we actually combine them together. First, I need to talk to you a little bit about the kind of objects and the kind of transactions that our system deals with because we have structured them in a way to support effectively our distributed system. So as in all blockchains we have user transactions and these need to be executed in order to evolve the state of the chain in our system. Our user transactions take explicit inputs in the form of objects. So we have an object centric programming model where a transaction denotes exactly which objects it will touch, okay? And the object is in particular referenced by an object ID and a version number, okay? And objects can be one of two types. They can either be owned by a particular address and these we call owned object or single owner objects or they can be shared, which means that anyone can operate on them.
00:30:55.714 - 00:30:56.012, Speaker A: Okay?
00:30:56.066 - 00:31:43.948, Speaker C: So an owned object, you can think of it as an object you own and no one else can actually change, right? So an account, for example, in the world of FastPay is a known object or a UTXO entry in the world that is an address actually, that is protected by an address in the world of bitcoin is an owned object, right? But owned objects can be like characters in RPG, games, magic swords and et cetera, et cetera, et cetera. The assets that you own, shared objects on the other side, they are the things that we're more used to, if you want, in, let's say, the ethereum world. A contract address in the ethereum world would be effectively a shared object, right? Because anyone can basically send messages to it and can access its state in a controlled way.
00:31:44.034 - 00:31:44.670, Speaker A: Okay?
00:31:45.120 - 00:32:57.236, Speaker C: So we explicitly denote the input objects. We also explicitly say which command we would like to send the object and that basically involves the package name of the move package, the programming language we use, the name of the command and some arguments that are just primitive types. And of course all of this is signed and what is it signed by? It is signed basically by the address of all the owned objects that are included in the transaction. And here we have a very important restriction which is that a transaction can only contain owned object by one owner and that is the owner that actually needs to sign the transaction. I'm simplifying a little bit, we have a few extensions but that's basically already going to carry us quite far, okay? And of course each of these object references refers to. A store that is basically the state of the blockchain at any point. And the store is simply a key value store from object IDs to version and versions to the owner if it is an address or if it is shared, the move type and the actual data of the object.
00:32:57.338 - 00:32:57.990, Speaker A: Okay?
00:32:58.360 - 00:33:29.540, Speaker C: And also we rely on a side data structure so this store can actually be only eventually consistent or loosely consistent. We don't have very strong consistency of this because this can be big and we might want to store it in a big key value store one day. Then we also have the object locks, as we call it, the owned object locks, that map for each object ID and version, whether that object is already assigned to a transaction or to no transaction.
00:33:29.640 - 00:33:30.048, Speaker A: Okay.
00:33:30.134 - 00:33:42.560, Speaker C: And we will see how this is actually used. And that store has to have at least one strongly consistent operation. We should be able to basically atomically check if it is none and update it with a transaction.
00:33:42.980 - 00:33:43.392, Speaker A: Okay.
00:33:43.446 - 00:33:48.288, Speaker C: And that should only happen once. The second time it should actually reject it if it is not none.
00:33:48.384 - 00:33:51.744, Speaker B: And so in that previous figure, both paths will be accessing.
00:33:51.872 - 00:33:57.064, Speaker C: Both paths will access that. Yeah, both paths will access that. And we'll talk about this in a second.
00:33:57.182 - 00:33:57.752, Speaker A: Okay.
00:33:57.886 - 00:34:03.320, Speaker B: One clarification. So ownership, it's like literally either one or infinity as far as the number of owners.
00:34:03.980 - 00:34:53.610, Speaker C: Literally one or infinity in this simplified model. In the real world, the real world is much more interesting than this. So in the real world, we allow multisig for addresses natively, right? So you can actually say an address is actually two out of these three or five out of these eleven or whatever. And we also have immutable objects that anyone can access. So it's a little bit more subtle in reality. But this simplified model will illustrate the core of the protocols and the other ones are quite simple to extend to. But in particular we don't have a club good in the sense of only those ten without coordination with each other can go for it.
00:34:53.610 - 00:34:57.790, Speaker C: That's an interesting thought. How to do that?
00:34:59.600 - 00:35:04.072, Speaker D: Good question about owned object locks. Is it a local view of each validator?
00:35:04.136 - 00:35:46.036, Speaker C: Yeah, absolutely. So each validator has a local database for the object store and a local database for the owned object locks. I'm not talking about global databases here. We are building the global database so we cannot assume there is a global consistent database. Yeah. All right, so let's actually now dive into the fast path, right, which is actually the path that all transactions go through. All transactions in our system first go through the kind of like check the transaction and create a certificate and then check the certificate before they actually get quickly executed or they go through consensus to be executed.
00:35:46.036 - 00:36:37.192, Speaker C: So let's actually see what happens here. So as I mentioned, a user builds one of these transactions that I showed you earlier and sends it to all validators in parallel. And each of the validator checks some local things local to the transaction as well as this kind of check on the locks database that is shared amongst all transactions. So what are the local things that it checks? First of all, it checks if the signature is correct. If it isn't correct, there is no point in actually looking at it if the objects exist. If the owner matches actually the owner of the objects, it also checks some other things. Like, I mean, does it parse correctly? Is the package available like basic checks there? So these ensure that it comes from the correct owner.
00:36:37.192 - 00:37:07.856, Speaker C: All the objects that are referenced actually do belong to that owner or are shared. That's really what this checks. And then the check that really gives us the consistent broadcast is this test and set that is atomic on each of the objects. So for each of the owned objects, we're going to now check if they already have been assigned to a transaction or otherwise we will assign this transaction to be associated with these objects.
00:37:07.968 - 00:37:08.630, Speaker A: Okay?
00:37:09.400 - 00:37:40.540, Speaker C: And if indeed this transaction both passes the local checks and also is the transaction that is assigned to all the owned objects that it referenced, then we consider that it is a valid transaction and we actually sign it and send back the signature to the user. And once the user collects two thirds of these signatures by stake for all the validators, right, for two thirds of the validators by stake, I should say, then that forms a certificate.
00:37:40.700 - 00:37:41.410, Speaker A: Okay?
00:37:44.100 - 00:38:22.440, Speaker C: And then once the user has that certificate, they can send the certificate back to all the validators. And now what happens here? The validator checks that it is a valid certificate, that indeed it is a transaction designed by over two thirds stake. All the local checks are rechecked. Again, you don't need to in some models, but we do. And then there is a choice here, right? The choice is the following. If that transaction only refers to owned objects, so the only object that basically contains are fully owned by that address, it can send that transaction to be executed immediately through the fast path.
00:38:22.520 - 00:38:22.956, Speaker A: Okay?
00:38:23.058 - 00:38:44.464, Speaker C: There can never be a conflicting transaction if on the other side it also contains, for example, a shared object. Not, for example, if it contains a shared object, it has to wait to execute it and in any case, it sends the certificate to the sequencing engine, our consensus algorithm for sequencing.
00:38:44.592 - 00:38:45.220, Speaker A: Okay?
00:38:45.370 - 00:39:56.164, Speaker C: And then eventually, either through the fast path or the normal path, effects of the execution will appear. They will be signed and will be sent to the user. And once two thirds of the effects are available to the user, we basically have our strongest form of finality settlement. Finality. So what properties do we get already just from this part? Right? So if we make the usual assumptions that we make in distributed systems, namely that less than two thirds stake is Byzantine, or at most two thirds stake is Byzantine, the network is fully asynchronous and cryptography works. We don't assume that adversaries can break our signature schemes, et cetera. Then if we see a certificate, if a certificate exists, right, it means that no other certificate can possibly exist that refers to the same object at the same version, okay? This is actually a very strong property if you think about it, because it means that the world of transactions, just raw transactions with signatures can be quite inconsistent.
00:39:56.164 - 00:41:42.392, Speaker C: Like a bad sender, right? Can say I would like to send my object to you or use my object in particular way and then also send my object to you, right? It can equivocate. So that's an inconsistent world. But the world of certificates is consistent with respect to the owned objects, okay? And that is already guaranteed by the existence of a certificate, okay? And furthermore, there is a liveness property, which is that if a certificate exists, then there is some honest validators that have all the state necessary in order to be able to move on and execute that certificate. So that makes sure that some good validators in the system will actually have enough data to move the state forward, okay? And that's all due to the checks that we do and this kind of atomic check and set for the first transaction we see per object, okay? So that's already a good set of properties here and this is exactly what allows us to move and execute certificates that are only on owned objects. Because now, since we are consistent with respect to owned objects just by having a certificate, we're ready to execute and life is good, right? So let's talk a little bit about the two types of finality that I keep talking about. Because now, since we're going to start moving towards shared objects, they will become slightly different from each other. So, as I mentioned, when a validator receives a certificate and does the checks on the certificate, it immediately acknowledges that the certificate is valid to the user.
00:41:42.392 - 00:42:23.544, Speaker C: And when the user receives two thirds of these acknowledgments, we basically say that the transaction has transaction finality. What does that transaction finality mean? It means that effectively, from now on, this transaction will execute. Come hell or high water, it will basically execute. There is nothing you can do about it, okay? You cannot revoke it, you cannot cancel it, you cannot take it back. It's going to happen, okay? This is pretty cool. You can turn off your computer, go and have a coffee. It's going to happen no matter what you do, okay? But you do not know what the effects of that transaction are going to be yet.
00:42:23.544 - 00:43:07.688, Speaker C: Maybe it will execute and it will basically hit a condition that says abort, right? So the transaction executes, but the actual semantics of the call may not execute. Like if what you want is an actual transfer to happen, it may not actually happen. Now it turns out for owned objects, you can fully simulate what will happen. So if you have sent a transaction with just owned objects and you basically have simulated what's going to happen, and indeed the transfer or whatever semantic thing you care about was checked locally that it would happen, you also are guaranteed that it will execute and this will happen. Okay, so that's actually pretty cool here.
00:43:07.774 - 00:43:09.176, Speaker B: So it's the validator that's in a.
00:43:09.198 - 00:43:43.484, Speaker C: Position to know that no, the user is in a position to simulate all transactions that are just on owned objects. If you think about it, right, you're the only one who can operate on your own objects, your local history. Exactly. You retrieve your own objects. You run the transaction as the validator would run it locally on these objects, you will come up to some conclusion about what the result of that execution is going to be. Well, guess what? The validator is also going to reach exactly the same conclusion. So for owned objects, that finality is very strong.
00:43:43.484 - 00:44:20.444, Speaker C: And in fact, since we then can take the fast path and execute immediately, the second form of finality will come literally milliseconds after that finality. So it's not particularly different. But then after execution, right. Either a few milliseconds later, if we're just talking about owned objects, or a bit later, as we will see, for shared objects, we also have a second notion of finality, which is settlement finality. This means that effectively, in a quorum of validators, the goods have exchanged hands. Right. So it means that from now on, you can make transactions on the basis of the settlement of the previous transaction.
00:44:20.444 - 00:44:34.240, Speaker C: So, for example, if Alice has sent some asset to Bob, bob can now, at the point of settlement finality, put in a transaction using that asset that will succeed. Okay, so that's the settlement finality.
00:44:34.320 - 00:44:34.900, Speaker A: Yes.
00:44:35.050 - 00:44:38.432, Speaker E: So I presume there is some stake involved with the validators.
00:44:38.496 - 00:44:39.156, Speaker C: That's right.
00:44:39.258 - 00:44:53.684, Speaker E: So what happens? Let's say I make a transaction. I receive two thirds votes with the current set of validators. And let's say a year from now, the set of validators have all changed to a new set of validators. And at that point there's no notion of time over here. There's no notion of ordering.
00:44:53.732 - 00:44:54.104, Speaker C: That's right.
00:44:54.142 - 00:45:03.404, Speaker E: The new set of validators can say, okay, right now I'm paying to George, but tomorrow I can go in and pay the same money to Tim and a new set of validators are voting for me.
00:45:03.442 - 00:45:40.040, Speaker C: No, that's a very good question. So our system is designed to operate within epochs. Within an epoch, the validator set and its stake is stable. And everything that I'm saying here is with respect to that stable set across the epochs, we have effectively a reconfiguration protocol where the stakes can change, et cetera. And I will make the argument that all the safety is preserved by that protocol. So you're right to worry about it. But I will tell you why you shouldn't worry about it.
00:45:40.040 - 00:45:47.828, Speaker C: And the reason you shouldn't worry about it does not come down to certificates because not to this particular part.
00:45:47.934 - 00:45:48.590, Speaker A: Yeah.
00:45:49.440 - 00:45:51.564, Speaker E: There is some notion of consensus over there.
00:45:51.602 - 00:45:55.596, Speaker C: When you move from one, you have to, you have to have consensus to do that.
00:45:55.618 - 00:45:55.756, Speaker A: Yeah.
00:45:55.778 - 00:46:07.552, Speaker C: There is nothing magical. We haven't had a miracle in being able to do that without consensus, shall we say.
00:46:07.606 - 00:46:08.256, Speaker A: Yeah.
00:46:08.438 - 00:46:20.980, Speaker C: So you can think, if you want, of a certificate as being valid within one epoch. That's I think the way to think about it and everything that I'm saying before I talk about reconfiguration is really with respect to that one epoch.
00:46:21.400 - 00:46:27.048, Speaker B: Okay, cool. And what is one epoch in one day? One day.
00:46:27.134 - 00:47:26.264, Speaker C: One day it's up to we could change it, but one day seem to be the natural unit for an epoch. It's stable enough, but not too stable. All right, so we will talk again about this settlement finality when it comes to shared objects, because in the case of shared objects, it does come a little bit later than this. But what I wanted to point out, and Karthik to some extent alluded to this, one of the challenges here is that once basically a transaction is final, even after two round trips here in the Acknowledgment, we need to guarantee that it survives across epochs and that it is included in checkpoints. Otherwise we cannot actually get safety in the long term. Right? And this despite the fact that things are happening, asynchronously in everything that I have shown so far. Okay, so that's going to be an interesting challenge.
00:47:26.264 - 00:48:15.448, Speaker C: Now let's talk a little bit about execution. Now, one thing that you may have noticed already when I presented the fastpath is that both the check of the transaction and the check of the certificate largely look at things that are local to the certificate and largely look at a very small amount of state that is related to these objects that are included in the transaction. That means that we can basically shard both the check of the transaction and the check of the certificate. And the only thing that we need to be very careful when we actually shard that across different cores or different machines down the line right now we do it across cores is that strongly consistent locks table that needs to basically record the first transaction that touches an object, but not the second one or the third one.
00:48:15.534 - 00:48:15.940, Speaker A: Okay.
00:48:16.030 - 00:49:21.100, Speaker C: So there is a very small critical region there where we have to make sure we have synchronization between different asynchronous processes that do the checks, but everything else basically can otherwise parallelize across all our cores and we do that. Now, when it comes to the execution for transactions, what we basically get is we get the transaction certificate and we also query the database for these particular objects that are included in the transaction. And then we just run the move command and what comes out is a set of effects which include which objects were mutated and at what version, which objects were created and at what version deleted, as well as some status and some events that they emit. What we basically do once we conclude the execution is then we go for the new objects or the mutated objects and create entries in our lock tables for the new versions of these objects or the new objects that were created.
00:49:21.180 - 00:49:21.424, Speaker A: Okay?
00:49:21.462 - 00:50:37.480, Speaker C: So as you process effectively transactions, you invalidate or set the locks to the transactions and create new locks to actually continue processing the objects as they evolve in the system. And we use lamport timestamps here to make sure that object IDs and versions are always fresh. You can never actually go back to any previous one. So there is a sense by which we always make progress forward, which allows us to make quite a few optimizations in not keeping state, to check that things are fresh, to avoid replays, et cetera. Now, one thing to notice here is that the processing of transaction again only touches a very small amount of state. And again, we parallelize that across all the cores in our system and we have designs to actually distribute that again across different hosts on the validator on the basis that we know exactly what state it touches, we can efficiently retrieve it, do the computation and restore all the new objects that are created. Okay, so shared objects, let's talk about shared objects because this is really where the challenge is here the remaining challenge.
00:50:37.480 - 00:51:02.944, Speaker C: What is the problem with shared objects? Why can't we use what I described so far to process them? The problem is that because shared objects could be included in anyone's transaction, it is difficult for these disparate parties to coordinate and propose a consistent sequence of transactions with a consistent sequence of version numbers that these transactions should touch.
00:51:03.062 - 00:51:03.536, Speaker A: Okay?
00:51:03.638 - 00:51:13.744, Speaker C: So the system has to come up by itself with a sequence of transactions and the versions on which these transactions should operate on the shared objects.
00:51:13.792 - 00:51:14.004, Speaker A: Okay?
00:51:14.042 - 00:51:29.940, Speaker C: That's a harder problem. The system must assign the version numbers and that sadly requires consensus, right? And that has many more critical paths effectively within the validator where we need to do synchronization, et cetera.
00:51:30.020 - 00:51:30.650, Speaker A: Okay?
00:51:32.060 - 00:51:51.708, Speaker C: So this is how we do it, right? This is how we do it. On that side, on the left hand side, we have the fast path and eventually a transaction comes in to consensus with a certificate and the certificates are sequenced. Now, what happens here is all the certificates are sequenced, all the certificates are eventually passed to the checkpoints.
00:51:51.884 - 00:51:52.512, Speaker A: Okay?
00:51:52.646 - 00:52:30.284, Speaker C: But if the certificate contains a shared object, what we do is we assign in sequence the next version number for the shared object that it is touching. And we can do this statically. We don't need to execute at that point to do this. And we can do this statically because we maintain a database of object ID. And what is the next version? Since the versions are determined by lamp or timestamps, we don't need to execute these transactions to work out what the new version is going to be. We just look at the input sequence numbers. We take the maximum, we add one that's basically going to be the output sequence number.
00:52:30.284 - 00:53:08.068, Speaker C: So all of this is extremely fast. It literally involves some computations on integers because that cannot be parallelized. That is the sequential aspect of our system. Okay, but guess what? Modern computers are extremely good at doing that. And then once basically we have assigned a version, we just basically store a database of. Okay, this transaction this transaction touches basically these particular sets of shared objects at these particular sequence numbers. And now what we do is we inject those certificates into our execution engines and our execution engine takes the certificate, looks at the transaction.
00:53:08.068 - 00:53:19.900, Speaker C: For the objects that are owned, the version number that they should access is already given by the creator of the transaction. For the shared objects, the version number has been assigned after consensus and we're golden.
00:53:20.560 - 00:53:20.972, Speaker A: Right.
00:53:21.026 - 00:53:29.712, Speaker C: We know exactly what state to execute on. There is no room for inconsistency between validators. All validators will execute on the same input objects. Yeah.
00:53:29.846 - 00:53:43.568, Speaker D: Do you run the risk of any collisions, like for the objects that are owned objects? Could a user pick the next version number in a way that it collides with one of these shared objects?
00:53:43.584 - 00:54:21.484, Speaker C: Yeah. So we have to be very careful about how we derive our object IDs and the versions in order to make sure that we never can recreate. No user can ever recreate the same object ID version. Okay, so the object IDs are actually cryptographically derived in the transaction by hashing all the inputs, the transaction, and that lead to basically the sequence of new object IDs that are allowed to be created. So that cannot go back. There cannot be cycles in hash function, so we're safe. And the version numbers are created using these lamport timestamps.
00:54:21.484 - 00:54:55.412, Speaker C: And we hope that since we're using U version, the number of transactions you have to do to actually make a lamport timestamp roll around is so large that we will be very rich before it ever rolls back. So it's someone else's problem. I mean, we did back of the envelope calculations of how many transactions you would have to do. We're talking about thousands of years, so we're not worried about it right now that it will roll around. The user has no discretion in setting version numbers. The system always sets what the next version number is.
00:54:55.566 - 00:55:01.884, Speaker D: Is there no notion of priority? The user can't presumably pay more to.
00:55:02.082 - 00:55:22.644, Speaker C: That'S a very good question. Right. So this really depends on how that component that does consensus really is designed. To us, it's a black box, right? So there is a sequencing engine underneath that gives you a sequence. What really matters? I think lots of people think of consensus as a sequence. In my view, this is wrong. And I'll give you a bit of an insight here.
00:55:22.644 - 00:55:48.732, Speaker C: To me, a consensus is a distributed protocol that gives you a sequence of sets of transactions. Then the system, and I'm talking now about the system around the consensus, can actually apply its own logic, if you want, for how to sequence deterministically that set within each of the commits. So there you can order by gas fee, for example, which is actually what we do.
00:55:48.866 - 00:55:49.164, Speaker A: Right.
00:55:49.202 - 00:56:14.020, Speaker C: So we take basically the kind of consistent execution we order within that by gas fee, and this way we prioritize, if you want, the more expensive transactions first. But there might be other approaches there. I'm not claiming this is the best approach.
00:56:14.760 - 00:56:26.440, Speaker B: So you're saying that in your consensus protocol, it actually does not have discretion over the sequencing. In some ways that's just like automatically interpreted the execution layer, what the corresponding sequence is.
00:56:26.510 - 00:56:49.608, Speaker C: So what I'm saying is that one would look at basically the sequence produced. Okay, let me back off here. Let me take a step back. So traditionally what state machine replication needs is a sequence. That sequence doesn't have like natural boundaries of commit sets.
00:56:49.704 - 00:56:50.108, Speaker A: Right.
00:56:50.194 - 00:57:04.212, Speaker C: That is a problem. Right. Because you don't quite know what the latency characteristics are. It might be that you get ten items and then the 11th is after 2 seconds of latency. Or it might be the case that they're all in the same commit set. So the 11th is actually already there.
00:57:04.266 - 00:57:04.580, Speaker A: Right.
00:57:04.650 - 00:57:24.760, Speaker C: This is not a very helpful abstraction for a system designer that actually cares about performance. So the way I see it is instead our consensus algorithm provides a sequence of sets of transactions and within each set you have the same latency, effectively. And now the higher level system is free to reorder those because reordering is free within that set.
00:57:24.830 - 00:57:25.112, Speaker A: Right.
00:57:25.166 - 00:57:38.272, Speaker C: As long as you do it deterministically. So one way of reordering is to say, let's take basically the most expensive first, right. And that's for free. And that's one way of doing it.
00:57:38.406 - 00:57:45.340, Speaker B: You're really saying there's sort of two different sequences you care about. One is just kind of like wall clock time and then the other is kind of actual execution in the virtual machine.
00:57:45.420 - 00:57:45.952, Speaker C: That's right.
00:57:46.006 - 00:57:53.108, Speaker B: You're saying the consensus is sort of like more concerned with the wall clock time. Transactions in a single block are kind of tied with respect to that.
00:57:53.194 - 00:57:53.604, Speaker C: That's right.
00:57:53.642 - 00:57:55.120, Speaker B: And then the ties are broken.
00:57:55.280 - 00:58:02.564, Speaker C: You can break the ties in many way. We do it in the simplest possible way. All good?
00:58:02.602 - 00:58:02.996, Speaker A: Yeah.
00:58:03.098 - 00:58:10.340, Speaker E: So is it up to each validator to decide how they do the sequencing within sets or is that specified by the overall protocol?
00:58:10.420 - 00:58:42.276, Speaker C: It has to be specified by the overall protocol. Right, because it has to be deterministic eventually, when we reach the assignment of transactions to shared object versions, all the validators have to agree. So to some extent, the way you break the ties and the way you actually eventually reach a sequence between shared object has to be agreed amongst all validators. That's the state machine replication paradigm. So the discretion is the system designer's discretion on how you take those commit sets and come up with a sequence. But everybody has to apply that. Cool.
00:58:42.276 - 00:59:18.830, Speaker C: All right, so this is the shared object path. Now, besides assigning shared object locks, which we do in order to do execution on shared objects, consensus is absolutely necessary for checkpoints and reconfiguration. For checkpoints, what we do is we take all the certificates, we stick them basically in a block. But because things are asynchronous, it might be the case that some certificates that get into the block depend on other certificates that exist, will eventually be processed but are not yet available.
00:59:19.220 - 00:59:19.776, Speaker A: Okay?
00:59:19.878 - 01:00:17.964, Speaker C: So actually, checkpoints have to ensure, and we do ensure that there is actually a complete sequence of execution. And we also ensure that the checkpoints order certificates in such a way that if you were to execute them all sequentially, which many of our full nodes do, because they're not super sophisticated about dealing with these things, we want to make it simpler for them. Right. You actually end up in the correct state of the database, and since things happen asynchronously both in the consensusless path and also in the execution path, this is not guaranteed, actually. So we actually have to do some more work there to make sure that the checkpoints are kind of nice and clean as if it was a traditional blockchain, because lots of people want to consume that. So we need some theorems here that are not trivial, such as if a certificate is sequenced, we need to ensure that eventually all the certificates that it depends on are also sequenced, otherwise we would never be able to make checkpoints.
01:00:18.012 - 01:00:18.704, Speaker A: Right?
01:00:18.902 - 01:00:32.208, Speaker C: And we also need to ensure that eventually all final transactions, even if they are final through the fast paths, that is not relating to the consensus, if you remember, do make their way into checkpoints.
01:00:32.384 - 01:00:32.724, Speaker A: Right.
01:00:32.762 - 01:00:51.496, Speaker C: So again, this is not a trivial property if you have fast paths. Cool. Okay, I seem to have missed a slide. I apologize. I see checkpoints. That should be a slide on reconfiguration. I apologize.
01:00:51.496 - 01:01:09.872, Speaker C: I copied the title. So that's the final component of the system that I want to talk about. It's not checkpoints, it's reconfiguration. So finally, we want one last thing from the whole of our blockchain, which is we want to be able to change the set of the validators periodically, once a day.
01:01:09.926 - 01:01:10.192, Speaker A: Okay?
01:01:10.246 - 01:01:17.332, Speaker C: We call that period, an epoch within the epoch we have the same set of validators with the same stake across epochs. We want to be able to change it.
01:01:17.466 - 01:01:18.150, Speaker A: Okay?
01:01:19.080 - 01:02:11.060, Speaker C: Now what we do for this is that eventually validators stop signing new transactions. That means that eventually new certificates stop being formed. And then what we need them to make sure is that when they basically see all the certificates that they have received being sequenced and being executed, they basically say I'm ready to close the epoch. And we have a theorem that says that once we have two thirds of validators by stake that vote that they want to actually close the epoch, we can close the epoch. And all things that were final in the epoch will eventually make it into the sequence, will be in checkpoints. And therefore when we open the new epoch from the database that results from these checkpoints and that sequence all the state will carry through and that gives us safety across the epochs for reconfiguration. Okay.
01:02:11.060 - 01:02:19.400, Speaker C: And this is basically an overview of all the components of the system and how they interact with each other.
01:02:19.470 - 01:02:31.724, Speaker B: So just on that last point, if you have a certificate for an epic it's guaranteed to be in the checkpoint that concludes that epoch or does it potentially carry forward to the next?
01:02:31.762 - 01:02:58.500, Speaker C: That's correct. So the guarantee is if I get finality for a transaction within an epoch, I am guaranteed that the transaction will appear in a checkpoint within that epoch. And that's it basically because if it didn't, it wouldn't carry over to the next one and therefore it's not finality.
01:03:00.760 - 01:03:04.212, Speaker E: So I guess for that last point about Liveness is like the concern that.
01:03:04.266 - 01:03:10.136, Speaker C: If a transaction grabs a lock but then fails and doesn't properly move the.
01:03:10.158 - 01:03:13.880, Speaker E: Object to some other owner or something, then you can't actually touch that.
01:03:14.030 - 01:04:01.384, Speaker C: Excellent, excellent question. Thank you for your question because I was about to move forward and forget about it. So upon change of epoch something else that is very important happens which is we reset all our locks in the owned object locks database. So that means that if your client is buggy you have an object at a particular version and you create two conflicting transactions, none of which manage to get a certificate within the epoch your object is locked right in FastPay. Your account would be locked forever. But here because we have as a point of synchronization the end of the epoch where everybody is guaranteed to have exactly the same state, everybody being the validators, we can basically pop and delete all of these logs and start anew for the next epoch. Which means that a bug will cost you a day.
01:04:01.384 - 01:04:11.420, Speaker C: It's not great, you're not going to throw a party about it, but it's not going to cost your fortune forever. Which is something that was actually concerning. Is that answering your question?
01:04:11.570 - 01:04:12.220, Speaker A: Yeah.
01:04:12.370 - 01:04:18.392, Speaker C: Thank you very much. Yeah, that's actually. A very important thing from the standpoint of a client.
01:04:18.536 - 01:04:24.508, Speaker E: How should I think of a fast transaction in the same epoch?
01:04:24.524 - 01:04:25.104, Speaker C: I think it's easy.
01:04:25.142 - 01:05:09.632, Speaker E: I see a two third certificate, I'm done, right? But today, I guess when you order everything, then I think the way I see a transaction is if it's a single owner transaction, single owner object, then I see that the transaction happened. You paid me money and then there are no transact. So when you're paying me money, I'm just trying to figure out if you already own it. And that is you haven't paid it to someone else in a sequence. So that is what you check for. But in this world you have this two third certificate. But once you have reconfigured multiple times as a client, do I need to know who the right set of validators were at that time so that I can check, hey, the two thirds are actually the two third set of validators that on that particular day?
01:05:09.686 - 01:05:55.310, Speaker C: No, not at all. Not at all. So within the epoch, if I'm the creator of a transaction within the epoch, either I see two thirds of acts from validators or two thirds of effects, and then I'm convinced that it's either going to happen for sure within the epoch or it has already happened within the epoch now good. Within the epoch. Now within the epoch, I can also transfer the certificate on the effect, the second point of finality to anyone else and they can convince within the epoch that the thing is done. So if I want to pay you to buy some shoes or something, right, I give you basically that certificate. You're like, okay, great, I know this thing has happened even without looking at the chain, I'm good, it has clicked on my account.
01:05:55.310 - 01:05:57.580, Speaker C: You give me the shoes. That's it.
01:05:57.650 - 01:05:58.124, Speaker A: Right?
01:05:58.242 - 01:06:44.124, Speaker C: Now, across epochs, we don't rely on cryptographic evidence of certificates from previous epoch. What you do is you have to follow basically the reconfiguration of the committee going forward. So when we reconfigure, the old committee signed the new committee. So you have effectively a chain of new committees. You make sure that you're in the last committee, the current committee of the current day. And then you go and ask basically you're like getobject literally get object at that version or get object by owner and you have two thirds of the validators telling you, oh, yeah, this object ID comma version belongs to you. And then you know it belongs to you.
01:06:44.124 - 01:07:17.880, Speaker C: You don't have to go back and ask. Or if you want to be super certain, which is an auditing function, it's a full node function. You literally take that last certificate, sorry, this last committee, but you also take the full sequence of all commands in the system and you can re execute them. Right, but that's like a full audit of a full node. Yeah, cool. All right, just to say, yeah, we implemented all this. The whole suite is a big system.
01:07:17.880 - 01:07:58.784, Speaker C: It's 250,000 lines of code. The actual mechanisms that I described is only about 30,000 lines of code. It's not like huge. Actually, now that we know what they should look like, we should be able to have them being like a kind of summer project for a student or something. But when you don't know what you're designing, it always takes you a long time and a lot of lines of code to actually work it out. It's about 8400 commits and about 70 people worked on all the different aspects of the system, including all the researchers that actually designed the system. So all of our researchers basically were designing and coding and doing all the kind of QA activities.
01:07:58.784 - 01:08:50.388, Speaker C: So everybody was involved. We had three testnets, and as part of our testnets, just the last testnet, in fact, in April, we basically threw millions and millions of transactions and objects at this system just to work out all the kinks and debug it. And we induced very large peaks of 130 to 200,000 transactions in transaction blocks. I haven't talked about transaction blocks. Peaks of about 10,000 transactions, single transactions, single certificates, as I describe here. And since 62 days now, or maybe like 50 days more like it, we have a main net and we also have about 12 million transactions and about 10 million objects in the system. So it's doing the job that it's meant to be doing.
01:08:50.388 - 01:09:27.888, Speaker C: And all of that software is built so far to do parallel stuff, both the execution and the checks and all that stuff. But within one machine, on different cores, right? We light up all our cores. And of course, we are waiting for the day this supply of transactions exceeds the demand for transactions, exceeds what we can supply to actually move to many hosts. We're not quite there yet. Okay, now, there are many aspects of the system I haven't talked about. I just want to call that out. Don't assume that because I haven't mentioned something or because I have simplified something, we don't do it.
01:09:27.888 - 01:10:08.684, Speaker C: We have many other features. One feature that I think would be interesting if people want to chat about it over lunch or something is programmable transaction blocks. I have described how you can parallelize operations on different objects. So if you own 100 objects, you can do transactions in parallel. And all these hundred objects you understand that this is the topic of the whole talk. But what if you want to do transactions that really depend on each other? You want to do 100 transactions, all have dependencies on each other. In theory, you would take delays of a few seconds every so often, right, to transmit the transaction, wait for certificates, maybe consensus, and then the next one, et cetera, that will take you minutes.
01:10:08.684 - 01:11:04.908, Speaker C: So we actually offer a primitive called programmable transaction blocks that allow you to literally put them all in one block and run the whole thing through, which is a very powerful kind of like scripting primitive for blockchain. So I'm happy to chat with you about it and anything else here. And in terms of performance, what we actually have at the end of all this is less than 500 milliseconds for owned object transactions. So this is quite remarkable. We can basically do smart contracts on objects that you own yourself for less than a second. Payments, inventory management, transfers and all of that at very low latencies. We actually observe like hundreds of transactions per second if you put many of these owned objects in blocks as well, or about 10,000 if you just have one transaction per block in the chain.
01:11:04.908 - 01:11:49.150, Speaker C: And for shared object transactions, we basically largely inherit the performance of the consensus. So we observe about two to 7 seconds of latency. You still get transaction finality after like less than a second, but then you have to wait for the actual execution to happen. So that's an interesting thought there. You can basically turn off your computer and go away in less than a second, but it's going to take a few seconds for the effects to actually manifest. And our performance again for shared objects is roughly the same. It's about seven K TPS for single transaction transaction blocks and it goes up basically as we have more of them.
01:11:49.150 - 01:12:17.770, Speaker C: And that's it. In conclusion, the Swee Foundation actually has research grants. Some of you may just be interns and your research groups might be interested in funding. So do actually send us a short proposal. If you're looking to do projects that relate to the Sui ecosystem, we'd be very happy to support you. And in conclusion, production systems basically are never neat. This is one thing that I want to really highlight from this.
01:12:17.770 - 01:12:44.112, Speaker C: Am I satisfied at a kind of abstract level about how the system looks like? Namely, it's this kind of mishmash of two different systems and all the complexities in order to actually try to get the best of both worlds? No. Did I become a scientist in order to design such complex and ugly kind of like mishmashes of systems? No.
01:12:44.246 - 01:12:44.832, Speaker A: Right.
01:12:44.966 - 01:13:33.724, Speaker C: But actually in reality, people want this. This is what real systems look like because some people want to do really low latency transactions and some people want to do transactions that look like a Dex and they also want to mix them. So a model like FastPay where you have a side chain and then you bring them back is not good enough. Right. So real systems are messy and in particular, when you take real systems and you have to add features, robustness scaling and performance, they become even more messy. But at the same time, dealing with that messiness without compromising on safety and also performance is a fascinating area of research. So I actually have no regrets that despite the fact that my intellectual purity would not find appeal in these kind of systems.
01:13:33.724 - 01:13:53.540, Speaker C: I'm actually very glad that I'm working on them because the wealth of problems that you find is unmatched by the problems you try to solve when you have a more clean theoretical model of what you're trying to do in your system. So I encourage everyone to look at Sui and other systems and actually engage with that messiness.
01:13:54.520 - 01:14:23.472, Speaker B: So, bit of a philosophical question, but when you said people want this, who are the people? How did you determine when building a system, right, that obviously didn't have any users at the time you were building it? What people actually want, like the complex design of Ethereum can sort of be explained by they built a relatively simple thing and they've kept adding stuff to it based on how people were using it. Whereas you're more right building a whole.
01:14:23.526 - 01:15:15.552, Speaker C: System from yeah, no, that's a good point. So it is actually a thesis of ours that effectively there will be demand for blockchains. Ultimately, I think it's a thesis that we all share here that to some extent, if blockchains are useful, they will be popular in terms of the volume of transactions that you will try to do. And it is to some extent a further thesis that the simplest transactions actually take a lot of the block space. Like transfers actually are quite prevalent in blockchains. I mean, this is how we basically came to that model. We looked at other blockchains and we're like, what are the common types of transactions? And really there are two types of transactions that are common.
01:15:15.552 - 01:15:58.424, Speaker C: There is the kind of simple transfers, be it in Ethereum, ERC, twenty S or Native or whatever, and Bitcoin is all transfers. So there you go, you have a simple chain that is all about transfers. For a while, Cardano and others were also largely just transfers. And then you have these other transactions, usually DEXs, that are basically kind of shared object transactions. And these are the two kind of things that happen in the systems. So we're like, if we can basically really nail the latency of the transactions where we can the transfers at least and expand a little bit the reach of that model, then we have an edge.
01:15:58.472 - 01:15:58.780, Speaker A: Right?
01:15:58.850 - 01:16:25.750, Speaker C: This is how we came to this. And the second way we came to this was literally, I mean, for the Fast Pay work in particular, that was the vision of Facebook Novi, that they're going to do just transfers. That was all basically DM was doing at the time at a phenomenal scale, at the billions of user scale. Now, this didn't play out right, but that was the client ask.
01:16:28.830 - 01:16:29.554, Speaker A: Yep.
01:16:29.702 - 01:16:34.030, Speaker D: Can you speak a little bit about the requirements on the validator? Can you run a validator of a laptop?
01:16:36.530 - 01:17:37.362, Speaker C: No, you cannot run your validator from a laptop unless you have one of these Mac M two S, at which point you should run your validator from your laptop, right? No. What I mean by that is we never really designed the system to run the validator from a laptop. If your system does a transaction rate, like a few transactions per second or ten or 20 transactions per second, I'm talking about bitcoin ethereum kind of levels of volumes, then you can run it from your laptop. It's not a problem at all. You need a modern laptop. You would have zero problems doing that. If your system does the thousands to 10,000 certificates per second or hundreds of thousands of transactions within those certificates, if you want, then you do need to have 24 cores and you need to have I mean, it's not just the compute that is the problem.
01:17:37.362 - 01:18:03.980, Speaker C: You need to have one of these NVE disks that offers very low latency and you need to have enough bandwidth to replicate that volume. Right. So that's the place where I'm like, we're not designing the system to run on a laptop because once it graduates from being a small capacity system, you have to process that data.
01:18:04.510 - 01:18:17.934, Speaker D: Even full nodes will have to. One more question about when you're saying the user collects certificates from the validators, how does it practically work?
01:18:18.052 - 01:18:21.726, Speaker C: Yeah, very good question. Yeah. So when I present the protocol, I present it abstractly.
01:18:21.758 - 01:18:21.906, Speaker A: Right?
01:18:21.928 - 01:18:40.422, Speaker C: There is Alice. She sends certificates to everybody. She gathers certificates. In practice, what happens is that our full nodes effectively act as a gateway as well. Not ours. Any full node acts as a gateway. So as a user, you pick a full node that you usually do business with, or one or two or whatever, and you send your certificate, your transaction there.
01:18:40.422 - 01:19:04.800, Speaker C: The full node basically deals with the protocol because once the transaction is signed, the rest of the process doesn't require any secrets anymore from the user, so it can be driven independently of the user. So we delegate that to full nodes that go do all the communications, gather certificates, and eventually give them back to users, and the user doesn't have to worry about that.
01:19:08.720 - 01:19:15.424, Speaker B: The validator set. So is there like a bound on how big that can be, or is there a minimum sort of stake amount?
01:19:15.622 - 01:20:03.884, Speaker C: So we do have a minimum stake amount. It's about zero point 25% of the stake. So that means that the theoretically largest possible set we could have is 400, right? Right now we're operating at a bit over 100. I have to say the protocol is not overly sensitive to the number of validators, so it extends to hundreds. I think extending to thousands is really about networking efficiency, et cetera. You would have to start doing different things at a networking layer to make sure you can basically talk to each other. But even though from a theoretical point of view, there is a kind of N squared per validator cost that goes up as the set N grows or N cube.
01:20:03.884 - 01:20:48.540, Speaker C: I guess for the whole system. The constant in front of that is so small in comparison with actually shifting data around that we don't really see it per se on the set. One thing that we observed that was actually much more worrying initially was the fact that since you do full replication you need to replicate to all the full nodes, for example. So initially we didn't optimize these kind of read paths and that actually led to more worries until we actually created distributed ways of sharing all that data in a kind of more hierarchical way. And now that traffic never touches really our validators.
01:20:53.520 - 01:20:59.928, Speaker D: How do you differentiate between the owned object and shared object? Is it like a move language construct?
01:21:00.104 - 01:21:18.864, Speaker C: We have a flag in the database, literally our database has a record for an object of a particular version. And in there you have an owner field that can either be owned by a particular owner or it is shared and that's it.
01:21:18.902 - 01:21:22.612, Speaker D: So the user, when it creates an object, it identifies which type it is?
01:21:22.666 - 01:21:48.452, Speaker C: No, it's the contract. Right? So all objects are mediated and created by contracts. They are proper move types and all that stuff. So the move contract creates by default owned objects and then you have a kind of like make shared function that lifts it into being a shared object and that sets the flag and there is no going back. There's no going back. You can delete the object and create another one if you want an owned object but the same object cannot go back into being owned.
01:21:48.596 - 01:21:49.448, Speaker B: Thanks for us again.
01:21:49.534 - 01:21:51.930, Speaker C: Thank you so much and thanks for your questions.
