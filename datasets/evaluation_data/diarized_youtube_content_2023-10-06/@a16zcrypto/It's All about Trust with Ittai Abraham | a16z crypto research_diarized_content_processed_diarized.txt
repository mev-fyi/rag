00:00:07.500 - 00:00:08.050, Speaker A: You.
00:00:10.020 - 00:00:39.930, Speaker B: All right. Welcome everyone to Today's, a 16 Z crypto research seminar. Very pleased to introduce itai Abraham, who I've known for a very long time. You might know him as the sort of lead author of the Decentralized Thoughts blog. He was a PhD back at Hebrew University, spent a lot of time at Microsoft in the Silicon Valley, which is where I first met him. Then he was at VMware for many years and he recently moved to intel as a senior researcher. Today he's going to be telling us about why it's all about trust.
00:00:40.460 - 00:00:56.396, Speaker A: Thank you. Thank you for inviting me and thank you all for coming. My talk. Yeah. So I'm going to start very high level and then slowly we're going to zoom in into gory details. So let's start. So really, you know, it's all about trust.
00:00:56.396 - 00:01:46.910, Speaker A: What is trust? And it's kind of an elusive thing and that's kind of the reason why I'm really interested in looking at it from different angles. And of course, the most important angle here is cryptography. And this is this kind of phrase I took from Boaz Barak, which is that kind of cryptography is a way of transforming trust with mathematics. And this is one area, obviously, that we're all interested in. And the other one, which I care deeply about, is distributed computing and kind of the intersection between distributed computing and cryptography. And really there the interest is how do you kind of replace a centralized model of trust with a decentralized one? And these are two really kind of fascinating topics and I think there's a lot of interleave between them. But obviously kind of the third thing that I'm really interested in is the economics of things.
00:01:46.910 - 00:02:58.432, Speaker A: The way at least I interpret how economics thinks about trust, again, if I had to just do it in one sentence, is because it replaces the assumed model of trust of good guys and bad guys with this kind of question of how can we incentivize cooperation, right? And so this is I guess the reason I'm excited about this area is because of the combination of all these three. And I would say more recently, what I've kind of come to realize, it's not just these three, it's also this topic of governance. And I guess to me it's governance about how we as a society make decisions and make sure that these decisions are kind of upheld over time. And in a sense, at least, my opinion is that a lot of the disruption of the intersection of these three things is going to also go into governance. And here's one quote I really like from a talk that Vitalik gave. He says, Bitcoin and Ethereum really are digital nations. And so it kind of makes you think about what are nations and how in this new world, how will they evolve and become okay, so here's this again, these four things cryptography, distributed computing, economics and governance.
00:02:58.432 - 00:03:41.136, Speaker A: And I guess my thesis here is where do blockchains? We've talked a little bit about trust, so where do blockchains kind of come up? And at least my high level view is that blockchains are kind of mechanisms for trusted coordination. So it's a mechanism, it kind of tries to incentivize things. It creates this coordination. So this is the way of replacing trust and really for it to be viable for a large population, it actually has to work at scale. And so what we'll talk about today is how to do this at scale, I guess. But before we do, I kind of want to start with a little bit of more high level overview. So this is some quotes I really like from Yuvan No Harari, from his very famous book.
00:03:41.136 - 00:04:40.900, Speaker A: It's basically, if you think about what a blockchain is as a mechanism for coordination, well, it's a story that a society kind of, right, it's some fiction that a large group of people believe in. And this is kind of from Yuvaza point of view, this is maybe the most important thing or the biggest superpowers that we as kind of humans have over other entities. It's the ability to have kind of a joint common story. And in a sense at least, I think that blockchains are kind of a way of providing a much more robust story around trusted particular. You know, Yuval talks about this notion of money, right? And money as the only trust system created by humans that can bridge almost any cultural gap thanks to money. Even people who don't know each other and don't trust each other can nevertheless cooperate effectively. And I think that's really kind of an amazing feat of humans how we can kind of collaborate and work together and again at scale.
00:04:40.900 - 00:05:37.208, Speaker A: Okay, so what's this talk going to be about? So this talk is basically going to be about these three parts, right? So there's going to be an introduction about a mental model, about how I understand blockchains specifically in the context of how to do scale them, how to make them scale and work with high throughput and so on. And what I'll talk about is this amazing flywheel that I'll call this decentralized flywheel, where you have ideas that are coming from both theory basically mathematics, right, and systems. So academics are building systems and real systems and blockchains and they're kind of contributing each one to the other. And some of the reasons I'm super excited about this type of model is it's completely permissionless. So you'll see contributions from many different people with many different types of backgrounds. You don't have to have a PhD, you don't have to be a super engineer. There can be many different types of contributions in this kind of process.
00:05:37.208 - 00:06:22.976, Speaker A: And that's to me really fun about it. And secondly, as somebody who spent a lot of efforts also looking at the theory side, this is really like a fun area in the sense that theory matters here quite a bit. So if you look at, say, how databases work, or how operating systems work, or how the cloud works, well, there is a little bit of theory and mathematics into that. But in this space there's a lot more. And that's really exciting for me. Okay? And what we'll do is we'll start with this mental model for blockchains and then we'll talk about these three separate or they'll be connected and depending on the amount of time and how much questions you have, we'll kind of move through them. But one is about consensus and the other one is about kind of Dag based protocols.
00:06:22.976 - 00:06:55.504, Speaker A: And finally we'll talk about layer two solutions. Okay? There's kind of this obvious flywheel that I won't talk about, which is zero knowledge proofs. But again, it's a huge kind of way where theoretical ideas, systems and real systems are kind of helping each other. And again, it's really important to note that it's not just a one directional path, right? Theory is helping systems and systems helping. Theory and blockchains are helping systems and systems are helping. So this is really fascinating in that sense. Okay, so, yeah, let's start.
00:06:55.504 - 00:07:14.864, Speaker A: And before we start, I'll just give you kind of a very high level overview of how you see this whole area of blockchain technologies. You've probably seen this many times. It's kind of this layered view. The bottom layer is basically doing state machine replication. If you know this picture, raise your hand. Good. And the second one is about execution.
00:07:14.864 - 00:07:59.540, Speaker A: This is where your smart contracts are running. This is kind of the engine. You probably know this picture, right? The third one is basically the applications themselves, right? So what type of applications you want to run? And the fourth thing is basically everything that's off chain, right? And so in this talk I'm going to focus on the two bottom layers, kind of the core aspect. But it's really important to understand what are the use cases and what is actually the workload. Because you can't create high scalability in the air, you want to understand what is your workload, right? So actually we're going to go a little bit back to thinking about these layers. But anyway, our focus is going to be on these two bottom layers for today. Let's start with some semi formal definition of Byzantine faltar and state machine replication.
00:07:59.540 - 00:08:35.120, Speaker A: And we'll kind of go over this and if you have questions, I'll kind of give you a little bit more. But I will not do it completely formal. Right? So I guess the high level idea here is that in the ideal world we'd like to basically build a system where you have clients and these clients are just talking to some ideal system. This ideal system never fails, never has any problems. It's always online, it's always honest and there's absolutely no problems about it. If you think about it, that's how you treat, say, your laptop or the cloud or your bank account, presumably. Right.
00:08:35.120 - 00:08:59.124, Speaker A: So something that never fails. Right. And the problem is that more and more in the world that doesn't really adhere to what happens in particular, there's these three problems that happen, right. So one is a lack of safety. So sometimes these systems don't give you a consistent state or don't give you a consistent state relative to other people. That's kind of one big problem. The other big problem is liveness.
00:08:59.124 - 00:09:22.048, Speaker A: Right. Denial of service. We've talked about kind of a purely safe couch, right. That doesn't make any changes. That also is something that we don't want to do. And more often than not, we actually want to think about state machines that actually have some sort of private state. And these are kind of secrets that are held inside the state machine, and we also want to make sure that those don't get leaked out.
00:09:22.048 - 00:10:00.616, Speaker A: In some contexts, we'll do that. And this is essentially what the holy grail of this type of technology state machine replication does, is it allows you to build systems through replication, such that even if there's an adversary, and this adversary is controlling some fraction of your servers okay, let's say in particular this number f here, and there is some fraction of these servers that are compromised. So the adversary can control some of them, but not all of them. Again, it can be one third, one half. It depends on the kind of the type of model and protocol. You still want to build a system that, from the client perspective, looks like an ideal system, right. So they get safety and liveness and privacy.
00:10:00.616 - 00:10:50.824, Speaker A: And kind of my thesis here is that providing this type of service is essentially this mechanism of trusted coordination. So this is essentially what if you want to create a mechanism for trusted coordination, this is kind of what you need to provide. Yeah, this is kind of the high level overview of how I think about a blockchain and its scalability challenges. So there's basically four phases, and you can see I've kind of played around with the names of these phases, and maybe you have better ideas about some of these names. But roughly speaking, the first phase is just the phase where the clients have to send whatever they want, the transactions or the operations to the system. So we'll call all this phase the input phase. Okay? So all the problems of making sure that these requests are persisted.
00:10:50.824 - 00:11:30.680, Speaker A: Right. So the data availability are here, the problems of what happens if there is two requests and I can sandwich in between them. So mev, this is all inside kind of the input phase. The next phase, once we get this input, we want to order these transactions. And the reason we want to order these transactions often is because we're replicating the system. And in a replicated system, it actually is pretty important that we all agree on the same order. The reason I used to write here that this is a kind of a consensus box and not an ordering box is because kind of consensus implies total ordering, that we're just going to order all the transactions.
00:11:30.680 - 00:12:04.736, Speaker A: But more recently, we're seeing systems where you just need to order the things that need ordering. And let's say there are two transactions that can commute between them. It's fine to not completely order them. So that's kind of why I'm calling this an ordering phase. Once we do that, now we're actually doing the actual business logic, right? This is the value that we're creating. So the value is actually executing these transactions, right? So this is the execution phase and again, it's a nice way of separating between these parts. And finally, once it's kind of executed, then you want to store it in some way that's persisted.
00:12:04.736 - 00:12:44.128, Speaker A: So if somebody wants to read it, again, the output exists. So in particular, we talked about kind of some of the challenges here. So the nice thing is that you can really separate out your systems and think about these as four separate challenges, right? And by kind of optimizing each one of these things, you can look at your system and see where are your scalability bottlenecks, right? What part is blocking you, and try to kind of fix each one of these in some sense separately. So this separation of concerns has been kind of very valuable. Yeah. So we talked about kind of the input phase. The execution phase is again very important.
00:12:44.128 - 00:13:36.048, Speaker A: There's a lot of work on different programming languages and how to run things in an efficient way. And what we'll see in the third part of this talk is really that in some sense the biggest challenge of scalability is getting better execution. And so we'll focus on that in the third phase. The fourth phase is basically about recording the information and recording it in such a way that's authenticated, right, that people can get a response that is trusted. And so all this work on authenticated data structures and vertical trees and so on is all going kind of into this fourth space. Okay, what I want to start with is actually just the consensus problem because in a way if you kind of ignore this consensus problem, then you just have a regular computer, right, that it. Just transactions arrive, you have to execute it and they have to kind of record it back.
00:13:36.048 - 00:14:45.168, Speaker A: And so this is kind of the part about consensus. And if you ask people about consensus, in particular the Byzantine fault tolerant version, maybe in 2015 or 16, they would say, well, yeah, you can do it, but it's very, very slow, right? And there was kind of this perception that it's not scalable and very inefficient. And I guess now there's kind of maybe an opposite understanding that in some sense if you do the right things and the right choices, this is not going to be your bottleneck of your system. And okay, why is that? So the first thing to kind of talk about is this latency versus throughput trade off when doing consensus, right? So in a sense, consensus is this distributed protocol that requires some sort of agreement between all the servers. However, you don't have to do this on each and every transaction, so you can batch things. So if there's a lot of requests coming, then you can basically decide that let's say every 10 minutes you'll batch another block, or every 17 seconds you'll batch, or every 20 milliseconds. And by batching this, what you're doing is you're basically trading off higher latency, right? Because when you're batching a transaction, then it's actually not being committed immediately.
00:14:45.168 - 00:15:23.296, Speaker A: It's waiting for a few seconds, a few milliseconds. But the hope is that now you're actually going to spend less efforts because you're doing consensus less often. So if you had to do consensus every ten milliseconds, that would be maybe expensive. If you're doing it once per second, it's not that expensive. And so what you're doing is you're basically reducing the amount of times you're running consensus and you're increasing your latency. But the good side of this is that now you're kind of increasing throughput as well by basically doing less consensus. The extreme version of this, which is basically somebody says, I have amazing throughput and here's how to get amazing throughput.
00:15:23.296 - 00:15:49.612, Speaker A: You basically do consensus every hour, right? And you will get amazing throughput. And yeah, maybe once an hour you have 100,000 people that have to run a protocol that takes 2 minutes. It's still not that bad. I mean, it's 2 minutes out of your hour. That's not going to take a lot of effort. And basically your throughput is just going to be bottlenecked by how fast you can input data, how fast you can execute it, and how fast you can output it and record it. Okay.
00:15:49.612 - 00:16:23.750, Speaker A: Depending on how that works. In a sense, be very careful about just talking about throughput because then it's very easy to get very, very high throughput. However, if you really want to, say run consensus every few milliseconds, then yeah, then it's expensive. And obviously the expense is a function of the number of replicas that we want to do right. And there are, again, lower bounds and we'll talk about them, but roughly speaking, the amount of messages that need to be sent is kind of quadratic. At least that's the worst case. Yeah.
00:16:24.200 - 00:16:36.404, Speaker C: I can think of two versions of this separation. One is that there's one system and you're just thinking of the older aspects separately. But another is like there's just literally different systems. There's one system that does the storage, one system that does the input.
00:16:36.452 - 00:16:55.488, Speaker A: One many of the modern systems that have very high throughput essentially have queues between these systems and they do work and they move it to the other system. And so you can really think about them as a factory. Right. And you finish this thing and it goes to this other factory. So it's like a pipeline of things that you're doing.
00:16:55.574 - 00:16:58.752, Speaker C: Can you think of consensus as a service? Like take me?
00:16:58.806 - 00:17:10.996, Speaker A: Yeah. All of this exists. All of this exists. So all of these these are all narrow wastes. It's very easy to replace this consensus and put something else replace the execution. This is why this is a very nice architecture. Yeah.
00:17:10.996 - 00:17:32.312, Speaker A: So you can definitely let's say you build a different the APIs in between here are very narrow. Right. So if you have a better data availability solution, you can kind of replace it and put it back. Right. If you have a better execution engine that runs a better smart contract language, you can do that as well. Can you comment on the trade offs between having this kind of modularity versus.
00:17:32.376 - 00:17:36.910, Speaker C: A monolithic design that tries to do all of them at the same time?
00:17:39.860 - 00:17:59.348, Speaker A: This monolithic versus is maybe loaded. So I would say there's this software architecture. Right. And I think that from a software perspective yeah. You can build amazingly good monolithic stuff. And also you can argue that it's easier to build these things separated. Right.
00:17:59.348 - 00:18:32.336, Speaker A: So if you have separate teams yeah. I would say it's nice to understand that there's a logical separation. How you build it is not necessarily physically separated. Right. I mean, I've seen amazing systems where everything is built together and amazing systems where everything is separated. And I use amazing in a good way, but you can also take it as other ways of seeing amazing. Yeah.
00:18:32.336 - 00:19:19.196, Speaker A: Any more questions about okay, so what we'll kind of do is in this first two phases is talk about this consensus part and why it's kind of not so bad as people think it is. Again, sorry. Yeah, I remember I forgot one thing I want to say. So the really important part here related to kind of separation of concerns is that when you're running consensus, right, even if you have a day's worth or an hour's worth of transactions, what you really are doing consensus on is just a digest of all that work. Right. So you're agreeing on a cryptographic digest of all that work. In particular, this implies that this input layer gives you kind of this nice data availability property.
00:19:19.196 - 00:19:42.628, Speaker A: So you know that if you have this digest, you know that you can for sure access the data. But the consensus itself doesn't need to see the data. It just needs to look at the digest. Sorry, I should have said that earlier. That's what makes it really scalable in the sense that even if we're taking a day's worth of transactions, we're running consensus on it. It's still just on one hash. Okay.
00:19:42.628 - 00:21:00.504, Speaker A: Before I talk about anything else, we have to mention Bitcoin and the Kamoto consensus. So I'll do this really, really quickly. You've probably heard me talk about this in other contexts and this is not kind of the core thing I want to say, but there's kind of this important separation in my view about who gets to vote and how you take these votes and decide about consensus. In particular, in Bitcoin, the decision about who gets to vote is proof of work and the decision about, given these votes, what's going to be the winning transaction. That's longest chain based Byzantine fault torant protocol in the synchronous model. And that's very different than many modern blockchains where the way that you decide who gets to vote is typically through proof of stake and the way where when you take these votes and you aggregate and decide how things are committed, you typically use kind of quorum based Byzantine Falter and systems in partial synchrony. For me at least when I use this type of language, then kind of saying comparing between proof of work and quorum based protocols, I know what that means, right? But for me these are like two incomparable things, right? So proof of work is a way to decide who gets a vote and a quorum base is basically how those votes get translated.
00:21:00.504 - 00:21:49.352, Speaker A: In particular, you can run a quorum based protocol with proof of work if you really, really insisted on it. And similarly, longest chain versus BFT, at least from my perspective, longest chain protocol is a Byzantine fault tolerance state machine replication protocol. But again, I understand kind of the way this is not used using these names. Okay, I just want to say one word about why this talk is going to focus on kind of the quorum based BFT. And before that, I just have to say the amazing thing from my perspective again, in hindsight about Bitcoin and from my perspective at least, first of all, it solves Byzantine faltar and state machine replication, right? That's kind of in the synchronous model. So it does everything that we wanted it to do. And the really, really amazing thing is it does this without a public key infrastructure and without authenticated channels.
00:21:49.352 - 00:22:28.110, Speaker A: And if you think about that, that's for me at least mind blowing, right? The fact that you can do this thing without having any trust assumptions, without having any previously set channel, without having any public key set up in advance with a very minimal setup. And that's amazing and in some sense unfair to compare this to these high throughput low latency other systems because they're kind of assuming a little bit more and so they kind of get more. But anyway, to me this is kind of an amazing thing and the amazing power of kind of mildly hard cryptographic puzzles. And that's where I'm going to stop about talking about Bitcoin and talk about quorum based Byzantine fault tolerance. Yes.
00:22:28.640 - 00:22:34.044, Speaker C: Do you think of the ideal hash function as a weaker assumption than the.
00:22:34.242 - 00:22:47.292, Speaker A: Other cryptographic assumptions you'll see that in all these protocols you do assume there's a public key infrastructure, that there's a public key for each of the replicas. So that's the important part. The important part is how do you build a public key infrastructure?
00:22:47.356 - 00:22:49.852, Speaker C: No, but bitcoin assumes like an ideal Hashman.
00:22:49.916 - 00:23:06.100, Speaker A: Yeah, sure. That's another nice really cool thing about how much cryptography you're using. But from my perspective this is the mind blowing part. The fact that you can do it without these assumptions. If somebody would ask me, I would say no, it's probably impossible.
00:23:07.160 - 00:23:22.812, Speaker B: In other words, it's not really when you talk about PKI, it's not the cryptographic assumptions you're emphasizing, it's the setup assumptions. So the fact that somehow and who knows why somehow this work was done before the protocol even started, it happens to be cryptography, but that's not the main point.
00:23:22.866 - 00:23:43.684, Speaker A: Right, exactly. Yeah. You could imagine in some sense even from a kind of a trust liability perspective right. You just don't have to set up anybody with a public key. And in all these other systems, well, you start with some set of 100 validators. Okay. That means there was some initial set that started the system.
00:23:43.684 - 00:24:16.204, Speaker A: Okay. There are other things I can say about why this is also centralizing, but not through the setup assumption. Okay. So I'm going to finally start with kind of part one and talk about quorum based BFT and let's talk about some hard things about consensus. So the first thing is obviously in asynchrony you don't know if you're not receiving a message, you don't know if the problem is that the server is down or that the message is delayed. Right. And this is kind of the hard part.
00:24:16.204 - 00:24:49.912, Speaker A: And in partial synchrony you kind of have this case where you want to maintain safety always and you want to make sure that you're making progress during phases where the system is kind of synchronous. And the problem is that you can't tell between each of these. And there's this kind of amazing result by Leslie Lamport in 1988 showing how to solve this. So solving State machine Replication in partial synchrony with emission failures. Raise your hand if you don't haven't seen Paxels before. Okay, good. So I'm going to go pretty quickly.
00:24:49.912 - 00:25:36.104, Speaker A: Right, so it's view based. In every view there is an elected leader and basically what you want to do is you want to maintain safety. And the canonical problem that we're worried about is that there is one leader that basically sends us some value and for some reason this leader sends the value to some parties and then stops working. That may happen and then the next leader coming in will kind of try to commit a different value and that is the canonical problem that Paxos tries to solve. So why isn't this trivial? Well, basically the idea is let's have a leader and the problem is that this leader may fail if this leader may fail. Now the next leader comes in and it might make a different decision. And so to overcome this, basically what Paxos does is two really, really simple things.
00:25:36.104 - 00:26:11.856, Speaker A: The first thing is this idea of lock commit, right? So instead of committing by just sending a message as a leader, I first kind of make sure that what I intend to commit is being recorded on the network. And that's going to be kind of a lock certificate. And the way I do that is make sure that there's at least N minus F people that receive this lock message before I decide to commit on something. Okay? So this is kind of the idea. Before you commit, you lock, you get N minus F locks and that's your confirmation. And we'll call that the lock quorum. And the second part of this solution is what happens when a new leader wakes up.
00:26:11.856 - 00:26:43.740, Speaker A: So when a new leader wakes up, it actually needs to check whether the old leader did something already tried to commit or not. So it doesn't know if the old leader succeeded to commit or not. But it wants to be very cautious. If there was any chance that the old leader committed, it wants to kind of use that same value in order not to break safety. And so the way it's done is very similar. The new leader also reads from a set of N minus F. Okay? And then what the new leader basically is hearing is all the previous locks of the previous replicas.
00:26:43.740 - 00:27:39.890, Speaker A: What you're getting here is kind of this quorum intersection property, right, that you know that if you kind of hit two different sets of M minus F there's, there's going to be at least one replica in the intersection and therefore you're guaranteed to listen to one of these locks. And then this is kind of where the tricky part comes in. So this is kind of the only tricky or hard part about Paxos is that when you're sending these locks, you have to send the view number of these locks. And if I read from different replicas, different locks, from different views, I have to choose the lock that has the highest view. Okay, we'll kind of go over why that's the case. But this is the one thing that Paxos does which is not completely trivial. So again, you listen to misf different locks and you have to choose the highest one.
00:27:39.890 - 00:28:21.880, Speaker A: And anytime you have a protocol that tries to do state machine replication and it doesn't have this aspect, it doesn't have to have signatures and Byzantine fault ions even in the emission model, even in the simplest case, it has to use this property. It has to somewhere embed this protocol. And here's kind of a quote from the Casper. This is basically the consensus protocol that Ethereum is using. So they say that they construct a novel correct by construction fork rule, follow the chain containing the justified. Checkpoint of the greatest height. Okay, so the Pexos way of saying at least here is choose the lock with greatest height.
00:28:21.880 - 00:28:57.332, Speaker A: So the lock is basically what a justified checkpoint is in kind of the ethereum language, and the highest view would be the greatest height. So this is kind of a trick. If you want to look at consensus protocols and see whether they actually are safe or not, the first thing you should look at is whether they're at least doing at least solving consensus as Paxos does. And so it's nice to see that this is always the case. I'll kind of just, again, very quickly draw this interrupt with a historical question. Yeah.
00:28:57.386 - 00:29:06.520, Speaker B: If this is too far afield, feel free to ignore lamport. What's the context with that versus dwarf lynch stockmire?
00:29:06.880 - 00:29:44.890, Speaker A: Yeah, 1988 was this amazing year where dwarf lynch Stockmeyer was published, view stamp replication was published and the part time Parliament was published. Leslie didn't actually define partial synchrony that was in DLS, but his protocol actually works in partial synchrony. Viewstamp Replication is barbara and Brian's work is very similar to a you know, this often happens in science where there's a lot of different groups coalesced on a very similar thing.
00:29:47.100 - 00:29:50.330, Speaker B: Yeah, the protocols themselves are fairly similar.
00:29:50.700 - 00:30:11.890, Speaker A: No. Okay, so this protocol so DLS doesn't have a clean Paxos version, I would say. And Viewstamp Replication maybe has something similar, but maybe had something that they had to fix later on. So I think the Paxos version is cleaner in that sense.
00:30:16.180 - 00:30:30.112, Speaker C: What property of the view are you using? Do you need monetization of it or just like that? There's something we all know and there's some condition we all know about how to compare. Would it work if we just put a hash random number on each one of them compared?
00:30:30.176 - 00:30:53.580, Speaker A: Yeah. No. So you really need some sort of think about as monotonic numbers. You can also do it with bounded timestamps, but yeah, just think about as monotonic numbers. You want to know who's early and who's late and you want to use the latest one. So here I'll call the highest view is the latest. And here it's called the greatest height.
00:30:55.200 - 00:30:57.304, Speaker C: Synchrony makes time meaningful.
00:30:57.432 - 00:31:26.840, Speaker A: No, let me give you the canonical I'm not going to explain the algorithm today, by the way. On Friday we're going to do this from scratch, from zero. So if you want to, you're very welcome. But here is like the canonical thing that you should always keep in mind about why we even need this type of problem. Right. So again, we're kind of in this partial synchrony mode and basically there's N equals three and F equals one. And this is really just about omission failures.
00:31:26.840 - 00:32:03.892, Speaker A: And we're going to have, let's say, three views. So this is view one and view two and view three. And again, think about these for now as some sort of logical views. Maybe there is some way of timing of deciding how they start and end, but let's for just for a second ignore that. And so here's kind of the leader of view one. So there's some leader and even though in the first view it doesn't really have to listen, it listens to let's say it receives two different things and they basically say, well, we have nothing because we just started, right? We're just waking up. And this leader maybe is sending some value.
00:32:03.892 - 00:32:17.636, Speaker A: Let's say it's sending the value X. Okay? So that's kind of what it is. And let's say at this point it crashes. That's it. And now we have a different leader. So here's different leader in view two. And it also kind of does a reading.
00:32:17.636 - 00:32:49.730, Speaker A: And let's say it needs to read N minus F. So that's just two. Two out of three is N minus F. And so it gets a bottom here and it gets a bottom here because this third one didn't see this X, right? So this was just sent to and since it receives no locks, it basically sends the value Y to everybody. But let's imagine that it doesn't succeed to send the value to everybody. It crashes after two. Okay? All right, now we're finally in view three.
00:32:49.730 - 00:33:31.330, Speaker A: And here we are. And this is the leader of view three. And it again needs to read from two parties, right? And so it's basically reading, let's say it's reading from these two. And what does it hear? Well, it hears this old value X, right? This is kind of the only thing it heard because this one didn't hear the new value. So it hears this X and also hears the value Y and it kind of needs to say, well, it actually wants to send the value Z. Right? But the question really is what does this leader three do? And so what Paxo says is that this is not enough information. What you need is not just saying that this is Y, but this is Y from view two and this is X from view one.
00:33:31.330 - 00:34:06.666, Speaker A: So now we've attached. The lock includes the view number. And now once we've done that, then now we basically look at these two locks and what do we do? Yeah, exactly. We peak the highest one, right. So we're going to send the highest one. And this is kind of the one that we're sending. And maybe after we send this, the way Paxos works is that we finally get confirmations and this is when we know that we can commit this is when we know we can commit Y.
00:34:06.666 - 00:34:39.000, Speaker A: And then we might even tell this to everybody else. Okay, so that's kind of how Paxos works. And these kind of two old views is kind of an explanation of why sorry, I mean, this is kind of not yet the explanation. This is just how the protocol works. Right. And the thing is that it could have actually been the case that I've continued this protocol here to actually commit Y. And that's the reason why it was very important for us to choose Y over here.
00:34:39.000 - 00:35:29.080, Speaker A: So I stopped this execution at this point, but it could actually continue on to commit Y. And basically in view three, I have no idea whether there was a commit of Y or not. Again, for those who are kind of more into these points, then kind of this point essentially where there's two locks. This is the point where the system becomes univalent and this is the point where there is a commit certificate and this is the point where there's enough people that learn. So basically there's kind of these three phases of a consensus protocol. In the end, everybody learns about the decision or enough people, a quorum learns. There's kind of a middle phase where somebody learns that happen to commit, but there's actually an even earlier phase where every execution from this point onwards will have to choose Y.
00:35:29.080 - 00:35:31.910, Speaker A: And this is kind of the point where the system becomes univalent.
00:35:32.490 - 00:35:36.666, Speaker C: We don't know that yet. Not everybody knows that.
00:35:36.688 - 00:36:39.520, Speaker A: Yeah, it's unobservable, but it's basically the safety proof kind of goes through that. It says, show me the first view where there was N minus F people that received a lock. And basically you can prove by induction that if you're following this rule, all later leaders are going to have to choose Y because the maximum between all of these is going to be Y and every new one is also going to show Y, right? So that's kind of the proof by induction. Okay, so that's paxos. And in a sense, I want to say this is what I call law of hardness preservation, right? No matter how you're going to solve this problem of Byzantine fault tolerance state machine replication, you're adding Byzantine failures, you're adding cryptography, you still have to fight this problem. There's not a lot of ways to do that. So this is basically the Paxos way of doing it from 1988, okay? But we still want to look at PBFT and now there's like two new challenges that we have to deal with.
00:36:39.520 - 00:37:28.990, Speaker A: One is that we're equivocating in a view, this is like really about the 1980s and now we're moving to roughly the year 2000. And I want to say all the Byzantine fault tolerant state machine replication protocols in partial synchrony, or almost all of them basically follow this PBFT path. And what PBFT does is takes Paxos and adds kind of fixes to handle malicious or Byzantine behavior relative to emission ones. Okay? That's kind of what it does. And it does that by doing two more things, right? So the one thing it does is it handles kind of the fact that maybe if I'm malicious, instead of sending Y, I will send Y and Y prime, so I'll send different values. And the way that's kind of solved is by having one more round of exchange. So I'll probably show that on Friday.
00:37:28.990 - 00:38:10.010, Speaker A: And this is maybe why. You see, Casper, for example, had like two rounds of N minus F of two thirds. So the first round of N minus F is kind of dealing with this Equivocation problem, making sure the leader is only saying one value. And the second round is basically solving this problem. It's making sure that if there was a commit in view two, then in view three, we're going to persist that commitment. That's kind of the reason why there's these two rounds and they're lower bounds, showing that this is kind of the best you can hope for. And the second problem that you have to fix is you actually actually force the leader to say the truth and choose the maximum lock.
00:38:10.010 - 00:38:54.486, Speaker A: If I'm a malicious leader, I might see all the locks and choose a different lock, ignore the kind of the Paxos thing that I'm supposed to do and do something else. And basically, we know of only two solutions, roughly speaking, to this type of problem. One is basically the PBFT approach where I take all the locks and I just send them over. Okay. And then you can basically check this is kind of verifiable computation, I did some work and then you can just check the same work that I've done. And you can imagine you can compress this, of course, if you need to, that's kind of one approach. And the second option is kind of the big problem there, big is that now I have to send a message that contains N different things, right? So it's kind of a linear cost message to send these locks.
00:38:54.486 - 00:39:38.246, Speaker A: So in terms of the amount of bits, it's maybe potentially large. And the second approach is this tendermint approach. And again, this is really surprising from my perspective, coming up with kind of this new view change approach, where instead of the leader sending all the locks, the leader just chooses, decides what is the maximum lock, so does what Paxos tells them to do and just sends it over. And what each replica does is it just says, I'll accept this lock if I haven't seen a larger lock before that. So it seems like a less safe solution, but surprisingly, it's actually safe. And we'll see maybe the problem is it's slightly less live.
00:39:38.428 - 00:39:40.920, Speaker C: Why can't you fake the lock number?
00:39:41.370 - 00:39:58.666, Speaker A: It's all cryptographically signed this round over here of Equivocation, which I didn't do because I'm not running good at time is basically getting you three N minus F signatures that were all agreeing on the lock and the view number I'm.
00:39:58.698 - 00:40:10.900, Speaker C: Missing why you need the N minus F for the first but not for the second? Because wouldn't I need the N minus F signature to prove that I'm not kind of making up my lock time?
00:40:11.590 - 00:40:35.862, Speaker A: Yeah. Part of what this protocol allows you to do is to say that when you have a lock certificate, you can't invent it because three n minus F parties signed on it. And so if you're worried about faking a lock, then it's very hard to fake a lock because you have to have n minus F out of which at least f plus one parties are honest work with you.
00:40:35.996 - 00:40:39.450, Speaker C: But the mistake can do with just hash fund pointers.
00:40:40.510 - 00:40:50.880, Speaker A: You can't change the they all have to agree, they all have to be in that view to sign on it. So if I'm in view two and you give me something in view four, I'm going to say I'm sorry, I'm not going to sign on it.
00:40:51.330 - 00:40:53.886, Speaker B: And everybody knows who the leader is in the view, right?
00:40:54.068 - 00:41:13.894, Speaker A: Yeah. Two questions. The first one is in tenderman, they have to wait. Delta, right? This is the caveat basically to get rid of that thing. Yeah, that's why I just said here maintaining safety and I didn't talk about liveness that's next slide. Yes. And then the second one is in the omission fault model.
00:41:13.894 - 00:41:41.550, Speaker A: The lock is basically my vault and then the certification is like just a one round of votes. Whereas in this Byzantine one lock is a two third votes and the certification is two phases of two third votes. N minus f signatures. Yes, absolutely. Yeah, you're right. I was kind of planning to do this on a whiteboard, but realized that I don't have enough time. So come on Friday and you'll see it in its gory details.
00:41:41.550 - 00:41:55.650, Speaker A: Okay. So I guess this is PBFT. It's amazing. It also has three different versions. Thesis version, the journal version and the conference version. So this is the conference version, which is the more practical one. If you're reading thesis version, it's more complicated, it's information theoretic.
00:41:55.650 - 00:42:51.250, Speaker A: But really almost all modern protocols are following this path and doing this. And so again, ask yourself, first of all, are they solving this problem and then are they solving the equivocation problem in around? And then ask yourself, are they actually making sure that the view change is not being manipulated? So these are the three things you have to verify to check that it's working. Here's kind of some of the things that happened after that. And I'll tell you a little bit about this hot stuff work. Basically what we tried to do is kind of be eclectic and take the best ideas of all these previous protocols. And so we started with PBFT, that was kind of our base protocol because that's the only base protocol that you can choose in some sense. And then there's a very nice paper by Mike Ryder actually showing how you can move from quadratic message communication to linear through a leader.
00:42:51.250 - 00:43:36.782, Speaker A: And so we kind of used that as well to show that you can take PDFT and make most of it linear. Okay, the only part that was kind of not working for us in terms of linear is this phase where you have to send all the locks. When you're sending all the locks, you have to send a linear size message. And if you're sending a linear size message to N people, that's kind of a quadratic amount of bits. And so we kind of, again, took this idea from Tendermit of using the view change there and just sending one lock instead of N minus F locks. And then the two other things that were really important in this protocol is we took these ideas from Bitcoin. So the first is that in PBST, there was kind of a stable leader that stayed until we found some fault in it and then we kind of threw it away and replaced the new leader.
00:43:36.782 - 00:44:10.986, Speaker A: And here what we kind of did, is said, let's actually replace leaders all the time. So this is kind of this leader rotation. This is, I guess, epitomized by kind of the Bitcoin protocol where every round there's a new proof of work winner. And the second thing kind of we did, again, copying Bitcoin is taking a chained approach where we're not just reaching agreement on one decision, but on kind of a chain of decisions. And so the top hash is actually committing to all the bottom ones. And again, if you kind of are in the weeds of PBFT, they used what I will call an array instead of a linked list. And that actually matters.
00:44:10.986 - 00:44:55.530, Speaker A: The details here matter. The final thing that we did is then looked at this kind of Casper FFG work and there there was this really nice observation that you don't really need to have two different types of messages. So in PBFT, there was like a message doing the non Equivocation and then the other message doing the, I guess the view change part. And there was this really nice observation that you can pipeline these and use the same message. And so, in a sense, at least from my perspective, what Hot Stuff did is take all these ideas that were out there and kind of just put them together into one cake and bake them. That was kind of this thing I didn't mention liveness. And indeed, liveness is still a challenge.
00:44:55.530 - 00:45:38.810, Speaker A: And here there's a separate story about responsiveness, right? Again, this is a very nice work by Rafael and Elaine. And there's kind of a side flywheel about synchronous protocols and about responsiveness. But I guess seeing that paper also made us realize this problem also with tendermint and with tendermint view change, which is that this two chain type protocols, they're good. But in a sense, there are cases where an honest leader doesn't make progress, right? This is this hidden lock problem. And so to overcome this, we kind of came up with adding one more round. So this is the three round hotspot protocol. And so this is where you get responsiveness and linearity and all these nice properties.
00:45:38.810 - 00:46:09.810, Speaker A: I didn't mention view synchronization here. I do kind of want to say a few words about kind of the short term hindsight. So given this work was done maybe five years ago, I would say there's kind of these two separate types of contributions. Right. So the first type of contribution is really about the fact that it's linear and responsive. These are kind of theoretical measures. It seems to be a good property that if you have a good leader and we're after synchrony, that this good leader will manage to commit.
00:46:09.810 - 00:46:52.610, Speaker A: And unfortunately, again, there was this hidden lock problem, which we can do on Friday, where there are some cases where that doesn't happen. This is kind of this protocol that got linearity and responsiveness, which from a theory side, you'll see in the next slide was very nice. But from a practical side, it seems that if you're kind of handling maybe 100 validators or a few hundred validators, then linearity isn't such a big problem for that scale and responsiveness. Also, there are so many problems with partial synchrony where you don't make progress just because of asynchrony that extracting the maximum amount of liveness. Isn't it's not clear that that's kind of the breaking point of these protocols.
00:46:53.750 - 00:47:02.100, Speaker B: On the responsiveness point? So an honest leader may make progress, but it still may be committing a block that was proposed by a Byzantine party. Right?
00:47:02.810 - 00:47:12.518, Speaker A: Yeah. Well, in these pipeline protocols, you commit, but you're also kind of pushing your own proposal for the next guy. So making progress is like a dual thing.
00:47:12.604 - 00:47:14.470, Speaker B: Okay, I see.
00:47:14.620 - 00:47:14.982, Speaker A: Good.
00:47:15.036 - 00:47:19.402, Speaker B: So you're not stuck with some proposal by a previous Byzantine person.
00:47:19.456 - 00:47:22.298, Speaker A: You're kind of making progress in some sense. You're pushing the system forward.
00:47:22.384 - 00:47:32.510, Speaker B: And then this is like responsiveness shows up twice on the right hand part of the slide. They're different or no? When you say optimistic responsiveness is important, that's the same one.
00:47:32.660 - 00:47:36.106, Speaker A: Yeah. I'm hiding here some this other notion.
00:47:36.138 - 00:47:39.582, Speaker B: Where just like, you automatically speed up to the network speed, right?
00:47:39.716 - 00:48:07.770, Speaker A: Yeah. So here's what I'll say. And you'll see this in the next slide. There are like subtle definitions of what responsiveness means and optimistic responsiveness. What I will focus on is just that using this property, you can actually build Asynchronous protocols. And I'll show that in the next slide. I guess what I wanted to say here is that these two properties were really like, great theoretical advances.
00:48:07.770 - 00:48:40.420, Speaker A: It's not clear that in practice, as of now, they're like the main advantage. And the opposite is that the things that didn't change the asymptotics of the theory, the fact that we're doing leader rotation, the fact that we're doing training, the fact that we're doing pipelining, it improves constants. Right. But it didn't change the asymptotics. So from a theory perspective, less Astounding, those are actually mattered a lot in this space. Right. And so these ideas were actually used and are used in many systems today.
00:48:40.420 - 00:49:23.262, Speaker A: So it's kind of this flywheel where some of the ideas are actually used in systems and some of the ideas are used in theory. Yeah, not great. Okay, so the next thing that happened is seeing how you can actually use these responsiveness protocols to use partial synchrony protocols and build out of them protocols that work in Asynchrony. And so this goes back to this beautiful paper by Kats and Ku 2006. I don't know, I have maybe eight or ten papers that basically extend the ideas there and wherever. I'm kind of stuck. I just read this paper again, I figure out something else I didn't understand, and then I said, okay, maybe I can do some other things based on my new understanding.
00:49:23.262 - 00:50:04.634, Speaker A: Anyway, amazing paper. And here's some of the things we've done from this work. And so one of them is basically, if you have a distributed key generation, then you can actually solve Asynchronous Byzantine agreements, so asynchronous state machine replication. And you can do this kind of with the optimal message complexity, which is kind of, again, the theory contribution. But from my perspective, the really important conceptual contribution here is that it shows how you can take a partially synchronous protocol and kind of massage it into an asynchronous one. And that was actually something that people didn't know previously to that. Previously you had like the Asynchronous island, which was like strange, and using these binary agreement protocols.
00:50:04.634 - 00:50:15.010, Speaker A: And then this partial synchrony, which seemed more practical. And the nice thing here is that this actually allows you to take these relatively practical partial synchrony protocols and kind of massage them into an asynchronous one.
00:50:15.160 - 00:50:18.770, Speaker C: What do you mean by asynchronous? Like you need to assume something, right.
00:50:18.920 - 00:50:22.230, Speaker A: That you get liveness even in Asynchronous.
00:50:23.210 - 00:50:25.190, Speaker C: How do you not get FLP?
00:50:26.250 - 00:50:56.206, Speaker A: FLP only says that there must exist infinite executions, but their measure can be zero. So in particular, these protocols are using a strong coin that's your randomness. And what this guarantees is that the time to commit a transaction is in expectation, a constant. So these are constant expected round protocols. That's kind of the best you could hope for. And so in a sense, like FLP is a non problem, right, given good randomness. Okay, maybe I should have mentioned this.
00:50:56.206 - 00:51:27.250, Speaker A: This is maybe another type of misconception, right, that FLP says you can't do asynchronous. That's not the case, right? Yeah. And then on top of that, this is kind of this flywheel, right? So we start with these systems and Blockchain and Casper. And then this goes to kind of theory. And you have these even more amazing results showing that, again, asymptotically you can kind of improve and get kind of what I'll call free broadcast or free consensus. You have a message of size order N, and it takes N squared to commit it. So just sending this message of size order N is N square bits.
00:51:27.250 - 00:51:58.414, Speaker A: So if all your work is order N squared, that's kind of asymptotically free, I'll call that. So that's kind of the one side of using a strong coin and then there's this other side of using a weak coin. And here I'll even be more hand wavy, but we have this very nice work about no waiting hot Stuff. So it's another version that uses a weak coin. Again using these ideas of katsunku solving asynchronous Byzantine agreement, this time without a strong coin, it's a harder thing to do. So you need like verifiable secret sharing and so on. And from that you can build this distributed key generation.
00:51:58.414 - 00:52:45.886, Speaker A: I think Sarah talked about that a while ago and those ideas also kind of brought us these information theoretic versions of Hot Stuff and that gives us kind of information theoretic versions of agreement on a core set. And this actually in the end actually helped for MPC problems. So a lot of amazing kind of theoretical things actually depended on using some of these, taking ideas from say, Tendermint and Casper and applying them. That's kind of really cool in that sense. Yeah, I wanted to show you how this works on a whiteboard. I'm not going to do it, so I'm just going to say very high level about how this connection between partial synchrony and synchrony works. And the problem with adversaries is if the leader is faulty, you don't know if it's faulty or delayed.
00:52:45.886 - 00:53:18.934, Speaker A: And so the way partial synchrony works is usually you just give it some time out. You say, well, if I don't hear from you in 5 seconds, we're going to throw you away and start the next few. That's essentially what it does. And the idea in kind of this new protocol is different. It's basically saying we're all going to be leaders. So each one of us is going to run a partial synchronous protocol and each one of us is going to be a leader in its separate bubble, right? So we're just going to run n copies of this protocol and after a while we're going to basically choose a random leader in hindsight. So choose that one of us was the actual leader, pop all the other bubbles and say those didn't matter, those were decoys.
00:53:18.934 - 00:53:41.246, Speaker A: And the only thing that mattered were this one leader that we chose. And this is really a blueprint of how this algorithm works. You run n instances of a single view. This is your partial synchronous protocol. It can be Pexos, it can be PBFT, it can be your favorite version of Hot Stuff or whatever partial synchronous protocol you want. The thing that you do need is this kind of strong version of responsiveness, right, in order to get this to work fully and asynchronous.
00:53:41.278 - 00:53:43.080, Speaker B: So it's black box if you have that.
00:53:46.330 - 00:54:19.120, Speaker A: Yeah. And that's what you do. You basically run these end versions. You wait until MSF of them finish and that will always happen because eventually you do have NYSF good leaders. And so this is where you're using responsiveness to say that you can always wait for these instances and then you choose in hindsight one of these and it's really kind of this magical transformation. You thought that Asynchronous is impossible and you actually just say, well if I solve partial synchrony I can just do this and get asymptotically optimal results for Asynchronous. That's kind of, kind of nice.
00:54:19.120 - 00:55:04.126, Speaker A: Okay, I want to do this on, but I'm not going to do it in more details. Yeah, sorry. So one more thing. Is that about partial synchrony is really about I would say no at a very high level. I think if you just need to get some system then any version of these two chain versions, right, it doesn't have to be hot stuff, can be PBFT or whatever is kind of good enough. I will focus on mention the ethereum use case where for them they actually care about this fact that you have in every round a leader that's secret, right? So it's hard to bribe and this leader is only going to speak once and again the advantage of speaking once is that it's harder for an adaptive adversary to attack. Because until I see you speak, I don't know who you are.
00:55:04.126 - 00:55:52.634, Speaker A: And after I see you speak it doesn't matter if I can corrupt you because it's already done. And so again, amazing work done here on both of these fronts. But the problem is that when you actually combine this with these pipeline protocols, you kind of end up needing this consecutive honest leaders in order to make progress, in order to commit. So you get all these nice properties of anti bribery and good protection. But now in order to commit you actually have to have let's say three or four honest parties leaders consecutively. And so the obvious question is can you actually do this without this consecutive property? And there are several ways to do this. One is kind of to build a reputation system where if somebody crashes or doesn't make progress, then we're just going to move them away from the rotation.
00:55:52.634 - 00:56:37.374, Speaker A: So that's kind of I think one approach which has its advantages disadvantages and the other one is this new work that we have that basically removes this need for consecutive honest leaders. And I guess in hindsight the technique is pretty simple. If you look at all these protocols I just mentioned, they look at commit certificates, they look at N minus F signatures. And so in this protocol we actually look also on these quorums but also on single votes. So if you kind of look at more information you can maybe do more smarter choices and then you can actually have a protocol that even if you have holes in between your honest parties, every honest party is actually making progress to the system. You don't have to have three of them together to make finality progress. Okay, so this is what I plan to do.
00:56:37.374 - 00:56:59.422, Speaker A: In less time. And so we'll talk about this Dag flywheel maybe pretty quickly. So reliable broadcast. Is this amazing? Asynchronous protocol. And these are the properties. And if we had time, I would show you the protocol. It's just this really simple four line protocol.
00:56:59.422 - 00:57:51.678, Speaker A: It's beautiful. If you ever earn this field, you have to know it because when you try, you make mistakes and you figure out why it's wrong. And there's a whole flywheel here, by the way, as well. So there are systems and real systems and a lot of new theoretical results, amazing theoretical results, just about improving this protocol that I'm not going to talk about anyway, that's Reliable Broadcast. And what does this have to do? Well, now we'll talk about how this Asynchronous protocols actually connect to these directed Eclic graphs. And so again, this is an idea that started with the blockchain space and from this idea from Hashgraph from 2016, and they were mentioning this idea of virtual voting and zero message embedding of any protocol. And we looked at it and we kind of didn't really understand why it's interesting and what should we do with it, but it kind of seemed like a fresh new way of thinking about these protocols, which looked kind of surprising.
00:57:51.678 - 00:58:57.680, Speaker A: And then this next thing that happened is this work in 2019 by this Alif zero group, where what they basically did is they said, well, instead of just building kind of this directly cyclic DAF of messages, let's make sure that every time I'm sending a message, I'm actually sending it through Reliable Broadcast, okay? So I'm not just going to run a regular message sending, but I'm sending it through Reliable Broadcast. And the only thing I want you to see is I'm just going to draw a cartoon of how Reliable Broadcast looks like. So basically the leader sends messages to everybody and then everybody sends an echo. So this is the echo phase. And after we receive enough echoes, let's say we receive M minus F echoes, we basically send a vote message. This is the vote message. And if you receive misf votes, that's when you basically say, I received the Reliable Broadcast.
00:58:57.680 - 00:59:38.880, Speaker A: Okay, so roughly speaking, again, I'm missing here like an important part, which is if you receive F plus one votes, you also send a vote. But let's ignore that for now when I'm sending a message to you. That would be just a direct message sending. But here I will say I received your message when this happened. I received MSF votes of people that received MSF echoes of people that saw my message, right? So there is some non trivial thing happening. Anyway, so the idea here is that we'll use this protocol and build a graph of these things. What does that mean? That means that if I'm sending one message, here's the message I'm sending, right? So this block basically represents the fact that I sense ran this whole protocol.
00:59:38.880 - 01:00:51.892, Speaker A: And now if there's N minus F people that heard this block, then each one of these blocks is actually a reliable broadcast that says that they heard this previous reliable broadcast. Okay? So this is like a graph of reliable broadcasts. And in a sense, what the protocol says, what this protocol says is just you build your new block after you hear from N minus F previous blocks, okay? So you run your reliable broadcast after you heard M minus F other reliable broadcast. If you think about it as a Dag, it looks really clean if you try to open it up, actually, each one of these things is doing some non trigger thing, right? It's a bit heavy operation. It's n power three operation. And I guess the amazing thing is that you can do a lot of protocols inside this abstraction. And this started with this work of Dagwider that basically show that you can embed this previous protocols and particular gather and verifiable asynchronous agreements, the one that we talked about earlier.
01:00:51.892 - 01:01:58.716, Speaker A: You can actually embed this inside this type of behavior. And the really surprising thing is that you can embed this and kind of keep an asymptotically constant costs so you're not kind of paying too much costs on it. And this kind of continued this work by Narwhal and Tusk showing that not only can you kind of build consensus protocols inside it, you can also do the data dissemination part very efficiently inside that. And this goes on with bullshark, which shows that not only you can kind of do that, you can build a very nice version of PBFT essentially, which all it basically needs to do is to look at F plus one votes in this Dag. And again, I was hoping to show you a little bit more about that. But I guess my point here is that there's all these new algorithms that are still they have to solve these old problems, right? So they're not doing something completely new, but they're looking at consensus in a different way by basically looking at this directly cyclic graph of rail broadcast. And that gives them a lot of abstraction and power in that sense.
01:01:58.716 - 01:02:40.036, Speaker A: And the cost to do that seems a little bit high. And there's some work now trying to improve the latency, so it's not completely free doing that. And it's actually a good question. Can you actually take down all these constants and make it extremely efficient? But the other aspect that I want to highlight is that this is kind of a scalability bottleneck in the data availability part, so it allows you to kind of run the data availability part irrespective of the consensus protocol. And I think in that sense, going back to the separation of four parts, this was kind of for me, one of the big advantages of these daggers protocols, that they were really cleanly separating the data dissemination part from the consensus.
01:02:40.228 - 01:02:43.580, Speaker B: Are they doing in a way that you couldn't have done with previous paradigms.
01:02:45.600 - 01:03:13.940, Speaker A: Everything could have been done but hasn't been done, I would say. So the fact that you're doing this, the observation that you look at this Dag of reliable broadcast and you realize that what it allows you to do is to kind of make a lot of progress in data availability even if you're not doing consensus. That is implicitly known in a lot of, I guess, PBFT systems, but I haven't seen it explicitly said or used.
01:03:14.010 - 01:03:19.184, Speaker B: I see, so you're just saying like piggyback on the reliable broadcast for the data availability. For the data dissemination.
01:03:19.312 - 01:03:36.844, Speaker A: Yeah, since it's asynchronous, you don't have to deal with anything. Right. So it'll just run. And the fact that you're doing this with N minus F means that you're kind of locking everybody's messages together, which is another nice thing. There are caveats here. Did I have here the caveats. Yeah.
01:03:36.844 - 01:04:04.256, Speaker A: So I said here the caveat here is that it's not directly for clients. So if I had more time, I would kind of try to explain this comment. Basically, this gives you really good throughput assuming that every replica has its own set of messages it wants to push. But in the real world, you have clients that want to send messages. And so if each client has their kind of representative, then that would work really well. Otherwise, maybe my representative doesn't work. So I have to send it to all the replicas.
01:04:04.256 - 01:04:15.368, Speaker A: And so now the client is sending the transaction to all the replicas. Now each replica is trying to push it. So now it's getting replicated end times in the system. So it's not clear. So in some sense, it's a very good happy path. Yeah. Okay.
01:04:15.368 - 01:04:54.432, Speaker A: Quite a bit more to say, but I'm going to run and just talk a little bit about kind of the layer two problem. And why am I talking about that? Because in a sense, I guess my hope is that what I want to convey to you is that consensus should not be your bottleneck. Right. We have today so many tools, right, to do this in partial synchrony and asynchrony with a Dag, without with two rounds, with three, with responsiveness, and again, it's all inside. Drinkless is on the hash. We understand this latency throughput trade off, so it's not easy to do, right. But at least from an architectural perspective, we know about a lot of solutions.
01:04:54.432 - 01:04:57.472, Speaker A: And that shouldn't be your scalability bottleneck.
01:04:57.616 - 01:05:04.410, Speaker B: This is assuming you're happy to make datability assumption, trust assumptions on top of the consensus trust assumptions. Right.
01:05:06.060 - 01:05:26.290, Speaker A: Well, I mean, going back to the this is going to be maybe the next slide. There are these three other parts input, execution, output. You have to solve those. So basically I'm saying okay, so you're arguing execution. Yes, I agree. Maybe this is the controversial part, right? Maybe you're saying input is also a hard problem.
01:05:27.380 - 01:05:47.808, Speaker B: I'm just saying the old school approach of L Ones was to actually kind of reuse the L One also to solve the data availability problem, right? So it means like a common set of trust assumptions gets you both the consensus properties and the data availability properties you want. Whereas I think in the architects you're.
01:05:47.824 - 01:06:17.168, Speaker A: Describing, they're kind of I'm going to talk exactly about that. Exactly about that, yeah. So that's good. I'm going to talk exactly about that. I'm going to give you brief history, which is initially, at least from my perspective, how historically Ethereum has thought about this or some of what I've heard from Vitalik and crew is initially they said, let's just build charting inside our L One. And the problem was it seemed too complex. There were kind of security problems that we can talk about and maybe centralizes a lot of the work.
01:06:17.168 - 01:07:19.990, Speaker A: And the next phase of evolution was like, okay, plasma is going to save us extend kind of lightning or payment channels to kind of generic execution. And what they realized at that point is that what is really a challenge there is data availability. So by pushing both the execution and the data availability outside of L One, you're now kind of creating a problem where if I want to complain against this plasma operator, I don't have the data. And so that was the problem. This is kind of where roll ups came into play, where the idea is that we're just going to push the execution off chain and keep the data on chain, right? That's kind of the high level idea. And the good thing here is that it kind of creates an ecosystem of competition between different roll up providers and it kind of solves the main bottleneck of execution, right, which is as an L One, you don't have to do the execution yourself. You can have somebody else do the execution, but you're still kind of doing the input and the recording of the data.
01:07:19.990 - 01:08:04.530, Speaker A: And that's something that you still have to solve, right, this data availability thing. But again, there are nice ways of making sure that an L One can actually scale to quite large amounts of data availability. Okay? So here's kind of in a few pictures how these schemes work, and I think there's a lot of interesting design questions and improvements that can still happen. So this is like the caricature version of what a Sharded system looks like if you have basically large machines. So this is like you have a large machine, it has 128 cores or whatever. And now what we're going to do is we're just going to do input and ordering. But when we come to execute, because that's our bottleneck, we're going to split our workload into a lot of cores and we're going to try to run that.
01:08:04.530 - 01:09:00.304, Speaker A: Okay? Obviously, as you know, the problem there was now you have these heavy validators and maybe you want to have smaller validators to make it easier for more people to audit the system. And so that is a trade off. But I think this is an interesting design spot that maybe hasn't been completely covered. And maybe the evolution of that is to use Optimistic Concurrency Control, right? So one of the problems in this design is you have to make sure that everything runs in the same amount because in some sense you have to wait till everything is executed. So if you have even one thread that's very slow, you're going to be as slow as the slowest one. And so kind of in this design of Optimistic Concurrency control, what you can do is actually choose to optimistically run until you get some point in time and then you can stop it and you can order all these things. And then sure, the executioner has to do a little bit more, has to check that there's no conflicts between these separate threads or do some more smarter thing like reexecute some of these transactions.
01:09:00.304 - 01:09:27.096, Speaker A: But that's kind of, roughly speaking, the idea of vertical scaling with optimistic and currency control. And obviously the next phase of that is horizontal scaling. That's done by just simply having several systems. But you can actually think about one system that is designed in such a way, right, that has basically multiple subsystems. And now you have this problem of now where do you take your security? Right? So you have end parties. You have to have each party be in one of these shards. Not in all of these shards.
01:09:27.096 - 01:10:14.636, Speaker A: You want to move them around because you don't want to make each of these shards now has less security. And so these are all these challenges in horizontal scaling. And so the next phase that came in is kind of these payment channels and this is kind of, again, the caricature of a payment channel where the only thing that's happening in the layer one is basically exiting and entering, right? Roughly speaking, unless there's a complaint, then basically you just enter and exit here and all the work is done in there too. As I mentioned earlier, the problem was that since data availability is happening here, when I want to complain against this operator, I have to show proof that this happened. And since all the data is happening inside this operator, this is kind of somewhat of a challenge. But again, maybe this idea can be revisited and relooked at. Yeah.
01:10:14.636 - 01:10:17.550, Speaker A: And so this is kind of how roll ups look today.
01:10:18.240 - 01:10:23.890, Speaker C: Data availability in l One is easier because I can become a node and then I have access to the data.
01:10:24.980 - 01:11:08.972, Speaker A: Yeah. So presumably the l One security is much higher. So the risk of a complete failure in the l One is small while in the l Two you're willing for it to be completely failed and you still want to maintain safety even if l Two completely fails. That's kind of the, I guess, high level view of how a roll up is viewed from a layer one perspective. So from a layer one perspective, layer twos are basically optimistic executions. But if they fail in terms of safety, then there is a way of recourse. And here the problem was that there was none because I didn't have the data.
01:11:08.972 - 01:11:50.424, Speaker A: And so that's kind of the next evolution of this thing, which is to basically make the data be available on l One, but run the transactions on there too. And okay, depending on whether you're doing fraud proofs or zero knowledge to make this execution, this validation very cheap. And that's kind of roughly speaking the idea. And what we're seeing here is kind of this evolution. And part of this evolution is happening because of the time it takes, right? So this goes back to the throughput latency challenge that we talked about earlier. So this gives you a lot of throughput because you can just, if you want, you add another layer two, you get more throughput. But the problem is latency.
01:11:50.424 - 01:12:37.188, Speaker A: Here's kind of this recent comment by Vitalik basically saying that for fraud proofs it actually takes a week to reach finality. And today in zero knowledge proofs, even the best ones take a few hours. Maybe it'll improve over time, but if I'm buying an ice cream or something, then it's hard to say. Just wait a few hours, right? There's kind of a motivation for a layer two operator to say, I want to provide you some immediate response. And so the dynamic that is happening is that these layer two providers are actually now becoming a full blockchain, right? So they're saying, well, maybe we'll also do some data availability. We'll provide you data availability so that if there's a problem, you can go and check it. And we'll also maybe do some sort of consensus so we'll decentralize the sequencer.
01:12:37.188 - 01:13:16.164, Speaker A: And this is kind of an interesting dynamic happening. And I guess what I want to comment this is, I guess the comments that I'm going to skip just because of lack of time, but about what are the security properties of layer two from a layer one perspective. So from a layer one perspective, I don't need you to do data availability or consensus, right? Because I'm going to take care of that myself. But maybe this layer two operator is saying I want to give my clients better service in some sense. And this is kind of my point about asymmetric trust. So I guess, again, beautiful area of research and this nice flywheel around. Asymmetric trust.
01:13:16.164 - 01:13:58.884, Speaker A: I won't go into detail about this work. There's early work in MPC and then Stellar, and this very nice work by Chris and Kashin and Takaman showing how you can basically build systems even if we don't all agree on who we trust, right? Let's say I assume that some of you are trusted and some of you think that other groups are trusted and so on. If there's a good enough core, we can actually do some things. And at least in my view, something around that is happening also between layer ones and layer twos. Right? So they're basically saying some clients will say, no, I just trust layer one. So I'm going to wait either a week or a few hours and maybe some other clients are going to say no. My trust assumption is that if it's actually final on your layer two, I'm going to trust that that's okay.
01:13:58.884 - 01:14:06.710, Speaker A: And then here's the ice cream. You can go and eat it. Okay. Yeah. And I'm going to skip that and say thank you.
01:14:07.080 - 01:14:37.144, Speaker C: About the layer two kind of think of banks are kind of layer two in a way money is held by the Federal Reserve, but when I use my bank account in bank of America, I just have something for them. But I think of this as probably okay because they tell me something, I can go to court or whatever and get so what am I missing?
01:14:37.192 - 01:14:57.396, Speaker A: Or why can't these are systems that are designed without having a fallback court mechanism. And so the problem is, what if your bank says, you know, you said you had a bank account. We're sorry, we looked and you don't. There is none. And so what do you say? You say, hey, look, you sent me this by mail. And they're like, no, we didn't sign this. This is like, you made this up.
01:14:57.396 - 01:15:16.536, Speaker A: So this is the data availability problem, right? The data availability problem is you go and you say, I have an account. They're like, we don't know what you're talking about. So the idea here is that you're going to post the fact that you have an account or the fact that you're making transactions on layer one, but the actual running of the execution that will happen in what you call a bank. Right? And that's kind of the layer two approach.
01:15:16.728 - 01:15:34.076, Speaker C: The way that it would take it to the layer two space is like somebody will open Alert Two and say, whenever I process something, I won't give you my full state or anything like that, but it will give you some signed message that you can take to a smart contract on layer one and take your money back or some kind of guarantee.
01:15:34.108 - 01:16:11.612, Speaker A: Yeah. So you're trying to solve things that plasma that's kind of like so it's good until you're managing like a million people or you have a lot of customers and then maybe I lost these messages or so on. So now data availability is dependent on me and I need to be able to prove something relative to everything you did in the bank. It turns out to be nontrivial. That's the reason why this plasma approach was kind of unpreferred. And I guess it's also because layer ones realize that it is not so hard to get very scalable data availability. So this is a feature that they can provide.
01:16:11.612 - 01:16:43.408, Speaker A: Maybe this was kind of the one slide that I missed here. There's actually an interesting question here. Where will the value be captured between layer one and layer two? Right. What's the kind of power dynamics there? And in some sense, that goes back to the workload. If the workload is such that there's very little cross l two transactions, you can live very happily inside your layer two. Then maybe you can argue that most of the value is going to be captured there. If a lot of what you're doing is actually moving between things, then the layer one is actually going to allow you to be this superhighway of moving between different layer twos.
01:16:43.408 - 01:16:54.030, Speaker A: And then presumably a lot of the value is going to be captured there. So it's an interesting question. It's hard for me to predict how the dynamic will go. All right. Thank you.
