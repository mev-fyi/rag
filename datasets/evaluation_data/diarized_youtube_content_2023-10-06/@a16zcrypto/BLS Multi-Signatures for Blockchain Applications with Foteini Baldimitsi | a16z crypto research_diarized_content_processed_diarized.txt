00:00:07.380 - 00:00:07.930, Speaker A: You.
00:00:09.900 - 00:00:27.956, Speaker B: Good morning everyone. Welcome to today's a 16 Z crypto research seminar. Very happy to introduce Fatini Baldinski, who we know well. She was a faculty fellow with us last summer, summer 2022. So it's great to have her back talking about something which is very relevant for us, BLS multisigs.
00:00:27.988 - 00:01:01.792, Speaker A: So, Fatini, thank you so much. Tim. Thanks everyone for being here. I'm really glad to be here again next since previous summer and I'm excited to talk to you about two recent results which are kind of still work in progress or works that are under submission. So any feedback would be very highly appreciated. So, what I'm going to talk about is two different research projects that I've been working on this year with my collaborators. But they have a common theme and the common theme is BLS multi signatures.
00:01:01.792 - 00:01:42.144, Speaker A: So I thought that it would be nice to try to combine them. I don't know if I will really spend an hour and a half, but let's see how it goes and feel free to stop me at any point with questions. So, first step, let me introduce what a multisignator is for those of you in the audience that haven't heard of multisignatures before. So, traditional digital signatures, they're just run by one signer that has a pair of secret key and public key. You want to sign a message, you sign it with your secret key and you get as output your signature. Now, the multisigner setting is a little bit different. So the high level idea is that you now have N signers.
00:01:42.144 - 00:02:23.120, Speaker A: In my picture, I just have three that they all want to produce a signature under the same methods M. So here, as you see, every single signer gets the same messages as input. They all have their own key pairs. And we also assume that this bold PK here is the combination of all the public keys of the signers. All right? So what would be a very, very trivial way to produce a multisignature? It would literally just be to output the concatenation of all these underlying signatures. But of course this is not great because this results in a linear size of the signature to the number of signers. So that's not good.
00:02:23.120 - 00:03:16.660, Speaker A: So most of the multisignature schemes that we know and that we use in practice, they actually manage to create a sort multisignature via two different ways. One is to assume an interactive protocol among the signers, or the most classic one is to assume some sort of aggregation across the signatures that gives a very short signature at the end. And we want this signature to have efficient verification. And again, we want this to be sublinear to the number of signs. Additionally, many multisignature schemes support what we call a key aggregation, which is an additional algorithm that manages to combine these public keys together again in a public key that is of sublinear size to the number of signers. And this makes verification even more efficient. So this is a high level idea of how multisignatures are defined.
00:03:16.660 - 00:04:37.432, Speaker A: So what do we care about multisignatures? Well, multisignatures have various applications. So in the non blockchain setting we have seen uses of multisignatures in collective signing of digital certificates for example. And most prominently in the blockchain setting, we have seen applications of multisignatures in multi user wallets where you might want to authorize a payment by requiring a certain amount of signers to sign the transaction in various layer two. Blockchain protocols like mixing protocols or the lighting network and additionally in proof of stake protocols where you want a set of Validators to sign together a new block that ends up on the blockchain. So in today's talk I will be focusing on those two applications of multisignatures and I will be discussing two results. The first one will be decentralized issuance of a special type of digital certificates and that is the case of anonymous credentials. And to do that I will introduce a new building block which is relevant to multisignatures but it builds more features and this is blind multisignatures.
00:04:37.432 - 00:05:41.296, Speaker A: And then in the second part of this talk I will talk about again a variation of multisignatures that is optimized for subsets of signers and I will explain what all this mean. The common theme between those two works, beyond the fact that they are on multi signatures, is that both of them are based on the concrete case of BLS multi signatures, which is one of the most well known and efficient multi signature schemes in the literature. So we'll get to all this in details. So let me start with the first part of the talk and that is the decentralization of anonymous credentials via multisignatures. So let me start by introducing what an anonymous Credential is. So in a high level, an anonymous Credential is a digital token and it typically contains information about the user. So here for example, you can have the name of the user, date of birth, address and so on.
00:05:41.296 - 00:07:06.780, Speaker A: And this digital Credential is issued by some special authority that we typically just call it the issians authority. So the important property of anorum's credentials is that after the user obtains their Credential, they should be able to use it to create proofs on their attributes in a privacy preserving way. So for example, let's say that this owner of this digital Credential wants to buy alcohol and they want to prove that their date of birth corresponds to an AIDS that is over 21 years old and this is the only predicate that they are wishing to reveal and nothing else about the rest of their attributes. So having an anonymous Credential should allow a user to go ahead and create such a proof. That only proves this very specific predicate to some proving server to some Verifier server over there. The important property is that this proof that they present to the Verifier should be unlinkable to the original Credential that they got in the first step. So in other words, this means that even if this ishing authority colludes with this verifier, with this server here, again, they shouldn't be able to link together that Credential with that proof.
00:07:06.780 - 00:08:04.604, Speaker A: And in the typical case of anonymous credentials, a user should be able to create more proofs on their Credential, potentially on different predicates or even the same predicate. And still those proofs should be unlinkable to each other and of course, still unlinkable to the original credential. So that's the basic setting of anonymous credentials. And if you think about how can we construct anonymous credentials, that's a pretty straightforward generic way that can give us anonymous credentials. So what would be this generic and simple construction? What is the credential? What can we do during the ACMs process? So essentially we could just have the ishing authority to compute a signature on top of all the attributes of the user. And this is the credential of the user. Okay, so this is something that it's verified, that it's coming from the issuance authority.
00:08:04.604 - 00:09:05.616, Speaker A: It's something that because of the digital signature, we can easily tell that it has been vouched for from the Asians authority. Then how can we produce those private proofs on our credentials? Well, we have these very powerful tools in cryptography that we call zero knowledge proofs. And it's very, very easy to come up with zero knowledge proofs on different predicates on our credentials. And at the same time, these proof should also be proving that we do own a valid signature from the issuer on this blind attribute that we are revealing some predicate on. So this all sounds nice. However, such generic constructions are not necessarily very efficient. And that's why there has been all this line of work in the literature that we call anonymous credentials, which essentially tries to create customized protocols for this process.
00:09:05.616 - 00:09:55.312, Speaker A: So we're not using any digital signature. We are using digital signatures with certain properties that will allow for efficiency and all its proofs. Or we use customized zero knowledge of proofs and not generic snacks. So that said, if you look more carefully in the literature of anonymous credentials, you could identify two basic categories of anonymous credentials. The first category is the one that we call multiuse anonymous credentials. And this fits better to the scenario that I have been explaining so far. What is the idea of multiuse anonymous credentials? These are credentials that I get issued once, so I talk to the issuance authority once I get my credential, and then I can use them multiple times.
00:09:55.312 - 00:10:44.960, Speaker A: I can use the same credential multiple times as long as I produce fresh proofs every single time I'm using that credential. So typically these credentials, as I said before, they are based on digital signatures and zero knowledge proofs. And again, as I said, a lot of work in the literature has been focused into constructing signature schemes that allow for efficient zero knowledge proofs on the underlying methods. The second type of anonymous credentials is what we call single use anonymous credentials, or what we very often also call anonymous tokens. What is the idea there? The idea there is that every single Credential that you get issued, you can only use it once. Okay. So you can think of this as like single use tokens.
00:10:44.960 - 00:11:36.144, Speaker A: How are these schemes typically constructed? They are typically constructed using a special type of digital signatures, which are called blind signatures, and I will explain soon and optional zero knowledge proofs if you want to reveal more complicated policies on top of your credentials. So there has been a very long line of work in the area. I'm not even including everything here. And anonymous credentials have also received interest from the industry, and we have seen various projects attempting to implement them. But as far as I know, there is again renewed interest for credentials from companies like Microsoft. So let's see how that goes. All right, so back to our decision between our distinction between multi use and single use credentials.
00:11:36.144 - 00:11:36.548, Speaker A: Yes.
00:11:36.634 - 00:11:41.572, Speaker B: Can you explain how signal uses and what anonymous credentials they are?
00:11:41.626 - 00:12:46.250, Speaker A: Yeah, so they're using some sort of multi use anonymous credentials, and they essentially use it to prove properties when they create some groups, I believe. So you want to prove that you satisfy some property to create some groups for messaging? I think that's what they're doing. I'm not sure how I haven't really followed at what point their implementation is, but there was a paper on it, I believe, last year at CCS, and I'm happy to give you references. Okay, so back to the distinction between the two types of credentials. The multi use credentials are typically used in applications where you have some sort of subscription service you want to sign up once, you want to get your Credential once and then be able to use it multiple times. And if we ever end up having digital identities with privacy, then again, this would be some sort of multi use Credential. You get it issued once, you use it many times.
00:12:46.250 - 00:13:11.728, Speaker A: While single use anonymous credentials are more natural for applications like single use coupons, electronic cash, electronic coins, electronic voting. And the main property of single use credentials here is that what I want to claim is that they're more efficient as long as you know that they're only going to be used for a small number of times.
00:13:11.894 - 00:13:21.140, Speaker B: They're also vehicle to vehicle communications. The US. Built this huge infrastructure for single use credentials, and the argument is efficiency on the device on the car.
00:13:21.210 - 00:13:48.140, Speaker A: I didn't know that. I'm happy to see references. Thank you. So, again, in general, the main idea is that because they don't necessarily require the use of zero knowledge proofs during presentation and presentation of a single use Credential is literally just verification of a signature. They're much more efficient although again, there is the caveat that you need to run the issians process every single time you want to obtain a new Credential.
00:13:49.040 - 00:13:53.564, Speaker B: Also, don't you typically need to keep some list of spent credentials?
00:13:53.612 - 00:14:56.784, Speaker A: Yes, I'll get into that. All right, so the topic of this work though, is to try to decentralize anonymous credentials. So now that we're all experts on what anonymous credentials are, let me complicate the system a little bit and talk about decentralization. So all the classic anonyms credentials constructions that we had in the literature were Credential constructions that were assuming a single issuer, a single Issuance authority. However, in certain scenarios, you might want to have more than one issuers issuing collectively a digital Credential. So the most relevant work for us in the setting, and again, I'm not including every single work here, was the work of Credential, decentralized Credential issuance by a paper that appeared, I believe, in NDHS 2017. Sarah.
00:14:56.784 - 00:15:53.712, Speaker A: Well, quite a few years ago, Sarah was one of the authors and they were using threshold blind signatures for the Credential issues before that. And since Matthew is in the audience, there was another, maybe the first attempt to do decentralized anonymous credentials, but the setting there was a little bit different. So you were not just assuming that these issues were specific authorities, it was embedded into a blockchain system. However, here the idea was the following. So the idea was that we want to construct a multi use anonymous Credential scheme where there is a fixed set of valid issuers and you have M issuers. And whenever a user wants to obtain a Credential, they can get signatures from T out of these N users. And this is what we call a Threshold signature.
00:15:53.712 - 00:16:51.460, Speaker A: So Threshold signatures and multisignatures share some common properties. So you could view a multisignature to be an N out of N Threshold signature. However, when you try to implement these schemes, you're going to see that there are certain caveats. So here, just to wrap up this part, it's not just that they needed a Threshold signature for the issuance, they also needed to convert that into a blind signature. Why? And again, I'm going to get into how the scheme works. But the idea was that they wanted to make sure that during issuance there is this angularity property between the Credential that is being obtained. So the caveat of this work that mainly comes by the use of Threshold signatures is the fact that there is this interactive and non flexible threshold key generation phase.
00:16:51.460 - 00:17:32.964, Speaker A: So this set of signers here is fixed. They need to run a protocol among each other in order to create their key pairs ahead of time. If you want to add a signer, you need to rerun the protocol. So this gets pretty complicated. And also the verification of those schemes on the chain was pretty expensive. So in this work, they were assuming that the decentralized credentials would be eventually presented on a blockchain. So in such a scenario, we really, really care about the cheapest verification possible.
00:17:32.964 - 00:18:12.944, Speaker A: And verification, at least in this original scheme, was pretty prohibitive. So what are we trying to do? I should also note that a few months ago there was also some work that was optimizing verification for the coconut scheme and they were essentially using Bat zero knowledge proofs in order to combat the verification costs on the chain. So our main goal when we started to work on this project was to come up with a trade off between this problem of requiring this interactive, non flexible kid generation phase among the signers. Yes.
00:18:13.062 - 00:18:14.688, Speaker B: Like, what was the actual construction they.
00:18:14.694 - 00:19:04.784, Speaker A: Were using in that the Zebra scheme? They were using the coconut construction, which is what? So this was a threshold blind signature construction on top of PBM signatures. I don't know how many details you want to know, but basically what they did. So in the original coconut construction, they didn't do any optimizations on verification. And again, I believe this was a 2017 paper. So I think they were just evaluating the cost of every single verification which required zero knowledge proofs. And this Zebra work essentially batched these zero knowledge proofs together. And to be fair, they also did some extensions to add some discussion on revocation and so on.
00:19:04.784 - 00:20:17.556, Speaker A: But I'm not touching that on this talk. But the main contribution was batching zero knowledge proofs with tools that did not exist in 2017. All right, so our main observation, the main problem that we're trying to tackle here and offering a trade off essentially, was to try to avoid this expensive key generation phase among the issues and have a very flexible issuance construction. So instead we said, what if we build blind multisignatures as opposed to blind threshold signatures, which offer a very flexible signing process so the user can just pick on the fly which signers they want to sign their Credential. These signers never need to interact with each other, and in this way we can have this issians process to be very, very ad hoc. So of course, this comes with a cost that you can either see it as a cost or a benefit. So multisignatures as opposed to threshold signatures offer signer accountability.
00:20:17.556 - 00:21:30.560, Speaker A: So when you're looking at a multisignature, you can immediately tell who are the issuers, who are the signers of this signature. And in that sense, this might be a feature you might want to be able to tell when you are looking at anonymous Credential, when you are looking at the proof of anonymous Credential, who were the issuers that vouched for it. It might be important, especially in cases of misbehavior, you want to be able to potentially penalize issues and so on. But the main disadvantage here is that this reduces the set anonymity. Okay? So now your Credential is kind of hidden only among credentials that have the same set of issuers. The good news is that if you run this on a blockchain setting, you kind of have control set anonymity so you can see how many credentials have been issued with the same set of signers and you can decide which signers you want to pull over when you're designing your Credential. So as I said, it is a trade off and it can be application, it can depend on the application on what you want to do.
00:21:30.560 - 00:22:04.350, Speaker A: A second difference from these prior works was that we focused on the case of single use credentials and none of these prior works were doing single use credentials. All of them were doing multi use credentials. Our construction is based on BLS signatures. And now I'm going to go over how BLS signatures work in order to first show you how our blind multi BLS signature works and then bind everything together for the Credential scheme questions up to here.
00:22:06.480 - 00:22:08.728, Speaker C: So for the scheme, the public keys.
00:22:08.744 - 00:23:19.312, Speaker A: Are independent, the public keys are completely independent. And when the user is starting an issuance process for Credential, they can just decide which signers they want to sign. So let's get into the construction of blind multi signatures first and to do so I've been talking about blind signatures for quite a while now, but let me now formally introduce what a blind signature is. So a blind signature is again special type of a digital signature which typically works as follows. So you don't now have a signer that owns the message and they sign the message on its own. Instead you have a signer and you have a user who owns the methods and wants to receive a signature on these methods from the signer without the signer ever learning what the message was. So the generic way to construct a blind signature is to start by having the user to blind the message, somehow commit to the message, somehow send this commitment to the issuer or the signer.
00:23:19.312 - 00:24:28.840, Speaker A: The signer is going to sign this committed value and then the user using the randomness that they used in order to commit to the message. In the first step, they can unblind the signature that they received, this sigma prime that they received from the issue. So the main properties of blind signatures is that they have to be blind or else unlinkable. And this essentially says that if you look at the transcript of the protocol, this commitment to your methods and this initial signature sigma prime, you shouldn't be able to link that to the final signature on the final message. All right? So even if the signer gets to see the final signature sigma and the methods M, of course they can tell that this is a signature that came from them. It verifies under their public key, but they cannot link it to any issuance process with any user. The second property, which is the natural property that we want from every digital signature scheme, is that of unfordability.
00:24:28.840 - 00:25:25.216, Speaker A: However, we cannot just assume plain unforgettability here. We cannot just say, oh, it should be the case that a user cannot forge signatures on their own. So in the security definition, we have to be a little bit more careful. So typically in the unforgettability definition of digital signatures, we're just saying that you have an adversary that asks for signatures from a valid signer and then this adversary wins if it outputs a signature on a message that it didn't query for. So we cannot do that any longer in the case of blind signatures exactly because the signer doesn't know which signatures it computed during the signing process. So we don't have any way to check if the signature that the adversary outputs at the end is one that the adversary queeted in the first place or not. That's why in blind signatures we have a special flavor of unfortunability that we call one more, unfortunately.
00:25:25.216 - 00:26:30.240, Speaker A: And it essentially says that if you have an adversary that gets Q signatures during signing queries from the issue, they shouldn't be able to output Q plus one valid signatures. And that's the main property of blind signatures. So in this work, and for the first time, we define what we call blind multisignatures. So here the setting changes a little bit and instead of having a single signer, now we have multiple signers. So all of them are signing the same methods. The user might be sending different commitments to each one of these issues, receiving their blind signatures and then aggregating the signatures to the one and final output multisignature here, or potentially, again, you could have interaction among signers. But everywhere in our work, we don't want to assume any sort of interaction among signers and the particular signature scheme that we'll use, the BLS signature scheme, will allow for the segregation.
00:26:30.240 - 00:27:24.508, Speaker A: So now, in terms of security properties, again you want correctness, you want this one more affordability that you had before, but you also want to be able to bind this with properties of blind, of multisignatures. So the setting becomes a little bit more tricky. And now this adversary that can obtain signatures can also corrupt all but one of the signers. And in order for this adversary to win, it should be the case that in the forgeries that it outputs at the end, it still includes this key from the onass signer that has not corrupted so far. So we need to essentially combine the properties of multisignatures and blind signatures. Finally, as before, we want blindness. And the blindness or an Ecbility property is very similar to blind signatures, as I stated before.
00:27:24.508 - 00:28:27.460, Speaker A: And we typically model that by having the adversary to run two sessions of signing. So here the adversary is the signer and they should not be able to decide when they see the signature whether the signature corresponds to signing method zero or method one. So again, we need to combine properties of blind and multisignatures together. All right, so having defined that new building block, let's see how we can build that out of BLS signatures. And the construction is actually pretty easy since we already have blind BLS signatures and BLS multi signatures. So we just need to put them together. So, to see how this work, let me first introduce how plain BLS signatures work and then get into the case of blind BLS and multi BLS.
00:28:27.460 - 00:30:02.500, Speaker A: So, BLS signatures, as I said before, is a very popular signature scheme that is defined over pairing friendly elliptic curves. So the idea is that in order for the BLS signatures to be verified, they need to assume what is called a binary pairing map that has this property here that will allow us to do verification by moving the exponents around. So how do BLS signatures work? You have to define a set of parameters, a group Z of paramorder Q with some generator equal Z, where the discrete loggers problem is hard when you generate keys, your keys essentially have the discrete log relation. You pick a secret key randomly from Z star Q, and you set the public key to be Z to the secret key. When you want to sign a message, you has the methods with a special has function that has its two curve points and you raise that to the secret key. And then in order to verify, you'd check if this pairing equation holds, you give us input the signature and the generator Z, and you'd check whether this is equal to the pairing that takes us input the hash of the message and the public key. And if you replace the public key here with Z to the secret key, you can see and the signature here with half of the message to the secret key, you can see that it's pretty trivial high verification works since it allowed since because of the properties of the pairing, you can move around the exponents.
00:30:02.500 - 00:31:05.288, Speaker A: So, BLS signatures are deterministic and unique. And let's now see how we can turn them over to blind signatures. So now, as I said, a blind signature, it is a two step process. So you start with a user that has the methods, and as the first step, they want to blind the methods that they hold before sending it over to the signer for a signature. How can we do that? We have the message and we randomize it with some random value R, by taking the multiplication of the half of the message with C to the R, and we send this commitment to the message over to the signer. The signer will literally just raise this value to its secret key in order to compute the signature and will send this blinded signature back to the user. The user now can unblind this signature by multiplying with a public key of the signer raised to minus R, where R was the blinding factor that used in the first step.
00:31:05.288 - 00:32:02.990, Speaker A: So again, if you do the algebra here, you are going to see that you will end up with a signature that verifies on the original message. M. Okay, so blind signatures blind BLS signatures are not something new. They were proposed by Baldira in 2003. Blindness holds unconditionally and unforgettability works under a strong assumption that is called the chosen target CD eight assumption. And the proof was done in the Random Oracle model. So a first contribution of this work, although it's kind of a side thing, is that we came up with a new enforceability proof for blind BLS signatures that uses a more standard assumption, the QD log assumption in the Random Oracle model, but also requiring the algebraic group model.
00:32:02.990 - 00:32:48.644, Speaker A: So to be fair, it's really hard to compare those two settings. It's not really clear which one is preferable or not. So on the one hand, here you have this chosen target assumption, which is not ideal, but you're only using the Random Oracle model. Here you have a more standard assumption, Qdlog, but you also require the AGN model. But the advantage of our proof is that it gives a much tighter security reduction compared to the original one. And this is kind of important if you imagine these schemes being implemented in a large scale with multiple signers signing messages at the same time. So the tightness of the reduction might be a benefit.
00:32:48.644 - 00:32:57.224, Speaker A: Here, again, you can view this as a kind of side contribution of this work. Having an alternative proof for blind pls fatina.
00:32:57.272 - 00:33:04.636, Speaker B: Could you say a little bit about why it seems like you need stronger assumptions for the blind construction?
00:33:04.748 - 00:33:58.370, Speaker A: Yeah, well, blind signatures are actually a very flimsy cryptographic building block, proving their security has been pretty tough. There have been results a couple of years ago, crypto that they actually so in particular breaking concurrent security. So if you want to have a signer that opens multiple signing sessions at the same time, it's pretty hard to get this level of concurrent security. People have shown that there are these ros problem based attacks that you can actually do if you have many concurrent sessions open. So getting OK security has been pretty tough and typically this is resolved by using hardware assumptions. Yes.
00:33:59.140 - 00:34:17.456, Speaker C: I can ask you an alternative way of blinding. When you just take H to the H of M and raise to the power of R, this is kind of maybe additive blinding, but you can also do multiplicative, I guess, right, when you for the C instead of so you're.
00:34:17.488 - 00:34:20.440, Speaker A: Saying just literally blind it by raising it to R?
00:34:20.510 - 00:34:33.560, Speaker C: Yeah, people looked into that. And then you compute Sigma Prime, you just raise it to one over R to remove the R.
00:34:35.290 - 00:34:38.618, Speaker A: I'm not so sure. Okay, I would have to double side. Yeah, I'm not so sure.
00:34:38.784 - 00:34:45.082, Speaker B: How do we feel about these assumptions or these reductions in the AGM and the Random Oracle model? Are we. Happy with them.
00:34:45.216 - 00:35:33.306, Speaker A: Yeah, that's a fair, you know, AGM. My view at least is that it is a new model that we still don't understand very well. So there are some proofs that we managed to do that were impossible by just assuming the random oracle before. There were certain flaws in some of these proofs as it was shown by some papers that had criticism of the Ada model. So at least my take is that it's a model that it's not very well understood yet, but this doesn't mean that we cannot play with it. So at least the fact that it gives us a more tight reduction, it's something that is interesting. It's not the only proof that we have for this game.
00:35:33.306 - 00:35:34.662, Speaker A: It's an alternative proof.
00:35:34.726 - 00:35:38.206, Speaker B: Yeah, there's like some kind of discrete, one more discrete log proof you could.
00:35:38.228 - 00:36:18.314, Speaker A: Probably yeah, well the chosen target is also some sort of like one more assumption. Yeah. So again, it's not the only proof, it's the proof that at least currently would give us the tightest reduction. So how much you believe in AGM, I think it's still something to know. It will take some time. And again, I'm happy to talk about this as a disclaimer. I should note that co authors in this paper are the inventor of the AGM model is Julian Lowe's and also John Kadz, who is one of the people that criticized the AGM model but they were still happy with it.
00:36:18.314 - 00:36:55.320, Speaker A: Again, as long as it gives a tight reduction at this point. All right, so let us now see how we can turn this BLS blind signature into a multi signature. As I said, this blind multi signature is a new primitive, but we already have blind BLS and multi BLS. So we essentially just had to combine them together. We didn't do anything super innovative in this cryptographic being blocked. So again, we're going to do a blind signature but with multiple issues. Each one of them has a pair of a secret key and a public key.
00:36:55.320 - 00:38:35.510, Speaker A: In BLS multi signatures you can do the public key aggregation in the following way. So you can multiply all the public keys of the signers together after computing the exponentiation of them to this randomizing factor AI, which for every public key works as follows you has together all the set of the signers and the public key of the corresponding signer for whom you want to compute AI. Okay, so you compute this randomizing factor for every single public key and then you multiply all the exponentiations of the public keys to this random factor together to compute your joint public key. So then the user can compute all these commitments to the methods that they want to get a signature on and they randomize them for different random values. They get back these blind signatures from each one of the signers. So I only have this for the first signer here, but essentially, as we said, before every single signer signs by just raising to their secret key, raising this commitment to their secret key. And then the output is again, the aggregation of all the signatures obtained by the issuers by again raising this to this randomizing factor.
00:38:35.510 - 00:39:09.810, Speaker A: So something to note here is that you don't necessarily have to do the key aggregation at this step. You can do it here. And the nice part, as I said, is that you can have flexibility. Even if one of the issuers doesn't complete the protocol and they disappear, that's perfectly fine. If the user is okay to have a Credential issued by the rest of the issuers. They don't have to do anything, they don't have to restart the protocol. Again, they can just combine together whatever signatures they got, potentially fetch another one and put their signature together, the signers on their side.
00:39:09.810 - 00:40:06.420, Speaker A: Again, they're completely oblivious on who else is signing the Credential. They don't have to know, they don't have to interact with each other. Okay, so everything is on the signer side, on the user side. Is it clear how the protocol works? Again, nothing new here. The way that we do this aggregation of signatures for the BLS setting and this aggregation of the public key, it's very standard, is exactly how it happens for BLS multi signatures. All right? Verification. The nice thing about, and one of the most important properties of BLS multi signatures is that you can combine together multiple signatures and the verification, it will still cost you just two binary pairings because you are only verifying using the combined the aggregated signature and this aggregated public key.
00:40:06.420 - 00:40:34.170, Speaker A: All right, so we do prove unforgettability of this BLS blind multi signature. Again, as I said, this is a new primitive and we essentially do a reduction to our blind BLS proof and thus we require the same set of assumptions. It wouldn't be impossible if we didn't want to use the setting to again reduce our proof to the original proof by bolderva. If you don't want to assume.
00:40:36.610 - 00:40:48.090, Speaker B: Maybe I missed it, but why are all the C values that the user is sending blinded separately? Is there a version of this where you use the same blinding factor?
00:40:48.250 - 00:41:55.190, Speaker A: That's a good point. So the reason that we do it separately is because we want at least at the point of Issuance designers to not, even if they talk to each other, to not see that they're interacting with the same user. But you could do it separately as well. Another note that I wanted to mention here was that the nice property now also of these BLS multi signatures is that you can support further aggregation of BLS multi signatures. So if you have multiple credentials issued by the same set of issuers when you're verifying them, you can also aggregate them together before verification and further reduce verification costs in pretty much exactly the same way. So you can keep on aggregating the signatures this way as long as APK is the same, as long as the set of issuers remains the same. Okay, so again in our Credential setting if when you're trying to verify this Credential on chain, you see that there are more credentials coming from the same set of issues from different users on different messages, before verifying them, you can aggregate them together.
00:41:55.340 - 00:42:01.142, Speaker C: If one of them doesn't respond with their signature, you can still aggregate and just leave out that.
00:42:01.276 - 00:42:47.542, Speaker A: Exactly, yeah, so I basically said that this key aggregation process doesn't have to happen first, it can happen here. Okay, so the order doesn't really matter. Okay, thanks. Yeah. So again, as long as the user is happy with the signatures that it got from issuers, they can just move along. The nice property is that you don't have to rerun the protocol. All right, so how do we use that primitive? Now in order to build single use credentials and let me first assume that we want to build a very, very plain case of single use credentials of anonymous tokens that do not have any special properties on, they don't try to encode into the digital token any attributes of the user or anything like that.
00:42:47.542 - 00:44:02.000, Speaker A: Think of these credentials just literally being random numbers, some sort of like random serial numbers. Think of this as being one time tokens that you want to get issued. So in that case, if you don't have attributes in the picture, then using our scheme is very straightforward. What do you do? You literally have the user pick this random serial number and run the Asians process with whatever issues they like. They can obtain the signature and then using this token essentially means presenting the signature and verifying the signature. Of course there is this very valid question in all these single use casino schemes that Joe raised before, how do we avoid double use of the same token? And the most simple way especially these tokens will end up being verified in a blockchain setting is to literally just observe all the signatures that are being presented and check if something has been used before in order to just drop it if it appears again. There are other techniques that people have built on top of that like adding expiration dates, changing the keys of the issues and so on, but these are all kind of orthogonal to what we're doing.
00:44:02.000 - 00:45:12.550, Speaker A: So now what can you do if you actually want to also have attributes in the issued Credential? So when you also want to have attributes, this naturally requires a two step process during issians. So you have to assume that every single user that holds these credentials will first run some sort of registration process with the issuers and in that registration process they will have to essentially disclose their attributes or formally do a zero knowledge proof on their attributes. And the point is that during this registration phase, both or this kind of account opening phase. Both the issuer and the user will agree on a common commitment over the user's attributes. And this will be kind of the account number of the user with this issuer. Whenever later you want to get a Credential signed by this issuer, you will have to somehow encode that same commitment to the user attributes during this issuance process. And of course, there is a challenge here.
00:45:12.550 - 00:47:12.090, Speaker A: How do you do that in a way that still maintains Amigability? Okay, so how if I know, if the issue knows that it's signing on top of the commitment of the user, how can we make sure that later, when this credential will be revealed, that the issuer cannot just trivially say, hey, this includes this commitment of this user that I have an account for, so it must be him. So a way to avoid that is during the issuance process, to have the user to send a randomized commitment along with a proof that this commitment is the correct randomization of its original commitment. Okay? And again, you can view this as a generic way to build a single use Credential from a blind signature. The interesting part here is that specifically for the case of BLS signatures, doing this zero knowledge proof on top of the commitment that needs to get signed, it is a little bit expensive because in BLS signatures you have to do this has to a care point of the methods. So this gets a little bit tricky because then in your zero knowledge proof, you have to prove something about the output of this hash function. An alternative protocol that has cheaper issuance but more expensive verification is and again, you can view those two protocols as generic ways to construct single use credentials with attributes from blind signatures. So this third protocol that we present, it essentially uses a novel cut and seuss technique in order to convince the issuer that they're using that the user is using the correct commitment of attributes in the issue Credential.
00:47:12.090 - 00:47:49.266, Speaker A: And the trade off here is that this requires multiple rounds. You will essentially end up with more than one signatures to verify at the end and you're going to have more expensive verification. So these protocols offer some trade offs. And again, depending on what your application wants to focus on, more efficient issues or more efficient verification, you can pick between one of them. Again, if you don't care about attributes, things are much easier. You literally just show signatures at the end and this is it. So just some basic verification, some basic evaluation results.
00:47:49.266 - 00:48:50.522, Speaker A: Evaluation is still work in progress in this work. So first of all, again, compared to prior decentralized Credential schemes that use threshold signatures, we do achieve a flexible selection of signers, but with small anonymity sets. And we also give the first construction for single use decentralized credentials. We have a proof of concept verification results using Ethereum. So assuming that you want to verify all these credentials on the Ethereum blockchain. So recall that Zebra was the optimized version that was doing like, zero knowledge batching on top of the original coconut credentials. So for our scheme, if you want to verify a batch of 64 credentials, the amortized cost would be with using May 2003 prices, it would be zero point credential, while for Zebra it will be 2.24.
00:48:50.522 - 00:49:19.778, Speaker A: And for a batch of 512 credentials, again, you can see that it goes from 0.4 to 1.6. Again, the important thing to note is that for our credentials and this was the basic scheme that you don't have any attributes or anything. It just requires a single BLS verification. While for verifying these coconut or Zebra style credentials, you have to assume that the underlying blockchain supports zero knowledge proof verification. Yes.
00:49:19.944 - 00:49:27.686, Speaker B: Are you using the pre compiles for doing pairing based? Are you using the BN 250, the.
00:49:27.708 - 00:49:32.674, Speaker A: Ethereum implementation, for the built in one? Yes. Which I know that there might not be ideal.
00:49:32.722 - 00:49:36.038, Speaker B: Got it. Okay. But that would be the same pre compile used for Snark verification.
00:49:36.134 - 00:49:37.562, Speaker A: I would assume so, yes.
00:49:37.696 - 00:49:55.518, Speaker B: From the previous slide, cannot you use group elements as attributes or just use some algebraic hash function like Peterson hashes? Like, I don't know, the attribute would be M or G to the M or something? Like, you don't want to use this ugly non algebraic function.
00:49:55.604 - 00:50:12.402, Speaker A: The problem comes from the fact that you do need to hash the methods in the BLS signature. So the problem is not how I construct a commitment. The problem comes from the fact that the signature needs to be a hash of the commitment.
00:50:12.546 - 00:50:14.898, Speaker B: And cannot you use some algebraic hash.
00:50:14.914 - 00:50:16.086, Speaker A: Function instead of oh, you mean a.
00:50:16.108 - 00:50:20.006, Speaker B: Different hash function hash or something?
00:50:20.188 - 00:51:57.140, Speaker A: Well, it still needs to map to an analytical point for the BLS construction, right? Yeah, I'm happy to talk about this more, but for BLS signatures, you still need the fact that the hash function maps to a point. All right, so moving to the second part, which is, as I said, much shorter, keeping the same theme of BLS multi signatures, we are going to look into a different application, and this is signing blocks in proof of Stake protocols. So I'm not going to spend too much time explaining what proof of stake is. But the thing that I want you to focus on is that in these Proof of Stake consensus protocols, in every epoch, you have a static set of validators and then subsets of these validators sign every single block that ends up on the chain. So to set up notation, we're going to have a fixed committee of N validators per epoch, and we're going to have subsets of M validators computing multi signatures in every single block. So multi signatures have been very popular in this Proof of Stake setting because you want to have sort signatures by a set of signers on the same method, which is essentially your block here. So BLS multi signatures have actually been a very popular primitive for Proof of Stake protocols exactly in order to do this signature aggregation.
00:51:57.140 - 00:52:41.970, Speaker A: So in the previous slides I showed you how blind BLS multi signatures work. So let me just abstract out the BLS multi signature part. So as before, you have this like group G of prime order Q with generator Z secret key and public key has the discrete log relation to sign a message. Every validator in the multi SIG setting is going to half the block, raise the output to its secret key and randomize it by this factor AI. And I'm going to say what this factor AI is. We saw that before as well. And you aggregate the signatures by just multiplying them together.
00:52:41.970 - 00:53:48.452, Speaker A: So this AI factor is the same factor that you use to randomize your keys during key aggregation. And again, this AI, and I should note here that those are two different has functions. This AI is again the hash of all the public keys of the signers of this block concatenated with the public key of the particular signer that you are randomizing. Their public key verification is again just checking this pairing equation. And the observation here is that in Proof of Stake, this process is going to repeat for every subset of M validators. Okay? So every single time that you have a subset of M validators that they want to sign the next block, they will have to repeat this process. So in this work we had this very simple observation and we said, well, in Proof of Stake protocols you do have this large committee, this larger committee of N members which is static during the whole epoch.
00:53:48.452 - 00:55:14.310, Speaker A: And then you literally just have subsets of this committee signing blocks again during the same epoch. So having this very specific structure, can we do something better? So this was the question that we asked and the observation here was that it turns out that it is possible to run this key aggregation process once per epoch for the full set of all the N validators, no matter which subsets of N validators compute signatures later. So what is the benefit of this? The benefit of this is that you're not going to save anything on verification, so you're not going to save anything on the binary pairings, but during the signature process you are going to save a lot of exponentiations by not having to repeat this key aggregation process. So here is how our scheme works. And just to see the difference, I'm keeping the original scheme of BLS multisignatures on the one side of my slide. So now again, we have this fixed committee of N signers, and then we have subsets of N signers, the validators that validate every single block. So the key generation algorithm is exactly as before.
00:55:14.310 - 00:56:09.190, Speaker A: Every single signer is going to create the key as a regular BLS multisig. But now we assume the following. At the beginning of the epoch, once the whole committee has been identified, all Ncommittee members will run this key re randomization process as follows. They will re randomize their key by taking these AI factors that you use for key aggregation using the full committee that you know ahead of time. So you're going to re randomize your public keys using this randomizing factor that uses the full committee, and you're also going to re randomize your secret key. Okay? You do that once you pay all these exponentiations once at the beginning of the epcot. Now, every single time that you want to sign.
00:56:09.190 - 00:57:13.160, Speaker A: Now, also keep in mind that when you're signing you have these subsets of these smaller number of validators. And I'm realizing that the selection of M for the number of validators is bad notation because I'm using M for my methods as well. But anyway, the idea now is that every single one of these invalidators, when they want to sign, they sign using a regular BLS signature, but using the randomized key, where again for the randomization they use the full set without really mattering who is participating in the subset that is signing now. And you aggregate the signatures again just by multiplying them together. The key aggregation doesn't need to do any new exponentiations. You're just aggregating the keys of all the subsets by multiplying together these randomized keys. So again, the observation here is that you have to do these exponentiations only once at the beginning of the epoch.
00:57:13.160 - 00:58:05.400, Speaker A: Every single time you sign, you just sign like doing a regular BLS signature and you just multiply the signatures together without extra expunciation. So this seems like a pretty reasonable way to save verification is the same as before. This seems like a pretty reasonable way to save some quite significant costs during signing. And you essentially save MX pronunciations per signature, assuming that you pay this cost once. And just to convince you, we have some implementation results. So just to decode this graph. So this is SMS, is how we call our subset multi signing scheme for different number of signers.
00:58:05.400 - 00:58:52.600, Speaker A: Here you can see the execution time that it takes to compute a signature. You see two versions of the scheme. So in BLS signatures, depending on which curve you want to have your signatures on, and in which curve you want to have your keys on, you might end up with different implementations. So these are the mean SIG and the mean public. Here the two different cases that you see depending on whether you want to minimize your public keys or your signatures here. And you can see for each case two different costs. How much does it cost to just aggregate the signatures together during signing and how much does it cost to aggregate the keys together and verify.
00:58:52.600 - 01:00:11.364, Speaker A: So obviously verification, because it also includes the padding computation, is always higher than just signatures, just signature aggregation. And again, the observation here is that you don't have to do any exponentiations on signature aggregation, you're just literally multiplying signatures together. If you compare that with the original scheme, the original BLS multi signatures, where these are basically these red and golden lines here, you can see that these exponentiations, they can actually have pretty high costs during signing. And while in other case, as you also observed from the previous slide, they kind of remain pretty flat with the increase of multiple signers because again, the only cost that you're paying is multiplications. So the interesting part though is that proving security of this scheme with these chains ended up very, very tricky. So when I first saw these chains of the BLS multi signature scheme, I said, oh yeah, that seems very natural, I bet that we can reduce security to the original BLS multi signature. So as it turns out, the original BLS multi signature scheme is secure under DeFi hellman in the random Oracle model.
01:00:11.364 - 01:01:29.336, Speaker A: And it is using a special technique that is called the forking technique in the security proof, which essentially means that you have to rewind the adversary and rerun the protocol again. So when we tried to work on a similar proof for our protocol, we realized that the fact that now our adversary can pick different subsets of messages during the signing queries, it could essentially make this proof technique to fail. So our first approach and the first proof that we have in the paper is a proof under a wikilesary that fixes the subset ahead of time and says, this is the subset for which I'm going to create a forgery. So if the subset is fixed ahead of time, this forking technique and this rewinding in the security proof works as before. So in this case, for this weaker adversary, we can get a security proof under the same assumptions under the original one. Now, if we don't want to assume this weak adversary, then we can again engage, we can again use the AGM model and have a security under discrete log. But unfortunately, this security proof will have an exponential security loss, which again is not ideal.
01:01:29.336 - 01:03:18.460, Speaker A: As a matter of fact, we saw that this exponential loss is inherited in this protocol and we actually saw that if there is an adversary that can break this problem that is called the random modular subset sum problem, they can break the security of the scheme. So in this problem, this problem, it basically says that if you have a set of indiggers and an integer target t, you're trying to find out if there is a subset of integers that sums to the target t. So this assumption is deeply related to how the security proof works here, and I'm happy to spend more time offline later explaining how that works. So in order to come up with our second proof, we had to use this assumption as well, and the interesting part that I want to say here though is that you don't necessarily need this assumption if you know that the size of your committee is not very large. So in particular, if you know that the total number of possible subsets is negligible to the output of this has one has function that you're using when you're randomizing your keys, then the probability of finding such a substitute sum solution is negligible. How does this translate in practice? So we have this analysis in the paper that I have not included here which basically says that if you know that your committee size is a few dozens or like a hundred committee members, then the probability of breaking the scheme using this RMS based attack is negligible. And you don't necessarily need to make this assumption for your scheme.
01:03:18.460 - 01:03:22.224, Speaker A: I'll take my questions first about those.
01:03:22.262 - 01:03:25.536, Speaker B: Exponents that you were getting from Hashing that you were putting into the thing.
01:03:25.558 - 01:03:26.736, Speaker A: And you're basically saying is there some.
01:03:26.758 - 01:03:33.220, Speaker B: Combination there is actually an attack on this. I assume because they were out the outputs of a random oracle.
01:03:35.320 - 01:03:47.130, Speaker A: There is an attack. You can model it as an RMSS attack. Yeah, I can tell you more. We have to go through the security, through details of the security group to explain that.
01:03:47.740 - 01:04:00.460, Speaker D: So I don't know if you mentioned this paper that was like four or five years ago, the accountable subgroup multisig paper for BLS, but it's a very similar thing. So I don't know how the construction.
01:04:00.880 - 01:04:06.584, Speaker A: I think that this accountable construction was part of this multi BLS construction.
01:04:06.632 - 01:04:10.030, Speaker D: Oh, no, this is a paper from like 2018 or something.
01:04:10.720 - 01:04:20.930, Speaker A: Well, this multi BLS construction oh, I see. I know which paper you're talking about. They're not doing the same thing. I'm pretty sure they're not doing the same thing.
01:04:21.620 - 01:04:37.124, Speaker D: I mean, their accountable subgroup construction is pretty similar. Basically saying an arbitrary group subset of the signers can combine to have a multisig, but you can tell which signers did it.
01:04:37.322 - 01:04:44.380, Speaker A: You can tell here as well. I don't remember how that construction works, but I'm pretty sure we have seen it and it is different.
01:04:44.490 - 01:05:07.664, Speaker D: I'm happy yeah. Oh, I'm not saying it's, but my point was they can prove security without the exponential loss. I'm curious what I mean, they have to use the forking llama to really gnarly proof and stuff, but I'm just curious why you think why it worked for their protocol and doesn't work.
01:05:07.702 - 01:05:31.108, Speaker A: I don't remember how the security proof works, but I'm happy to take a look. Okay. Yeah. And I think that's pretty much it. Thank you everyone. As I said, this is a combination of two works. The first one is a work with my student Joanna, who is actually sitting back there and collaborator from University of Maryland and CISPA.
01:05:31.108 - 01:05:46.540, Speaker A: And the second work is work that is done at Misten Labs research for the optimization of PLA signatures. And I should say that we have also used the help of a 16 Z for their second work. It is a project that we have discussed with Lera and Dan.
01:05:47.200 - 01:05:58.652, Speaker B: I was wondering if you know something. It's called proof of possession. So I was just wondering how you compare your work with proof of possession because in that case, you don't have to do the random Oracle exponentiation.
01:05:58.716 - 01:06:55.700, Speaker A: Right, so that's a good point. So where do you use proof of possession? So in many of these multi signature schemes well, there are two different ways to construct multisignature schemes in order that they avoid the so called Rock key attacks. One approach is to use this proof of proof of possession, which essentially says that every user needs to prove knowledge of their secret key that corresponds to their public key. So that you cannot have rock key attacks or you can build signatures like the BLS multi signature that essentially secure against rocky attacks without the need of proofs of possession. So in the proof of stake setting, we have seen uses of signatures that require proofs of possession. So again, this is a trade off here, really, on whether you want to do proofs of possession or not. So again, in certain cases, we've also seen criticism on proofs of possession for the blockchain setting.
01:06:55.700 - 01:07:06.740, Speaker A: The fact that you require this extra knowledge proof, it can be annoying, but yeah, again, like proofs of possession or not, it is a trade off that is well known for malware signatures.
01:07:06.900 - 01:07:39.360, Speaker C: For the consensus for the second project, when you were hashing a list of public keys yeah. On some of your previous slides where you showed the construction yeah. Here say yeah, can you just use the hash of the latest block, say, instead of the list of the public keys? Because you can imagine this hash will attest to registrations of all of those public keys and possibly you can just reuse it if you're part of multiple consensus protocols with the same validator set or multiple committees.
01:07:39.700 - 01:07:44.020, Speaker A: So you're saying randomize your keys in a different way yeah, in different instances.
01:07:44.520 - 01:07:49.750, Speaker C: By using just the hash of the latest block, assuming everything kind of is written there.
01:07:50.360 - 01:08:03.764, Speaker A: Yeah. That's interesting. We can certainly think about that. Yeah, I'm not sure that it would help us with the problem of the subsets being defined in the security proof. I'm not exactly sure what the benefit.
01:08:03.812 - 01:08:10.940, Speaker C: Would be on the but yeah, your proof maybe will need a Tweak, maybe you're forking.
01:08:11.840 - 01:08:15.710, Speaker A: Yeah, that's interesting. That's definitely interesting. Yeah.
01:08:18.730 - 01:08:31.050, Speaker C: Does the hash function need to be do we need to feed it a superset of the public keys for the correctness to work? What do we need from the thing we're feeding to h?
01:08:31.120 - 01:08:50.846, Speaker A: Yeah. So this has function takes all the public keys of your original committee and you run it. So every time that you want to randomize, every time assigner they want to randomize their. Own key. They have to run this hash function with all the keys of the committee and their own key so that they get different outputs.
01:08:50.878 - 01:09:13.942, Speaker C: I guess I was saying that you can add the hash of the previous block. For example, you can clearly add randomness to it, but I was asking whether it's important that you at the very least feed it a superset of the public keys of the committees that you will later need to verify. Basically, if I give you just one subset, and it was guaranteed that that is the only subset you wouldn't need to hash.
01:09:14.006 - 01:09:47.220, Speaker A: So you need to be a little bit careful here because of these rocky attacks in multisignatures, you need to make sure that whatever you're has in there, it does somehow include information for all the possible signers. So you need to be a little bit careful with what you're having here. So assuming that the information of the previous block is already including all the information that you need, as what Lera was saying before is already including all the information you need for the next committee selection, this might work. But again, you have to be very careful because of these rocky attacks. Thank you.
