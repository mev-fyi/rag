00:00:07.380 - 00:00:07.930, Speaker A: You.
00:00:10.140 - 00:00:32.044, Speaker B: Welcome, everyone, to today's a 16 z crypto research seminar. Very pleased to welcome Jochem Noy from Stanford, where he's getting close to finishing up his PhD. He's done for many years, really excellent work in blockchain consensus, particularly with applications to the ethereum ecosystem. And we'll hear about his latest work on that topic today. So all yours.
00:00:32.132 - 00:01:04.632, Speaker C: Thank you so much. Yeah, it's great to be here. Thanks for having me. Welcome, everyone. So the topic of today's talk is optimal flexible consensus and its application to ethereum. That's a topic that I hope you'll appreciate, spans kind of the gamut from relatively consensus theoretic all the way to relatively applicable kind of to the ethereum stack as it is today. The talk is designed to take 40 minutes, so we'll see late start.
00:01:04.632 - 00:01:55.876, Speaker C: Maybe we'll still finish in time, though. This is joint work with Srivatsan, my PhD colleague at Stanford, lei Yang from MIT, another PhD student, and my advisor, David Che. And if you want to know all the nitty gritty details, the paper is up on archive. There will be a link later also to the source code that kind of comes with the ethereum part of the story. So let me get started. Here is my one slide recap of the consensus problem, which is really at the basis of blockchains that we see in the wild out there, right? So what happens in a blockchain? There's a bunch of replicas, a bunch of validators, a bunch of nodes running the system. They receive transactions from users, right? And then they run a certain protocol.
00:01:55.876 - 00:02:34.144, Speaker C: They talk to each other. They run what we colloquially call the consensus protocol, more specifically, the replica logic. Here, I write as Pi. And then there are some clients that have a wallet and they're interested in whether their transactions have taken place and in which order these transactions take place. So Alice and Bob, right, and the replicas are producing some sort of advice string that goes to the clients, and the clients then use what I call a confirmation rule here, denoted by C, in order to recover the sequence of transactions that was agreed upon by the blockchain.
00:02:34.192 - 00:02:34.404, Speaker A: Right.
00:02:34.442 - 00:03:18.580, Speaker C: So Alice recovers a log and Bob recovers a log. What do we want from this ordering service that is the blockchain? What properties do we want from such an ordering service to be useful? Typically, there are two properties. One is what we call liveness, which is that if there is a transaction that is honestly produced and being sent to all the replicas, then eventually that transaction should make it to the output log of every client. So that's somehow non censorship, if you will. And the second property is what we call safety, which is that any two clients, at any two points in time, they have a log, and these logs should be consistent. So one of the logs should be a prefix of the other log, or vice versa.
00:03:19.000 - 00:03:19.460, Speaker A: Okay.
00:03:19.530 - 00:03:25.588, Speaker C: So far, I assume you probably have seen this. And then the setting I'm in right here is why is this problem hard?
00:03:25.674 - 00:03:26.020, Speaker A: Right?
00:03:26.090 - 00:03:59.392, Speaker C: The problem is hard because there is delay in the network, right. When these nodes talk to each other, their messages don't arrive instantly. In fact, I'm operating in what we call the partially synchronous setting. So there might be a network partition. So sometimes for a sustained period of time, maybe there are some nodes that cannot talk to each other. And then also, some of these replicas, they're malicious, so I'm assuming F fraction of them might not even be following the protocol that we specify. And in that case, these consensus guarantees, we don't get them all the time.
00:03:59.392 - 00:04:04.016, Speaker C: We get them under the assumption that the adversary is not too strong.
00:04:04.118 - 00:04:04.480, Speaker A: Right.
00:04:04.550 - 00:04:33.096, Speaker C: So I'm going to introduce two parameters, if you will. One is called the liveness resilience, and I'm going to say, okay, I want to make sure that liveness holds if the adversary is weaker than this liveness resilience parameter. And then the same for safety resilience. Right. So there's TS, and I want to make sure that if the adversary is not too strong, then there is safety in the system. Logs are consistent.
00:04:33.128 - 00:04:37.096, Speaker B: So I should be thinking about, like, the permission setting where F is just a number of replicas.
00:04:37.208 - 00:04:39.464, Speaker C: Yes, permission setting.
00:04:39.512 - 00:04:40.110, Speaker A: Correct.
00:04:41.120 - 00:05:09.720, Speaker C: Okay. And classically, right. The protocols, you probably have seen a few in the past few weeks. Classically, the protocols are parameterized such that the liveness and safety resilience is one third. All right, so that's kind of the setting. Now one third, is that good enough? And these are the numbers from, like, a few days ago, right? You look up 8 million out of 22.7 million ETH.
00:05:09.720 - 00:06:06.068, Speaker C: Staked are currently with Lido. That's one organization, arguably a Dow, basically, but still they share some infrastructure, they share some governance process, et cetera. And that's controlling basically more than a third of the stake at the moment. And so given this scenario, our clients, depending on what their application is, might have certain thoughts, what they would like the protocol to provide to them. So, for example, Alice, in our example, Alice is working for one of the larger exchanges. And the exchanges have the problem that people make deposits in a currency, then they transact on the exchange, and then they withdraw in some other currency. And the question is, if somebody deposits a million dollars worth of ETH, at what point should you make that balance available for trading on the platform? And in particular, at what point should you allow for that balance to be withdrawn in some other currency?
00:06:06.244 - 00:06:06.632, Speaker A: Right.
00:06:06.686 - 00:06:15.964, Speaker C: In particular, you really want to make sure if somebody deposits ETH, exchanges it for Bitcoin, and then withdraws the Bitcoin that the ETH stays with you.
00:06:16.082 - 00:06:16.652, Speaker A: Right.
00:06:16.786 - 00:06:39.228, Speaker C: You absolutely do not want to see a safety violation. You do not want to see the log roll back. And so a user in this setting, a client in this setting, might reasonably say, with respect to the safety and liveness resilience, it'd be better if the system provided higher safety resilience, even if that comes at the expense of lower liveness resilience.
00:06:39.404 - 00:06:39.804, Speaker A: Right.
00:06:39.862 - 00:07:29.716, Speaker C: It's okay to not confirm a deposit for a bit if the system is currently under stress, but it'd be good if something is confirmed, you cannot roll it back. So we call this a safety favoring client. And then, on the other hand, maybe there's Bob, and Bob wants to build an optimistic roll up. And for safety on the optimistic roll up, Bob relies on L1 liveness, because otherwise fraud proofs cannot be posted on the L1. And then if there's fraud on the L two, that cannot be proven, that cannot be prevented, and then you lose L two safety, possibly. So Bob says, really? I care about the minimum of these twos safety matters. Yes, but liveness matters also, because otherwise I lose safety on the L two.
00:07:29.716 - 00:07:49.380, Speaker C: So let's not reduce the liveness resilience further than where we are. So we call Bob a liveness preserving client. Okay. Given these two clients, obviously their needs at the moment cannot be satisfied by one protocol.
00:07:49.540 - 00:07:50.250, Speaker A: Right.
00:07:50.780 - 00:07:53.076, Speaker C: And so we want to accommodate, maybe.
00:07:53.118 - 00:08:13.744, Speaker B: Just to tie that together on the previous slide. So what's interesting here is Lido. You hear very different opinions about whether it matters that Lido is 35% or not, right. Some people worry about it, some people don't. And so what you're saying is with this client specific sort of setting, basically, whoever you are, however you feel about Lido, you can set your parameters accordingly. Right?
00:08:13.862 - 00:08:14.396, Speaker A: Correct.
00:08:14.518 - 00:08:19.510, Speaker B: It can be sort of client specific opinions about how much to worry about that right.
00:08:24.200 - 00:09:33.644, Speaker C: If you don't worry about Lido, you don't have to change things. But it would be nice if the protocol supported this kind of flexibility that you can basically choose your resilience depending on what you think is more important for your application of this requirement. There is kind of a formalization of a consensus problem that is called flexible consensus. And so the idea there is that it comes from this paper originally, at least that's the first reference I'm aware of called flexible Byzantine fault tolerance. And there's another related work which kind of points to the fact that the discussion is really very much about kind of strengthening the safety resilience, which is called strengthened fault tolerance. And the idea here is that maybe can we design protocols such that every client can kind of choose their own liveness safety resilience pair, and then we guarantee these consensus properties for all clients that have chosen resiliences that actually turn out to be true, given how strong the adversary is. We try to do the best for every client that has made a reasonable assumption about the world and about the adversary power.
00:09:33.644 - 00:09:55.884, Speaker C: And so then this previous kind of consensus formulation changes in the following way, right? Alice and Bob, they now both have a resilience pair rather than having a global resilience pair, right? So every client now has a resilience pair. And what that really means is Alice and Bob also use different confirmation rules.
00:09:55.932 - 00:09:56.800, Speaker A: Really. Right.
00:09:56.870 - 00:10:12.580, Speaker C: But different only in the sense that they're parameterized kind of for different operating points. Now, what we want to make sure, we want to ensure the consensus properties for all the clients that have chosen resiliences that are sufficiently high compared to the adversary.
00:10:12.660 - 00:10:12.904, Speaker A: Right.
00:10:12.942 - 00:10:27.800, Speaker C: So if a client chooses a liveness resilience greater than F, then that particular client should enjoy liveness. And if two clients choose safety resiliences greater than F, then these two clients.
00:10:27.880 - 00:10:35.548, Speaker D: Should be consistent, should the consensus protocol will be engineered independent of the list of these pairs?
00:10:35.724 - 00:10:36.064, Speaker A: Yes.
00:10:36.102 - 00:11:07.288, Speaker C: That's a very good question. Correct. So the question that we're asking is what are these kind of pairs, right, that a particular consensus protocol can support? So ideally exactly. So ideally, we would like to have a protocol that can support as large a range, as large a possible choice of these resilience pairs as possible, basically. But it's not clear that that's actually possible. So we'll see in the talk. But yes, that's precisely the question.
00:11:07.288 - 00:11:12.776, Speaker C: So what resilience pairs? What is kind of the maximum flexibility that a protocol can provide?
00:11:12.958 - 00:11:23.768, Speaker B: So there's another way of thinking about that. One thing would just be like you're given TKL and TKS, and now you could imagine trying to design a consensus protocol that just worked for that specific pair.
00:11:23.864 - 00:11:24.268, Speaker A: Correct.
00:11:24.354 - 00:11:29.484, Speaker B: But then what you'd love is one protocol to rule them all, where basically you simultaneously got all of those pairs.
00:11:29.532 - 00:12:06.988, Speaker C: Okay, exactly. So in the original problem, right, we only had one resilience pair, and we are allowed to design our protocol with respect to that one resilience pair. Now, we're given a set of resilience pairs and we want to come up with a protocol that satisfies, that supports all of these resilience pairs at the same time. So now there are two questions. One is, for what set of resilience pairs? Can you give me one protocol that supports all of them at the same time? And then the second question is, is there some sort of kind of maximum set that you could possibly hope for? And is there even one protocol that can do the whole set?
00:12:07.154 - 00:12:08.510, Speaker A: Correct. Yes.
00:12:09.440 - 00:12:57.960, Speaker C: Okay, so this is kind of the problem, right? So it's clear, hopefully, what is the problem we're studying, right? We want to design these flexible consensus protocols and we're interested in supporting this maximal possible set of resilience pairs and what's possible even, right? And just as Tim pointed out, now we can, okay, we'll plot this, right? We plot the liveness resilience over the safety resilience. Remember the standard protocols that we use out there, for example, hot stuff or even Casper FFG kind of part of Ethereum has a system wide replica quorum, right? So this is a quorum that gets used as a parameter by the replicas of two thirds. And the operating point we're in is this one third, one third resilient.
00:12:58.040 - 00:12:58.332, Speaker A: Right.
00:12:58.386 - 00:13:33.796, Speaker C: What I presented earlier as kind of the classical result. And the problem with these protocols is, well, all the clients kind of have to follow this point, right? So clients, they're not really able to choose what operating point they want. They basically have to choose this operating point. Okay. Now, in order to understand what is possible kind of in this diagram, right. The first thing we should keep in mind is that there are some impossibility results of we know that certain logs are definitely not possible. They're not even possible for a single log protocol.
00:13:33.796 - 00:13:37.652, Speaker C: How could they possibly be possible for multilog protocols?
00:13:37.716 - 00:13:37.944, Speaker A: Right?
00:13:37.982 - 00:14:33.790, Speaker C: So there's the famous DLS 88 result, which basically imposes this constraint. It says, okay, everything kind of on the right hand side of this diagonal line here. There we cannot expect to support any client that makes a resilience choice of that kind. Okay. And then yeah, within kind of this trade off, bob wants to operate basically down here in the bottom right corner and Alice would basically want to operate in the top left corner. And so this particular choice of protocol with this one system wide replica quorum, bob is fairly happy with, but Alice is not really happy with this. And now what we can do in these protocols is we can tune the quorum and then if we change the quorum, this operating point basically starts walking up that diagonal line.
00:14:33.790 - 00:15:06.340, Speaker C: And so we can choose a different protocol instance that is parameterized differently. Basically, the replica code is different code. And in that different protocol we're now all operating, all the clients are operating here in the top left corner. So now Alice is happy, but Bob is not happy. That's basically summary of no flexibility is no good. In that sense, we would like to support flexibility. So these traditional PBFT style protocols as they are, they do not give us any flexibility.
00:15:07.880 - 00:15:17.928, Speaker B: Is it as simple as just moving up and down this boundary is just sort of choosing what's your threshold for what constitutes a supermajority when you do voting yes.
00:15:18.014 - 00:15:21.764, Speaker C: That is basically what happens if you tune the replica quorum.
00:15:21.812 - 00:15:22.410, Speaker A: Exactly.
00:15:24.060 - 00:15:26.412, Speaker C: The main point here, I think you pointed it out before.
00:15:26.466 - 00:15:26.684, Speaker A: Right.
00:15:26.722 - 00:15:52.596, Speaker C: But I want to make sure that it's absolutely clear is this is a system wide parameter, right? So all the validators in Ethereum need to be running this parameter. It affects how they cast their votes, et cetera. And so it's not possible that different clients can kind of assume a different quorum and then operate at different points here simultaneously. This is like one operating point for the whole protocol. There was another question.
00:15:52.778 - 00:15:54.324, Speaker A: No. Yeah.
00:15:54.522 - 00:16:21.340, Speaker E: So I'm just curious. So you said different clients are operating at different points. And let's say Bob chooses a safety threshold. That's really bad. So then he has to adjust his threshold up again. Is there a way basically the protocol just does that. So the protocol starts with always being safe, but maybe it's not live for some time, and then it kind of adjusts itself to whatever it thinks.
00:16:21.340 - 00:16:26.110, Speaker E: Some assumptions, of course, of the adversary doesn't grow.
00:16:27.280 - 00:16:29.100, Speaker C: This is basically just Bob's decision.
00:16:29.180 - 00:16:29.712, Speaker A: Right.
00:16:29.846 - 00:16:36.800, Speaker C: So in some sense, the idea here is that if Bob chooses a resilience that is lower than the adversary, in some sense, Bob acts adversarially.
00:16:37.700 - 00:16:38.076, Speaker A: Right.
00:16:38.118 - 00:16:47.468, Speaker C: In some sense, Bob is not like, you cannot really guarantee stuff about Bob if Bob basically says, I'm willing to accept any block even if it has zero signatures.
00:16:47.504 - 00:16:48.090, Speaker A: Right.
00:16:48.700 - 00:17:17.040, Speaker C: And so in that sense, it's only a choice kind of on the client side. So you could implement the client in such a way that the client kind of autocorrects if there is a safety violation or something like this. But it's really up to the client at this point. It doesn't seem to be kind of core to the consensus kind of problem formulation. Okay, cool.
00:17:17.190 - 00:17:17.664, Speaker A: All right.
00:17:17.702 - 00:17:50.696, Speaker C: But I've also mentioned there are already flexible consensus protocols, right? So what operating points do they achieve? I don't want to go into all the details of how exactly these protocols work, but suffice it to say they have an additional parameter. So in addition to the system wide replica quorum, there is now also every client chooses some local quorum and applies this local quorum in a particular way that I don't want to go too much into detail, but the result of it is that all of the operating points on this line are achievable.
00:17:50.888 - 00:17:51.292, Speaker A: Okay?
00:17:51.346 - 00:18:16.050, Speaker C: So I pick a replica quorum, and then for that replica quorum, I get a line. And if I'm a liveness preserving client, then I have a way to choose my local quorum in such a way that I'm at the bottom right. And if I'm a safety favoring client, then I have a way to kind of choose my local quorum, to choose my local confirmation rule in such a way that I'm as far top left as possible.
00:18:16.420 - 00:18:17.024, Speaker A: Okay.
00:18:17.142 - 00:18:31.364, Speaker C: But in this particular case, there is still quite a gap between what the most safety favoring client can actually achieve in this protocol versus what you could seems like you could hope for.
00:18:31.402 - 00:18:31.990, Speaker A: Right?
00:18:34.140 - 00:18:39.816, Speaker E: Where does the current ethereum mechanism fit on this thing? But doesn't ethereum have the dual kind.
00:18:39.838 - 00:18:43.708, Speaker C: Of so it's got LMD Ghost, which is on the left, and Casper on the right.
00:18:43.874 - 00:18:45.740, Speaker E: Is there two points for ethereum?
00:18:46.160 - 00:19:24.410, Speaker C: No. Okay. For the ethereum part, we basically only look at the Casper part because the dynamically available tip is in this setting, which is like the partially synchronous setting isn't even safe at all. And so the dynamically available kind of second log we basically sweep it under the rug. And we're looking at kind of the Casper output, the finalizations and the finalizations, as they currently are, they're operating down here. But you're foreshadowing kind of what I'll talk about in a moment is that there is a way to kind of make Casper flexible also.
00:19:25.340 - 00:19:26.090, Speaker A: Okay.
00:19:26.700 - 00:19:38.700, Speaker B: You could also imagine re asking the question in the synchronous setting if you wanted, right, and you might get a different graph. Say again, you could re ask the question in the synchronous setting and you might get a different graph.
00:19:39.360 - 00:19:54.924, Speaker C: In fact, this impossibility result goes away, et cetera. Yes, you can totally ask the question in the synchronous setting. Okay. And then in Fbft and in Sft, again, you can now tune the system wide replica quorum.
00:19:55.052 - 00:19:55.536, Speaker A: Right.
00:19:55.638 - 00:20:05.060, Speaker C: Currently for this particular line, we've chosen it to be two third, but we can crank it up, right? And then this line starts kind of walking up to the top left.
00:20:05.130 - 00:20:05.364, Speaker A: Right.
00:20:05.402 - 00:20:20.616, Speaker C: And now the parameterization becomes better for Alice, our safety favoring client, but it becomes worse for Bob. Bob is now away from where Bob would like to operate. So these protocols, they give us some flexibility, but there are some gaps.
00:20:20.728 - 00:20:21.148, Speaker A: Right?
00:20:21.234 - 00:20:22.296, Speaker C: What about the gaps?
00:20:22.408 - 00:20:31.816, Speaker B: Nakamoto consensus is also flexible in some sense, depending your confirmation rule. Like you can tune this safety parameter.
00:20:31.928 - 00:21:10.600, Speaker C: Yeah, Nakamoto has a type of flexibility, not really with respect to the resiliences safety and liveness resilience. They're coupled in the protocol, but with respect to the latency and the error probability. Yes, confirmation error probability. Okay. So what I will be talking about is a family of protocols that we call OFLX protocols, which stands for optimal flexibility. And what they achieve is this tradeoff curve. So basically now with this protocol, we can have a single protocol instance in which we can support both safety favoring clients and maximally liveness preserving clients simultaneously.
00:21:10.600 - 00:21:19.404, Speaker C: And that is done. This protocol technically still has some sort of a system wide replica quorum, but at this point there's no purpose in tuning that.
00:21:19.522 - 00:21:19.996, Speaker A: Right?
00:21:20.098 - 00:21:54.132, Speaker C: So all that's being tuned here, know, the system runs with one quorum and then clients have kind of their own client specific confirmation quorums, their own confirmation rules. So in this know, Alice and Bob are both happy because the protocol can support both of them. And there's a theorem which says these OFX protocols give us optimal flexible consensus in the sense that any set that we're given of resilience pairs that satisfy the impossibility result and have the property that we want to strengthen the safety resilience.
00:21:54.196 - 00:21:54.376, Speaker A: Right.
00:21:54.398 - 00:22:00.120, Speaker C: So the safety resilience is greater than the liveness resilience. Any of these pairs we can satisfy simultaneously.
00:22:00.640 - 00:22:01.196, Speaker A: Cool.
00:22:01.298 - 00:22:52.600, Speaker C: So how do we build this? Interestingly, there is a generic construction in which we can take a closed box kind of protocol off the shelf, add some stuff to it, and then it becomes an optimally flexible protocol. And that's what I'm going to go through first, because that builds some intuition of kind of what are the mechanisms that are at play? And then we'll see that there are different ways to kind of realize these mechanisms. And there is a way to realize this in ethereum in such a way that we don't even have to change the Replica code. Okay, but let's first look at kind of the generic construction, right? So we want a consensus protocol. This box is my consensus protocol. In a block diagram, there are transactions coming in, and then we have all of these different logs of all of the different clients coming out, right? And Alice's is this one. It has no liveness resilience and one safety resilience.
00:22:52.600 - 00:23:23.428, Speaker C: And Bob's is this one with like, one third, one third. And our protocol, again, has some Replica logic, right? So there's a Replica in the picture, and it has a client logic. So there are clients in the picture, right? And here's how the protocol works. So first we take this off the shelf, closed box protocol operating at one third, one third resilience pair. And that protocol. We're just putting all the transactions come in. The Replicas put them into this protocol, and that protocol outputs some log.
00:23:23.514 - 00:23:24.052, Speaker A: Okay?
00:23:24.186 - 00:24:02.444, Speaker C: That's an internal log, though, right? We're going to do something to that log. Here's what it looks like, right? So the protocol is running, transactions get confirmed, right? This is from the perspective of this Replica. This is kind of the internal log that it sees, right? And then when the internal log has grown a little bit, there is this second stage of the protocol which we call permalock and vote. And what it really does know, the Replica looks at this output the internal log at this output log, and then says, okay, this is my log. I'm going to lock on it. I have to be careful with the language lock on this, right? So I'm applying a lock here. And then I'm going to cast a vote.
00:24:02.444 - 00:24:42.910, Speaker C: I'm going to let Alice and Bob know, hey, by the way, I'm locked on this part of the log, of the internal log. This is the internal log that I hereby vote for, right? These are the post votes. I'm voting for this. And we'll see what voting means, right? So Alice and Bob, they collect these post votes from our Replica, but also from some other Replicas. And then once they have enough of these post votes, they have their client local quorum. So they choose their quorum in such a way that liveness is satisfied and consistency is satisfied. They choose their quorum, and then they confirm their respective output log.
00:24:42.910 - 00:24:46.136, Speaker C: And now the internal log keeps growing.
00:24:46.248 - 00:24:46.716, Speaker A: Right?
00:24:46.818 - 00:25:29.416, Speaker C: And what does a Replica do? Well, periodically it says, okay, here's the new log, right? This is what I'm going to lock now, right? And again, I'm going to cast my vote, and more Replicas cast their votes. And so Alice and Bob keep confirming stuff. And what does it mean? Okay, we have talked about post voting. We have kind of seen what this locking thing is, but we haven't really seen in what sense it's a lock. It's a lock in the following sense if the internal log were to have a safety violation. So suppose suddenly there's a new output that's consistent with earlier output. Then the replica looks at this and says, no, no, this is inconsistent with what I've already locked.
00:25:29.416 - 00:26:19.340, Speaker C: I've basically given a promise to Alison, Bob that I will never endorse anything inconsistent with the votes that I've already given them. So I'm not going to lock this guy and I'm not going to vote for this guy. I'm not going to give a vote to Alice and Bob for this conflicting log. Okay, so the key mechanism to keep in mind about what this protocol does, right, is it's running a consensus protocol at one third. One third. And then it has this permalock and post vote stage. And what really happens in the permalock and post vote stage is that if a replica has ever post voted on a certain log, right? Then if the replica is honest, we can infer that the replica must be locked on that particular log and that it will never cast a post vote for something inconsistent.
00:26:21.440 - 00:26:24.776, Speaker B: Did you say that? So clients should speak to multiple validators.
00:26:24.808 - 00:26:25.760, Speaker C: When they do this, correct?
00:26:25.830 - 00:26:26.256, Speaker A: Yes.
00:26:26.358 - 00:26:27.424, Speaker C: Clients a little bit of a change.
00:26:27.462 - 00:26:28.656, Speaker B: In the model, right?
00:26:28.838 - 00:26:37.380, Speaker C: Clients receive the votes, not necessarily talk to multiple replicas, but they see all the votes from different replicas just like they do right now.
00:26:37.450 - 00:26:38.070, Speaker A: Yeah.
00:26:39.320 - 00:26:47.060, Speaker D: So here's the case that the safety of the consensus is violated because the quorum, the amount of visiting notes is higher.
00:26:47.130 - 00:26:48.596, Speaker C: Yes, very good. Correct?
00:26:48.698 - 00:26:48.964, Speaker A: Yes.
00:26:49.002 - 00:26:51.700, Speaker C: So we'll see in a moment what happens.
00:26:51.850 - 00:26:53.280, Speaker B: Could you repeat what nurse said?
00:26:53.370 - 00:27:46.552, Speaker C: The question was, how is it possible that there is this safety violation here, basically, on the first stage protocol, right? I think you were pointing out the fact that this can only happen if the adversary is stronger than one third, right? Exactly. Because this protocol is safe up to one third. Okay. So let's look at basically why does this give us flexible consensus, right? Can we reason through the liveness and the safety of this protocol? And it's actually fairly straightforward, so bear with me. In 5 minutes we have this. So let's think about liveness, right? Let's assume we're looking at a particular client, K, and that client, K, has made a reasonable choice in terms of its liveness resilience, right? Its liveness resilience is actually greater than F. And also it's a client that has chosen a point in this feasible triangle, right.
00:27:46.552 - 00:28:24.224, Speaker C: Somewhere here on this diagonal line, right. Then look at the graph, right? If you choose a point anywhere on this line, it must be the case that your liveness resilience is smaller than one third. And we have assumed that your liveness resilience is adequate. So F is even smaller than that. So what does that mean? That means the base protocol in this condition is safe and live. And if the base protocol is safe and live, it means all of these logs across honest replicas will be consistent. And if they're all consistent and they're all live, then all the permalocks that honest replicas will ever adopt, they will all be consistent.
00:28:24.224 - 00:28:58.050, Speaker C: And the liveness gives us that if any honest replica sees a certain output log, right, eventually all honest replicas will see that output log. And because that output log is consistent with any previous permalock that it might have adopted, they will actually also vote, post vote for that particular log. And because of this, every client eventually, if the client has chosen the quorum adequately, in particular in such a way that votes from all honest replicas are enough to actually meet the quorum, then the client will be live and will eventually confirm that output log.
00:28:59.220 - 00:29:10.944, Speaker B: So is it fair to say the only thing that you're worried is going to go wrong with liveness is when in that left picture where you kind of are lock on something and then there's a fork you're not going to vote for, but then if there's no forks in the first place, that problem can't happen. And so you're fine.
00:29:10.982 - 00:29:11.424, Speaker A: Yes.
00:29:11.542 - 00:29:27.988, Speaker C: Two things I'm concerned, right? One is outright this guy not be live. Because if this guy is not live, then no transaction actions go through. And the other option is precisely this, right? What if this guy is unsafe? Then my replicas here might be locking on inconsistent stuff and then I might never get enough votes to confirm.
00:29:28.084 - 00:29:28.536, Speaker A: Yes.
00:29:28.638 - 00:30:04.836, Speaker C: And the idea is, well, if that happens, then the liveness that I'm expecting, I'm expecting too much liveness. This cannot happen. For any possible choice of liveness resilience that is actually feasible. So suppose we have two clients. Both of them have chosen adequate safety resiliences greater than F. And remember, we choose the quorum in a particular way. Now, for contradiction, assume there are some conflicting confirmations, right? So assume some client outputs kind of the red log up there and some client outputs the blue log down there.
00:30:04.836 - 00:30:32.184, Speaker C: Clients, I mean, outputting logs here, right? Not the intermediary logs, logs actually coming out of the protocol. Then what must have happened, okay, the client who output the red chain has seen a certain number QK of votes for that particular chain, right, for that chain or longer chains. And at the same time, the client who outputs the blue log has seen a set QK prime of votes for that blue log.
00:30:32.322 - 00:30:32.720, Speaker A: Right?
00:30:32.790 - 00:31:17.196, Speaker C: But now, because of the choice of the quorum, okay, these are basically all the replicas that voted for red. These are all the replicas that voted for blue. But there are only so many replicas, right? This black. Rectangle here, right? And then we can reason that because the quorums are chosen as large as they are chosen, it must be the case that in this intersection, these two quorums must actually intersect and the intersection is greater than F. In particular, there is an honest replica in this intersection. But now we've already seen earlier, right, that an honest replica would never vote for two inconsistent things because of the construction of the protocol. So this cannot be.
00:31:17.196 - 00:31:21.224, Speaker C: So the protocol must be safe. So what have we seen, what have we learned?
00:31:21.272 - 00:31:21.484, Speaker A: Right?
00:31:21.522 - 00:32:26.400, Speaker C: In some sense there are three mechanisms at play here. Three I don't know what to call them features or three gadgets somehow in this protocol, right? One is this idea of permalocking. And the idea of permalocking is we're using that in order to avoid contributing towards conflicting confirmations both in the future and conflicting confirmations in the past. In the sense that if we vote on something, if we post vote on something, the post votes tell our clients, hey, I have permalocked on this. And what it means is if a client sees a post vote from an honest replica for a certain log, it knows that this replica will not in the future and has not in the past post voted for anything inconsistent. And then the third mechanism is this client specific quorums that basically allow us to output these flexible logs. And now we can ask, okay, post voting and permalocking, these are kind of two mechanisms that we need on the replica side, right? These are like system wide kind of mechanisms.
00:32:26.400 - 00:32:47.636, Speaker C: On the other hand, the confirmation rule, these client specific quorums, they are client logic. They're kind of local decisions. Every user basically of the system can choose their own confirmation quorum and we can look at different constructions. There are different constructions in this OFLEX family how we implement these three mechanisms.
00:32:47.748 - 00:32:48.072, Speaker A: Okay?
00:32:48.126 - 00:34:12.864, Speaker C: The construction you guys have just seen is kind of the generic construction as an add on for any protocol, right? You take any protocol that's one third, one third resilient, you add this additional permalocking and vote and the client specific confirmation quorums and you'll get an optimal flexible protocol essentially adding post vote, permalock and confirmation rule external to the protocol. Then the second construction is well, if we look at these chained PBFT style protocols, they basically use chaining in order to unify a bunch of rounds of voting that happen in the PBFT protocol into one streamlined kind of proposal and vote procedure. I don't know if you guys have seen hot stuff or streamlit or any protocol in the past few weeks, but yeah, suffice it to say they're all relatively similar. They all use this trick that votes can kind of one vote can really kind of serve multiple purposes at the same time and we can basically, for free add on top this kind of purpose of being a post vote. So we get post voting basically for free. In these protocols, we still need to add a locking mechanism and we still need to add these client specific confirmation quorums. And then the next step is OFLEX confirmation rules for Ethereum, where it turns out ethereum.
00:34:12.864 - 00:34:58.272, Speaker C: So Casper in particular, Casper FFT is one of these chained PBFT style protocols. So we get the votes for free, we can reuse the votes that are already in the protocol. And it turns out there is a performance optimization in the specification that effectively already implements the locking logic that we need so we don't have to add the permalocking kind of on top of the protocol. What this means is, looking back at what's replica logic and client logic is, we get optimal flexible consensus for Ethereum without making any system wide changes. Everybody can just start following this new confirmation rule and then have the flexible guarantees.
00:34:58.336 - 00:34:58.852, Speaker A: Yes.
00:34:58.986 - 00:35:03.830, Speaker E: Can you remind me how many replicas the client talks to? Is it just one, or is it.
00:35:04.840 - 00:35:14.504, Speaker C: The client needs to hear votes from all the replicas, just like needs to talk to one? But these can be relayed, right? So anybody can give them to you.
00:35:14.542 - 00:35:44.400, Speaker E: So is it true that if I just talk to everyone and I collect whatever, I just talk to everyone separately, and then locally, I can say, okay, if my safety threshold is 80%, then if 80% of the nodes, say, this portion of the ledger is finalized, then I finalize it locally. That's my confirmation pool. And now I can do this with no changes to the protocol, just with the added model assumption that the client talks to all the nodes. So there's no extra voting or locking.
00:35:44.820 - 00:35:59.552, Speaker C: No, this doesn't work. Yeah, this would be exactly Fbft's construction. So flexible BFT is pretty much doing this. It's basically saying, what if a client, just instead of applying this system wide quorum, applies a local larger quorum?
00:35:59.616 - 00:35:59.844, Speaker A: Right?
00:35:59.882 - 00:36:09.996, Speaker C: But otherwise there is no post voting and no permalocking. Then you get exactly the trade off of the Fbft curve, which is not the full trade off. There are some gaps, but in there.
00:36:10.018 - 00:36:11.470, Speaker E: Also, you need to talk to everyone.
00:36:12.480 - 00:36:52.820, Speaker C: There also the relaying happens. The talking to everybody doesn't really change anything because it's all signed anyway. Let me give you one sentence of intuition. Why this is the problem with this construction is that even though the client locally applies this very high quorum, that does not stop the replicas basically from turning against confirmations that the client has locally made. So you locally see this very high quorum, but the replicas apply this much lower quorum. And so the replicas, if the adversary is sufficiently strong, the replicas basically start casting votes that are inconsistent with your high quorum confirmation. And so the replicas basically turn against the clients.
00:36:52.820 - 00:36:55.784, Speaker C: And that's precisely what the post voting and permalocking rules out.
00:36:55.822 - 00:36:56.024, Speaker A: Right?
00:36:56.062 - 00:37:02.452, Speaker C: The permalocking does exactly this thing where you lock and you will never vote for anything inconsistent, no matter what.
00:37:02.526 - 00:37:05.164, Speaker E: So I guess this works, it's just not optimal in that case.
00:37:05.202 - 00:37:10.270, Speaker C: Yes, you get some flexibility, but you don't get all the way there.
00:37:11.040 - 00:37:14.040, Speaker B: Can you say a bit more about the permalocks and Ethereum?
00:37:14.120 - 00:37:14.376, Speaker A: Yes.
00:37:14.418 - 00:37:15.836, Speaker B: Given that there's people in the room that know locks.
00:37:15.868 - 00:37:16.112, Speaker A: Correct.
00:37:16.166 - 00:37:45.480, Speaker C: I'm coming there right now. So this is the figure one of Ethereum consensus, right? Just to kind of answer Noah's question, what we're looking at here, ethereum's protocol, is this composition of LMD Ghost and Casper FFG. It outputs two logs. But what we're interested in specifically, we're interested in high safety, right? And safety is kind of the domain of Casper FFG. And so what we're interested in is kind of boosting the safety of Casper FFG.
00:37:46.140 - 00:37:46.552, Speaker A: Okay?
00:37:46.606 - 00:38:16.804, Speaker C: So we're trying to kind of make finalizations even more safe. We want to have superfinalizations, if you will. Okay? So here is quick, as concise as hopefully possible summary of Ethereum's Casper part, right? So ethereum is a blockchain. So there are blocks, and then the protocol is basically propose a block, and then we do a bunch of voting, right? And we vote for a certain block, and then somebody proposes a block again, and there's voting again.
00:38:17.002 - 00:38:17.750, Speaker A: Okay?
00:38:18.360 - 00:39:11.620, Speaker C: And then there is this traditional Casper FFG finality rule, right, that says if you see roughly it's not exactly two thirds on one block, you need to see two thirds of votes on a bunch of consecutive blocks. But basically, if you see two thirds of votes, then you finalize this block A, and then you output it according to the finality kind of default confirmation rule of Casper FFG. Okay? Now here's the thing about the locking mechanism. In Ethereum specification, they go one step further. They not only say, if you see this, then that's your finalized log, they also say, if you see the situation that something is finalized in your opinion, you will lock on this guy. And even if later you see a conflicting block that also has enough votes to be finalized, you will not output that. You will always be locally consistent.
00:39:11.620 - 00:40:09.000, Speaker C: The reason they do this is performance optimization, right? Once you have finalized stuff, you can basically forget about every block and every vote on everything that's inconsistent with it. And so you don't have to kind of walk the block tree every time. You just basically keep track of your neogenesis, kind of the most recent finalization, and you do consensus only with respect to that. So they have this as a performance optimization, but it turns out that's exactly the locking mechanism that we need in order to implement the permalocking. So the next thing that happens in Ethereum is that, okay, so you have a rough sense now where the permalocking comes from, right? Leaves us with two more things, the client specific confirmation quorums and somehow these post votes, right? And so here's the next thing in ethereum votes are not just floating around kind of next to blocks. They also get included on chain. They also get reported on chain.
00:40:09.000 - 00:41:08.280, Speaker C: So in this case, the block C would contain all the votes that are necessary, would at some point contain enough votes so that based on C and its prefix, you can reason that A must be finalized. Okay, so that's an important property. And now we come to the post votes and client specific confirmation quorums. Namely, if C has a certain number of QK of votes, what does this mean? This means any replica that has voted for C must have seen all the votes that attest to the fact that A is finalized. So any replica that votes for C must have locked according to the protocol rules for A. So that's exactly the mechanism that we were going after earlier with the post roads. Remember, every time I see a post vote, I can infer something about the locking about the permalock at honest replicas.
00:41:08.280 - 00:42:05.644, Speaker C: So this is the confirmation rule. The new confirmation rule is the following if you see a block A and a block C in such a way that A according to the votes included in C or in the prefix of C, A is finalized and you see QK votes on C, then you confirm with this OFX confirmation rule, you confirm A. And the reasoning is exactly this. If an honest replica has voted on C, you know that if the replica is honest, it must have locked on A. Which in turn means if there was another condition where somebody could be confirming right. Where you have A block F and votes finalizing F are included in H and H also has a quorum, then you know that this replica would not have been voting for that block. Because that block is like these two blocks are inconsistent with A, and the replica must have already been locked on A.
00:42:05.644 - 00:42:19.984, Speaker C: So you will not confirm F. So we're safe, basically. So, yeah, the two kind of ingredients here, right? We have the client specific quorums that enable the flexible resiliences. You've seen where they come from, right?
00:42:20.022 - 00:42:20.176, Speaker A: Here.
00:42:20.198 - 00:42:34.980, Speaker C: These QK post voting help us inform clients about what permalocks replicas have adopted. And permalocking is this mechanism to make sure that replicas would not vote for two conflicting confirmations.
00:42:35.880 - 00:42:36.692, Speaker A: All right.
00:42:36.826 - 00:42:40.884, Speaker C: Any more questions about this? Otherwise, I'll come to experiments.
00:42:40.932 - 00:42:41.624, Speaker A: Yes.
00:42:41.822 - 00:42:54.764, Speaker D: When you were saying that it's a performance optimization for Ethereum to kind of not switch. Why would you see a conflicting branch if the number of Byzantine nodes is less than one third yes.
00:42:54.802 - 00:43:13.104, Speaker C: Then you don't see it. Yes, but we want safety up to one, right? We want that basically, if we tune QK up to one, that then unless everybody is malicious, we will be safe. So we have to deal with the parameters where F is greater than one third.
00:43:13.222 - 00:43:13.744, Speaker A: Okay.
00:43:13.862 - 00:43:14.672, Speaker D: And another question.
00:43:14.726 - 00:43:14.944, Speaker A: Sorry.
00:43:14.982 - 00:43:22.020, Speaker D: So Ethereum is collecting as much votes as it can possibly collect. So the principle could just stop at two thirds.
00:43:22.440 - 00:43:22.804, Speaker A: Yes.
00:43:22.842 - 00:43:23.892, Speaker C: Oh, that's a very good point.
00:43:23.946 - 00:43:24.212, Speaker A: Yes.
00:43:24.266 - 00:43:29.108, Speaker C: So Ethereum collects as many as possible for incentive reasons, right?
00:43:29.194 - 00:43:29.444, Speaker A: Yeah.
00:43:29.482 - 00:43:31.384, Speaker C: And that's a very good point, actually.
00:43:31.422 - 00:43:31.576, Speaker A: Right.
00:43:31.598 - 00:43:49.644, Speaker C: If Ethereum had done another performance optimization to not include all of these votes on chain but stop at two thirds, then this would no longer work. Yes, that's a very good observation. Actually, maybe it would still work because these votes don't have to be included in blocks. You only have to see them.
00:43:49.842 - 00:43:50.204, Speaker A: Right.
00:43:50.242 - 00:44:03.952, Speaker C: So all you have to see included in blocks are the two thirds that are attesting to the finalization. These votes, you can read them in the newspaper. Basically, no matter where they come from, if they're properly signed, you know that they come from the Replicas, then you're good.
00:44:04.086 - 00:44:11.412, Speaker B: But to Lara's point, I mean, currently you can literally just read off the current Ethereum blockchain basically all the information you need for this.
00:44:11.466 - 00:44:11.876, Speaker A: Right.
00:44:11.978 - 00:44:20.500, Speaker C: And that's exactly what we have done. Correct. So precisely, all the information is on chain.
00:44:20.580 - 00:44:21.016, Speaker A: Right.
00:44:21.118 - 00:44:59.572, Speaker C: We can take the Beacon Chain API and read it off our consensus client. Our consensus client, and basically plot what would the logs have looked like for the normal Casper finality? For the OFLEX finality with a safety resilience of 33%. So that's basically QK two thirds, and then for safety resilience, 98%. And what we see is this is like an average afternoon. Basically what we see is on chain, participation is fairly high.
00:44:59.626 - 00:44:59.892, Speaker A: Right.
00:44:59.946 - 00:45:16.180, Speaker C: Everybody is voting as they should, and we see that the logs just move along pretty much in lockstep. The Casper log is a little bit faster and the other ones catch up because they have this extra roundabouting.
00:45:16.340 - 00:45:22.600, Speaker B: So basically what you're doing here is cranking up the safety and checking if Liveness would have broken down.
00:45:22.670 - 00:45:23.610, Speaker A: Is that right?
00:45:24.640 - 00:45:26.396, Speaker C: Yes. That is something you can do.
00:45:26.418 - 00:45:26.652, Speaker A: Yes.
00:45:26.706 - 00:45:29.272, Speaker B: I feel like that's how I'm sort of interpreting this graph.
00:45:29.416 - 00:46:23.632, Speaker C: Yeah. So in this graph, I'm basically running both of these confirmation rules and I'm just interested to kind of see how quickly they catch up with each other. But what you could do is you could just always run whenever your system is kind of in operating by itself mode, you always run with a very high safety resilience. And then if you detect that Liveness has stalled, presumably because participation is low, we'll get to that in a moment. Then you notify some operator and the operator is supposed to figure out why is this is there a hard fork going on? Has there been any bugs? Can we basically just lower the resilience, the safety resilience, in order to get Liveness? Or should we stick here and not finalize for the moment, or not super finalized for the moment? This is another plot. Yeah. The source code of this proof of concept is on GitHub if you want to check it out.
00:46:23.632 - 00:47:11.276, Speaker C: This is another plot where we basically tune the safety resilience of the OFLEX confirmation rule and we're plotting the mean latency against Casper finality. And what you basically see is that Casper finality has an average latency of 15 ish minutes. And because of the extra round of voting that we need for of know, we need about 4 minutes more. And then once we paid for the 4 minutes, no matter how much safety we want, we basically don't incur much loss in latency anymore. There's a little bit of a spike back here. Those are basically the cases where participation drops quite a bit and then the higher resiliences start stalling. And we see that in this plot here.
00:47:11.276 - 00:47:56.152, Speaker C: So this plot is from during the Shanghai hard fork. And so what happens is participation initially is high, then the fork happens, the participation drops because not everybody has updated their validators. And once participation drops, the high quorums, the high safety resilience rules, they start stalling, right? The low, like fairly low, still higher than Casper, but like fairly low safety resilience rules, they quickly start catching up because participation is still fairly high. And eventually participation stabilizes again. Everybody has updated their validators, everything nice and then even the high safety rules catch up again. So this doesn't happen very often. It happens when there is like client bugs.
00:47:56.152 - 00:48:48.980, Speaker C: We have another plot from this recent finality stall. It happens during hard fork with the very, very high safety resilience things. But usually the plot looks like this, right, almost all the time. This is just to give you a sense. And yeah, that's pretty much all I had in a summary in one slide. We are interested in building these kind of optimal flexible consensus protocols where we can support the maximum set of safety liveness resilience region pairs in one protocol at the same time, right? It turns out it's actually possible. We have a bunch of constructions, one very generic one where the three key ingredients, you add them basically to literally any protocol that you pick off the shelf and then more specialized ones where you need to do less changes to the protocol.
00:48:48.980 - 00:49:03.970, Speaker C: For chained PBFT style protocols and for theorem, it turns out you don't even have to change the replica logic at all. And yeah, you pay a little bit in latency because the extra on a voting, but most of the time it just works.
00:49:04.580 - 00:49:05.490, Speaker B: Thanks again.
