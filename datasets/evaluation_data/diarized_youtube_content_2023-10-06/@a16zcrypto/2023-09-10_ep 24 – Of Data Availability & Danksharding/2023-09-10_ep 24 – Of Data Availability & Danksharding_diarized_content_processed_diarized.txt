00:00:00.330 - 00:00:00.880, Speaker A: You.
00:00:03.810 - 00:00:56.000, Speaker B: Hi everyone. Robert Hackett back here again with another episode for Web Three with A 16 z. I recently chatted live with some of our researchers on the topic of data availability sampling and Dank sharding, which is relevant to blockchain scaling as well as paving the way for more advanced blockchain networks and user applications. While much of the discussion is especially relevant to Ethereum, some of the concepts we cover are also applicable to advances in computing and networking generally. This discussion involves some specific math which we cover and briefly explain in the episode for quick context. As you listen, polynomial commitments, which you'll hear more about, are a tool that helps reduce the amount of data needed to verify complex computations. An interpolation, another term you'll be hearing is a way to reconstruct data from a limited set of data points.
00:00:56.000 - 00:01:20.870, Speaker B: Be sure to check out the paper referenced in this episode for a deeper explanation at A 16 zcrypto.com Das. That's Das for data availability sampling. As always, none of the following is investment, business, legal or tax advice. See a 16 z.com disclosures for more important information, including a link to a list of our investments.
00:01:22.410 - 00:01:45.950, Speaker C: Okay, so today we're going to be talking about data availability sampling and Dank sharding. Now, if you're unfamiliar with those things, don't be scared because I'm here with a couple of experts who are going to break things down for you. We've got Dan Bennett Stanford professor, eminent cryptographer and A 16 Z crypto research advisor. And Lara Nicolenko, research partner at a 16 Z crypto.
00:01:46.370 - 00:01:48.106, Speaker D: Great to be here. Thanks Robert.
00:01:48.218 - 00:01:49.582, Speaker A: Thanks Robert. Great to be here.
00:01:49.636 - 00:02:28.234, Speaker C: Thanks Lara. Thanks Dan. So as mentioned, Dan and Lara recently wrote up a great post and have a proposal related to ways to improve protodank sharding, which is this upgrade that is planned for later this year in Ethereum. There is a lot of rich detail in that post, so we'll dig in there. But before we get into all the details, I thought maybe we could zoom out and back up and start with the bigger picture here. So let's start with Dan, help us understand the subject of today's talk data availability sampling. And maybe you could do it without breaking anyone's minds here.
00:02:28.234 - 00:02:35.134, Speaker C: Maybe we could keep it as succinct and accessible to a broader audience as you might be able to.
00:02:35.332 - 00:03:01.686, Speaker A: Sure. So maybe we can zoom out a little bit before we talk about data availability sampling. Maybe let's talk about the general goal here. So this is part of the efforts to scale Ethereum. And so one of the ways to scale Ethereum is using roll ups. And roll ups actually need to push a lot of data on chain. So a roll up allows you to take say, 100 or 1000 transactions and process them all as a single transaction on the Ethereum layer one.
00:03:01.686 - 00:03:33.760, Speaker A: And then all the data associated with those transactions basically gets pushed on chain and as a result, there's a need to actually store quite a lot of data on chain and the question is how to do that. And that's exactly where Danksharding comes into play. So it's a really beautiful, beautiful idea. It came from the Ethereum Foundation, particularly from Dankrad Feist. It's a really elegant construction and basically Lara and I wanted to give an overview of how this construction works and potentially look at some options in which it maybe can be improved a little bit.
00:03:34.130 - 00:04:01.706, Speaker D: Yeah, roll ups essentially allow execution to scale for Ethereum. But the question is, how do you make the data scale? And Denshard in that availability sampling basically adds this missing piece to Ethereum that will allow it to achieve full scaling. This post is kind of quite technical. Some aspects we don't explore, like networking, which is also interesting to research and to write about. But we're mainly focusing on kind of cryptographic aspects of bank sharding and I.
00:04:01.728 - 00:04:19.434, Speaker A: Would actually even add that there's really beautiful open questions that are still left to think about for researchers. If you are interested in this area, there's like beautiful questions around coding and polynomial commitments. There are really quite interesting questions. If they could be resolved, we would end up with even more efficient systems.
00:04:19.562 - 00:05:03.210, Speaker C: That's excellent context. I definitely want to ask you a little bit about those open areas for potential inquiry in a little bit. But before we do that, let's talk a little bit more about this proposal that would free up space on the Ethereum blockchain. So the blockchain is basically this giant record of transactions happening from the time that this system launched way back when to the Genesis block. How are developers thinking about freeing up space, making it get greater throughput scalability, cheaper transactions? All of these things that sound great and make it sound like the system will be much more usable. How do you actually get there? What is required to get these efficiency gains?
00:05:03.370 - 00:05:36.530, Speaker D: I think the main challenge is that you cannot just ask validators to store more data. It won't get you too far. If you want to scale the blockchain to increase the block size by several orders of magnitude, you have to split your block and disperse it to validators so that each validator only stores some fragment of the block. And there where the ideas from error correcting and razor coding comes in that allows you to do that. So basically increasing the block size without putting too much burden on the validators, I would say that's the main technical difficulty. That's what Densharding is solving.
00:05:36.690 - 00:05:49.846, Speaker C: You mentioned erasure coding and it sounds like that's a key piece of this technology that enables it to work. Maybe you could provide some more detail there. What is erasure coding? How does it apply in this context?
00:05:50.038 - 00:06:24.534, Speaker D: Sure, absolutely. So you basically take a block with users data and you expand it. You raise your code, it turn a smaller block into a larger block, and this larger block can tolerate some emissions from it. So you can lose some portion of the block. In case of den sharding, you can lose 25% of the block and still be able to reconstruct these missing pieces from what you have. So when you disperse this expanded block to the validators, and the validators go down, because some of them are Byzantine or faulty, you can still reconstruct with those validators. And that's okay.
00:06:24.534 - 00:06:39.830, Speaker D: If they go down and they lose their pieces, the rest of the validators who are still online and still honest can recover those missing pieces. And that's why the properties of erasure coding are useful here, just to substitute for Byzantine or faulty validators.
00:06:39.990 - 00:07:21.746, Speaker A: And maybe we can even explain it a bit more with an analogy. It's like if you're watching a Netflix movie and say the Netflix servers are sending packets to your computer and you're watching the movie, well, imagine, I don't know, like 10% of the packets actually don't make it through, and your computer only gets to see 90% of the packets. So normally you would start to see all sorts of lossy video and degradation in performance. With erasure coding. What happens is, if the movie is coded using an erasure code, even if only 90% of the packets get through, the laptop has enough information to actually reconstruct the entire movie. What's interesting is, so erasure coding is used everywhere. Like communication networks wouldn't be able to work without erasure coding.
00:07:21.746 - 00:07:59.406, Speaker A: And maybe, again, just another example. When you have a deep space probe, it's sending messages back to Earth. There's a lot of noise, and a lot of them get either dropped or they get garbled. And yet on Earth, we're able to recover the signal and get those crisp images from Mars. That actually also is done using a slightly stronger technique called error correcting codes, where we're not only losing packets, but also we are recovering from packets being distorted, being changed in value. What's interesting in the context of the blockchain is that all the data is being signed. So we actually don't care so much about data corruptions because the signature layer will detect data corruption.
00:07:59.406 - 00:08:39.294, Speaker A: So really all we care about the only thing that sort of a malicious node can do, someone who's trying to prevent the data from being reconstructed, the only thing that that node can do is, in some sense, remove pieces that make up the data. So we don't care so much about recovering from actually corruption of the data, because that's taken care of by the signature. But we do worry quite a lot about pieces of the data simply missing. And as a result, maybe whoever is trying to reconstruct the data is not able to do it. And so that's exactly where erasure coding comes in, where we know that the only thing that can happen is that a certain piece of the data is missing. It can't be garbled. So if we got it, we got the right one, and that's because of the signature.
00:08:39.294 - 00:09:01.966, Speaker A: But if it's missing, we have to somehow recover. And that's exactly as Laro was saying. That's the idea of erasure coding. And the way you do that is basically you take your original data. In the case of Ethereum, you would take a block and you would expand it a little bit. Actually, you expand it by like a factor of four to get sort of more data in the block, so that the data is now redundant in the block. And now you break it into little pieces.
00:09:01.966 - 00:09:40.650, Speaker A: And now you can ask every validator, oh, you don't have to store the entire block, you only have to store this small piece of the block. And if enough validators actually do their job and store those little pieces, and when we need to reconstruct a block, they send those pieces back to us. If enough pieces get back, then we are able to actually reconstruct the entire block. And in Denk Sharding in particular, again, it's a beautiful proposal. The recovery rate is 75%. So if 75% of the validators respond and we're able to recover 75% of the pieces, then we're actually able to reconstruct the entire block. That's kind of the core mechanism that's used in Dank Sharding.
00:09:40.750 - 00:10:10.800, Speaker C: That's really useful. So it sounds like erasure coding is this kind of technique that enables you to apply some redundancy and backups so that you don't lose all of the data, so that you can still sort of assemble it, get access to it, see that it exists. Dan, you mentioned that the reason for doing this data availability sampling is to prevent bad actors from doing certain things like getting rid of some of the data. What exactly are we defending against here?
00:10:11.250 - 00:10:40.182, Speaker A: Yeah, that's a great place to go next. So in fact, what happens is with these proposals for solving the data problem in Ethereum, there's going to be a new transaction type that's going to be introduced. This is called a Blob carrying transaction. And what it would allow folks to do is basically embed blobs. Each Blob is 128 KB. So embed blobs into blocks. So normally blocks are made up of transactions that just do all sorts of things to the Ethereum state.
00:10:40.182 - 00:11:22.482, Speaker A: So now, in addition to just the regular transactions that we know and love, there's also going to be a Blob carrying transaction or a few Blob carrying transactions per block. And as I said, each one is going to be 128. Long term plan. And actually, maybe Lara can talk about the transition to how we get there, but the long term plan is there could be quite a few data carrying Blobs in every block, which would actually make the block quite large, right? I mean, each Blob is 128 KB. If you put a bunch of those together, you could end up actually you will end up with like blocks that are 30 megabytes. It's just not reasonable to ask validators to store these huge blocks. Today the blocks are only like 100, so so these will be much, much larger blocks.
00:11:22.482 - 00:11:59.954, Speaker A: And so the idea is to basically break up these large blocks into little pieces. Every validator or every node will actually store only this little piece. And now the problem is, well, what happens if they say that they stored the piece but in fact they didn't? Right. What do we do then? And that's exactly where data availability sampling comes in, which is a very efficient way to test at block creation time that in fact, everybody received their pieces. Everybody currently has their pieces and currently the block can be reconstructed even though it was broken into many, many small pieces. And these pieces are stored, distributively across the network and just takes over.
00:11:59.992 - 00:12:32.186, Speaker C: I just want to make sure that I understand something here. So you said blocks today are about 100. Idea is that after all these upgrades, they're going to be about 30 megabytes. And part of the reason for that expansion of the block size is to accommodate this new type of data, this blob data that has a different purpose than sort of what you usually shove into blocks, which is just pure transaction data. This blob data is really related to helping these off chain layer two networks store some data in a more ephemeral manner. Did I get that right?
00:12:32.288 - 00:13:03.314, Speaker A: Yeah, I'm glad you reiterated that because that's important. So these blobs basically are going to be used by these roll ups, right? So the roll ups have to store the roll up data. And so instead today what they do is they store it as what's called call data, which is somewhat expensive and kind of not what call data was meant for. So instead they're going to store this data as blobs in blocks. And what's interesting is these blobs are actually not going to be available to the execution layer of ethereum. They're just going to be stored as blobs. In blobs, the execution layer will only see hashes of these big blobs.
00:13:03.314 - 00:13:42.274, Speaker A: They won't be able to access actually individual bytes or elements in the blobs, which is the new mechanism today. The way this is stored is, as we said, in call data. And call data is all available to the execution layer of ethereum. So because of this simplification, in fact, storing blob data is going to be a lot cheaper than call data. So in principle, the goal of this is to reduce the costs of layer two systems, right? Because today they have to pay quite a lot to store all their data as call data. In the future, once Dank sharding is deployed, or even once prototype sharding is deployed, in fact, the costs will be a lot lower. So l two systems will become much cheaper and easier to use.
00:13:42.392 - 00:13:54.946, Speaker C: That's great. I love that we're having a sophisticated conversation about cryptography and very technical software and yet we keep using the word blob. Reminds me of the anti Sci-Fi movie Attack of the Blob. But Larry.
00:13:54.978 - 00:13:55.174, Speaker A: Yeah.
00:13:55.212 - 00:14:03.686, Speaker C: Maybe you could chime in now and talk about this trend and how we get from where we are now to that vision in the future of expanded block sizes.
00:14:03.878 - 00:14:57.194, Speaker D: Yeah, for sure. Before I dive into that, just to add two comments to what Dan said, I want to mention that it's important the blobs are going to expire and the validators don't give any guarantee that they're going to be storing these blobs forever. Right now the expiry is set roughly to 30 to 60 days. That's what I guess Ethereum Foundation is thinking about. So after this period during which you will have an opportunity to download all of these blobs and store it locally, the network will drop them, but the commitments to those blobs will persist. And if you need so if you have the blobs themselves, you can always resupply them to the execution layer with call data. So as long as you store the blobs, you can prove that those are the correct blobs that you have by resupplying them just because the chain continues storing the hashes of those blobs, the commitments I also want to mention another important thing is that the fee market for those blobs is going to be different.
00:14:57.194 - 00:15:29.894, Speaker D: So it's going to be another fee market. They're going to be priced a little differently. So Ethereum is kind of going to have these two pipes, one pipe. If it gets congested, you are paying larger fees, say for execution and if a data pipe gets congested, you are paying larger fees for storing the data. So we don't yet know how expensive the storage is going to be. The intuition is that it must be less expensive than cold data today. But again, we have to experiment to exactly see how much cheaper it's going to be.
00:15:29.894 - 00:16:17.138, Speaker D: And proto Dank sharding is actually a step towards full Dank sharding, but it's like an experiment that we're going to carry to see how validators are going to handle this additional load and how expensive going to be the fees for storing those data blobs. So on the way to Dank Sharding, we're going to do this experiment with protodank sharding. In protodank sharding, basically you don't apply any erasure, coding or error correcting. All you do is you add this special transaction type that carries data blobs and those blobs are going to have an expiry and that's it. So the block size is going to be increased by a little bit in Ethereum. So right now, as Dan was saying, it's around 100 KB. With prototype sharding it's going to be around 500.
00:16:17.138 - 00:16:28.226, Speaker D: So so it's not a big of an increase, but we're still going to test all of the hypothesis that hopefully will check out and Ethereum will continue moving to full dank sharding.
00:16:28.338 - 00:16:59.598, Speaker C: So, Larry, you said a number of things in there. I want to make sure that all those points came across. You mentioned that the Blob data is going to have a different fee market. So is the right way to think about this? Like you have different highway systems. Or perhaps you have a lane on a highway where you have an EasyPASS or something. And maybe it's cheaper for someone in this HOV easy pass style lane. You get lower fees to put this kind of data on the blockchain versus someone who's just a regular commuter having to pay a toll.
00:16:59.598 - 00:17:08.520, Speaker C: I know I'm kind of mixing some metaphors here, but I'm wondering if that's like a physical analogy to describe these differences in fee markets for different types of data.
00:17:09.290 - 00:17:45.534, Speaker D: Yeah, I would say that it's hard to imagine this new data fees being more expensive than what we currently have. So the hypothesis is that it's always going to be cheaper to put your data in those Blobs if you don't care about accessing this data from the execution layer. So it's as if opening another lane on your highway that will just increase traffic. But for certain types of transactions, you will go into this lane. And for those transactions that are kind of data heavy, and for transactions that are execution heavy, you will continue going through the main lane, if that makes sense.
00:17:45.732 - 00:18:07.426, Speaker C: Yes, it does. And it sounds like one of the reasons why it can be cheaper is because it has this expiration date, as you mentioned. I think you said that the current idea is maybe that's going to last 30 to 60 days for this Blob data, at which point it would simply, I don't know, vanish and there would just be a trace remaining, a sort of commitment, as you described.
00:18:07.538 - 00:18:08.342, Speaker D: Yes, exactly.
00:18:08.476 - 00:18:53.350, Speaker A: Maybe to add to that, basically what happens is when you submit transactions, there's a fee market, as Lara was saying, in that if there are lots of transactions being submitted all at once, say there's a big NFT Mint and everybody wants to issue transactions, then of course the price per transaction immediately goes up. Well, Blob data is going to be a parallel fee market, so presumably there are fewer people submitting Blobs than there are people submitting transactions. So hopefully there will be less congestion over Blobs. But in principle it could happen. Hopefully not, but it could happen that all of a sudden, for some reason, there's huge congestion over Blobs, and then the fee market for Blobs will actually go up. But in principle, again, because there's less demand for submitting Blobs than there is for submitting transactions, the hope is that the cost for Blobs will be lower than the cost for transactions.
00:18:53.770 - 00:19:36.018, Speaker C: Okay, that's really helpful to understand as well. So we've laid out a sort of timeline for these updates. Perhaps people listening in. Maybe you're familiar with the merge which happened last year, in the fall, this big Ethereum upgrade that basically eliminated the environmental impact of Ethereum in terms of its energy consumption. And now we're entering this new period of upgrades which I think vitalik co creator of Ethereum has termed the surge, meaning that all of a sudden these updates are going to enable the blockchain to scale a lot more powerfully than it's been able to before. So part of this update, one of the first ones, is protodank sharding. It's happening later this year.
00:19:36.018 - 00:19:48.040, Speaker C: What happens in between that upgrade and full on Dank Sharding? What are the differences between the two? And when do we get that fully formed vision of Dank Sharding in the future?
00:19:48.570 - 00:20:34.098, Speaker D: That's a great question. I think there are still some research problems to figure out along the way, especially around networking, because when those Validators are storing only fragments of the block, they need to help each other reconstruct those fragments. If some Validator falls asleep for a while, it wakes up, it wants the help of other Validators to help it reconstructs its missing fragments. So it's quite involved networking protocol that I think is still in the making for Dank Sharding. And there are other exciting research problems that potentially can improve the scheme. But I think so far it looks like quite a clear path to get from protodank Sharding to Dank Sharding. And the question is just maybe how to make it better, how to improve different aspects of it, make it more efficient.
00:20:34.274 - 00:21:08.126, Speaker A: Maybe it's worthwhile adding that in the protodank sharding approach, there are at most four Blobs per block, which is why each blob is 128 times 128 gives us half a megabyte, which is why that's kind of the limit on block sizes in protodank Sharding. And that's actually imminent. That's supposed to happen later this year. And then, yeah, going all the way to Dank Sharding still takes some work. In fact, there was this very big event recently with generating parameters jointly for Dank sharding. And so there's a lot of work to do in preparation of getting to full Dank Sharding.
00:21:08.318 - 00:21:23.430, Speaker D: Yeah, that's quite exciting. I think they're still accepting contributions to participate in the trusted setup ceremony. So it's still ongoing. It's a large community effort that's really fun to watch and people come up with lots of creative ideas to contribute. So check this out.
00:21:23.500 - 00:21:39.900, Speaker C: Definitely that's super cool. I actually participated in some of the trusted setup ceremonies for some of the early privacy coins. Is this trusted setup ceremony as ostentatious as some of those where you had people kind of like burning laptops and exploding hard drives and things like that?
00:21:41.250 - 00:21:58.930, Speaker D: From what I've seen, it's just people come up with different creative ways to generate entropy. Some using their pets, dogs and cats. Some are creating some sophisticated marble run machines and such. There was even one contribution done from a satellite. The highest altitude contribution.
00:21:59.510 - 00:22:14.520, Speaker A: Yeah, actually, we have to mention that it was pretty cool, actually. This company, CryptoSat, actually has satellites in orbit that sample noise up in space and then contribute and participate in the trusted setup protocol. So that was pretty cool. Pretty cool to see.
00:22:14.890 - 00:22:26.362, Speaker C: Wow, that is awesome. Did not know there was crypto in space just yet. Dan, you said that proto Dank Sharding is going to have four Blobs per block. What is Dank Sharding going to enable? How many Blobs are we talking here?
00:22:26.496 - 00:22:45.642, Speaker A: Yes, by the way, it's proto Dank Sharding is up to four Blobs per block. They're actually targeting two, but up to four. And then Dank Sharding, as we said, they're targeting at most 30 megabyte blocks. So just divide 30 megabytes by 128. Tells you it's on the order of 100. I guess it's about 100 Blobs.
00:22:45.706 - 00:22:51.970, Speaker D: Yeah, I think the current plan is 128 target and maybe up to 256 Blobs per one block.
00:22:52.310 - 00:22:52.914, Speaker C: Great.
00:22:53.032 - 00:23:06.600, Speaker A: And that's exactly when the blocks become quite large, and then it becomes a little difficult for the validators to keep the entire blocks themselves. And that's where we have to start breaking them up into pieces, and each validator will store one piece.
00:23:06.970 - 00:23:38.370, Speaker C: Got it. I appreciate the basic division. Maybe some of the more technical math that you get into in your post would be a little bit harder to convey here, but that makes sense. Maybe we could talk a little bit about some of the proposals that you made in your recent post. So you did this research and you found that with some adjustments, you could potentially get even more efficiency out of EIP 4844, which is the more technical name for protodank Sharding.
00:23:38.870 - 00:24:38.110, Speaker D: Yeah, the bulk of the work was just to understand the Dank Sharding proposal in full, and then we observed some different ways to look into the mathematics cryptographic components of it that hopefully unravel new toolkits that we can use there. So the rough idea, not going too much in depth, is you fit a polynomial through your block. It's a bivariate polynomial because your block is rectangular, and then you evaluate this polynomial at more points, kind of expanding the block. And the idea is that if you view this as bivariate polynomials instead of dencriding was viewing it as a list of multivariate polynomials, you can then apply techniques for bivariate evaluation, bivariate interpolation, and possibly even try to apply bivariate correcting codes to that. But that's very much an open door for more research and exploration. So in this post, we try to explain where more research can be done to improve the scheme in that direction.
00:24:38.270 - 00:25:13.674, Speaker A: Yeah, maybe I can add to that. I mean, Dank Sharding is such a beautiful idea, really. The Ethereum Foundation deserves a huge amount of credit for it, and Dunkard in particular. It's really quite an elegant construction. Initially, we were just kind of trying to understand the details and it took some time to kind of recover exactly all the details of how everything works. And we figured maybe it'll help the world to also have another write up that explains how the mechanism works. And initially, I guess, the original Dunk Sharding, the way it's described, is all using univariate polynomial commitments where we take each row, we view it as a polynomial.
00:25:13.674 - 00:25:52.246, Speaker A: Erasure coding is all done using polynomials. Maybe I can even teach a little bit of erasure coding here in one sentence, in that suppose you have two points in the plane and you want to do erasure coding on them. What you can do is you can just pass a line through those two points. And now you can just publish instead of just those two points, you can publish those two points plus maybe two additional points on the line. So now you have four points total. And you know that if two out of those four points make it to the receiver, the receiver can use the two points that it received to recover the line and then recover the original two points. That's the whole idea of erasure coding.
00:25:52.246 - 00:26:28.578, Speaker A: So of course, instead of lines, we use higher degree polynomials to achieve different thresholds. But that's the idea. Basically, we have two points, we pass a line, we get more points on the line. If only two points of the line make it to the receiver, the receiver can reconstruct the line and recover the original points. And so Dankstarding actually does this really quite elegantly by looking at the block as a matrix, as a rectangular set of data. And then it basically extends using these line ideas both horizontally and vertically. And that actually gives the quota block where then pieces of that rectangle is sent to the different validators.
00:26:28.578 - 00:27:19.058, Speaker A: What's interesting is now that you have a rectangle, you can kind of think of it as a two dimensional object that very naturally leads to thinking about this as a bivariate polynomial, as exactly as Lara was saying. And what Dank Sharding does is it gives actually a very interesting way to commit to these bivariate polynomials. So it kind of builds a way to commit to bivariate polynomials by using commitments to univariate polynomials. And so it turns out that the reconstruction mechanism that was done in Dengue charting is also based on construction along lines and columns, also construction as done using univariate polynomials. And then I guess as we were working through this, there was this realization that, hey, everything here really is about rectangles and bivariate polynomials. Maybe it is a way in which the reconstruction can also be done by using sort of interpolation of bivariate polynomials.
00:27:19.154 - 00:27:54.478, Speaker D: So in fact, you take your block and you expand it to x by the factor of two in both directions. So you have four x more points as a result. But those four x points only encode a small quadrant. So in principle, you only need one quadrant in order to interpolate and recover all the rest. Of this encoded block. So 25% of the points should be enough. But as Denk Sharding works by doing univariate interpolations, it needs 75% of the block to do column and row wise reconstruction.
00:27:54.478 - 00:28:33.746, Speaker D: If you do Bivariate interpolation directly, you should be good with just 25% instead of 75%. So that will improve the number of elements you need in order to reconstruct to recover the block. It will also improve communication and data availability, sampling, but it's all due to improved reconstruction. Now the question becomes kind of a mathematical question of how do you do efficient Bivariate interpolation? There is an obvious need for kind of a better algorithm there. And we were researching this a little bit, and it appears so far to be a little bit underexplored. So maybe there were no applications for Bivariate interpolation before. Maybe it's just a hard problem we didn't know.
00:28:33.746 - 00:28:40.770, Speaker D: But that's definitely an interesting direction to basically try and improve Bilinear interpolation algorithms.
00:28:41.190 - 00:29:48.754, Speaker A: So what I love about this is, just like Lara was saying for the more algorithms folks in the audience, is that there's been a ton of work on doing univariate interpolation. If I give you points on a univariate polynomial, like points on a line, and I ask you to reconstruct that polynomial, there are very good algorithms for univariate polynomial interpolation. And it turns out the Bivariate interpolation problem, somehow it seems like it received less attention. And what's really cool here is all of a sudden the blockchain ethereum Dank Sharding is creating an application for this really natural algorithmic problem of Bivariate polynomial interpolation. We really need it here. If we had a good algorithm for Bivariate polynomial interpolation, we could actually make Dunk Sharding better, because reconstruction, as Lara was saying, would go down from 75% to 25%. So to me, this is really beautiful in that the blockchain theorem, Dank Sharding is creating this new area of research, or at least prioritizing this new area of research, showing we really need better algorithms, new algorithms, efficient algorithms to do Bivariate polynomial interpolation.
00:29:48.754 - 00:29:56.700, Speaker A: So hopefully that will encourage and spur a lot more research on these types of algorithms, and presumably they will be actually very useful for this problem.
00:29:57.230 - 00:31:17.720, Speaker C: So I love that we dug into the methodology here and didn't shy away from the mathematics. I know some of it might sound a little complicated, Bivariant polynomials and univariate polynomials, but I especially appreciate how you sort of describe these in terms of geometry and shapes, because I think everybody here can really picture a line going through some points or how a rectangle functions. So I think that really helps ground the kind of work that you're doing. I want to double click on this statistic that you mentioned where the current proposal as it exists, would require 75% of samples in order to be reconstructed, whereas what you're proposing would trim that down to 25%. So that's a giant difference, 75% to 25%. But it also sounds like just for a casual observer, if you're only having 25%, the fact that it's like just less than 50%, it sounds like, is that really enough to make assurances that this data is available and was made available? When you go down to 25%, it sounds like, I don't know, you might be cutting some corners or stuff. So how do you assure people that in fact just having 25% of data samples is actually enough and that things can work at that level?
00:31:18.410 - 00:32:31.494, Speaker D: Yeah, that gets us to the topic of data availability sampling and what it achieves. I guess because this reconstruction thresholds 75% or 25% basically determine how many samples you need to get high assurance that the data is there. The way you do the sample is that you ask the network of validators to give you back one element, random element of this encoded block. And if you get it back successfully and you can verify also validators give you back the proof of the validity of this piece and you can verify it against the commitments that the chain persistently stores. So when you get back the successful sample, that makes you sure that the data is available with probability that is one minus either one quarter or three quarters depending on how your reconstruction algorithm works, whether it requires 25% of the data, 75% of the data. So every time you do a random sample and it comes back successfully, your basically false positive. The probability that you think the data is available when it's not drops down exponentially and the rate at which it drops down depends on how much data you're required in the reconstruction.
00:32:31.494 - 00:33:13.660, Speaker D: So if you reconstruction only requests 95% of the data, you do less samples and your assurance the false positive rate goes down quicker than if you only have a reconstruction algorithm that requires 75% of the data. So depending on how efficient is your reconstruction, you might need fewer samples in order to have the same assurance that the data is available. So that's why you not only improve the reconstruction here, but you also improve the number of samples you need to do for your data availability sampling. And data availability sampling is interesting because it's probabilistic. So the more samples you do, the higher your assurance the data is available. Right, and you can always amplify this probability by doing more samples to make it clear.
00:33:14.030 - 00:33:40.814, Speaker A: I think actually Lara, what you just explained is really, really important. So that's like the heart of Dank Sharding and Data availability sampling. So I'll just say it one more time so that the audience will hear it twice because that's really at the heart of it. So maybe think of the block. We said the block is going to be encoded as this rectangle, right? So somehow we go from a block to a rectangle. The specifics of that is done using serial coding. But let's pretend we went from a block of data to a rectangle.
00:33:40.814 - 00:34:31.938, Speaker A: So now imagine this rectangle is literally a rectangle of dots. Every dot corresponds to one piece of the data that's going to be distributed to one validator. So now to do data availability sampling, somebody wants to verify that enough of the dots are actually available. We know that if more than 75% of the dots are available, then the block can be reconstructed using the erasure coding method. Or maybe if what we're saying would be used, then only 25% of the dots are sufficient to reconstruct the original rectangle. But how do you know that 75% of the dots are available? So that's exactly the data availability sampling mechanism. What you do is you can kind of imagine like, you were throwing darts at this rectangle, right? So every time you throw a dart, you hit a random point in the rectangle, and then the validator that holds that point has to prove, yes, I really have that point.
00:34:31.938 - 00:35:13.758, Speaker A: Now, you want to verify that 75% of the dots are available. So imagine you have this rectangle. Maybe only 75% of the dots are there. Some of the dots disappeared for some reason. You want to verify that 75% of the dots are available, because if 75% are available, you can reconstruct the entire rectangle. And so what are you going to do to verify that 75% are there? You're going to throw a bunch of darts at the rectangle, and for every time the dart hits a dot, the validator that it hits has to prove that the dot really is there. And so if you throw 100 darts and all 100 darts come back saying, yes, the data really is there, that gives you a pretty good idea that more than 75% of the data is available.
00:35:13.758 - 00:35:56.014, Speaker A: Because if less than 75% is available and you throw four darts, you expect one dart to hit a missing dot. And so if you throw 100 darts and all of them come back saying the data is available, you have pretty good assurance that more than 75% of the dots are there. And so that's the idea of data availability sampling. You just try lots and lots and lots of random points, like 100 of them. If all of them are there, then you have pretty good assurance that more than 75% are available. And you can reconstruct the data. And you see, if you want to get 75% assurance you need to throw 100 darts, if you need only 25% assurance, you would need to throw fewer darts than that.
00:35:56.014 - 00:36:42.314, Speaker A: So that would basically reduce the amount of data that's needed to satisfy the sampling mechanism. Now, maybe it's worthwhile saying that once the data availability sampling check succeeds, so all the 100 darts come back saying, yes, the dots really are there, then the validator says the data is available, and then goes ahead and signs the block as saying this block passed data availability sampling. Yeah, and that's used later on in consensus. So that's what the test is. The test basically data availability sampling is a very efficient way to test that enough data is available to reconstruct a block without actually reconstructing the block completely. So I think it's good to hear it twice and maybe even a third and a fourth time. But that's kind of the core idea that makes this all work.
00:36:42.512 - 00:37:06.530, Speaker C: I love that and I especially love the physical analogies that you're using with a dartboard and throwing darts. I think that really brings it home for people. So we're nearing the top of the hour here. I want to get ready to close out. But before we do that, just maybe I'll throw it to you both in one sentence. What is the upshot of all of this? Why does it matter to get these efficiency gains?
00:37:07.270 - 00:37:43.098, Speaker D: Well, I would say the ultimate goal, of course, is to scale the blockchain and these new techniques would allow to do that for Ethereum to achieve full scaling. That's really interesting approach, I would say, because in the beginning ethereum was thinking about doing full sharding and arriving at quite complicated designs. But having roll ups help scale execution of Ethereum and leaves it up to Ethereum to scale its data availability layer, basically increase the space while roll ups increase the execution capacity. And that piece together will give us cheaper and faster blockchains.
00:37:43.274 - 00:38:08.520, Speaker A: Yeah, lara said that perfectly. I mean, really, the scaling story for Ethereum is roll ups. And the way to get roll ups to be more efficient and cheaper to use is by solving the data problem. And Danksharding is a very elegant and efficient way to do that. So the upshot is a scalable version of Ethereum where roll ups are much cheaper to use than they are today.
00:38:08.970 - 00:38:42.226, Speaker C: That's great. And if you get cheaper transactions and the ability to make more transactions, I think that opens up Ethereum to do all sorts of new applications that weren't possible before with high gas costs and fees. So this has been great. Thank you all for joining us. I hope that you all learned a little bit about data availability sampling and Dank sharding. If you'd like to learn more, as mentioned, you can check out Dan and Lara's post. It's a really great piece, so I highly recommend reading it and checking out all the resources contained in there.
00:38:42.226 - 00:38:54.294, Speaker C: Thank you all for joining us. I'm looking forward to this weekend. I'm going to go to my local bar and teach everybody about erasure coding. So thank you all and take care.
00:38:54.412 - 00:38:55.590, Speaker D: Sounds great. Thank you.
00:38:55.660 - 00:38:56.054, Speaker C: Thank you.
00:38:56.092 - 00:38:56.886, Speaker A: This has been fun.
00:38:56.988 - 00:38:58.570, Speaker D: Bye bye.
00:39:01.710 - 00:39:39.080, Speaker E: Thank you for listening to Web Three with a six and Z, you can find show notes with links to resources, books or papers, discussed transcripts and more at asics and Zcrypto.com. This episode was technically edited by our audio editor, Justin Golden. Credit also to Moonshot Design for the Art and all thanks to support from Asics and Z Crypto. To follow more of our work and get updates, resources from us and from others, be sure to subscribe to our web three weekly newsletter. You can find it on our website@asicscrypto.com. Thank you for listening and for subscribing. Let's go.
