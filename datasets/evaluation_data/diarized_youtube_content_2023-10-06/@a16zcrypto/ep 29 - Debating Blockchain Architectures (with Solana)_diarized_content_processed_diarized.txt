00:00:00.250 - 00:00:00.800, Speaker A: You.
00:00:04.610 - 00:01:04.634, Speaker B: Welcome to Web Three with A Six and Z, a show about building the next generation of the Internet from the team at A Six and Z crypto that includes me, your host, Sonal Chaksi. Today's episode is one of our deep Dive hallway style chats with a six and Z crypto general partner Ali Yaya and Deal team partner Guy Wu Oulette in conversation with Salana co founder and Salana Lab CEO Anatoli Yakovnko, who also worked at Qualcomm for over a decade, where he was a senior engineer and engineering manager, among other things. I mentioned that because we discussed engineering, hiring, systems optimization and more. In this episode, we actually kick off the first half of the episode discussing and debating blockchain architectures, including Solana and Ethereum, monolithic versus modular, etc. And really what are the trade offs and what to optimize for or not, depending on what you're building, and to make crypto happen at scale now and in the future. We discuss how innovation plays out in theory versus practice. And in the second half of the episode we cover company, community and ecosystem building, including the nuances of leadership in open source.
00:01:04.634 - 00:01:39.450, Speaker B: We also briefly discuss devrel governance, some of the Solana backstory, and the differences between hardware and software innovation, including touching on the Salana phone. As a reminder, none of the following should be taken as investment, legal, business or tax advice. Please see asicsmc.com disclosures for more important information, including a link to a list of our investments. Be sure to also check out the show notes for links to related episodes or resources on some of the tech topics that come up, especially on the evolution of programming, role of AI and VDFS, or Verifiable delay functions. The first voice you'll hear is Ali's, followed by Tolle's and then guides.
00:01:43.250 - 00:02:21.258, Speaker C: Well, Tolly, it's really great to have you here. You are one of the most original and contrarian thinkers about blockchain architecture and the technical aspects of how to build a blockchain and decentralized computation generally. I think the overarching theme is the big question of what the end game of blockchain scalability is. And there's obviously so many different approaches, there are various trade offs to the different ways of doing things. And so to start off, I would just want to start with how do you see the end game for decentralized computation, what do blockchain architectures look like, and then maybe we can take it from there.
00:02:21.344 - 00:03:02.754, Speaker A: Well, thank you for having me. I'll be a bit more contrarian than I actually am. I'm going to take a very extreme position here. I think that settlement will become less and less important commoditized like it has in traditional finance, and you still need stuff for settlement guarantees, but those can be achieved many different ways. And I think what's really hard is what's really valuable to the world is having synchronized piece of state that's globally distributed and globally synchronized. And you can think of this as like Google. Spanner does this for Google, but in a lot of ways Nasdaq, NYSE, they do that for financial markets.
00:03:02.754 - 00:03:50.406, Speaker A: They guarantee that there is a certain chunk of memory that you can rely on that gives you the best price for stuff. And from a very high level blockchain systems. They are permissionless marketplaces. They're very programmable, very open, you can do lots of stuff with them. But ultimately at the bottom of the stack is some kind of marketplace and it's very valuable for all those marketplaces to be completely synchronized as close to the speed of light as possible around the world because then everybody can use that as a reference point. You can still run local markets and do stuff, but if you know that this is like the global price for X, Y and Z, it makes all of the world's finance more efficient. And I think that's the end state of blockchains synchronize as much state as you can at the speed of light.
00:03:50.406 - 00:03:52.150, Speaker A: That would be the end game.
00:03:52.300 - 00:04:14.686, Speaker C: One way to ground this conversation is to align on what we think the most interesting applications will be at the end game. To imagine like now there are 100 million or billions of users who are using crypto in some way or another and are interacting with blockchains. What do you think the biggest drivers of activity on a blockchain will be once we reach that world?
00:04:14.868 - 00:04:55.930, Speaker A: My belief is that the kind of applications that you will see that are actually touching a billion users are very similar to the web two kind, but it's more transparent. It's truer to that vision of a long tail internet with a lot of different smaller companies that are able to control their data. They're not just being rapidly commoditized by Spotify and all these giant and aggregators, even though those companies are building great products and doing great work. But I think the long term is you have creators with more control, more self publishing and real internet wide distribution and markets that drive all of that under the hood like as fast as possible.
00:04:56.080 - 00:05:14.498, Speaker D: Another interesting way of framing this question is the set of trade offs you mentioned. You think settlement will become less important in the future? I'd be curious when you think about the endgame and Solana as the place that a lot of global commerce, specifically finance happens, what are some of the auxiliary components or perhaps complements to that?
00:05:14.584 - 00:05:53.206, Speaker A: Well, that system is not designed as a store of value. That's very different problem. It doesn't try to actually be as tolerant to internet failure because it's actually trying to be as fast as possible. So you use all the available resources on the internet. It actually relies on a world where you have mostly free cross border communication and finance. It's not like bunker coin. And I think there is a need in the world to have a bunker coin that will survive, god forbid, global wars and cold wars and things that really bisect the world and these massive geopolitical challenges.
00:05:53.206 - 00:06:19.158, Speaker A: Something needs to survive that. And that's a valuable thing in and of itself. But I think in a very optimistic world that I believe is happening right, like things are getting more and more connected. I think we're going to see ten gigabit connectivity between us while you're driving in the US. To a person driving in China. I think that's happening within like five to ten years. In that world where you have this completely interconnected world.
00:06:19.158 - 00:07:22.934, Speaker A: I think this globally synchronized state machine is something that absorbs a lot of the execution side of it and then you have a lot of places where settlement can occur because the settlement part is very easy to guarantee empirically. This is my, again, very I'm taking this position for discussion. We've seen, I don't know since 2017, hundreds of different even kinds of proof of stake networks, like design wise and like many, many different instances of them. And we basically have seen zero quorum failures because settlement is very easy to get right. Once you have Byzantine fault tolerance and over 21 participants that are fairly decentralized, you actually don't see settlement failures. All of these other problems that are auxiliary to that I think are already solved, like Tendermint, I think is a wonderful system that has been unbreakable empirically despite having to go through like a luna sized crash. It wasn't the quorum that failed.
00:07:22.934 - 00:07:59.702, Speaker A: Settlement actually worked without any faults at all. So I think we are overspending on settlement in terms of security and resources and engineering and way underspending on research and Rd on the execution side, which is the place where most of finance makes money. And I feel personally that if these technologies are going to impact and touch the world at scale, like at truly like global scale, it has to be better than traditional finance by price, by fairness, by speed even. So, this is where we need to focus all our R and D on and compete there.
00:07:59.756 - 00:08:38.114, Speaker C: You're highlighting the fact that settlement is one of the things that you can choose to optimize a blockchain and that actually maybe people over optimize blockchains to do settlement, and that there are other things that you can also optimize for that maybe are more important. There are certain things along the lines of performance, like say, throughput and latency, as well as composability, and things that are also important and often are in opposition against the security of settlement. One thing which might be very helpful is just a high level description of Solana architecture. Basically like your vision for how a hyperscalable blockchain looks.
00:08:38.232 - 00:09:11.478, Speaker A: Okay, let me give you a brief overview of what Solana is from like an architecture. It's actually pretty simple. It's designed to transmit information around the world as fast as possible to all the participants in the network at the same time. So there is no sharding, there's no complicated consensus protocol, so to speak. We've actually tried to keep things pretty vanilla, or like, we got lucky, I guess I would say, with one hard computer science problem. And this was Fox Synchronization. And this is the idea for using a verifiable delay function as a source of time in the network.
00:09:11.478 - 00:10:01.046, Speaker A: And you can think of that as if you have two radio towers that transmit at the same time over the same frequency you get noise. And the first protocol that people thought when they started building cellular networks is that, why do I give each tower a clock? And they alternate by time when they can transmit. And literally, FTC is like a truck full of goons that will drive out to your cell tower and shut it down if it goes out of sync. And you can't do that in an open permissionless network like around the world. So the idea for Solan actually came from that kind of Eureka moment that you can use a verifiable delay function to schedule block producers so they don't collide. Because in a network like Bitcoin, if you have two block producers that produce a block at the same time, you have a fork. And that's the same thing as the noise in the cellular network.
00:10:01.046 - 00:11:21.638, Speaker A: So if we can force all of them to alternate by time, you get this nice Time Division protocol where every block producer takes a turn that's scheduled, and they never collude, and therefore forks never occur, and the network never gets into that noisy state. And then everything else that we've did after that has been pretty standard kind of optimizations that folks have done in operating systems and databases for 2030 years. We transmit the blocks around the world like BitTorrent, so in pieces with eraser coding to a bunch of different machines. Actually, that eraser coding ends up looking very similar to data availability sampling, and kind of has the same effects, and they retransmit the bits to each other and reconstruct the blocks, and then they vote. And it just kind of continuously keeps doing that. And the main takeaway from how Solana is designed is that we try to really make sure that every process that the network or the code base and the way that it's engineered is designed such that we can scale it with more cores. That if in two years we get twice as many cores per dollar that we can literally tune it and say, okay, we now have twice as many shreds per block or twice as much compute per block and therefore the network can do twice as much stuff.
00:11:21.638 - 00:12:03.534, Speaker A: And that happens naturally, really, without any architecture changes. And that's kind of the main goal that we really wanted to achieve. Because this is based on my experience at Qualcomm. I was there from 2003 to 2014. Every year we saw hardware improvement and an architecture improvement on the mobile devices. And if you didn't write your software such that you don't have to rewrite it the next year, you're kind of screwed as an engineer, because you would get a device that's twice as fast with twice as big SIM delays, and you'd have to like, okay, I have to rewrite this code now just to take advantage of this. So you really had to think ahead that everything that you're building is only going to get faster and faster.
00:12:03.534 - 00:12:37.326, Speaker A: And that works out pretty well. And that's been, like, the biggest learning experience in my engineer career, is that you can pick an algorithm that's very well tuned and it can be the wrong thing, because the benefits of using that algorithm become very marginal as hardware improves and scales. And the complexity of implementing it right now becomes, like, a major waste of time. So if you can just do the very simple, naive thing and just scale with cores, you actually are like probably 95% of the way there.
00:12:37.428 - 00:13:03.062, Speaker C: That idea, the idea of proof of history as a way of synchronizing time across validators, that was a very groundbreaking idea. That was like a fundamental difference in how to implement consensus in a permissionless decentralized network. And so that as the core insights of what makes Solana so fundamentally different from all other permissionless consensus protocols is a very important kind of core part.
00:13:03.116 - 00:13:45.074, Speaker A: That was actually, like it's one of the pieces in the Amdl's law, like, when you're trying to optimize. And this is why it's so hard, I think, for folks to replicate Solana in terms of both node count and latency and throughput is because classical consensus implementations, they're very kind of step function based. You have a whole network, like tenermint or even hot stuff, they all have to agree on what the current block is before they go to the next one. And cell towers don't do that. They don't do a round of communication and say, okay, we did 120 millisecond slot between 10,000 participants. Let's all go to the next one. They actually already have a schedule and you just transmit and go.
00:13:45.074 - 00:14:18.480, Speaker A: And because you don't have this required step function around, you can actually run the network very quickly. I think of it as like, asynchronously, but I don't know if that's the right term. But you have a set of leaders and they just continue transmitting at their slot and you never stop and wait for consensus to run. And that's only possible because we have this strict concrete understanding of time. I honestly think that maybe even redundant. There are clock synchronization protocols that you can build. They're just very hard.
00:14:18.480 - 00:14:35.982, Speaker A: I remember working with them, with the CDMA team, and it's a massive undertaking to have reliable clock synchronization. And Google uses permissioned atomic clocks, and that's just not something that's feasible on a permissionless network. So that's the philosophy behind Solana.
00:14:36.046 - 00:14:36.914, Speaker C: That's very interesting.
00:14:37.032 - 00:15:09.758, Speaker A: Before I started building Solana, I was like trading and interactive brokers and OANDA and forex.com like algorithmic trading for fun. None of it ever made any money. But this was at a time where flashboys was prevalent across traditional finance. And whenever I thought that I had a good algorithm, my orders would be a little late. I could literally see it would take much longer for that order to make it to the market and the data would arrive slower. So I was fighting both sides of this kind of like black box.
00:15:09.758 - 00:16:01.530, Speaker A: And I think the fundamental goal for these open permissionless systems, if we want to disrupt finance, is to make it impossible for that to ever happen, that the system is open, anyone can participate in it. It's clear how to get access to get prioritized rights or fair rights, however you want to construct it and it's clear how to get access to the data without any delays or without anyone in the middle. And for that to all happen as fast as physics allowed, as fast as engineers can make it, I think that's the fundamental problem that if blockchains can solve, it would be very impactful to the rest of the world and would benefit a lot of people globally. And that could be a building block that then you can use to disrupt the ad exchange monetization model on the web, like make it all web three with micro payments and attention tokens and yada yada.
00:16:01.610 - 00:16:11.954, Speaker D: I think there's an important distinction between pure latency and atomicity, especially within a single state machine. Perhaps you can expand on which you think is important and why.
00:16:12.072 - 00:17:12.706, Speaker A: Yeah, part of this is that, well, it's impossible to make the entire state atomic because that kind of implies a single global write lock for the entire state. It implies a very sequential slow system. So you need atomic access to state and you need that to be guaranteed because it's really hard to build software that acts on remote state that isn't atomic because you don't know what the side effects are on any of your computations. So this idea of being able to submit a transaction, either all of it executes or it entirely fails without any side effects is just part of what the properties of these computers they need to have them. Otherwise I think it's pretty impossible to write reliable software for them. You just can't build any kind of reliable logic or definitely not financial. You might be able to build things that are eventually consistent systems, but that's, I think, a different kind of software.
00:17:12.706 - 00:17:54.738, Speaker A: So there's always kind of that tension between making the system atomic and performant. Because if you guarantee this optimicity, it means that you are picking a particular writer at any given moment globally to that particular part of the state. And that resolution requires for you to have a single sequencer and something to linearize those events. And that creates points of where you can extract value and attack the fairness of the system. There's a lot of tradeoffs there and solving them I think is a really hard problem that we've seen. Not just Solana face those, but Ethereum with Flashbots and things like that.
00:17:54.824 - 00:18:14.554, Speaker C: One of the things that is often debated, especially in the Ethereum community, is the notion of verifiability of execution. And at least from the perspective of the Ethereum community, it's very important for end users or people who don't have extremely powerful machines to be able to verify the activity in the network. I'm curious what your view is.
00:18:14.672 - 00:19:15.630, Speaker A: I think the end game for both systems is very similar. If you look at the end of the Ethereum roadmap and you have either Dank Sharding Subcommittees or Shards or whatever you want to call them, the idea there is that the overall total network bandwidth is greater than any individual node. So the network is already computing or processing more events than any individual node is configured for. You have to take into account the safety and security considerations of such a system and there's protocols to post fraud proofs and things like that in data availabilities like sampling schemes. All those are actually applicable to Solana as well. So if you take one step back, it doesn't actually look that much different. You have a system like a black box that creates so much bandwidth that it's not very practical for a random user, that's not a sophisticated user to go verify it.
00:19:15.630 - 00:20:12.922, Speaker A: So they're going to rely on these sampling techniques to guarantee that the data is there and the very robust gossip networks that guarantee that nodes cannot be eclipsed and they can propagate fraud proofs to all the clients and stuff like that. And that's the same kind of guarantees between Solana and Ethereum. The main difference I think is Ethereum is very much constrained by Bitcoin and this idea of being global money and really holding on to that narrative, trying to compete with Bitcoin and this idea of store of value. And I think there is some narrative reasons there to have very small nodes that user can even if they're partially participating in the network, they can still do that versus having the network all run by professionals. That kind of makes sense. Honestly, I think it's a fair thing to optimize for. If you don't care about execution, all you care about is settlement.
00:20:12.922 - 00:21:14.574, Speaker A: Why not minimize the node requirements to their lowest possible and allow people to partially participate in the activities in the network? I don't think that creates a system that's any more trust minimized or secure. At the end of the day for the vast majority of the world. Like at the end of the day, you're still going to rely on data availability, sampling, Das and fraud proofs. And the user, to validate that the chain has done something wrong, only needs to execute a bunch of signatures from the majority of the chain on Solana. That single transaction describes all the individual pieces of state that that transaction touches and that actually runs on any device like a browser in a mobile phone can easily execute that one transaction. That the majority side that did the invalid state transaction. And because everything is specified ahead of time on Solana, it's actually even easier to construct on Solana than with like EVM or any smart contract can touch any state and randomly jump between them during execution in some way that's almost simpler.
00:21:14.574 - 00:21:26.214, Speaker A: But I think from a very high level, at the end of the day users are going to rely on this Das plus fraud proof and that's kind of like the same for all designs at this point.
00:21:26.332 - 00:22:34.366, Speaker D: So I think the biggest difference or distinction between what I see as the long term Ethereum roadmap and the long term Solana roadmap is around zero knowledge proofs and validity proofs specifically as opposed to just fraud proofs. And the idea that even if a user does have to have some level of trust in a centralized RPC and throughput reaches the point to where no individual user, perhaps even in a data center, can run enough hardware. To fully verify the chain the idea that someone is creating a proof or you have aggregate proofing together that is then distributed to individual users who can verify it and instead of putting your trust in Crypto economic mechanism you're putting your trust in the ZK proof systems. And I remember when we had discussed this a little bit when we were at crypto startup school and you were kind enough to come speak. You seemed to think the Zke EVMs were almost impossible to audit and that they would not be robust for several years and so it wasn't even really worth trying to go down that path today because perhaps it was too early. I don't want to misrepresent your views, but I would be curious to ask you broadly why Solana has not prioritized ZK proofs and validity proofs the same way it seems like Ethereum has.
00:22:34.468 - 00:23:28.174, Speaker A: I think there's two challenges there. The way that we have prioritized them is there's a company called Light Protocol that's building application specific zero knowledge proofs and these are hand optimized for the specific program code. So the proving times are fast, the user doesn't really notice them during their interaction with the chain and you can actually compose them. You can have a single Solana transaction that calls five different ZK programs. So that is like an environment that saves some compute resources or creates privacy for the user but that's not really verifying the entire chain. And the reason why it's really really hard to verify the entire chain is that zero knowledge systems don't do a very good job with a lot of sequential state dependencies. Like the classic example is the VDF or Verifiable delay function.
00:23:28.174 - 00:24:26.226, Speaker A: Every time I talk to look at zero knowledge performance you can really see it fall apart when you try to prove a sequential shot recursive shot to 56. It's because that sequential state dependencies on execution really blows up the amount of constraints that the system has to have and verification improving takes a very long time. I don't know if this is the best in the industry, but latest thing that I saw on Twitter is about like 60 milliseconds for 256 byte shaw. That's a very long time for a single shaw instruction, right, when like a normal intel chip with native shaw to 56 instructions can do like 11 million shaws per second. So for sequential computation, classical computation is going to be necessary anyways. And in an environment that's designed for execution with lots of marketplaces, you actually have a lot of sequential dependencies. You have markets that are very hot.
00:24:26.226 - 00:25:20.226, Speaker A: Everyone's submitting data directly into just that one pair and everything that's triangulating around that pair depends on that. So your execution that's sequentially dependent actually blows up and that becomes a very long system to prove. And solana does not prohibit somebody running a zero knowledge prover in a recursive batch verified way against the entire computation. And that just maybe happens to run once every 2 hours or eight days or whatever on some gigantic machine. That's fine if it's feasible. But what users need is if I'm trading, I need that guarantee that my information was written to the chain within like microseconds right milliseconds, and that I'm getting the state and some guarantees on that state also within milliseconds. And that's where the money is going to be made.
00:25:20.226 - 00:25:55.402, Speaker A: So I think we need to solve that problem and that needs to be actually competitive with traditional finance. And if that's true, then you can start working on zero knowledge and figure out how do we provide these guarantees to users that basically don't want to verify the chain and don't want to rely on these events, but maybe we can at least do it once every 24 hours or something like that. I just think that there's two different use cases and the primary one is we got to actually solve the mechanical markets problem first, not the problem for the long tail of users that don't want to run a node.
00:25:55.466 - 00:26:06.974, Speaker D: So it sounds like what you're saying is proofs, validity proof. CK proofs are excellent at settlement, but they don't really help you for execution because the latency is too long and the performance is too poor. And that's likely to be true in the longer term.
00:26:07.022 - 00:26:33.658, Speaker A: It's been true so far, unless you guys disagree. And that's my gut instinct, simply because the more active the chain gets, the more sequential kind of hotspots and state dependencies you get. It's not like these perfectly parallelizable things that never talk to each other. That was true. I think we would be done by now. It's just you have a bunch of really hard to parallelize, really hot spotty code that spikes all over the place.
00:26:33.744 - 00:27:36.430, Speaker C: Well, the other counterargument may just be that the curve of improvement of zero knowledge proves in particular on the prover side, is exponentially improving. Maybe five years from now, ten years from now, the overhead might go down from something like 1000 x on top of just performing the computation itself to then generate a proof, which is what it is today, down to something that is much, much more feasible, especially with the help of dedicated hardware. And I would love your perspective here, given that you come from a background of hardware engineering. But it could be that we live in a world where zero knowledge proofs are so good because the demand for zero knowledge proofs in the world is so high that it drives a lot of energy towards optimizing them. At every level, at the algorithmic level, at the code optimization level, and at the hardware level to make them very competitive to the point at which it might actually be more efficient to have one node perform the computation and generate a proof and distribute that proof to everyone else, as opposed to having every node having to run the computation themselves.
00:27:36.580 - 00:28:47.458, Speaker A: So I think that trend will actually work well for program optimized zero knowledge systems because then you can look at the kinds of applications that aren't experiencing the other effect, which is there's more and more stuff happening on chain and the number of constraints blows up faster than you can add hardware. And this is based on my gut. My feeling is that as the demand side gets bigger, like as you see more and more computation happening on chain, it's going to be harder and harder for zero knowledge systems to keep up in a low latency way. Not 100% sure that it's even feasible, but I think it's very possible that you can build systems that take very large recursive batches, but you still have to run classical execution and you snapshot it like every second. And then you throw like an hour's worth of compute on a big parallel farm that does verification between every snapshot and then does recursives from there. But that will take time and a big pile of hardware that's going to run behind the actual execution of the chain. I think that's the challenge.
00:28:47.458 - 00:29:16.494, Speaker A: It's not clear to me that ZK can't catch up unless demand flattens. Like, I think eventually demand will flatten out and obviously if Moore's Law stops, we're kind of screwed as humanity to begin with. But assuming Moore's Law continues and hardware keeps improving and at some point crypto demand is saturated, like Google searches per second are probably saturated at this point, then you'll start seeing that flip happening. I think we're just far from that.
00:29:16.612 - 00:30:13.778, Speaker C: The other big difference between the two models, which is related to this point, is that kind of the ethereum, roll up centric vision of the world is one where you do essentially shard computation. You shard data availability, you shard. Bandwidth and network activity. And as a result it's conceivable that you could end up with a greater amount of throughput in aggregate because you can add roll ups on top of a single L one almost indefinitely. But of course you'll suffer on latency to finality. And so part of the question then is what ends up being more important? Is it throughput overall, throughput down the line or is it latency? And it may be the case that both are important, but for different applications. It could be that both architectures actually fill in a niche for certain kinds of applications that maybe require one thing versus the other and both of them therefore continue to exist side by side.
00:30:13.864 - 00:30:36.086, Speaker A: I think actually you can do pretty good latency to finality even in that system. If you take the subcommittee latency, the Subcommittee finality as good enough. That really depends on the overall performance. How often does a subcommittee's block get rolled back? Maybe it's designed such that it's never that could be almost possible.
00:30:36.268 - 00:30:44.170, Speaker C: But it may be that you're still constrained by the throughput that you can get given that you have to generate zero knowledge proofs so that the one can verify them.
00:30:44.240 - 00:31:45.002, Speaker A: I think the problem with that environment, there's like a couple of problems, but I think the main one is that you have your roll up and your sequencer. And this is where I think people are going to try to extract value is in the sequencer and the roll up construction. And they're going to build like a tradefice style system where you have sequencers that are more or less colocated. They extract all the mev from that environment and they operate no different than Citadel or Jump or Broker dealer or whatever you want to call it that's routing orders and putting stuff in the middle of them and that's fine. But those systems already exist and this design doesn't actually break open that whole monopoly. What I think is better is if you had like a totally open permissionless system where it wasn't possible for those middlemen to actually even place themselves and to start extracting value that it was truly synchronized. Global state machine.
00:31:45.002 - 00:32:44.158, Speaker A: Even if that State Machine has lower network bandwidth than this massively Sharded system, it's still providing more value. And it's very likely that it's actually going to be cheaper to use because the Sharded approach is like creating a bunch of different small pipes and generally pricing in any given pipe is going to be based on how much capacity is left in that single pipe, not overall network capacity. It's very hard to build a system where all the network bandwidth is completely shared. You can try there's designs for roll ups that will try to put blocks and whatever is available, but they're all going to be competing and bidding and it's not going to be as simple as here's one giant pipe. The capacity is priced and the price is based on the remaining capacity of that one giant pipe and because it's one aggregated source of bandwidth, I think it's going to get much, much lower pricing and then speed like performance at the end of the day.
00:32:44.244 - 00:33:07.398, Speaker C: I think at one point I had heard you say that you don't believe that the demand for block space is infinite. Yeah, which is a great point and it's very interesting point with implications for how you would architect the blockchain. So I'm curious, what do you think the demand for blockchain is for block space is at the limit or at equilibrium once we have billions of people on web three.
00:33:07.564 - 00:33:59.580, Speaker A: Imagine if at Qualcomm the engineers were told the demand for cellular bandwidth is infinite code designed for infinity. That would be absurd, right? There's like a target that people think is reasonable and it's like one gigabit or ten gigabit and that's 100 times more than people are currently using. And if you can design for that demand, you don't have to think about every arbitrary long tail design choice. You can actually look at how much hardware you need and decide do I need to Shard or not? And what's the simplest implementation and how fast is it going to the pricing for deploying it is going to get to be so low that it's negligible. My gut feeling is that there is some limit for 99.99 most valuable transactions will probably fit within the first 100,000 TPS. That's my gut guess.
00:33:59.580 - 00:34:28.466, Speaker A: And 100,000 TPS system is actually fairly doable like today's hardware can do it. Salana Soft architecture can do it. And you can see that from what Fire Dancer folks are posting. The Labs client right now I think would struggle. It would require an elab experiment maybe to prove it, but I think 100,000 TPS my guess is all you need for probably the next 20 years of blockchain space.
00:34:28.568 - 00:34:47.142, Speaker C: Well, there is this effect of induced demand. It is very possible that if block space is so affordable and so trivially easy to use, that the demand for block space will skyrocket just because people will want to use it for all sorts of things that are maybe not the 100,000 most valuable transactions, but are still transactions that people would want.
00:34:47.276 - 00:35:08.058, Speaker A: There's still some floor price, right? The price per transaction has to cover the bandwidth costs to every validator. Like egress costs are going to dominate validator costs. If you have 10,000 nodes, you roughly need to price the per byte usage of the network to be like 10,000 times x normal egress costs.
00:35:08.154 - 00:35:21.566, Speaker C: That sounds expensive unless you Shard, right? So I guess it would be the question. Do you imagine that at some point solana hits a limit and becomes a Sharded architecture down the line or do you think that the monolithic architecture is enough indefinitely?
00:35:21.758 - 00:36:23.090, Speaker A: The thing that so far people have done with Sharding is that they build systems that are much lower bandwidth than solana and sharded those. So they hit those kind of capacity limits and they started to getting into biding wars to access the bandwidth and that greatly exceeds the egress cost already. And at 10,000 nodes of egress, right, if we purely priced egress, I think the last time I checked it's like one dollars per terabyte of egress to every validator in salana it comes out to even with forex overhead something like four times ten to the minus six per transaction. There is a floor price, you can't use it to stream video but the price is so low that you can use it for search. You could basically have every search hit the chain and then get the results from your search engines as replies unchained.
00:36:23.170 - 00:37:27.994, Speaker D: I think this is actually an interesting point because we start the podcast with the question of what is the end game for blockchain scalability implying that blockchain scalability is the most important problem. And I think Chris has used this analogy before, the idea that a lot of the advances in AI and ML over the last decade can be attributed purely to much better hardware and that that was really what unlocked a lot of this. And I think occasionally we talk about blockchain scalability as serving the same purpose that if we can just get to 10,000 10,0000 a million TPS, finally everything will work. But an interesting counterpoint to that is today Ethereum does twelve transactions per second and Ethereum itself still does more throughput than any of the individual L two S at fairly high prices, usually dollar transaction fees. While on Salana a lot of at least the simple transfer transaction fees are fractions of a penny. And when we talk about this, usually it's under the guise of if we get the next order to magnitude of throughput there will be a lot of new applications that we can't reason about or can't think about today. And to some extent Solana has for the last couple of years been the place where you would be seeing those applications be built.
00:37:27.994 - 00:37:47.806, Speaker D: And yet a lot of the things built on Solana look very similar to a lot of the things built on Ethereum. So I'm curious your thoughts whether greater increases in throughput or reduced latency do unlock new applications that we can't reason about today or whether most of the things we'll want to build on a blockchain over the next ten years will be pretty similar to the designs we've already proposed.
00:37:47.918 - 00:38:43.182, Speaker A: I actually think that most of the things would be pretty similar. I could be surprised. I think every use case that have hit any kind of traction has been like a marketplace or an asset like an NFT or a marketplace for NFT. And what's more interesting is how do you build business models around those kinds of properties in web three where users own the data that users actually get back as part of participating in those networks and use that somehow outcompete and growth like the big tech companies, this is the harder nut to crack. Like how do you apply these new tools? But I think the tools have basically already been discovered. And the reason why Ethereum transactions are so expensive is because the state is so valuable. Like Ethereum represents very valuable state in its chain and the markets have liquidity.
00:38:43.182 - 00:39:21.178, Speaker A: So then when you have that state available and anyone can write to it, they will bid up up to the economic opportunity costs for them to be first to write to that state and that'll effectively spike the fees and that's what creates the valuable kind of transaction fees on Ethereum. For that to happen on Solana, applications need to create that kind of valuable state like whether it's NFTs or whatever opportunities exist in those bits that are now for sale. For anybody to write to, that needs to happen often enough to where hotspots occur and people start bidding up the fees.
00:39:21.274 - 00:40:38.982, Speaker C: I'll give a counterpoint here. I think it's very easy to underestimate the creativity of a whole planet filled with creative developers and entrepreneurs. And actually just historically, if you look at for example, the first wave of the web and the internet, like starting in the 90s, it took quite a long time in order for the main sort of drivers of interesting applications and value to really be developed. And in the case of something like Crypto, we've only really had programmable blockchains since Ethereum starting in around 2014. And something like Solana has only really existed for four years or something like since 2019 or whenever Solana initially launched, which is not really all that much time for people to fully explore the design space and then separate kind of related and important fact is that the number of developers in the space is extremely small. Still, we've got probably tens of thousands of developers who know how to write smart contracts and who really understand the promise of blockchains as computers. So I would actually bet that we are very early on in the development of interesting ideas for what it is that you can do on top of a blockchain and the fact that you have this general purpose computer that can now support programs that make commitments about how they behave in the future.
00:40:38.982 - 00:41:39.014, Speaker C: The design space that creates is so broad that I will bet that we will be very surprised for what kinds of things people build. And they likely won't just be things that have to do with trading or marketplaces or finance. It could be that they take the shape of things like shared data structures that are very valuable but play a role that's not financial in nature. So a good example of this could be a decentralized social network where the social graph is on chain as a public good which enables all sorts of other entrepreneurs and people building technology to build on top of it. And that graph, because it's on a blockchain and because it's open and universally accessible to all of those developers, becomes a very valuable piece of state that a blockchain maintains. And in that world you could imagine people wanting to issue a very large number of transactions for any sort of reason, like to update this data structure in real time. And if those transactions are cheap enough, I imagine developers will figure out a way to leverage them in the way that they have historically.
00:41:39.014 - 00:42:03.042, Speaker C: Like whenever computers get faster, developers figure out a way to use that additional computational capacity for some improvement to their application. It's never the case that we have enough computational power. People always want more. And I think the same will happen with blockchain computers as well and there will be no limit. Maybe it's not infinite, but I think that the ceiling on demand for block space must be much, much higher than what we imagine it is.
00:42:03.176 - 00:42:16.454, Speaker A: The counterpoint is that the internet use cases were actually discovered pretty early on. Search and even social graphs and ecommerce were like pretty early on. I think in the 90s there were.
00:42:16.492 - 00:42:53.938, Speaker C: Things like, for example, ride sharing was something that would have been very difficult to predict. The form that search actually ended up taking would have been very difficult to predict. Social networking broadly, streaming of videos and things like that would have also been inconceivable at the beginning and I think same here. We can think of some of the applications that people might build on top of a blockchain, but some of them feel impossible given the constraints, the infrastructure constraints today. But we could run through a list of the likely things that will likely become very big down the line once those constraints are lifted and once more people come into the space to build them. So I think it's actually quite analogous and we might just be surprised by how big it all gets if we just let it run.
00:42:54.104 - 00:43:37.586, Speaker D: There's an interesting card game called Dot Bomb that actually Chris mentioned like five years ago. He ended up sending me a picture of it after the first time we met. But the point dot Bomb is a card game from 2004 where the goal is to lose money as slow as possible. You can't actually win or ever make money. You're running a bunch of different startups with internet ideas from the without exception, every idea that was supposedly a terrible idea, including magic internet money, online grocery delivery, and online pet stores, became at least a billion dollar business sometime after 2010. And so I do think a lot of these ideas that start out potentially being very bad or failing in their initial implementation do come around and become very well adopted in the future.
00:43:37.688 - 00:43:55.430, Speaker A: It's very interesting that all those ideas actually caught a bit of fire in the 90s, that they were all plausible. People could all imagine them and that's actually a pretty decent signal that people want that problem to be solved, but they're willing to pay money for it. Just the info wasn't there.
00:43:55.500 - 00:44:13.686, Speaker D: So the question then is, if it's not scalability, that's the blocker. Is it just cultural acceptance of blockchains? Is it privacy? Is it user experience? What do you think? Anatoly is the big unlock that will take adoption from where it is today to internet level mainstream.
00:44:13.798 - 00:44:50.534, Speaker A: This is kind of colored by my experience of the internet itself. I remember being on the web and going in bulletin boards and all this other stuff. And there was like this shift where at least I went to college and got an email address and everybody working had an email address and they would start receiving links of memes and funny things. And they were constantly online and then they were constantly on the web. And you saw Web UX got a little better. Actually, hotmail started working and Facebook took off. And because of that, people had a mental shift where they understood what the web was.
00:44:50.534 - 00:45:29.640, Speaker A: I think initially it's just very hard for people to even grok what a URL bar is, what a URL is, what does it mean to even click on something and go to a server? Like, I had to teach my parents that. And we have the same issue with self custody. And for people to actually understand what those keys mean, what that seed phrase means, what does it mean to own a wallet and transact with it? There's a mental shift that needs to happen, and it's slowly happening, I think, with everyone that eventually buys crypto and deposits it on their own self custody wallet. They get it once they have that experience. But how many people have had that experience so far? It's not a very large number.
00:45:30.010 - 00:45:37.334, Speaker D: On that note, you guys built a phone. Perhaps you can tell us the inspiration for building a phone and how you think the rollout has gone so far.
00:45:37.452 - 00:46:16.594, Speaker A: Yeah, well, because of my experience at Qualcomm, I knew it was a very kind of constrained problem that we could solve, and it was going to not take the entire company to pivot into phone making business. It was literally like, we need to hire these five people. I'm very close friends with like, two of them. They've worked at Google and Qualcomm. They know exactly how to go do this, and we can see what happens. So it was an opportunity that had low marginal cost to us as a company, and that seemed like a big swing that could potentially change the shape of crypto or like the mobile industry and see what happens. So it seemed like a worthwhile thing to take.
00:46:16.594 - 00:46:57.406, Speaker A: We built a device with an OEM partner. They were planning to release a high end phone, and we worked with them to add crypto specific features in it. And we had awesome reviews from folks and developers find it as an alternative app store as a pretty compelling thing. We're still not sure whether there is again macro conditions. Plus is there an application in crypto that is so compelling that people are willing to switch from iOS to Android? Some people are, but not like massive numbers of people. It's very, very hard to launch a device. Basically every device that has been launched outside of Samsung or Apple have failed.
00:46:57.406 - 00:47:38.510, Speaker A: People would be surprised to know, I don't think the Pixel sells even a million devices a year. And that's Google, right? It's very hard to compete and the reason for that is because the pipelines have been so optimized or manufacturing those devices. About Samsung and Apple is that any new startup that tries to build a device, they're going to be late on the hardware by the time they launch because both Samsung and Apple will do a recognition and how quickly they can do it. It's just very hard to compete. So you need to have something that is like a religious reason for people to switch. Maybe crypto is that reason. We haven't proven that out yet, but we haven't disproven it yet either.
00:47:38.510 - 00:48:02.840, Speaker A: We still haven't seen that breakout use case where self custody is the key feature that people need and they're willing to change their behaviors. But what's cool is the tech is there, there's other OEMs that can go enable it. And I think we started seeing at least Google change their NFT policies after soccer is announced. So if we had anything to do with that decision, I'll take 100% credit.
00:48:04.170 - 00:48:17.674, Speaker D: You're one of the few founders who's actually both built hardware and built a decentralized network. Building a decentralized protocol or network is often compared to building hardware because of how complicated it can be. Do you think this analogy holds and makes sense? What is the same? What is different?
00:48:17.792 - 00:48:59.014, Speaker A: Hardware is harder. We're still playing on easy mode, just everything goes wrong. We've had issues like Qualcomm where a tape out that goes bad costs the company like tens of millions of dollars per day. It could be very catastrophic. You have bugs that break subsystems that you discover six months after everything's done and you have to go jump through crazy hoops and software to fix them. It's much, much harder. I think just the stakes are much higher at a big ship company than something like software where you can still quickly go discover you can do 24 hours.
00:48:59.014 - 00:49:01.926, Speaker A: Patch is still doable in software which makes it a lot easier.
00:49:02.038 - 00:49:11.062, Speaker D: It's nice to hear it's not as hard as we think. I'm curious whether it's faster to change salana or faster to change a core part of Qualcomm architecture.
00:49:11.126 - 00:49:12.798, Speaker A: Faster to change salana for sure.
00:49:12.884 - 00:49:30.094, Speaker D: Okay, we were talking about how to bring the next million billion developers into crypto. Solana has done an excellent job, I think, of building its own community and has a really strong one. I'm curious some of the approaches that you guys took as you were thinking about not just building the company, but the ecosystem.
00:49:30.222 - 00:50:17.774, Speaker A: So there was, let's say, a bit of luck involved there. We started Salana Labs in 2018 and this was at the tail end of the previous cycle. And a lot of our competitors actually raised multiples more than we did. And we had a small team that was a bunch of ex Balkan folks that worked for almost a decade with and we just didn't have the funds to go build an optimized EVM. So we built a runtime that we thought could demonstrate this key feature, that there's such a thing as a very scalable blockchain that's not limited number of nodes or bandwidth or latency. We really wanted to hit all three of those. And if we were to also try to go use EVM, if we used Vanilla Gap, it would have been very slow and all our improvements would not have been even noticeable.
00:50:17.774 - 00:51:12.398, Speaker A: You'd have to spend a lot of engineering time rewriting EVM and making it work. So we focused on building this very fast network and nothing else. And when we launched, we had a very bare bones Explorer command line wallet, but the network was fast. And this is what the hook was for the developers because there weren't any other alternative fast cheap networks at the time, none that were programmable, none that could give you that kind of speed and latency and throughput. And the fact that it was rust actually ended up being kind of the score thing that allowed a developer community to flourish because a lot of folks couldn't copy and paste solidity code. They had to build everything from scratch. And that building process is actually like how you get onboarded as an engineer.
00:51:12.398 - 00:52:15.142, Speaker A: If you can build the primitives that you're used to in stack A and stack B, you kind of learn Stack B inside out. And if you start appreciating the trade offs and you become its champion, right? You can then go be the religious salad that says this particular stack is better than this other one. Writing those programs is like writing a kernel driver. The spec was very similar and that was something that I didn't anticipate and probably would have made the mistake of trying to build EVM compatibility had we had more funding. The fact that we had limited runway, limited engineering time, that was the forcing function that forced us to prioritize only for the key important thing, which was performance of this one state machine. My intuition was that if we can unblock developers with that one thing, give them a very fast, cheap network that they can't get anywhere else, they can unblock themselves, they can build all the stuff that were missing over the weekend or whatever. And that actually happened.
00:52:15.142 - 00:52:46.882, Speaker A: And that was again surprising and awesome that it did. I'm not sure if it would have happened if the timing like the macro environment wasn't right. This was 2020 is when we launched, literally. We announced March twelveTH, then March 16, the stock market and crypto markets collapsed by 70% and then three days later we were alive. I'm so glad that we were alive three days after that collapse and not three days before. I think that timing those three days might have saved us.
00:52:47.016 - 00:52:51.010, Speaker D: That's a very hopeful description of building through a crypto winter.
00:52:51.590 - 00:52:55.406, Speaker C: Another important factor here is how we win the developers.
00:52:55.518 - 00:53:33.018, Speaker A: So this is something that what's kind of counterintuitive to understand is that you have to go chew glass to build your first Solana program and that requires people to actually put in the time. And we call the chewing glass devs that overcome the chewing glass part. Not all of them, but enough. Once enough of them do, they will build the libraries and tools that kind of make it easier for the next developer. And that is actually kind of a work of pride for those developers to do that. It becomes a library that they share or something like that. It's just a natural thing that engineers do in open source ecosystems.
00:53:33.018 - 00:54:18.874, Speaker A: So software naturally scales. I think this is something that we really wanted to let the developer community go and build and kind of chew glass and turn it into something useful for other devs without us being super opinionated about it, because that really allows those folks to own it and really makes them feel like they have true ownership of the ecosystem. So we try to work on problems that they couldn't solve over the weekend, which are like these long protocol problems. I think that's kind of like where that ethos comes from. You're willing to chew glass because you're getting something in return out of it, you get ownership of the ecosystem and we are able to focus on making the protocol cheaper and faster and more reliable.
00:54:19.002 - 00:55:09.598, Speaker C: I'm curious what your thoughts are on the developer experience and the role that programming languages play down the line as the space gains more mainstream adoption. Part of it is that it's quite difficult to become onboarded onto the space and to learn how to use the tools and to learn how to think in this new paradigm. Programming languages might have a big role to play on that front because smart contract security has become such an important part of what an engineer in the space has to do and the stakes are so big that it's just a very high barrier for anyone to cross. And ideally we will end up in a world where the programming language helps you much more than it currently does through tools like maybe formal verification, but just maybe more help from the compiler, more help from automated tools that give you a sense for whether your code has any chance of being correct and bug free.
00:55:09.684 - 00:56:16.470, Speaker A: Yeah, formal verification is, I think, going to be a must for any kind of DeFi application. And this is where I think a lot of the innovation happens is in the execution side of DeFi, like building new marketplaces and those are the things that are under the most threat of hacks and those are the things that really need formal verification and tools like that and obviously audits and all these other things. I think there's a lot of other applications that kind of quickly converge on a single known implementation that becomes kind of trusted through lindy effects. Once you kind of establish a single kind of canonical thing for a class of problems that's actually easier than a new startup that's building a new DeFi protocol that has to go take on a bunch of implementation risk because no one has coded that before, and then get people to trust that code and put money at risk inside that protocol. And that's where you need all the tools. Like, I don't know, LLMs that can do unit testing and Fuzzing I think could be really helpful. Formal verification for sure.
00:56:16.470 - 00:56:34.074, Speaker A: Better compilers like Move I think is a pretty good language that's been developed by the Ex Libre folks. But there's also like Rust itself. There's two different brewers that folks have been working on, like Connie and Prousti, that are getting better and better with every year.
00:56:34.192 - 00:57:27.274, Speaker C: I completely agree with that. It feels to me like the world of programming is changing in a very interesting way because it used to be that most programming was just the traditional kind of imperative programming, that people do the things that are very JavaScript like and are very web two. And the model is you write some code and it's likely incorrect and it breaks and then you fix it and it's fine because the stakes are low. But increasingly there are more and more applications that are mission critical. And for those applications you need a very different programming model, one that gives you greater guarantees that the code that you have written is actually correct. But then on the other side it's also interesting because there's another kind of programming that's emerging which is machine learning, like using data to synthesize programs that then do interesting things, especially when the stakes are not nearly as high. And both of these two things are kind of eating the original form of imperative programming from either side.
00:57:27.274 - 00:57:59.474, Speaker C: And there will be less and less just normal vanilla JavaScript code in the world. There will be more code that's written by machine learning algorithms from data, and then there will be more code that's written through much more formal techniques that look more like math and formal verification for the things that are more mission and life critical. And crypto very much falls in the latter category, but it may also benefit from machine learning tools to be able to sort of find bugs and be able to debug or sort of get to the point. Where you have a better ability to determine whether a program has any vulnerabilities.
00:57:59.602 - 00:58:30.494, Speaker A: Yeah, I mean, I could even imagine at some point prover optimized languages like Cock or Acta or Idris being the language that you write your smart contracts in, and then you tell an LLM, go translate this to Solidity or Solana anchor, and then you run the prover against the implementation. That could be very powerful and seems now feasible. Like, I probably wouldn't have believed it two years ago, but there was very much a step function that happened with GPT Four.
00:58:30.612 - 00:59:06.506, Speaker C: I love that idea. You can use an LLM to generate the spec for a program that satisfies some formal verification tools requirements. Then you can ask the same LLM to generate the program itself. And then you can run the formal verification tool on the spec and the program and see if it actually satisfies the spec. And if it doesn't, it'll give you an error, which you can feed back into the LLM and tell it to try again. And you can do this recursively until eventually the LLM, with the help of a formal verification tool, generates what's? Hopefully a verifiable formally verified program. And you can describe the program in English, which is amazing.
00:59:06.608 - 00:59:12.758, Speaker A: Yeah, hopefully the complexity of the spec isn't so large that a human can't understand that's.
00:59:12.774 - 00:59:17.610, Speaker C: Right, exactly. Because then the spec might have bugs. That's the issue with formal verification.
00:59:17.770 - 00:59:48.258, Speaker D: Yeah, we're talking about how to build a strong ecosystem in public. But one of the other things that at least to me was remarkable at the time a lot of blockchains would be launched and almost immediately decentralized to the extent that the core team stopped participating in discussions on forums or stopped trying to help other partners participate. And you guys seem to take almost from the network launch BD and go to market very seriously. And I think that was probably a pretty big advantage in building the Solana ecosystem.
00:59:48.354 - 01:00:29.794, Speaker A: This is like a biology quote. So he said that decentralization is not the absence of leadership, it's the abundance of leadership. I was like a Linux nerd in high school, so I remember how hard it was to get Linux taken seriously in a qualcomm at a big company that actually even proposed. The idea of running Linux on a mobile ship seemed ludicrous when I first joined. And it took a very large community wide effort to go convince and show that open source was meaningful. And I think that's something that I felt that we needed to do as well. That the network needs to be decentralized, but that doesn't mean it's leaderless.
01:00:29.794 - 01:01:36.506, Speaker A: You actually need a lot of experts that can continuously tell people about the benefits of using this specific network and its architecture and continuously get more people on board and create more of those leaders that can be evangelists across the entire world. But that doesn't mean that everything happens under one roof. If the network is permissionless and the code is open and anyone can contribute it and run with it, it should naturally actually decentralize, you should naturally see leadership spring up out of places that you never expected and that we saw happen as folks jumped into our hackathon. And maybe they were the team that figured out how to build an object framework like Anchor and that became like Coral and Armani's team that's building wallets and NFTs and all this other thing. And he's as much of an influence on Solana right now as I am. Probably people might listen to him more than me, unlike a lot of the UX things for sure. So that kind of effort is just something that we didn't plan for but we didn't plan to ever stop.
01:01:36.506 - 01:02:46.398, Speaker A: The goal for us is just to grow everything else around us that our voice becomes just one of many versus like we need to become quiet and exit or something like that. One of the criticisms that we used to get is that we don't do enough PD and there's kind of this trend in crypto that when you bootstrap an ecosystem, you give a bunch of grants to big branded companies to go build products and stuff. And that sometimes works, but it's very rare. And internally we've almost had the belief that we should be actually getting paid. If we work with a big company, they should pay us at least cost to go build an integration for them because that's at least a signal that somebody internally is desperate to solve a problem for that company, right? And they want to go to market as fast as they can and they're willing to pay us to do that. And that means that there's a very strong internal driver to launch something at that big company and get to PMF or product market Fit. And this is why I think Solana probably has done a lot fewer big brand announcements or big brand partnerships.
01:02:46.398 - 01:04:06.598, Speaker A: Instead, we've massively focused on hackathons like regional hacker houses and ground like ecosystem building, trying to connect devs between each other, trying to get them to participate into the Swoop where maybe their entry point is at hacker house at a city and they meet some other devs that have raised capital on Solana. And that's been like the flywheel. We try to get people connected around the world and connected to developers one on one as much as we can and then get them all into a hackathon. They all compete and that competition drives them to go build their first or maybe their second product in their second hackathon and then get noticed and actually make a pitch deck go from an idea that they pitch in a hackathon to this is a fundable company and go through that whole process. And that's where a lot of the traditional BD folks at foundation or Labs have been focused on, is like that cycle trying to get as many companies started in the ecosystem because there's very few products that are onboarding venture scalable numbers of users right now in crypto. In my mind, that means that we just don't have enough ideas. We don't have enough founders that are taking shots on goal and figuring out what are the business models that are actually scalable to millions of users.
01:04:06.598 - 01:04:34.862, Speaker A: So we need to work in the top of the funnel. You need 20 different startups. I think maybe one to two of them will figure out something with PMF and that just gets them into that next stage. And that stage is just as brutal. You need 20 of those people in the next stage before one of them breaks out. And to be honest, I don't think in crypto we've seen anyone break out of that next stage yet. And that's been, I think, the biggest challenge.
01:04:35.006 - 01:05:07.514, Speaker C: A related question here is how do you engage the community to develop parts of the core protocol itself? That's like one of the trickiest balances to strike for any blockchain ecosystem. Like on the one hand, you can engage the community actively, but then you maybe have less flexibility and the process of governance involves many more people and coordinating is hard. And then on the other hand, you could control things more in a more top down way and evolve more quickly as a result but then sort of suffer in your community engagement. And we're curious how you strike that balance.
01:05:07.642 - 01:05:55.982, Speaker A: So generally the way that I've seen it work, I'm not at the foundation anymore, but while I was there, well, we saw people that were already actively contributing into something that they wanted to work on and then they go through a proposal process. There's like an RFP and there's a grant or whatever attached to that and that works out because you got to find engineers that are capable. It's very similar to an interview process. Like when I hire somebody at Labs, if they cannot push PRS into the code base within 90 days, some kind of mismatch happened, right? It could be that our culture doesn't align well with that person or whatever. Doesn't mean that that person's bad. It's just something's not working. And you really want to make sure that those events are few and far in between.
01:05:55.982 - 01:06:28.002, Speaker A: And the grants process kind of runs the same way. It's like you find engineers that are already committing and contributing to the code base. They already know how culturally to get their code merged and how to deal with the open source direction of an open source repo. Grind. It's a grind. How does Linus manage all those committers? Somebody has to do that job and get stuff and put it together. It works well when you find people that are able to figure it out on their own and then you can give them grants or whatever to take on bigger projects.
01:06:28.002 - 01:07:07.326, Speaker A: And those grants are important because a lot of these problems, they're like L six, L seven engineer problems. You need like one or two really smart people that are willing to work on this for over a year and it takes them six months to get up to speed. And generally those folks don't want to move jobs unless they have a three to five year kind of line of sight on a commitment to work in the same project. So those grants kind of all have to account for that. You find really good people, people that can commit code and are willing to work for three to five years and then this one hard thing and it's holy.
01:07:07.358 - 01:07:11.478, Speaker D: What do you think is the best way to run decentralized governance for a protocol today?
01:07:11.564 - 01:07:41.786, Speaker A: For like a layer one? The approach that we took that seems to work is very much like Linux. It's kind of moving forward by avoiding vetoes from any participants. It's taking the path of least vetoes. And honestly, there's a lot of participants that can veto any kind of change. It's validators. It's partners like Circle or Coinbase that's just running nodes that can say, hey, this change is bad. We can't run it for X, Y and Z reasons.
01:07:41.786 - 01:08:22.694, Speaker A: And that approach actually works really well because the core function of protocol development is to improve the protocol. And that's a very almost boring thing. We got to make the system faster, more reliable, use less ram. No one's going to object to any of those changes. There's not any really major controversial changes outside of maybe like the fee market mechanics and things like that. And when those happen, ideally we have a process called the SIMD Process solana Improvement Documents. You post these designs and then people talk about it and there's tests and you run data and eventually gets into Master and that's three months out before it actually gets to mainnet.
01:08:22.694 - 01:09:21.486, Speaker A: So there's a lot of opportunities for everyone to go look at this code and decide whether it's a good or bad idea before it even gets merged. It's kind of a slow process, but it's actually not that slow. If you've ever worked at a giant company, basically fairly similar to how you'd get like a big architectural software change into at a place like Google or Qualcomm, you got to talk to a lot of people. You got to pitch it, you got to communicate it, make sure all the key partner, key people that are touching that code base are comfortable with it and then slowly kind of get it through. It's harder to make big sweeping changes. I think that's the reason why proof of work to POS merge took a very long time in Ethereum is that those changes are even if everyone agrees on them, you just kind of need to bake them. It's like baking time for any because a lot of smart people all looking at the same thing may actually realize something.
01:09:21.486 - 01:09:37.202, Speaker A: There could be a bug or some design, like something that you missed three months later, like after everyone agreed. And you need time for those concerns to prop up and actually go through those arguments three, four or five times before everything actually finally crosses the line.
01:09:37.336 - 01:09:44.338, Speaker D: Some tactical advice on how to hire contributors for the Labs entity would be great. How do you think about hiring people to work internally?
01:09:44.434 - 01:10:18.294, Speaker A: Yeah, engineering wise, we tend to have a pretty high bar. At least we hire fairly senior people. And this is what I would recommend a lot of startups do. The way that I hired early on is I would be working on something and I would know exactly how to do it, and I would hand it off to the new hire and tell them, this is how I would do it. And I would expect him to either finish it within 90 days or surpass me, like, tell me, okay, you're wrong. You should actually be doing x y and Z. And the reason why I did that is because I could actually evaluate them during the interview.
01:10:18.294 - 01:10:48.762, Speaker A: Like, I could test them, hey, this is the problem I'm working on. I need somebody to take it over so then I can go do the unknown thing. And it's very hard to do that in a startup. If you're a CEO and you give somebody an unknown problem, you don't know if they can back, hey, I couldn't do this or not. Whether they're a good hire or not. We have a devrel function and we added it after kind of the ecosystem grew a bit. Think of it as a PM for the ecosystem and you need the ecosystem to grow a bit before you actually have that role to fill.
01:10:48.762 - 01:11:14.142, Speaker A: And we hired one when I was spending too much time answering questions and being on discord till two in the morning helping unblock devs is when I was like, okay, somebody else could be doing this. I know what this job is now. So I always thought of hiring in those early days as scaling specifically myself, which is kind of a very selfish thing to think of. But it's weirdly, like, I think the right thing to do for a CEO.
01:11:14.286 - 01:11:54.682, Speaker D: I think there's an interesting question. When you're building a community, you can build a community around an idea, but a lot of communities form around a person or a set of people. And you were talking earlier about how decentralization is not a lack of leadership, but an abundance of leadership. And at least from what seems to have happened over the last decade in crypto, every successful project has a messiah or a prophet, so to speak, who is spreading the good word. And especially the big ecosystems have at least one of these people. I'm curious the direction of causality here, whether you think a successful project makes the original contributors profits or whether the original prophets and messiahs make the project successful.
01:11:54.826 - 01:12:32.214, Speaker A: I think it's the successful project forces itself onto somebody that doesn't fall apart under the pressure. That's what happens. It's very hard, I think, to create that artificially. You need to have something that is catching some PMF, some fire, and everyone's pouring in. I happened to be in the place that I needed to organize all these folks and contributors and tell them what to do, right? Just at least suggest what they should be working on. That's how it starts, and I think it's the idea that drives it. And then eventually an engineer can contribute.
01:12:32.214 - 01:12:46.802, Speaker A: Maybe like, I'm a really good principal engineer or whatever, and I can drive big projects, but I quickly became just one of many, and my contribution as a single engineer becomes smaller and smaller, marginally less important.
01:12:46.936 - 01:13:16.858, Speaker D: So maybe it's not the person, but it's the idea of the person. And satoshi epitomizes this. Well, that even more than a decade after they stop actively contributing to bitcoin, people still revere satoshi as almost a god like figure. And I do think this is interesting, especially relative to the fact that with the exception of satoshi, almost all project founders are actually just one person. And docs and I'm interested in your sort of experience going through this.
01:13:16.944 - 01:13:35.310, Speaker A: I think it's actually becoming less so as time goes on. There's kind of a spike in that happening, and it wasn't something that I was used to, but as people got to know me and started contributing to the code base and everything else, and it kind of started growing on its own, it kind of shrunk to normal levels.
01:13:35.390 - 01:14:45.046, Speaker C: I think you're right that it decreases over time. But I also think that you're giving yourself far less credit for galvanizing a movement from the very beginning, just like coming up with the idea of proof of history and then evangelizing that idea. Catalyzing activity from the initial group of developers getting funding initially to start building all of this, spreading the narrative, making Solana a publicly known idea for how it is that you can build a decentralized blockchain and ecosystem. I think all of that is essential in the early days and sort of you and vitalik and other kind of messiahs or prophets for blockchain ecosystems actually play a role. Satoshi played that for bitcoin in the early days, and I honestly think that bitcoin is probably the exception. And it can only really work in that way because it was the only game in town, it was the first, and nothing else in the space really existed. And now I think in a world where the space is much bigger and there are many other competing projects that draw people's attention, I think that the leadership of a set of visionaries at the very start is important.
01:14:45.046 - 01:14:54.310, Speaker C: But of course, as the ecosystem becomes more decentralized, that does become less of a factor. And I think then the ideas themselves are what drive the activity.
01:14:54.390 - 01:15:01.670, Speaker D: Maybe the answer is you just have to be very humble to be a messiah and anyone who would stand up and say it's because of me is disqualifying themselves.
01:15:01.760 - 01:15:02.350, Speaker C: Exactly.
01:15:02.500 - 01:15:05.230, Speaker D: So we'll move to a bit of a lightning round now.
01:15:05.300 - 01:15:05.850, Speaker A: Totally.
01:15:05.930 - 01:15:10.942, Speaker C: How important do you think privacy will be for blockchains in the future?
01:15:11.076 - 01:15:50.758, Speaker A: I think there will be kind of like a shift. Like we had the best SL where it was a few forward thinking people using it and then all of a sudden I think a big payments company or whoever will adopt it, it'll become the standard. I think there'll be a shift, it's just I'm not sure when it'll take a breakout moment. I think you need it to become a feature that if you don't have it, you're just not able to compete. We're just not quite at that level of maturity in the market but I think we will get there once we get like 2030 40 million people all using blockchains on a daily basis. Every merchant in the world needs PNL privacy. That's just like the bare minimum.
01:15:50.854 - 01:15:53.978, Speaker C: Will we get privacy through cryptography or through trusted hardware?
01:15:54.074 - 01:16:15.300, Speaker A: I think cryptography I think cryptography is fast enough and good enough we don't need already. We had one of the been lucky to hire one of Dan Bonet's students and he's built bulletproofs that are already fast enough and scalable enough for us to handle privacy for the world for payments. Absolutely. Cryptography is good enough already.
01:16:15.850 - 01:16:24.978, Speaker C: Next lightning round question. What are the implications of a Solana style architecture for mev? Does the leader have too much power to reorder transactions?
01:16:25.074 - 01:17:14.066, Speaker A: This is the big question. So the way that we originally designed this was that you can actually schedule more than one leader per slot. And the theory there is that if we shrink the slots to be as close to speed of light around the world as possible, which is like roughly 120 milliseconds, and you have multiple of these, then you end up with something that is like a discrete batch time auction that's happening every 120 milliseconds around the world. And the user can pick the block producer out of the ones that are all available. Either one that's geographically closest or the one that is offering the best rebate. And we believe that that's going to be basically the best you can do. Like theoretically that's probably the closest, most efficient way for finance to run it's.
01:17:14.066 - 01:17:47.330, Speaker A: Either I pick latency and I send to the closest block producer or I pick the best rebate because I'm doing that latency dollar trade off. That was the theory. We haven't tested multiple leaders per slot yet, but we're getting close to where I think that might be doable, maybe next year and then we'll see if that is robust from reliability perspective. But I think once you have it, you really do have a system that basically forces mev competition to be at its optimal to where the amount of extraction is as small as possible.
01:17:47.480 - 01:17:52.062, Speaker C: What is your favorite systems level optimization in the Solana architecture?
01:17:52.206 - 01:19:05.130, Speaker A: Turbine, I would say this was how we propagate blocks. And this was an idea that we had early on. And it was one of the things that we wanted to prove as early as possible to demonstrate that each node in the system that we can scale in the network to very large number of nodes and we can transmit a lot of data. But the amount of egress that each node has to share, like the amount of load egress that each node has to take on, is fixed, that there's a cap at how much egress they have to do. So if you want to think of it from a high level, every leader, when they create a block, they chop it up into shreds and they create erasure codes for those shreds, and then they transmit one shred to a single node and then that node sprays it to the rest of the network. So because it's all intermixed with erasure coding you as somebody that's receiving that data, you have very high reliability guarantees on that data because your number of nodes that are going to propagate that data to you is very large and the probability of 50% of them failing is very small. It basically requires 50% of the network to be down.
01:19:05.130 - 01:19:42.514, Speaker A: And if 50% of the network is down, you're not going to be making progress anyways on anything. So that was a very cool optimization and the way it's designed, it's very low overhead and very performant. And even after we had the Fire Dancer team actually from the ground up, we told them, hey, if you guys find any algorithm or anything better, tell us. We can definitely change the core code to make sure that things scale better. Turbine is pretty close to as efficient as possible, so very happy about that one to conclude.
01:19:42.562 - 01:19:58.614, Speaker D: Anatoli I'm curious how you see crypto adoption evolving over the next ten years in 2033. How do you imagine most users, namely users today who have no idea what a blockchain even is interacting with applications deployed on Salana?
01:19:58.742 - 01:20:59.118, Speaker A: I think what we should see are some breakout applications and payments because there's clear advantages to using crypto like USDC and digital dollars for payments, especially interest bearing dollars and all these other things you can do with programmable monies way, way better than current financial system. I think once know Congress passes a couple bills, I think payments is going to be a breakout use case once we have payments. The other side of it, I think that's also growing are like social apps. So applications that kind of bridge NFTs and social either messaging apps or social graph applications, those are slowly growing. Right now, I feel like they're primed to take off and actually get to real sizable numbers. I think between both of those, you should start seeing use cases with 1020 30 million people. Once you get to that stage, it's actually possible to iterate on what actually people want out of DFI and give them those products.
01:20:59.118 - 01:21:18.418, Speaker A: What's missing for everything else to break out is we need an active user base of 20 30 million people that you can sell products to that will actually use them, not for the token, but for the actual utility of that product. So that's my guess. Payments and then social super apps, maybe something like that.
01:21:18.504 - 01:21:25.542, Speaker C: What advice would you give to builders in the space or builders outside of it who are maybe curious about crypto and Web Three?
01:21:25.676 - 01:22:13.794, Speaker A: Yeah, I would say that this is like the best time to do it. It's when the markets are a little bit like macro suppressed it's before any big cycle, there's not a lot of noise. You can actually go focus on product market fit and anything that you actually discover with regards to product market fit is what's going to accelerate massively when macro turns around. And you want to do that before that happens. If you want to work in AI, I don't think people should be afraid to go start AI companies right now or crypto companies or whatever and go try it and go build those ideas. But I would say that folks should try to build things that are bigger ideas instead of copying something that exists. The best analogy that I've heard for this is that when people discovered cement, everyone was focusing on building bricks out of cement.
01:22:13.794 - 01:22:31.260, Speaker A: And then one person was like, I can make a skyscraper. And they figured out or combined rebar and the building that nobody could have imagined was possible. So the new tools that we have are basically cement. You got to go figure out what that skyscraper is and go build up.
01:22:31.630 - 01:22:33.494, Speaker C: I honestly think that was a good way to end.
01:22:33.552 - 01:22:44.730, Speaker D: That's a great answer, Anatoli, and it's always inspiring to hear you talk about being a founder. So thank you so much for being with us here today and helping light the way for future founders.
01:22:44.810 - 01:22:45.714, Speaker A: Thank you for having me.
01:22:45.752 - 01:22:46.690, Speaker C: Thank you so much, guys.
01:22:46.760 - 01:22:47.540, Speaker A: Thank you.
01:22:49.670 - 01:23:15.090, Speaker B: Thank you for listening to Web Three. With a Six and Z, you can find show notes with links to resources, books or papers, discussed transcripts and more at asics and Zcrypto.com. This episode was produced and edited by Sonal Toxy. That's me. This episode was technically edited by our audio editor, Seven Morris. Credit also to Moonshot Design for the art. And all thanks to support from Asicsmz Crypto.
01:23:15.090 - 01:23:30.560, Speaker B: To follow more of our work and get updates resources from us and from others, be sure to subscribe to our Web Three weekly newsletter. You can find it on our website at asics and Zcrypto.com. Thank you for listening and for subscribing. Let's go.
