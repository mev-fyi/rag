00:00:06.300 - 00:00:06.850, Speaker A: You.
00:00:08.900 - 00:00:34.920, Speaker B: So welcome, everyone, to this week's a 16 Z crypto research seminar. Very lucky that today we have joining us Patrick O'Gray, who's the head of engineering at Avalabs, which is the sort of organization behind the avalanche blockchain. And Patrick is going to give us an update about kind of all the cool stuff that they're up to over there. So Patrick, the floor is yours.
00:00:36.220 - 00:01:01.596, Speaker A: Thanks, Tim. I appreciate the invitation to come and chat with, you know, I watched a few of these conversations online. I thought they were super. Well, you know, happy to participate. Today I'm going to talk to you about scaling avalanche with subnets. Someone on our marketing, my comms team built this logo and I love the neon feel of it, so I use it everywhere now. But the goal of this conversation is to talk about avalanche.
00:01:01.596 - 00:01:45.132, Speaker A: Like, first of all, avalanche consensus. What is it? How does it work? Why is it a little different than how other people do stuff and then avalanche the network. So how is avalanche the network structured? I think a lot of people may have heard about us subnets. Subnets and how they compare to like zones, pair chains, l two s, like the full l one scaling story. And so talking about what they are and what you can build on them and then talking about where we're going with it. So what the particular architecture allows us to do and how we think that that really positions avalanche well for the multichain world that I think we feel strongly is the next step here. The way I've structured this talk is I have tons of slides of random images, pictures, diagrams and everything like that.
00:01:45.132 - 00:02:09.312, Speaker A: Feel free to interrupt me at any point to ask a clarifying question. I think some of these topics are pretty nuanced, but I think this room is know well researched on these things. So please stop me if you're confused by anything that I say. So yeah. First off, I'm Patrick O'Grady. I'm the head of engineering here at Avalabs. I've gotten a lot of awesome experience about how people think about blockchain, use blockchain, develop on blockchain.
00:02:09.312 - 00:02:57.508, Speaker A: And so I think it's been a lot of fun. I think a lot of it is definitely supported by some of the stuff you guys do with different people in your communities and stuff. Although I may write the tweets for a lot of the stuff we do, I stand a very talented team stand behind me and I just want to give a shout out to them before we get going here. First off, we'll start off with what is Avalanche? The consensus story. So the way you should think about avalanche is it describes two different things. So one is the consensus protocol that came out in 2018, and then another one is the network that's actually built around the consensus, which shares the same name. So I'm not going to go too deep into the other consensus kind of stories within our brief history of consensus research.
00:02:57.508 - 00:03:46.660, Speaker A: I don't need to provide much context there, I think, for this group, but I will dive deeply into the avalanche side of it. So as we think about our brief history of consensus research, you have the quorum based protocols that many projects use for BFT. A lot of stuff, powering Paxos and different sort of distributed systems research coming out in the think that was very commonly used within permissioned networks, particularly for crash fault tolerant systems. Paxos is one of those and then never really used widely in a permissionless or like adversarial context. Nakamoto comes along 2009, longest chain idea. Brilliant bitcoin, awesome stuff. People start to revisit the original voting based classical protocols as people move to this proof of stake transition.
00:03:46.660 - 00:04:50.644, Speaker A: In 2018, a paper called avalanche drops from a group called Team Rocket, and they propose a radically different approach to how consensus should be achieved. Instead of doing, knowing your consensus set, doing like a deterministic process, instead, let's do repeated subsampling of the network to achieve fast and scalable consensus. And so the way that this works, or the big idea is that, let's just say I have a really cool demo that I'll run showing this live, which will be, I think, kind of connect the dots after I explain a bit. But as you know, in a consensus situation, the whole goal is to basically propose is not to pick a particular color, but just ensure all the honest nodes pick the same color. So let's just say we have blue and orange. And so in avalanche, the way that this works is, let's say that you're the red node here. You'll basically subsample some set of k nodes by stake weight.
00:04:50.644 - 00:06:03.232, Speaker A: So in the main net this defaults to 20 or so people. If you get alpha, so that would be on the main net that defaults to 15 of those 20. Agree on a particular color, you then consider that poll or that round of the poll successful. And then if you get beta and I'll show this kind of in writing after this in a row of polls, then you accept. And so the idea is very straightforward where it says, okay, we have this room, let's say of 1000 participants, I want to try and run avalanche consensus so what I'm going to do is I'll pick 20 of random people in this huge room, ask them what they think, if they think 15 of those are the majority of people, or supermajority of people. It's all parameterizable based on who's running the consensus, say the same thing. I can have some notion that the network is starting to tip in a given direction and that if I get so many in a row of that certain majority that I specify in my node, I can be with very high probability say that the rest of the network will eventually agree on that.
00:06:03.232 - 00:06:43.004, Speaker A: And so the magic with avalanche or the breakthrough or the idea is that if you do this, it actually works. I guess it's the crazy idea with the repeated subsampling. And so in some sense you may wonder like, okay, you're doing all these samples, you have to do 15 in a row. That seems like a lot. Why isn't it really slow? Or why is it able to even perform as fast as it does? Typically, finality on the main net is between the order of 500 milliseconds to 1500 milliseconds across 1300 nodes. Well, a few tricks. First of all, you can pipeline these votes together so you can accrue confidence across multiple blocks for a single chain of things.
00:06:43.004 - 00:07:35.100, Speaker A: Second, you can perform multiple queries at once. So let's say if you have 15 polls, you can do like let's say 4567 polls at once. And then as long as you process them in order, you're still guaranteed to have the same properties. And then lastly, when there's no conflicting proposals for blocks or anything like that, which is typically the case, that usually only means, let's say 20 people, you're asking 15 times is only like 300 round trips with no leader. So compared to a traditional or other protocols that may do like an n squared sort of thing, there's no hot path on a particular leader that collects votes. There's no fancy signature step that's needed to be done. So you can have these really fast round trips that are basically just like HTTP requests going across already existing channel between different nodes that's already like a mutually authenticated TLS connection.
00:07:35.100 - 00:08:20.300, Speaker A: So that's just the overview of kind of what the idea is. And now I'll break out into kind of the steps that actually make up this protocol so you can understand, I think, a bit more of how we actually get to this final step. Before I do that, I'm happy to pause if there's any questions and if there's anything that's clarifying there. Before I go into maybe more of the composition of the algorithm, and then I'll show a demonstration of how this actually looks or like, what the intuition is behind it. Cool. So the way that you should think about avalanche is it's actually layers of simpler algorithms stacked up. And what we call this is the snow family of consensus.
00:08:20.300 - 00:09:11.070, Speaker A: So the snow family of consensus is basically broken up into three constituent layers, which is slush, snowflake, snowball, and then avalanche is built on top. So slush is the simplest one. Slush is just this crash fault tolerant algorithm that says, okay, you're running consensus, you're going to default to the first color you hear about as your preference. So someone gossips you a block, and it's, let's say, in the simple case with the coloring, like blue, you'll pull the network for some parameter m time. And then after m time, no matter what, you'll just stop. And so if you get a priority of Alpha K for one direction, you'll switch your color. Very simple.
00:09:11.070 - 00:10:17.088, Speaker A: So this is like the first step, right? We're like, okay, you could probably game this. It's not byzantine fault tolerant, resistant at all. But if the network is working together over some period of m time, you can have some idea that, great, eventually we'll probably align in the same color based on the polling and based on what you hear about, which is with random perturbations in the network, especially if the network starts tilted to one side. Once you have slush, the idea is, okay, well, how can we change this notion of counting or confidence of the node to give slightly more guarantees, or maybe a few more guarantees? And so then that comes with Snowflake. And so before I continue, I'll say that all of these are in the avalanche consensus paper. So if you are curious, like you want to read it more long form, I'll share that link afterwards and you can take a look at. It's a pretty short paper, so if you're curious to learn more, but the idea with snowflake is, let's add a counter that captures the strength of our conviction, basically.
00:10:17.088 - 00:11:25.210, Speaker A: So instead of just saying, okay, if I do another poll of alpha divided by k and it changes my perspective, let's also keep track of how many consecutive polls of alpha divided by k have yielded the same color. And so that instead of after going for m amount of time, instead I say, okay, after I received beta, or what we call beta consecutive, then I'll stop. So it adds this notion of instead of time having some sort of accrued confidence based on the observation, a node's observation of the network. And so still we do the same thing, though, if the color changes. So, like, if I do alpha k and I'm going orange, orange, orange, orange, and then it changes and I see a pole for blue, I'll reset my counter and start over. So I'm still flopping all over the place, but I have a little bit better idea of what it means to actually accept the last week is what we call snowball. Snowball adds confidence to the preference change.
00:11:25.210 - 00:12:46.828, Speaker A: So instead of changing the color, if you get another poll and a different color instead, what you do is you only change your color based on your total accrued confidence. So instead of saying, okay, I performed alpha over k, I saw a different color, I'm going to change right away. But what if I've already seen like 25 poles in a row of orange? Should I be changing my preference to blue? No, you should keep it as orange because it's likely that you got a small perturbation of the network. So this confidence basically adds one more layer to this process. And then upon that, both avalanche and then the linear chain mechanism for avalanche called snowman are built. So now I'll pause for a second, show you a really cool video, and then I'll go over it again because I think you'll understand a little bit more of the intuition behind it. So, avalanche, very simply, let's say you have this group, this big grid of blue and orange, every node running the same protocol here, they all start querying each other and over time, randomly, the whole network will shift either orange or blue just based on the random polling of the network.
00:12:46.828 - 00:13:35.388, Speaker A: And so there's a demo that one of the co founders of the company, Ted Yen, made, which I stare at for hours a day sometimes when I'm just doing these demonstrations because it's so fun. And so you can basically tune any of these parameters to see how the network changes as it pulls. And this is all using snowball. So the last, basically most mature part of that consensus family. So in this case, as a simple demonstration, let's just say that we have 20 squared nodes and then we're doing this repeated subsampling with k ten and alpha eight. So if I do a poll as a particular node, and eight out of ten of those respond the same color, I will basically say, that's my color. And then I'll count that as a successful poll.
00:13:35.388 - 00:14:13.340, Speaker A: If I get beta of those in a row with so let's say 15, then I'll have very high confidence and basically finalize in this demonstration, we don't actually ever finalize. We just have continuously increasing confidence just to kind of show you how this works. And so in this case, I'll slow down the simulation a bit. I think by default, it runs really fast. So you can see, I'll start it off. So on the bottom, you'll see that this is the number of polls that are being performed. And then in the diagram, you can see that as the color gets darker, it's the proportion of the network that has a deep confidence about one of the colors.
00:14:13.340 - 00:15:03.676, Speaker A: So you can see at the beginning, some people shift light blue, and then they may have developed some sort of counter preference in one direction. But then eventually they demonstrate their color as the other, as they see more consecutive poles and generally more just successful polls of the other color. And eventually they come to one conclusion. Now, you can really tweak this and see how it behaves under very different circumstances. Like, oh, does it go faster if you have a larger k? When you start to tweak some of the different parameters, like alpha, you start to change the safety parameters of the algorithm. So let's just say, for example, let's say you were pulling 20. Like, let's say your poll was defined as 20 random people.
00:15:03.676 - 00:16:17.990, Speaker A: And then if you get eleven of them, you say it was successful. Well, you may very quickly, just by random chance, stumble upon a run that will give you maybe finality in the wrong direction. And so you have to think about how to tune these parameters in a way that's safe, but also gets you the performance guarantees you need. And so I was talking to Tim about this right before the presentation started, was we had some research come out last week from Christian Kashin's group about a safety analysis of a lot of these protocols. They actually found a bug in the original avalanche consensus paper, which was a great find and appreciate their research. And the avalanche protocol as implemented was aware of this problem and also addressed it in a different way. But we've been sponsoring more research into this consensus algorithm for suggestions of how to improve its performance or just kind of more formal analysis of its safety guarantees, because it's very different to analyze in some ways than some of the other protocols, which are maybe more deterministic, but it's fun to at least visualize, I can guarantee you that much.
00:16:19.080 - 00:16:26.760, Speaker B: Patrick, quick question. So when the nodes are polling each other, do they only share their color? Do they also share their confidence?
00:16:27.580 - 00:17:12.500, Speaker A: They do not currently share their confidence. They actually don't share their color when asking someone else either. So the way it will work is you basically say, hey, node a, what is your preference, given this chain? And then it will respond back to you with its tip or, like, of that chain itself. But we don't respond with confidence. People have discussed that, but you end up, as you may imagine, into a very interesting and weird byzantine world where it's like, all right, if I'm starting to tweak my confidence based on everyone else's confidence, it's like a whole nother layer of what it means to actually be finalized.
00:17:13.320 - 00:17:17.252, Speaker B: Every byzantine node would be maximally confident all the time when it responds.
00:17:17.316 - 00:18:00.048, Speaker A: Yeah, exactly. Or to some people, it would say, try to keep them as close as possible to 50 50, and say, like, oh, I'm not confident about anything. And then a whole nother group, it would try to convince on different sides in a different color. Like, very interesting games. I think that you could probably use it if you figured out a way to do it systematically to improve maybe the liveness characteristics of the network. But it raises very interesting questions around safety. Also, does every node just pull nodes immediately attached to it, or is it, like any node uniformly advantage in the network gets? So it's stake weighted.
00:18:00.048 - 00:18:42.224, Speaker A: So avalanche itself is a very network heavy protocol, I think, compared to other networks. I describe it as pretty fully connected. Most nodes are connected to other nodes. And so basically, your node ID or your node certificate is like, in a way, a hash of your TLS cert. And so when you make a connection to someone, you can very quickly authenticate who they are. And then when you're doing the polling process based on BaSicaLLY the consensus set, you just randomly sample based on stake weight. So let's say you have 1300 nodes.
00:18:42.224 - 00:19:31.936, Speaker A: I'll sample with replAcement, the top 20. And every node will sample different groups just because they have different random seeds. Yeah, you can keep resetting this, and sometimes it'll go blue, sometimes it'll go Orange, sometimes it'll stay longer, kind of in the middle. And we see all these things on the network. I think the thing to think about and kind of the thing I don't have a slide here for which I wish I added is in the paper, it talks about that the liveness in the worst case is reduced to square root n. When there's square root n adversaries, liveness starts to degrade. And that only is really in the case where you have dueling proposals.
00:19:31.936 - 00:20:39.560, Speaker A: And so in many cases, with the WAy that most of the chains on AValaNChe work, you very rarely actually have blue and orange to vote on. You typically just have blue, in which case it looks more like a crash fault tolerant kind of performance mechanism for liveness rather than Byzantine, because the block itself encodes basically the producer signature. And there's like this kind of allowed or soft producer list for some period of time that makes it really quick, I guess, in a sense. So I think originally a lot of the ideas were fully leaderless. And then what we saw was that under certain conditions, a lot of people tried to game the networking layer for mev or some sort of benefit on chain. And so when we started to look at snowman plus plus, which was the mechanism we used to reduce contention on the network, that dramatically improved consensus performance as well across the network in terms of reducing finality and also reducing the number of rejected.
00:20:42.060 - 00:20:55.244, Speaker B: So Patrick, you alluded to this leader list versus not distinction, so maybe you could say a little bit more about that snowman plus plus, are they kind of like time slots and like a designated black proposer? How does that work?
00:20:55.362 - 00:20:56.972, Speaker A: Yeah, later.
00:20:57.026 - 00:20:58.028, Speaker B: That's fine, too.
00:20:58.194 - 00:21:45.976, Speaker A: Yeah. So I'll cover this now, and then I'll get maybe more into some more of the network architecture, and then I'll make sure to leave some time at the end so that we can dive into all sorts of interesting nuances of both that and the consensus mechanism. When avalanche first came out, it's composed, actually, this is a great segue into explaining some of the different topologies of the network. So when avalanche first came out, it's basically one network with three chains, at least at the heart of it. One of those chains, the x chain, is a dag, which has no leader. So validators produce vertices. Those vertices can be produced by anyone, and they're kind of confirmed independently.
00:21:45.976 - 00:22:27.230, Speaker A: You only have an issue if two vertices both rely on the same utxo or someone is proposing a transaction that double spends. You're not going to guarantee that that's going to finalize. But in the case of leaderless production, that originally also applied fully to the p chain and the c chain, which are linear chains. Right. And so early on, when there was less traffic on the network, it wasn't really an issue. There was no mempool. So people just produced blocks when needed, and then they were gossiped through the network and voted on using the consensus mechanisms I talked about before.
00:22:27.230 - 00:23:17.500, Speaker A: But when the C chain took off and there was a lot more activity. So the C chain is our EVM based blockchain. When that took off, there was a lot of defi activity and a lot of incentive to try and get your block or transaction before others people started to play games on the networking layer to take advantage of that. And so as a result, that caused the rejection rate of blocks to go up and in many cases also increased the time to finality from like the 500 to two second range to maybe like the 1.5 to five second range. And so the solution both to make the network more efficient and to reduce the time to finality was to introduce a soft leadering mechanism. The soft leadering mechanism is an expanding window at each height for who is allowed to basically propose a block.
00:23:17.500 - 00:24:05.992, Speaker A: And so the idea is it's actually not time based, it's just based on the height and then the validator set at that time. So if the network is going faster, you don't actually have to wait for the next slot for the next proposer. Instead it's just based on the height. So let's say height ten, height eleven. So in our own testing, you could run a virtual machine that, let's say you were just playing around with this, wanted to produce a block at every 100 millisecond, you would have a preferred leader at every 100 millisecond slot or however fast your virtual machine was running. And so this, I really wish I had. That slide basically reduced the rejection rate, I think around the time it was activated, from something like 30% to zero, 3% maybe.
00:24:05.992 - 00:24:11.020, Speaker A: So that was a lot less work for the network to do. Cool.
00:24:11.090 - 00:24:13.336, Speaker B: That was specifically the C chain.
00:24:13.528 - 00:24:33.430, Speaker A: So that applies to any linear chain. So that was the P chain, the C chain, and then, as I'll explain with different subnets, many of them also use the same proposer VM mechanism, we call it. And so that just comes out of the box. Now if you really spin up a subnet, so all the subnets run it. Most of the chains run it too.
00:24:34.840 - 00:24:53.396, Speaker B: Just so I understand. So it's kind of like a soft version. So if you think about a traditional longest chain where there's like a single person, basically like say a proof of stake slot based one, where there's sort of potentially one person to produce it, you're saying there's a small set, any of whom are eligible for that particular block height.
00:24:53.508 - 00:25:34.448, Speaker A: Yeah. So it's basically an expanding window. So if you start off at one, let's say the preferred person has 5 seconds to produce something, but if they produce it right away, it immediately goes to the next slot for the next person to propose on top of. But then if that first person doesn't produce within 5 seconds, then it's open to that first person and then the second person. And then if that doesn't work after 10 seconds, then it's open to three people and then eventually it falls back to the all to all leader list. So the thing with avalanche is it doesn't actually produce blocks consistently, it's quiescent. So if there are no transactions to produce, no blocks will be produced.
00:25:34.448 - 00:26:04.240, Speaker A: So if someone's not producing at a particular time, it doesn't necessarily mean that they're adversarial or like they're trying to break the network or anything like that. There may just be no blocks to produce. So it's a little bit more interesting, I think than just a little different than I think some of the traditional slot based things where you're assuming that if someone doesn't produce a block at a given time, even if it's empty, that means that they're offline or they should be slashed or something like that for not participating in the protocol.
00:26:05.700 - 00:26:10.950, Speaker B: You're saying here there's another explanation, which is just that there was nothing to, nothing to do.
00:26:11.800 - 00:26:43.516, Speaker A: And so with some other subnets, especially some of the newer ones, let's just say you're starting off and you got 1000 people, you may not be having a huge overflow of transactions right away. And so because of networking quies, you end up saving a lot of resources, blocks and everything like that, just that are empty. And so it's an interesting property of the network because of the way that this polling works and the rewards are actually uptime based rather than block based. And then.
00:26:43.538 - 00:27:00.484, Speaker B: Sorry, one other question. I'm intrigued by you talking about sort of the network layer manipulations before you went to snowman Plus plus. So I'm wondering, should I think about a node not gossiping transactions to prevent other people from proposing blocks and taking them for itself? Or what should I think about?
00:27:00.682 - 00:28:09.764, Speaker A: Yeah, so this is one thing I'll touch on a little bit later too, which is because typically the block production process works where a single node will produce a block and then send it out with its, let's just say in the pre snowman plus plus case with no signature on it. And so it sends it out and anyone can then start to basically vote on that block. Well, if you get your block out to more nodes before other people do, you probably would have a preference in the network for that block to be accepted because people would have started pulling it sooner, they would have started to prefer it sooner. And so you started to see people compete on the network layer to say oh yeah, I can propagate my block faster than everyone else because there was no menpool. You just put your information in a block and then shared it. And then obviously there's an incentive to have that happen, especially when there's like DeFi or captive TVL. But then when you switch to the snowman plus plus case or what also is known as the proposer VM, what happens is it's bounded.
00:28:09.764 - 00:28:47.862, Speaker A: So if you're producing within 5 seconds you're going to be able to get a block accepted. So it was a pretty massive change. The thing is it's not architected on the consensus layer. It's actually architected as a virtual machine that sits on top of the same consensus. So to actually release this soft leadering approach we didn't actually change anything within the consensus engine itself. Instead we just changed the block validity rules. How do you exchange assets between the different chains? Is there like a built in way to do it or is it kind of standard bridging stuff? No.
00:28:47.862 - 00:29:30.402, Speaker A: So I haven't touched on that part yet. Let me get back to kind of providing the network architecture and then I'll answer I think probably those set of questions because I want to make sure I get to more of the subnet related topics and how they relate to all of these other areas. I'll cover it at that time. That's right. Cool. So now that we've covered, at least loosely, some of the consensus, some interesting questions around some of the virtual machine design and how we think about proposers and consensus within particular chain, I want to also give a brief overview of avalanche itself and its topology. So avalanche is think of it like a network of networks in a sense.
00:29:30.402 - 00:30:18.526, Speaker A: So there's a primary network which has the staking set where everything happens like that. And then you can create these subnetworks on top of it which we call subnets. Now the way that it works is any validator has to always validate the primary network and then they can opt in to validate subnets as well. Now subnets are running any sort of custom virtual machine that the subnet creator wants to deploy. For most cases for avalanche right now a lot of people deploy evms as the subnet virtual machine but it's a much more flexible interface. I'll get into that is just a GRPC shim for any language. So whether it's goling rust you can write any type of virtual machine that actually runs as a subnet.
00:30:18.526 - 00:30:57.630, Speaker A: Now up until this point people have wondered why we require or why the network requires that people also validate the primary network when they're just trying to validate their subnet. It'd be the equivalent of, let's say in cosmos. To validate your zone, you had to validate the hub. And at this point, without cross subnet messaging, which I'll talk about, there isn't really a major reason for it outside of just kind of keeping the ecosystem cohesive. But with cross subnet messaging, this all changes. And so I'll talk about that a little bit later. People visualize it in different ways.
00:30:57.630 - 00:31:44.270, Speaker A: So I always present multiple diagrams to help people understand, I think, a little bit more about how the structure actually works. So in this case you can see that there's like this route, kind of this heart, the primary network with what are called the PX and C chain, which I'll get into. And from there you have this loose proliferation of user created subnets that may contain any number of validators. The important distinction, I think most people say, is like, oh, does it borrow security from anywhere? No, they're actually sovereign networks. So the security is provided just by the validators that are a part of it. Avalanche itself, the primary network does not provide any additional security to the subnets by default. You could add that mechanism if you wanted to, but right now they are just sovereign networks.
00:31:44.270 - 00:32:33.470, Speaker A: So within the primary network, most people that have ever used avalanche have used the c chain and that's the EVM compatible chain that most defi happens on. But there's actually two other chains in the primary network as well. The other one that people tend to use is something called the P chain. So that's the platform chain where it coordinates all the staking and the subnets. Within a single network like this we have something called atomic memory. So within a subnet where you can think about it like putting arbitrary blobs into this key value store between chains that can then be read on other chains. And so the way that communication happens between one single subnet because everyone is validating the same thing is just to have this, think of it like a bucket you can put stuff into and then take stuff out of.
00:32:33.470 - 00:33:48.162, Speaker A: So when you move assets between the C chain and the P chain, for example, which is oh, I'm going to take my staking rewards and put them into Aave, what you'll do is you'll export a Utxo from the pchain into this blob store and then on the c chain you'll try to read it in the consensus mechanism manages those dependencies to ensure that if you have a pchain export and then are importing it at a later C chain block, the network doesn't halt and it establishes an ordering over blocks that can then be accepted. So you can actually have multiple imports and exports in both p and C chain blocks. So the notion is that within a single subnet, messaging and actually communication is pretty well defined because you have a shared validator set. The difficulty comes when you're trying to communicate across subnets which have different validator sets. And that's where the cross subnet messaging story comes in. And so the joke I always say is no one really knows what a subnet is, I think in some sense because it's kind of used so broadly in some areas. And really it's just a collection of validators from the primary network that just are also validating another chain.
00:33:48.162 - 00:34:23.582, Speaker A: And so this company, I think two or three weeks ago came out with this really cool visualization tool that shows everyone on avalanche. And then what validators are also validating different subnets and chains. So you can see overwhelmingly so far still almost all validators are just part of the primary network. So they're just validating the XP and C chain. But you're now starting to see some proliferation to different subnets. I'll zoom in here so you can see a little bit more easily. So the biggest one that people have seen on avalanche so far is the DFK chain.
00:34:23.582 - 00:34:50.106, Speaker A: And so this is Defi kingdoms. The other really big one is the swimmer network, which was Krabata. And so they run some set of nodes, community nodes. Right now those are both proof of authority networks. So that's the original subnet mechanism. But when they're getting started, it's important for them to make sure that everything makes sense. They have control over the contract, so they want to ensure that there's some level of performance and then they don't really do any staking or anything like that.
00:34:50.106 - 00:35:49.722, Speaker A: It's just fully managed by them. But when this change happens, so as people have basically graduated from the C chain into the subnet world, it has had major impacts on the primary network. So this was the gas price on the C chain before and after Krabata migrated to their own subnet. And so over time, what we've seen is that projects will start on the C chain or on the primary network, and then when they get to some level of scale where their users are complaining that they want to use their own token or they want lower fees, then they'll move into their own subnet and then run their own virtual machine that's compatible with their game. So this has been really popular amongst different gaming companies that want to target a level of throughput that they just know they're not going to get either on the primary network or even on an EVM. They want to produce their own virtual machine. So I think most people, as I said, use the EVM.
00:35:49.722 - 00:36:32.422, Speaker A: But the really important part about subnets is it's actually just a generic virtual machine language. And we've really written like a shim between the EVM, particularly geth, and into the consensus engine to allow it. But the actual virtual machine interface is just a GRPC layer. So we give you arbitrary access to basically do anything within the engine or the networking layer that your virtual machine may want to do. So the way that you should think about it is avalanche. Consensus is this lower level engine. It handles the networking, the storage, the consensus, and the API.
00:36:32.422 - 00:37:11.986, Speaker A: And then it will say, hey, periodically I got this blob of bytes from the network. Can you parse it for me? And then if you parse it, it'll hand it back to the engine. And then the engine later on will say, hey, now that you've parsed this blob, can you verify it? Is this a good block? Great. Okay, now accept it or reject it. So you can abstract away a lot of the complexity of the consensus engine to actually build custom virtual machines that can do all sorts of random stuff. And so I created a demo which I call the spaces VM. So if you go to this website, I used to have a QR code up here, but I decided not to that for security reasons.
00:37:11.986 - 00:37:53.222, Speaker A: So if you go to this website called Trispaces XYZ, it's basically this generic name service site. Like, think of it like about me, which lets people register names and then store stuff at those areas. And so this is built as a fully custom virtual machine on top of a subnet. And so I can type in my name and I can view the space of all the stuff I've stored there. But this is a fully custom virtual machine running on the avalanche basically VM interface. But I basically added like EIP seven one two type data compatibility. So you could still use metamask with it.
00:37:53.222 - 00:39:16.126, Speaker A: But it's a fully custom virtual machine. And the way that I think about what that provides for avalanche, as the protocol is more to think about avalanche like this Hydra. So you have some set of virtual machines that are popular now. But as different virtual machines become popular, you just write a shim to this virtual machine interface and then all of a sudden now it can run this entirely new experience. And so it's a very future forward design mechanism, I think, for how we think about the growth of the network and has allowed us to continue to explore all sorts of new virtual machines, stuff like the move VM, stuff like WaSm, stuff like file storage and things, without having to redesign the core of the avalanche repository and consensus engine. So I think we are going to release some sdks in a few other languages pretty soon. But one of the things that people have really played around a lot with is some more of the networking stuff because you can send basically arbitrary gossip message back and forth between anyone in your subnet, which is super useful for building some cool p to P messaging app with no blockchain at all or anything.
00:39:16.126 - 00:40:09.394, Speaker A: But the VM interface and everything gives you that flexibility to really create random things. And so I think the one thing that the cosmos SDK does particularly well is provide really nice wrappers around many of these low level primitives. And that's been a huge part of, I think, their success. And so we look to do something similar, I think, with wrapping some of these in higher level ideas that make it easier for virtual machine developers to get started. Excuse me, but I think to tie it up and then I'll start to loop back around to people that have different questions is really, we have this avalanche, this consensus, we've implemented these subnets. The majority of them are proof of authority. They can't talk to each other yet.
00:40:09.394 - 00:40:52.990, Speaker A: And so what's coming, so the big thing was something called elastics subnets was just, actually just activated on the avalanche main net two days ago. So elastics subnets will people come to us and they're like, hey, I want to launch a subnet, but I don't want to control it. I don't want it to be a proof of authority mechanism where I have to explicitly add validators. Well, elastics subnets, we agreed. We're like, I don't think the end goal of subnets is also to be proof of authority. So we added this notion called elasticsubnets. And so elastic subnets allow you to basically transition a subnet from being proof of authority into proof of stake.
00:40:52.990 - 00:41:45.514, Speaker A: So you can define a custom token and then reward mechanism. And so when people stake to your subnet, they become validators. If they are uptime, for some percentage of uptime, they are then rewarded. Now the big benefit of this is that all of this logic actually happens externally to your virtual machine. So you can enable staking for anything and not really have to worry about the actual staking mechanism itself. So the virtual machine may not even have blocks, it may just be like some sort of like p to p thing, but you could still actually reward people for participating if they're up for some period of time. So that has been, I think probably one of the largest releases that we've had in avalanche, outside of the snowman plus plus kind of soft proposer mechanism, but that was really more reliability focused.
00:41:45.514 - 00:43:08.486, Speaker A: I think this is by far in a way the largest feature that we've added. And then I think the other area that has people really been asking a lot of questions about has been more of the cross subnet messaging scheme. And so it's great to have Dan here because his work, as many of you know, is seminal in this area. But the idea very simply I think, is that the primary network, which everyone is a validator on this is why it's important that everyone's a validator. When they register to become a validator, they store a BLS key plus a proof of possession on that network. Well, because the subnets are so closely connected to the primary network, and everyone runs the primary network during the subnet block execution context, you can actually just do a DB lookup or a memory lookup on your node to see really what the validator BLS pub keys are at a particular time on any other subnet. So the way that this works is if you're on subnet a and you're trying to do some sort of like interesting cross subnet messaging mechanism, you can generate a multi signature amongst all the validators on that subnet, send that message to any other subnet.
00:43:08.486 - 00:44:12.400, Speaker A: That message delivery and format is fully defined by the subnet. So it's just an arbitrary signature. The subnet b can then say, okay, I got this blob, it says it has the signature I can look up right inside of the primary network on my node what the keys should be at that time for the validators from subnet a, and then use that to just trivially authenticate that message. So you don't actually need to pass any headers around or kind of keep different subnets up to date with each other's state context, or validators. You can just query the primary network to see pretty quickly whether or not a message should be valid or not. And so a lot of people have said subnets are interesting, but until they can communicate with each other and until I can stake on them they're not as interesting to me, and we've heard that resoundingly and we obviously feel similarly. So a lot of our recent research has gone into making that possible.
00:44:12.400 - 00:45:01.774, Speaker A: And so I think this one is particularly exciting for us because we think it will unlock really a lot of intra subnet activity. But I'll pause again now. I think I may have prepared a few too many slides for the slot for timing. Sorry Patrick, quick question on the BLS signatures. Do you have different thresholds and different subnets? Yeah, so this is I think one area of research we're super excited about. So the subnet that receives a signature can decide how it wants to treat that. So what we would expect to happen is the subnet b in this case would say oh, I want to accept messages from subnet a and D but not C.
00:45:01.774 - 00:45:45.438, Speaker A: And for a my threshold on its stake participation is 70%, but on d I need 95% to be considered valid. So it's wholly dependent on the receiver side whether or not it accepts a signature at all. And if so, like what stake percentage? The really convenient thing is the recipient subnet, as I said, has full visibility into the staking set of any other subnet at any height. So it can say oh, this message was a little old, maybe 6 hours old. I don't want to accept it. I only want a message within the last 3 hours to minimize kind of the churn assumptions and then from there some required stake participation. It's more of a primitive, I think than some of the other approaches to this problem.
00:45:45.438 - 00:46:01.140, Speaker A: It's really just like a message create, like a signature creation and a signature verification mechanism. The message format, the delivery, the guarantees you get around reliability of that delivery are fully up to the subnet on how they want to handle that.
00:46:05.510 - 00:46:09.826, Speaker C: I want to circle back to snowman plus plus. Is now a good time to do that?
00:46:10.008 - 00:46:21.206, Speaker A: Yeah, totally. I think the last few things I have are kind of just more generic mechanisms. So yeah, I think it makes sense to go back to that. I'll open it up now. Yeah, fully for whatever questions you want to ask.
00:46:21.308 - 00:46:39.040, Speaker C: Sweet. I'm curious how so you touched on this, but I didn't fully grock it. I'm curious how snowman plus plus went from block builders validators being able to sort of produce their own ordering and get blocks in and not get other people's transactions in to a state where.
00:46:39.890 - 00:47:28.430, Speaker A: All blocks were getting in. Yeah. So the way that we developed it actually is a virtual machine wrapper around an inner virtual machine. So think of it like you have this virtual machine we define called the proposer vm, and its job is just to wrap arbitrary inner blob bytes. Doesn't really care what they are, but you sign them as a validator and then you assign basically a pchain height or time at which you did that. So when a validator gets a block at a particular height for a particular chain, it knows who is supposed to basically what the preferred proposer list is and at what time those people should be allowed to propose. So I'll make it very tangible.
00:47:28.430 - 00:48:07.782, Speaker A: We're at height ten, we've accepted height ten or whatever. We have height ten verified, and now we're going to height eleven. Let's say you're the second allowed proposer, so you'd be allowed to propose anytime after 5 seconds after that original block was created. And I'm proposer a or like the first one I can produce at any time within that 1st 5 seconds. When I gossip my block to everyone else they just run a simple validity check on the block header and say oh, I got this signature here of this node id. Is it one that's supposed to be running right now? Yes. Great, I'll start to actually verify the inner block.
00:48:07.782 - 00:48:55.320, Speaker A: No, I'll drop it, this is invalid right now. And so the cool part though is that because it's implemented as a virtual machine, you could feasibly create your own proposer mechanism based on something totally different. The architecture we always strive for is more the creation of primitives that are loosely stitched together, and we look to the community for interesting adaptations of that. Proposer VM is a great start, but oh, you want to do something even fancier, like you want to have strict windows, you could do that with timestamps if you wanted to. Like you could write a strictly slot based proposer mechanism on top of the snowman consensus, which is totally open or like leaderless. Yeah.
00:48:56.430 - 00:49:06.394, Speaker C: Is the short version that to mitigate spamming you have a proposer selection mechanism which you didn't have before. Before you were just doing sort of gossip, and then collective signatures get over.
00:49:06.432 - 00:49:41.846, Speaker A: A certain threshold and we accept it. Yes. The difference though, the collect a certain number of signatures part, there is no part of that in avalanche. So once a block is actually gossip, people just vote on it. There is no signature passing, people don't sign any headers, which is a huge reason why it's so fast. I have bytes and then people just basically vote on the hashes of those bytes. But in terms of what the big change was, just the number of people allowed to produce a block or payload at a particular height starts off as one instead of starting off as everybody.
00:49:42.028 - 00:49:46.670, Speaker C: And proposer selection is done sort of probabilistically based on stake weight.
00:49:46.770 - 00:50:06.366, Speaker A: Exactly. Got you. Okay, cool. Thank you. Yeah, like I said, I think you could do a really cool VM around it. That's more like strictly slot based VRF stuff. We would love to find more ways to have more creative approaches to it.
00:50:06.366 - 00:50:19.822, Speaker A: I think at that time we were like, we should really fix this. And so we have the most reasonable, safest option. And then from there, we really want to push people to develop interesting adaptations for their use cases.
00:50:19.966 - 00:50:22.020, Speaker C: And that was like November last year.
00:50:23.290 - 00:50:37.034, Speaker A: Yeah, I think it was right around the time of avalanche rush, which I think it dropped in the middle of October. Okay, very cool. Middle of end of October. Yeah, we have a whole write up on it too, if you want to read it and see some of the diagrams and stuff. Cool.
00:50:37.152 - 00:50:38.074, Speaker C: Yeah, I'll check that out.
00:50:38.112 - 00:51:42.480, Speaker A: Thank you. Yeah, I think the only other thing potentially to touch on is how we think about where the avalanche go deployment is actually going. So within a subnet, you can actually logically separate the chain runners across physical hosts. And so within a subnet, you could actually have multiple chains running on multiple hosts to scale out, throughput, scale out all sorts of state management, storage stuff, and then have avalanche go act more as almost like a network proxy that just decides which hosts receive which traffic based on what chain they're actually validating. And so I think now with really the completion of the core vision related to the elastic subnet component and the messaging have really focused all of our attention back into hyper virtual machine optimization. So people know us as like, oh, great, you can run eth. Cool.
00:51:42.480 - 00:52:17.420, Speaker A: Back to now let's make a virtual machine that's hyper optimized and custom built for avalanche consensus. I think that should be the gist of it, but happy to circle back anywhere. I think there was obviously a lot of topics covered and happy to dive in either to the consensus design the virtual machine, design, the network know different trade offs that I think the network makes in different ways. But yeah. Thank you for your time.
00:52:17.790 - 00:52:19.446, Speaker C: I think there's a question in chat.
00:52:19.478 - 00:53:38.894, Speaker A: Patrick, related to the VRF comment. Is there anyone thinking about using something analogous to fiat shamir heuristic to cut back on the heavy bandwidth requirements you mentioned? So one thing we've talked about doing is now that we have this BLS factory or tool at our disposal to do these periodic acceptance proofs where the values in the network kind of gossip signatures. I mean, anyone that's looked at e two will say wow, that sounds interesting. I think it's an interesting idea for people that have the ability to produce BLS multi signatures is to have something similar so that people that maybe aren't participating in consensus or don't want to do the polling can just wait and listen for validators to produce or some percentage of stake to participate in a single signature and use that instead as the way to listen and accept. For many API nodes or even exchanges, that's more than sufficient for their needs, and it would mean that they dramatically reduce the amount of polling. Now, the one thing I'll say is heavy bandwidth here is a relative term. The messages that are gossiped or like part of the poll is just 32 bytes.
00:53:38.894 - 00:54:16.350, Speaker A: So each poll per host is not large. We don't send the blocks back and forth. You basically get a block and then you pull its hash. So the network requirements are relative to others that maybe just listen to the signatures, maybe higher because of the amount of pulls that have to be done. But it's not like every block takes megabytes or gigabytes of traffic to verify. Good question though. Oh, the last thing I'll say while I fill time everyone has any questions is we're super interested in supporting different people working on zero knowledge virtual machines.
00:54:16.350 - 00:55:09.940, Speaker A: And so people that are doing we think that our VM basically abstraction is really good just for hyper optimizing for zero knowledge proof verification. And we'd love to work and are looking for different teams to support that are working on basically zero knowledge based virtual machines that would be a really good partner in that way because we can provide arbitrary implementations on the VM side. And then super quick finality of those proof verification, which I think would go really well with people doing these larger kind of zero knowledge scaling techniques. So I just wanted to, before I forgot, throw that out as something that's super interesting to us right now. Yeah.
00:55:12.070 - 00:55:49.950, Speaker C: I'm curious if you guys have thought at all about sort of like flashbot style direct auction over transaction ordering at the I guess this is c chain level, I guess this isn't subnet level, but I remember sort of in the pre snowman plus plus days, I guess like September last year, there was a ton of people offering sort of like a turbine style transaction spammers. And then that sort of moved into direct large stake weighted validator connections. I'm curious if you've seen that expand at all. I haven't looked into it in quite a while, or if maybe there's sort of more of a democratized effort or anything of that nature.
00:55:50.690 - 00:57:24.074, Speaker A: Yes, on the topic of transaction propagation in the mempool, our major concern is just the most efficient mechanism for transporting things. So our mempool just is amongst the people that are actually going to produce blocks, aka the validators. It's much more similar to Solana's approach where they just basically send transactions to the next producer like some set of producers, instead of just gossiping it uniformly like in the ethwood or something like that. So the mempool is a little, not opaque per se, but just like less widely disseminated. Because just to reduce networking overhead, we have seen and have seen people play around with the idea on subnets and custom virtual machines in particular, of changing the block proposer mechanism to be very different, very different being take into account all sorts of different factors, or on the transaction execution, have it natively swap whatever token you want into something else, or make the transaction fee part of the application, so that you don't even have this dual market thing where you're paying whoever's producing and then you're paying the application. The application is the production of blocks. And so we have seen a lot of people dabbling around like commit, reveal, SGX, like all sorts of crazy things for clobs, in particular subnets that are.
00:57:24.112 - 00:57:27.210, Speaker C: Specific to application specific clobs.
00:57:27.730 - 00:58:34.820, Speaker A: I think in the next few months you'll see a lot of subnets come out, particularly virtual machines that are more application optimized. So have a clob that just the matching is on chain, and then the order book is kind of gossiped around the node via the VM interface primitives, and then you can trivially paralyze the execution, right? Like, okay, if you don't have conflicting orders, if the transaction is verbose enough, you don't actually need to do any generic execution to know what state it's going to touch. Let's just say in a simple transaction I'm sending to you, well, I know the two database keys that are going to be touched are me and you. So any other transactions that don't touch that I can do at the same time. So if you have multiple books or they don't trample on each other's orders in some interesting way, you can do very interesting performance things outside of just changes to the block production mechanism that maybe reward people in very different ways. Like, oh, big orders, we prefer first, even if they pay a lower fee. You could do very weird things if you have sufficient research.
00:58:34.820 - 00:58:53.974, Speaker A: I think the next one that's coming out is called dexalot. They're working with layer zero on kind of like the bridge component to connect to that subnet. Yeah. Awesome. Thank you. Sure. If you have any cool ideas for clobs, also let me know.
00:58:53.974 - 00:59:10.990, Speaker A: I think we're working on creating more like reference examples, I think, of what people can do in this weird and new, totally custom vm world. And I think a lot of people ask about NFT minting. They ask a lot about clobs. They ask a lot about super high throughput transfers.
00:59:14.370 - 00:59:18.974, Speaker B: Thanks so much for the talk and all the Q and A. Yeah, no.
00:59:19.012 - 00:59:52.940, Speaker A: I appreciate you having me again. I love talking to the technical crowds about stuff. I think what I'll say is there's always, with Avalanche, there's a lot to unpack in terms of the consensus, the network architecture and everything like that. So sometimes it takes some time to really understand, I think, some of the depth of it. And I think over the next few months, we're really excited to see what the community kind of puts out, really into all these interesting primitives we're trying to unblock for them. I think there's just so much cool stuff happening right now, and we're really excited to see where that goes. Thanks.
00:59:52.940 - 00:59:53.750, Speaker A: Bye.
