00:00:06.330 - 00:00:09.870, Speaker A: You. Morning everyone.
00:00:10.020 - 00:00:40.406, Speaker B: Welcome to this morning's a 16 z crypto research seminar. Very happy to introduce Justin Thaler who as you know just showed up this week as a faculty fellow. He'll be with us for the rest of the summer, which is very exciting. Justin's a professor at Georgetown and today he's going to give the first of his two part tutorial series on Snarks. Just a couple of reminders. So the second part will be Tuesday at the same time at 11:00 a.m.. And then actually in the meantime, later today at 02:00 p.m.,
00:00:40.406 - 00:00:43.074, Speaker B: we'll have Scott Commoners as well as a seminar.
00:00:43.122 - 00:00:43.334, Speaker A: Okay.
00:00:43.372 - 00:00:44.630, Speaker B: So Justin all yours.
00:00:44.710 - 00:01:19.140, Speaker A: Thanks a lot. Yeah. So I'm going to tell you a little bit about Snark design. Well, what Snarks even are how they're built and how they're used in roll up projects which are pretty exciting right now as they're being built for blockchain scalability. Today will be not Super Technical Tuesday. Tim said I could just go crazy. Okay, so what is a Snark? So in a Snark some untrusted prover claims to know some witness satisfying some property.
00:01:19.140 - 00:02:15.560, Speaker A: So a couple examples that come up a lot in cryptography. So the prover might claim to know a pre image W of some designated output of a hash function. So here the witness would be a W such that H of W equals Y where sort of the prover and Verifier have agreed in advance on the hash function H and the output Y and the prover is claiming to know W and the Verifier doesn't know W and wants to make sure the proverb does. Another example which is related but not quite the same is the witness might be a private key corresponding to some known public key for some crypto system. Okay, so there's always a trivial proof for the proverbs claim which is to just send the witness W that it claims to know to the Verifier. Then the Verifier can just check that W satisfies the claim property. For example, in the first bullet point here, the Verifier could just evaluate the hash function at W and confirm it equals Y.
00:02:15.560 - 00:02:45.162, Speaker A: Okay, so what a Snark does is achieve the same goal but with hopefully much better cost for the Verifier. So this is a funny acronym. It stands for Succinct non Interactive Argument of Knowledge. So let me tell you what each of these letters mean. So Succinct means that the Snark proof must be shorter than the witness W. So there's like a trivial proof it's just the witness. So anything that's shorter than that I'm going to cost Succinct in the research literature.
00:02:45.162 - 00:03:18.746, Speaker A: Some authors will reserve it to mean like logarithmic in W or even smaller than that. I just think anything nontrivial should be considered Succinct. Ideally, checking the proof should be faster than the runtime of the Verifier in the trivial proof system. That's done actually captured by a letter in the word in the acronym Snark. So if I have to clarify that issue, we'll call it a work saving snark. Like the verifier saving work compared to the trivial proof system could be you have a short proof that takes a lot of time to check. Right? But that wouldn't be work saving.
00:03:18.746 - 00:03:35.230, Speaker A: Okay. The other letters. So non interactive means the proof is static. It can be posted on a website or sent in an email. And then argument of knowledge just basically means if the prover can't break some crypto system, then it really must know w in order if it's going to produce a convincing proof.
00:03:37.250 - 00:03:44.754, Speaker C: Is there like some notion short relative to the size of the statement too? Because it seems like for the hash pre image, like this arc is never.
00:03:44.792 - 00:03:48.420, Speaker A: Going to be shorter than the width. It can be.
00:03:49.750 - 00:03:51.860, Speaker C: What if the hash pre image is 80 bit?
00:03:54.950 - 00:04:41.180, Speaker A: Sorry, that shorter than okay. Yeah, there's like an asymptotic question versus a concrete question. There are Snarks with non negligible soundness error that might be shorter than 80 bits in principle anyway. I think that's possible. Anyway, in practice you're not going to get a snark proof that's smaller than several hundred bits. So if your witness is that short in practice, you're not going to be more succinct than that. But there are statements that could have larger witnesses.
00:04:41.180 - 00:05:00.180, Speaker A: The prover is proving to know many witnesses or many pre images or something. In practice there is a limit to succinctness the most succinct snarks. It's three cryptographic group elements. So you're talking, what is that, like 600, 800 bits? I forget.
00:05:01.320 - 00:05:07.060, Speaker B: And so just from concrete issue, if you're thinking about shorter and faster, just like logarithmic.
00:05:09.100 - 00:05:59.564, Speaker A: Yeah, we'll get into issues like transparent snarks and non transparent snarks. There are non transparent snarks where short really means like three group elements in a cryptographic group that's like 600 800 bits. I think if you want a transparent snark, you're probably not going to do better these days than logarithmic in the statement size or logarithmic in something. Okay. And I'll say more about what fast means. Like you can hope for a few milliseconds for these non transparent things when the proofs are really short and tens or hundreds of milliseconds for the transparent things, I think for the verification. Okay.
00:05:59.564 - 00:06:24.900, Speaker A: And something just mentioned the roll up application does not require zero knowledge. If you don't care about privacy, you just care about blockchain scalability. You don't need zero knowledge. So I will not call them ZK roll ups. I think it's a bit of a misnomer. I will call them validity roll ups today if I have to clarify what kind of roll up I mean, okay, so here's the plan for today. First I'm going to tell you about how are snarks designed.
00:06:24.900 - 00:07:04.980, Speaker A: Then I will give you sort of a taxonomy of existing Snark constructions and their pros and cons and that'll give you really it boils down to right now like four classes of Snarks out there today that people are using or might use soon. So kind of you can fit it all in one slide and understand how they relate to each other. And after that we'll turn to roll ups. I'll tell you what is a roll up, how they're using Snarks. And there won't be time today, I don't think. But if there is more time, I'll start the slightly more technical overview. So if parts one and two do feel a little bit technical or just dry because you don't care about cryptography, just come back for part three.
00:07:04.980 - 00:07:59.120, Speaker A: You won't need to know parts one and two for part three. So how are Snarks designed at a high level? Most Snarks work via a two step paradigm. So ideally you would start with something like a computer program written in a high level language. And that computer program is just going to take the witness that the prover claims to know as input and check that the witness satisfies the property claimed by the approver. So in like this hash pre image example from earlier, where the prover claims to know a pre image under a cryptographic hash of some hash output, the program would just evaluate the hash on the witness and check if it equals the claim value. So applying a Snark directly to a computer program conceptually can't really be done. So the first step of this two step process is to transform that computer program into an equivalent model that is amenable to probabilistic proof systems.
00:07:59.120 - 00:08:45.380, Speaker A: And typically this is some kind of circuit satisfiability instance or some kind of generalization of this. Some people like something called R One CS, it's a generalization of circuit satisfability. You don't have to worry about the details. There's a movement now to go sort of beyond even more intricate things, even beyond R One CS. But this transformation from the program to something like a circuit that's called the front end of the system. And then you would apply a Snark for circuit satisfiability to the circuit that's called the back end. So all of these Snark names out there in the literature, which maybe you've encountered some of them before, they refer to the back end, the probabilistic proof system where the prover is now sort of claiming to know a satisfying assignment to a circuit instead of like a witness, a pre image of a hash value or something.
00:08:45.380 - 00:09:04.010, Speaker A: And they're equivalent claims, right? The front end is ensuring that the circuit sat instance is equivalent to the original computer program. Essentially it's just circuits are kind of like a lower level language, if you will, than C or Java kind of. Does that make sense?
00:09:04.940 - 00:09:30.880, Speaker B: One question, step one, you talk about turning to an equivalent model of manimal to probabilistic checking. And so I'm wondering if you can say something about like why? And you never talked about probabilistic checking before. You said the goal is just succinctness I suspect what's going on is succinctness will come. There'll be a particular way that the succinct proofs will mean something like a bunch of probabilistic checks. And that's why you wrote that step.
00:09:30.950 - 00:10:03.470, Speaker A: Yeah, that's right. So what's happening in these proof systems, in these Snarks is conceptually that the verifier is sort of randomly challenging the prover, but then the randomness and any interaction is completely pulled out of the protocol using cryptography and kind of replaced with sort of the keys of a crypto. The randomness is kind of replaced with the keys of a crypto system, if you will. And in that way you wind up with a static proof where the verifier never has to send a challenge to the prover or anything.
00:10:06.000 - 00:10:24.204, Speaker B: In principle, you could imagine any number of approaches to what a system proof might represent. But like the dominant paradigm is, what it represents is the results of a bunch of random challenges. Given that that's the dominant approach, we shouldn't be surprised to see that step one means you need to basically repropare the same program in a way that you can check through Ram challenges.
00:10:24.252 - 00:10:40.456, Speaker A: I see. Zach. Yeah. You said everything much better than I did, so I won't try to repeat it. Think about the difficulty of step one versus step two is like reducing every general program to the circuit representation, like.
00:10:40.478 - 00:10:41.816, Speaker D: Relatively easy compared to step two.
00:10:41.838 - 00:11:28.804, Speaker A: Or how is that so easy and hard might not be the right wording. The question is also the overheads like program runs for 100 steps, but the circuit has a billion gates. That's not great. But also if the circuit has a billion gates and sort of the proverbs runtime is equivalent to evaluating a circuit with 100 trillion gates, that's also not so good. So there's going to be overhead in both steps. Some computer programs are much more amenable to being turned into small circuits than others. Asymptotically speaking, we sort of have both steps close to the best we can hope for, which is up to like, log factors.
00:11:28.804 - 00:12:19.770, Speaker A: In either step, we kind of can't hope to do any better. But concretely, those log factors matter and the constants matter and everything in practice can be quite nasty. But no, the infrastructure itself, kind of building it and even understanding it, maybe that's what you mean by difficulty can be quite challenging. If you don't care about really squeezing as much overhead out as you possibly can, making these as performative as possible, there are fairly straightforward ways to kind of do both steps. I mean, even the simplest things for step two use some of the most celebrated ideas in theoretical computer science. But if you really want these things to perform, everything is kind of challenging to understand the details of and to make sure it's bug free and all of that, they just get complicated. Does that answer the question?
00:12:24.060 - 00:12:39.710, Speaker B: Basically, step one, it should be intuitive that yes, we should be able to do something like that that's kind of conceptually I want to say straightforward but we're used to kind of different models of computation simulating each other whereas step two there's not clear operator that should be possible correct?
00:12:40.500 - 00:13:30.190, Speaker A: Yeah. And I would say to unpack step one just a little bit, the world's kind of dumbest program to circuit transformations would yield it wouldn't turn into a how do I want to say this? Yeah. Like Cook's theorem, like, your laptop is really built out of a circuit. Right. So there's this question of is it circuit satisfiability or circuit evaluation? And you run into this issue where the prover is claiming to know a witness the Verifier doesn't know. So there's kind of nondeterminism already built into the problem itself. But the really efficient transformations from programs to circuits, they will extend the witness with advice, inputs that help keep the circuit small.
00:13:30.190 - 00:14:52.708, Speaker A: This is actually I have a sort of book and I have a reading group and this is one of the concepts that's sort of hardest to describe verbally is like the circuit satisfability instance. The witness to the circuit is actually the witness in the prover's head with extra stuff in it to help the compilation from programs to circuits. So, yeah, snark programming is something like the practitioners refer to and they're talking about how you put in these extra advice inputs to the circuit to keep the circuit small and yeah, so even like relatively straightforward things here are going to use those advice inputs and hence it's a different way of thinking than Cook's Theorem might be, say, a little bit. So I hope that color is a little helpful. Most of this talk is on backends but I will talk about front ends because in the roll up know the overheads at both steps are really though you know here's the picture is computer program gets turned into some kind of circuit set instance. And then the proverb and Verifier, they both sort of know the circuit somehow and they apply a snark for the proverb to establish it knows a satisfying assignment to this circuit. The last thing I'll say is the front ends that people are using, a lot of the front end the idea that they would start with like a C or Java program is purely aspirational.
00:14:52.708 - 00:15:42.824, Speaker A: People aren't really doing that in practice. Many people are writing in very low level languages such as like Bellman and what else do they use? Circom. They're almost just writing out the circuit gate by gate and then other approaches will have something slightly higher level than that. But it's still like an assembly like language. And so obtaining front ends that really would take an arbitrary C program and spit out some circuitsat instance that's not just horrible is really not an open problem. So just some extra color there about where we are and what the programmers are really going to be writing if they want to use these snarks. Okay, so with all that said, my main focus and my main expertise really is on the back end.
00:15:42.824 - 00:16:28.650, Speaker A: So our back end goals are we want a Snark for circuit satisfability, where the prover is not much slower than just what it would take to evaluate the circuit gate by gate on the witness with no guarantee of correctness. So that's sort of the best you can hope for is like prover has to at least convince himself that the witness satisfies the circuit. Right? So we want the proof to be as small as possible. So yeah, the smallest but non transparent Snarks will be a few hundred bytes. Or it turns out if you want a nice version of non transparency, they'll be half a kilobyte to a few kilobytes. Actually, even the transparent ones, we can get down to a few kilobytes and we want the verifier to be the fastest possible. Again, from a few milliseconds is sort of the fastest we have now to maybe ten times that or something.
00:16:28.650 - 00:17:05.200, Speaker A: There are other properties we would want beyond just fast prover and fast Verifier. So now we get to yeah, I've been saying this word transparent. I haven't told you what that means and I will say in just a second. The other thing we would ideally like is for the Snark to be post quantum secure. Meaning if the prover has a quantum computer and is trying to find convincing proofs of false statements, it still can't do it. Okay, so what does transparency mean? So in a non transparent Snark, the prover will have to know what's called a structured reference string. So this is a very long string.
00:17:05.200 - 00:18:04.710, Speaker A: They'll call it a proving key in practice, typically it's as big as the circuit, so it's big and there'll be a trap door such that if the prover knew the trap door, it could easily find convincing proofs of false statements. Okay, so to generate the SRS kind of someone will have to know the trap door. SRS is structured reference string and you'll have to sort of trust that party to then forget the toxic waste, dispose of the toxic waste. So what people do now, there's been a lot of really nice work to sort of minimize the trust assumption. People will run like a giant secure multiparty computation protocol, hundreds of participants, and unless they're all getting together and colluding, no one knows toxic waste. So kind of as a trust assumption, it's fairly minimal now, but as a headache and something you don't want to have to do over and over again, it's still a giant pain. Right.
00:18:04.710 - 00:18:23.550, Speaker A: So I'll say more about how various Snarks differ in terms of the kind of trusted setup they require. But yeah, if Snark is transparent, it means you just don't have to worry about that at all. There is no trusted setup, there's no trapdoor that someone might know and could forge proofs of false statements if they knew it.
00:18:24.640 - 00:18:31.256, Speaker B: Sometimes I hear about these distinctions between setups that are circuit specific versus that's.
00:18:31.288 - 00:18:32.172, Speaker A: Exactly what I was getting.
00:18:32.226 - 00:18:33.144, Speaker B: So you'll get into that.
00:18:33.202 - 00:19:19.630, Speaker A: I'll get into that more later. Yeah, but I can say now you don't want to have to run a different trusted setup for every time you tweak your circuit or ideally you'd like 200 people get together, run the trusted setup and forever in the future. Anyone doesn't matter what circuit they care about they can use the same SRS as long as their circuit isn't too big. And that's kind of what people are doing today. Like Zcash and others ran some of these trusted setups a while back and now other projects get to use those same strings as long as they believe that someone in that ceremony from long ago people call these ceremonies was honest and didn't collude with every other person in the ceremony. I see there's something in the chat. Should I pull that up? Sure.
00:19:19.630 - 00:19:28.476, Speaker A: Best simple. Oh, okay, good. Thank you. Someone like the definition of a Snark.
00:19:28.668 - 00:19:29.410, Speaker D: Okay.
00:19:29.780 - 00:19:57.844, Speaker B: One other question. I wonder if it would be helpful to we keep talking about these circuits in the abstract, which in some sense we're all used to. But looking ahead toward roll ups especially sort of discarding the privacy aspect of canonical circuit, let's say, for roll ups built on ethereum, should I think of it as taking as input like state route and a bunch of merkel proofs and a transaction and then sort of also there's an alleged consequent state route and I'm just checking that that's legit.
00:19:57.972 - 00:20:12.910, Speaker A: That's exactly it and I'll say more about that later. And then for the simpler example of just a hash pre image the circuit would literally just evaluate sha three on the input that's there.
00:20:14.740 - 00:20:24.268, Speaker C: Thought about the transparency issue and I guess how much do you think it's a real issue in practice with all the multiparty setups.
00:20:24.284 - 00:21:21.056, Speaker A: And stuff versus a branding issue with yeah. My personal feeling right now which sort of evolves over time and probably will continue to, is that the biggest downside of the trusted setup is it limits the size of the circuit you can deal with. And if we ever want to apply these Snarks in blockchain applications the statements being proven tend to be very simple in a cosmic sense. You could imagine using Snarks in other contexts where the statements are vastly more complicated. Circuits would have to have trillions or more of gates not going to run trusted setup for that. And there are techniques you can use to try to mitigate this like recursive composition which I'll say a little bit about but that's what I feel. I'll say more about where trusted setup has been like a thorn in people's sides.
00:21:21.056 - 00:22:04.240, Speaker A: If I have time later in the talk I'm not so worried about the trust assumption. I think if 200 people get together that they're not all employed by the same person or something. They're probably not colluding. Last thing I'll say there have been bugs like Zcash. The Zcash bug was they included extra stuff in the SRS that shouldn't have been there and that would have let people potentially create Zcash out of thin air sort of something like that. So just maybe that means that the complicated nature of these Snarks within SRS I don't want to draw too strong a conclusion from that but it was an SRS issue that led to that bug.
00:22:05.140 - 00:22:22.776, Speaker B: So just to connect the dots there, you said your main concern about trusted setups is limiting the circuit size you can handle. So is that because you said that the reference trim needs to be basically order size of the circuit and then the question is the point is just like people actually carrying out the NPC just realistically it was only so long.
00:22:22.878 - 00:22:55.564, Speaker A: Yeah, exactly. And it's got to be distributed to anyone in the world who wants to operate as the prover which in the blockchain setting could potentially be anyone in the world. Yeah. So it's just expensive to generate and distribute and if it's trillions of gates, I mean that thing has to be a trillion cryptographic group elements. You could distribute that to everyone in the world. That doesn't sound like it'll work to me. The verifier of course doesn't have to know the whole SRS.
00:22:55.564 - 00:23:15.000, Speaker A: It only has to know like a few group elements or something. But yeah, unless you want the prover to have to have a massive memory machine or something, it's not going to be nice and every single person in the NPC to have the same memory. That's my take. It could continue to evolve.
00:23:16.220 - 00:23:32.236, Speaker B: Does that mean for the circuit we just discussed about validity of sort of a state transition in ethereum? It almost feels like you're saying it's sort of inconceivable you'd have a non transparent like that you'd have something with a trusted setup that could handle that circuit, is that right?
00:23:32.418 - 00:24:16.120, Speaker A: Oh, yeah. I mean, well, people are running roll ups now, and the reason circuits can get big there is because they want to handle a batch of huge number of transactions. And so the circuit's not just doing one state transition plus merkel authentication pass for any piece of data access, but know hundreds of thousands or something like that. Okay, so here we get to sort of the more technical part of today. So if you want to zone out it's really not going to matter but maybe you're interested. So here's kind of the main paradigm for developing a Snark for circuit satisfiability. This has three steps in and of itself.
00:24:16.120 - 00:24:55.654, Speaker A: So first you design something called a polynomial IOP. I'll say in a moment what that is. Then you combine that with a cryptographic primitive called a polynomial commitment scheme. I'll say shortly what that is and the combination of the two will give you a succinct argument, but it'll be interactive, and then you can pull the interaction away and just be left with a static proof by applying something called the Fiat chimere transformation, the details of which don't really matter for this talk. So let me tell you what is a polynomial IOP. But first I'll clarify the statement that all Snarks are designed in this three step paradigm. So that is almost true.
00:24:55.654 - 00:25:57.798, Speaker A: There's just, like, one exception which happens to apply to the first Snarks that were deployed in practice, especially in Zcash, was maybe the main initial deployment. So these Snarks are based on something called a linear PCP, which, while the letters are different, is actually fairly close to a polynomial IOP. And in fact, if you look at the crypto that's used to turn the linear PCP into a Snark in this class of Snarks, it looks very similar to a certain popular polynomial commitment scheme. So I really don't feel like I'm lying to you very much if I just kind of hand wave a bit and say all Snarks are designed in this three step paradigm. It's a slight lie, but not a major lie, okay? And in particular, these Snarks that went into the initial version of Zcash and became very popular and still are, they require a circuit specific, trusted setup. So if you're not happy with that, then you really have no choice but to use this three step paradigm. So I like this paradigm because you can sort of get a unified view of every Snark that's out there.
00:25:57.798 - 00:26:46.638, Speaker A: And given how many Snarks are out there, that seems like a useful view to have. Okay, so let me say a little more about what is this weird thing called a polynomial IOP? I'm actually only going to describe sort of a special case of that. It just makes it a little easier to present. So imagine you have a proverb and verifier, and they're willing to interact with each other, which means the prover can send a message to the verifier, and the verifier will respond with a random challenge to the prover, who will respond with some response, and the verifier will send another challenge and get a response. And at the end of the interaction, the verifier is going to accept the prover's claims as valid or reject them as invalid. So a polynomial P is just like that, except the first message in the protocol specifies a polynomial. And I really mean, like, in many cases, it's a univary polynomial.
00:26:46.638 - 00:27:42.684, Speaker A: So like A plus BX plus CX squared plus DX cubed polynomial, except the polynomial might be really big, might be as big as the circuit that the prover is making a claim about. So you don't want the prover to actually have to truly send all of the coefficients of the polynomial of the verifier because that'd be a huge message. It's not succinct at that point, the protocol, and you certainly don't want the verifier to have to read all the coefficients of the polynomial because then it won't be work saving for the verifier. So instead we're going to tell the verifier, look, the proverb is going to kind of send you this polynomial, but you're not going to be allowed to learn the whole polynomial. All you're going to be allowed to do is evaluate the polynomial at one point. So that's what a polynomial IOP is. You sort of have this initial message that's special.
00:27:42.684 - 00:29:03.252, Speaker A: So it's like some giant polynomial and because it's special, the verifier doesn't learn the whole polynomial, it's only allowed to evaluate the polynomial at a single point. But then the rest of the protocol is like the interactive proof I just described where whatever message the prooper sends the verifier, the verifier is going to read the whole thing, respond, the proofer is going to send the message, the verifier reads the whole thing. So it's an interactive proof, but where the first message is this weird giant polynomial that the verifier only gets to evaluate at one point. Does that make sense? Any questions about it? So in the real world, we don't have a way for the prover to send a giant polynomial to the verifier and the verifier to evaluate it at one point without reading the whole thing, right? So that's where cryptography comes in. So polynomial commitment scheme is just sort of exactly what you want to implement polynomial IOP without the prover, like literally sending the whole polynomial to the verifier. So at a high level, it lets the prover bind itself to the giant polynomial by sending just a really short commitment. Typically, this will either be what's called a Merkel Hash, it's just one Cryptographic Hash value, a couple hundred bytes or something, or it will be something like a Pederson commitment that's just a single element of a Cryptographic group.
00:29:03.252 - 00:30:04.040, Speaker A: If you don't know what that means, it doesn't matter. Point is, it's just a really small message from the Approver to the verifier. The verifier can read that tiny message in full. And sort of once the verifier knows this commitment, you think of the prover as sort of being stuck with whatever polynomial it sort of had in its head. You kind of think of that polynomial as being like shoved inside the commitment, and the commitment is locked and the prover can no longer alter the polynomial once it sends this locked box to the verifier. So then later, when the verifier chooses the input X that wants to know h of x, the polynomial committed polynomial evaluated at x, the polynomial commitment scheme has to come with a manner in which the prover can provide the evaluation as well as a proof that the evaluation is consistent with the committed polynomial. And this is just exactly what you want to sort of implement the polynomial IOP without the prover, like literally sending the polynomial in full to the verifier.
00:30:04.860 - 00:30:06.116, Speaker B: So the way you've described.
00:30:06.148 - 00:30:06.392, Speaker A: It.
00:30:06.446 - 00:30:11.924, Speaker B: The verifier could in principle choose their X as a function of the commitment they see from the prover.
00:30:11.972 - 00:30:12.388, Speaker A: Correct.
00:30:12.494 - 00:30:18.780, Speaker B: I assume that whenever you do this, v is just choosing X randomly independent of the actual commitment. Is that right?
00:30:18.930 - 00:30:23.164, Speaker A: Yeah. So in the end, X will just be random and the verifier reason about.
00:30:23.202 - 00:30:26.408, Speaker B: What each might be via the commitment. I can't do that.
00:30:26.514 - 00:31:08.104, Speaker A: If you don't care about zero knowledge, the verifier has no reason to try to read anything into the commitment. The proof systems will be sound. If X is chosen at random, the verifier just wants soundness means for anyone who hasn't heard of this, it's a security. It means that if the prover is making a false claim, the prover can't find a proof that convinces the verifier the claim is in fact correct. So the verifier just cares about soundness. So let's make sure the proverb is not lying and to guarantee that it's going to be enough for X to just be random. If you cared about zero knowledge, you're certainly going to have to worry about whether this commitment reveals any information about the witness.
00:31:08.104 - 00:32:15.676, Speaker A: And in fact, it might if you're not careful. But we won't care about zero knowledge today. Other questions? Okay, so now that you know what a polynomial IOP is and a polynomial commitment, I can give you the taxonomy of Snarks. Essentially, there are a bunch of different polynomial IOPS out there, there are a bunch of different polynomial commitments, and you can just mix and match the two with some caveats. But most polynomial IOPS will sort of combine with most polynomial commitments, and any combination of the two gives you a snark with sort of a different cost profile. So costs are sort of inherited both from the polynomial IOP and the polynomial commitment. One thing I'll mention is these properties that are important in practice, whether for practical reasons or branding for both transparency and post quantum security, they're determined essentially entirely by the polynomial commitment schemes.
00:32:15.676 - 00:32:50.060, Speaker A: So any polynomial IOP could be yield a transparent and plausibly post quantum Snark if combined with a transparent and plausibly post quantum polynomial commitment up to some subtleties that's really not worth getting into, certainly not today of details of quantum security proofs. But if we're just talking about plausible post quantum security, we really don't even have to worry about those subtleties. So, yeah, transparency and post quantum security, it's all coming from the polynomial commitment, not from the polynomial IOP.
00:32:52.560 - 00:33:12.964, Speaker D: So the reason you would mix and match is that the IOPS are doing some operation with the commitment and then based on their runtime of whatever the IOP is, and then it's making a black box pulse, this thing that has its own right. And so that's why it may make.
00:33:13.002 - 00:34:14.724, Speaker A: Sense to mix and match rather than just yeah. So basically the polynomial IOPS come with a bunch of costs of their own, right? So the prover is going to obviously the proverb will have to use a polynomial commitment scheme to commit to some polynomial and later prove an evaluation of it. There's no avoiding that. Right? But on top of that, the prover is going to have to compute the polynomial that needs to be committed and then send other messages to the verifier in the protocol. So sort of outside of the polynomial commitment, there's prover time, there's communication, there's proof length messages the prover sends to the verifier, which have to be ultimately in the Snark proof, there's the verifier time to check those messages. So the total cost of the final Snark is like the sum of sort of the cost of the polynomial IOP and the cost of the polynomial commitment. So if your polynomial IOP has, like, bigish proofs, there's not much point to using a polynomial commitment with really small proofs.
00:34:14.724 - 00:34:20.264, Speaker A: If it has worse costs on other axes. That's kind of the reason for okay.
00:34:20.302 - 00:34:27.308, Speaker D: Can I rephrase my question? All of these things, you mean are just sum? They don't interact in a funky way?
00:34:27.394 - 00:34:52.836, Speaker A: No. Yeah. Other than subtleties about how some polynomial IOPS can't quite use some polynomial commitments. And that just boils down to the prover in the polynomial IOP sort of wants to represent the polynomial via certain set of coefficients, and the polynomial commitments might or might not work with those kinds of coefficients. And that's like way in the weeds. We don't have to get into that.
00:34:53.018 - 00:35:00.150, Speaker D: But for the most part, if for some reason I decide all I care about is proof of time, there's the best IOP and there's the best.
00:35:01.900 - 00:35:55.000, Speaker A: But then the desire for transparency and post quantum security can then change which one you have to use other questions. Okay, so at a high level, there are kind of three classes of polynomial IOPS. The details I'll maybe say something about on Tuesday, but today it's not really going to matter. It's just sort of you can design a polynomial IOP sort of based on something called interactive proofs, something called multiprover interactive proofs, and something that I call them constant round polynomial IOPS. And I've roughly listed them just in order of increasing prover costs from top to bottom and decreasing verifier costs. So it sort of makes sense that prover costs and verifier costs are in tension, because if they weren't, then there'd just be something to rule them all. There's something with the best proverbs and best verifier and why everyone would just use that.
00:35:55.000 - 00:36:06.220, Speaker A: So it tends to be stuff with the fastest prover has slightly bigger proofs and somewhat slower verifier, and stuff with the fastest verifier has slower proverb.
00:36:06.720 - 00:36:09.468, Speaker B: Is there a sense that that tension is fundamental or just sort of an.
00:36:09.474 - 00:36:44.112, Speaker A: Artifact that currently wait for two slides? Okay. And then there are four classes of polynomial commitment schemes. I'm going to ignore the last one because it's not practical. So let me just focus on the top three. So the first one is the only one that's not transparent. So this is basically where trusted setups come from in Snarks. And the most popular variant here is due to KZG.
00:36:44.112 - 00:37:19.810, Speaker A: Everyone calls them KZG commitments, Kate, it all. And so I've highlighted it in green. So for each category I've highlighted in green, the commitment scheme that right now is sort of the most popular or widely known. And in each case they have the shortest proofs. And so I think what's happened, the reason the ones in green are so popular is in the blockchain settings, these proofs get stored on chain and they get verified on chain. So people are sort of moving to the extreme of good verification costs. So the ones in green in each category are the ones with the best verifier cost.
00:37:19.810 - 00:38:07.104, Speaker A: So there's another category which is transparent, but it's not post quantum because it's based on the hardness of discrete log, which is a problem quantum computers can solve efficiently. So the most popular one here is probably bulletproofs, maybe some of you have heard of it. So basically KZG like evaluation proofs are constant size. That's like three group elements or something, not three anyway, some constant number of group elements. Bulletproofs is log number group elements. And then the last category is both transparent and plausibly post quantum. And the most popular one here is Fry fast read Solomon interactive Oracle proof that stands for and the proof length there is like log squared times the security parameter or something like that.
00:38:07.104 - 00:38:21.220, Speaker A: So I've listed the three categories also in sort of this time in increasing order of proof length. So constant size, evaluation proofs, log size, polylog.
00:38:23.880 - 00:38:26.724, Speaker D: Question, is that logging?
00:38:26.772 - 00:39:14.468, Speaker A: What the IFP size? Okay, so in most Snarks, the polynomial committed to is like as big as the circuit and possibly even bigger by a constant factor. And also some of them you're committing to multiple polynomial, the proofs committing multiple polynomials that are as big as the circuit. There's only one category of Snarks. And this gets into how there are these trade offs between proven time and verifier time. Interactive proofs can make the committed polynomial only as big as the witness to the circuit and not as big as the full circuit. But to support general efficient transformations from computer programs to circuits, the witness will have to have these extra advice inputs. And in general, there might be a ton of advice inputs.
00:39:14.468 - 00:39:46.530, Speaker A: So that difference between circuit size and witness size sort of in general is only a constant factor anyway. But there might be some settings where you don't actually need many advice inputs to have a small circuit. And then these interactive proof things start to look really nice for the prover because it's only committing to a tiny polynomial. And that's why these things are so hard to compare to each other, because you get into interactions between the front end and the back end and is the witness big or is it small? Does that answer your question?
00:39:50.210 - 00:40:06.166, Speaker D: For post quantum secure, all that means is that once quantum computers exist, I should no longer use something that's not post quantum secure. It doesn't mean that once quantum computers exist, there's something insecure about things that already happened.
00:40:06.348 - 00:40:34.560, Speaker A: That's right. So basically, a quantum computer could find a convincing proof of a false statement, but if you're confident no one had that quantum computer when the proof was generated, you don't have to worry about it. Another thing I'll say is, if you do care about zero knowledge for all these Snarks, it's the soundness that might be broken by quantum computer. Zero knowledge is perfect. So the quantum computer will not let you violate the privacy of someone 50 years from now or something. So that's nice.
00:40:35.170 - 00:40:36.720, Speaker C: Is that always true?
00:40:37.490 - 00:41:03.110, Speaker A: Essentially. Basically, in theory, like, asymptotically you can kind of choose, do you want perfect zero knowledge? And no, I'm going to get this wrong for succinctness. You're always going to need computational soundness anyway. Yeah, let's just leave it as like every Snark in existence. The way they're made zero knowledge makes it perfectly zero knowledge.
00:41:06.730 - 00:41:13.900, Speaker B: The way you discuss it's almost like the only reason you could do that is if you wanted, if you really cared about post quantum, or is there a different reason you might?
00:41:15.150 - 00:42:21.910, Speaker A: Yeah, when you get way into the weeds, there are some other reasons involving what field your circuit is defined over. So certain computations sort of naturally want to work over some field or another. And if you use one of these two category discrete log, that field kind of has to come with a nice elliptic curve that kind of matches the field in some sense, whereas this IOP based stuff is just based on hashing. So you don't get field size restrictions from that, but you get field size restrictions from other things. Like, most of Fry needs FFTs, so that creates other restrictions on the field size. So way in the weeds of how these work, there are some other reasons you might want one or the other further complicating the comparison between all the Snarks out there. Other questions.
00:42:21.910 - 00:42:46.364, Speaker A: Okay, so now you sort of know the full taxonomy of Snarks. We roughly had three classes of polynomial IOPS, four classes of polynomial commitments. Of course, in each class there are multiple. So you're talking something like even if there were only one inch class, that's already like twelve Snarks. And there's more. And so there's like this commentator explosion. Let me make it even worse for you.
00:42:46.364 - 00:43:15.620, Speaker A: So, Tim, this is your question. Oh, sorry. My next slide is the answer to your question. Yeah. So I want to run through the taxonomy of some of these kind of twelve possibilities and tie them to the name of the systems people are really using. Okay, so the first class was sort of this exception class that wasn't quite based on polynomial IOPS book polynomial commitments, but if you squint, it's very similar. So the most popular Snark in this class is now due to grow 16 because it really has the shortest proofs, just like three group elements.
00:43:15.620 - 00:43:57.292, Speaker A: And the downsides here Tim mentioned in a question, the trusted setup for this Snark is circuit specific. So you change the circuit, you need a brand new trusted setup. It's not post quantum. And the know I would characterize as pretty slow and space intensive, but the verifier is really fast and the proofs are really short. Okay, so there was interesting progress a few years ago that kind of the big benefit was to turn the circuit specific trusted setup into a universal one. So all that matters there is how big the circuit is, not like the specifics of its wiring. And so that's the big benefit.
00:43:57.292 - 00:44:40.800, Speaker A: And so the main examples here are Marlin and Planck. And people are using these a whole lot today. The proofs are bigger, the prover is slower than the already I called slow proof of grow 16. There is a counterpoint, which is the Snarks that are truly based on polynomial IOPS. So everything that I'm going to describe but the top category, they can kind of use more general circuits. So you might wind up in a situation where for a given circuit size, the Planck prover is significantly slower than gross 16, but you can apply the Planck prover to a smaller circuit. And so, like slower prover applied to smaller circuit turns out in some cases to be faster than faster prover applied to bigger circuit.
00:44:40.800 - 00:45:45.424, Speaker A: Things are complicated to compare. Okay, the next category now moving to the transparent Snarks are take any multi proveractive proof or interactive proof and combine it with one of the fast prover polynomial commitment schemes, which of course tend to have bigger proofs. These things tend to always be intention prover cost and verifier cost. And the benefit of this other than transparency and plausible post quantum security is going to be this is how you get the fastest prover in the literature. But people aren't using these today because the proofs are pretty big and verification costs are very important in the things people are using these Snarks for today. And the final category is like, it doesn't really matter what polynomial IOP you use, but if you use the Fry polynomial commitment, you're going to get the shortest proofs among plausibly post quantum Smarks to be transparent too. And the downside is the prover is pretty slow and the proofs are still fairly large, like lambda that's the security parameter times log squared is bigger than log is bigger than constant in some cases by significant factors.
00:45:45.424 - 00:46:11.212, Speaker A: And I'll say more about that later. Okay, so that's the world of Snarks. If you imagine world like non blockchains where your phone is having a Snark like a server, something like Snark three is the most useful, where your phone can prove anything, right. Between the server doesn't care about circumstances. Yeah. And then you might want to choose a snark that people aren't really using today. Okay.
00:46:11.212 - 00:46:48.404, Speaker A: And then finally to get to Tim's question, so this is sort of ongoing work. So I don't have a specific result to tell you. So I'm just going to say it's what I consider a very promising approach to try to resolve the tension between prover time and verifier time. And it's not even a new idea. It's very simple. It's known, but the devil's in the details of, like, will it actually concretely be nice? So the idea is to take a fast prover, but kind of big proof, maybe slow verifier snark, and compose it. I'll say in a moment what that means with a small proof snark.
00:46:48.404 - 00:47:37.252, Speaker A: So what this means is the prover, like, in its own head, is going to use the fast proofer snark to compute a proof, but it's going to be a pretty big proof. It's not going to want to send that proof to the verifier. Instead, it's going to use the small proof snark to prove that it knows the big proof and send that proof to the verifier so that proof is small and can be verified quickly. But as long as the big proof snark, I say it's big proof. That's sort of a relative statement. As long as the verifier of the big proof snark is, like, sublinear in the size of the original statement being proven, computing the small proof. What you're doing is you're turning this verifier into a circuit and applying the small proof snark to that circuit.
00:47:37.252 - 00:48:03.200, Speaker A: So as long as this verifier is work saving, that circuit is going to be smaller than the original circuit that the fast prover was applied to. This might be hard to grok online, but in a sentence, in principle, what this does is it makes the prover time dominated by the fast prover prover time and the verification cost dominated by the small proof. So you get the best of both worlds.
00:48:04.740 - 00:48:26.520, Speaker B: Concrete parameters would help. Right. So like fast proof or big proof that could be like linear time proof or square root proof size or something. Or square root, like verification time or something like this. Right. And then meanwhile, maybe the small proof slow proof, or maybe that's a quadratic proving time, but you're invoking it on the sort of square root size proof. So that's just another linear term.
00:48:27.660 - 00:49:07.696, Speaker A: Yeah, so that's exactly it. Except in practice, quadratic prover would not be that bad. It'd be like quasilinear proverb with just like, gross constant and log factors. But square root is exactly right. So the fast proverb big proof, those snarks really do have square root size proofs. So, like, quasilinear in square roots is still smaller than linear in yeah. The reason why this is still ongoing work is because doing this requires taking the verifier, the first snark writing it down as a circuit and then applying the prover of the second Snark to that circuit.
00:49:07.696 - 00:49:56.758, Speaker A: And that's just like a nasty thing to do. People are doing things like this all over the place. But my guess is just the groups doing that are just not very familiar with the Snarks, with the fastest prover. So I think that this very simple, just one composition of two Snarks actually is going to wind up yielding very soon. Snarks with very nice cost profiles. They're not going to literally dominate on every axis, but I just think there'll be very nice profiles that might be attractive even in the roll up context. Very recently there was a nice workout that sort of there are lower bounds with caveats.
00:49:56.758 - 00:50:41.910, Speaker A: So you can sort of prove that without using certain kinds of cryptography. You can't hope for constant proof size. You can prove things about the kind of cryptographic assumptions you need to base your Snark on asymptotically speaking. The Snarks we do have are really good. We have Snarks where the prover runs in linear time, can't hope to do better. We have Snarks where the Verifier runs in constant time, but there's an SRS or logarithmic time, but there's no SRS. So the lower bound you could hope for is to say like a transparent star can't do better than logarithmic time for the Verifier.
00:50:41.910 - 00:50:49.066, Speaker A: And I'm not aware of a result of that, but I don't does that.
00:50:49.088 - 00:50:58.286, Speaker D: Make it does, but if as stated, this could potentially do better than all.
00:50:58.308 - 00:51:00.686, Speaker B: Of the things it was just before.
00:51:00.868 - 00:52:03.250, Speaker A: So yeah, so if you don't care about logarithmic factors and practical performance, you don't need composition to get everything you could ever want if you care about the difference between log and constant. So you really do care about for the Verifier. Yeah, I don't even know if composition is going to get you there without a trusted setup. So I mean, for this slide to be really a statement where you care about log factors and constants and concrete performance and yeah, people are composing Snarks today, but basically they're not using MIP and IP based Snarks that have the fastest prover in that composition. And I think essentially if they did, you get like new cost profiles that would be very attractive.
00:52:04.390 - 00:52:09.766, Speaker D: And then how does the transparency and post quantum security compose if you were to do this?
00:52:09.868 - 00:52:34.080, Speaker A: Great. So if either one of the constituent starks is not transparent, the result won't be transparent. If either is not post quantum, the result won't be post quantum. If they both are, then transparency will be preserved and post quantum is sort of heuristically preserved. But that's not a formal statement. So like plausibility is going to be preserved, I think is fair to say.
00:52:34.610 - 00:52:37.706, Speaker D: That'S because plausibility is an informal term.
00:52:37.818 - 00:53:44.750, Speaker A: Yeah, means like we you're not I would be surprised if you came up with a non contrived example anytime soon. That broke that where they're both independently post quantum and you compose them and you can show the result is not post quantum. I'm not an expert on formal post quantum security proofs, but if you came to me tomorrow and said, here are two natural Snarks that are both quantum but their composition isn't, I'd be like, wow, by the way, when you compose known transparent Snarks today, the security is like even classically is going to be heuristic. This is in the weeds of crypto, but without the composition, you could unconditionally prove them secure in something called the random Oracle model with practitioners are quite comfortable with. But if you want to compose them, you're going to lose security in the Random Oracle model. So security is already going to be heuristic anyway, and I'm very comfortable with that in practice and I think all practitioners are. I mean, they're out there composing Snarks today.
00:53:44.750 - 00:53:51.660, Speaker A: So, yeah, this whole plausibility with post quantum is sort of no worse than the situation. Even with classical security and composition.
00:53:53.440 - 00:53:57.810, Speaker B: Is there an easy intuition for why composition can mess up random Oracle based security?
00:53:58.180 - 00:54:38.832, Speaker A: Yeah, so you have to represent the Verifier as a circuit to then apply the next Snark to. So in practice, the Random Oracle gets replaced with the cryptographic hash function, so the circuits have to compute the cryptographic hash function, like in the circuit. Cool. Okay. So I really was hoping to be able to come here and be like, look at my exciting new Snark, but I couldn't. So I'm just going to say I really hope that this leads to something. Okay, so now we get to the final part of the talk.
00:54:38.832 - 00:55:02.330, Speaker A: Looks like I have 30 minutes, is that right, Tim? Yeah. Okay. Roll ups. Okay, so let me tell you what the context is for roll ups and then how they work. So the idea is that public blockchain should be verifiable by weak nodes. You hear from the Bitcoin community, they'll take this to an extreme. They'll say if you're not running your own node, you just can't trust anything.
00:55:02.330 - 00:55:58.040, Speaker A: So you want sort of verifying the blockchain to be doable by certainly a laptop, maybe even a Raspberry pi. If you talk to the really people who are extreme about this, a less extreme view is like if you need an Amazon server to run an Ethereum node, you can't call Ethereum decentralized at that point. So we ideally want it to be possible for weak devices to verify that all transactions that have been processed by the blockchain to date are valid. Something like that. Okay, so there are two costs to running any blockchain node. The first is that they have to do computation to validate transactions and in a little more detail, like for basic transfers and swaps and things. I think this computation is mainly just like checking digital signatures and making sure people don't overspend their accounts.
00:55:58.040 - 00:57:02.520, Speaker A: But in general, on Ethereum smart contracts, the nodes might have to execute arbitrary EVM bytecode and just a little more information about that if you're not familiar. So smart contracts and Ethereum typically written, I believe in solidity, it's a high level language and then compiled to something called Ethereum virtual machine bytecode, EVM bytecode, and the Ethereum nodes execute that bytecode. So in principle, these smart contracts can do like arbitrary computation. So the nodes kind of have to be prepared to do arbitrary computation. But then I think my understanding is a lot of the volume on Ethereum is just people sending each other money and stuff and that's very simple transactions, not doing any crazy computation inside them. So in addition to the computation nodes have to do, they have to store all the data that live on chain with some caveats about light clients and full nodes. And I myself am not super well versed in those differences, but conceptually, yeah, you've got computation and you've got storage.
00:57:02.520 - 00:57:49.710, Speaker A: Those are the two costs. Okay, so, yeah, let me just call the data stored on chain like the state of the world. And for simplicity, you can just think of them as all Ethereum account balances, just if you want to make things concrete. Okay, so the very first idea of a roll up is just to treat the blockchain, or really the blockchain nodes, as computational weak Verifiers and the roll up service as an untrusted prover. And this idea alone can save the Verifiers work. So rather than checking the digital signatures on a ton of transactions directly, you could imagine some proof gets posted to the blockchain that all the digital signatures are correct, and then the Verifier just has to trust the proof, not check each one individually. So this is great, this will save the blockchain nodes work, but it doesn't directly reduce the amount of data they have to store.
00:57:51.600 - 00:57:55.820, Speaker C: You can compress all the widgets probably like you can get rid of all the signatures.
00:57:56.180 - 00:58:14.544, Speaker A: Yeah, so we're going to get to that as a second idea, I think. Okay, you're saying instead of putting digital signatures with each transaction, put a proof of knowledge of a digital signature with each transaction.
00:58:14.592 - 00:58:20.732, Speaker C: Yeah, if you have one proof that you know, signatures for like, transactions in a block, that'd be safe.
00:58:20.816 - 00:58:42.620, Speaker A: Great. Yeah, so true. And I think that's another way of phrasing what I'm going to call the second idea. If you disagree, let me know and we'll talk more. Okay. Oh, by the way, from now on, in case you haven't seen this terminology before, we're going to call the blockchain like Ethereum. We're going to call that an L one or the chain and the roll up, we're going to call that L two, layer two.
00:58:42.620 - 00:59:25.084, Speaker A: Okay, so here's the second idea for roll ups. This is to try to address the storage issue. And again, I think Joe just said this more or less. So we're not going to store the state of the world on chain, we're only going to store a cryptographic commitment to it. And you can think of that this is often just like a single hash value, 256 bytes or something. So the binding nature of the commitment scheme will now ensure that once the commitment is on chain and everyone agrees on it, no one can alter the data that's committed to. So if someone comes and says, look, here's a piece of data that's consistent with this commitment and here's the authenticating information that kind of establishes this consistency claim, the cryptography is ensuring that their claim is true.
00:59:25.084 - 01:00:29.880, Speaker A: And indeed, this is the data that was committed to by the on chain commitment. So the key conceptual consequences of this is you no longer need to rely on the consensus mechanism of the blockchain to ensure that no one is messing with the data that the blockchain is really representing. Like, as soon as there's a cryptographic commitment to the data on chain, the binding nature of the commitment scheme means no one can go and alter the data. This raises the issue of now the data is only committed to it's not just like stored on chain. So you can ask like, well, what if someone in the future needs to access some of the data that used to be on chain and now it's only committed to? This is referred to as data availability issue. And there are sort of two approaches people are pursuing to this. I guess it's actually, my understanding, is a rich design space, but at a high level the two approaches are like store it on chain anyway or use economic incentives to get off chain entities to store the data and make it available.
01:00:29.880 - 01:01:22.102, Speaker A: So as far as storing it on chain anyway, it can live in cheaper storage called call data. This is like the Ethereum terminology, which my understanding is this is only stored by full nodes. There are definitely people who know more about this than I do. And moreover, Ethereum is considering changes to the protocol itself to make this sort of semi on chain option more scalable. So I know they're considering letting call data expire after a year and then sort of move to kind of the second option for kind of stale call data. If they implement Sharding, which is going to happen sometime after the merge, it's kind of late in the game plan that will increase the amount of onstain storage. It'll do something like, I'm probably going to say this wrong, but there's something like multiple sort of related but blockchains getting stored out there.
01:01:22.102 - 01:01:54.850, Speaker A: Each Shard is sort of kind of like its own chain. I shouldn't have said anything because I'm probably saying it wrong anyway. It's going to increase the amount of on chain storage. But if we're going to stick with this on chain option, it will sort of put an upper bound on the possible benefits of roll ups because no. Matter how fast the Verifier is for verifying the proofs, the dominant cost will just be like storing all this stuff on chain. That is not actually part of the proof, it's just the data that's kind of committed to has nothing to do with how efficiently the proofs are verified.
01:01:58.550 - 01:02:20.278, Speaker D: Like a very minor user of Ethereum, can I feasibly just store like, I don't know, I'm interested in my own account balance, maybe like one or two smart contracts. Can I just store that information myself or am I going to have problems if I don't know some local state when I try and spend my money or interact with you?
01:02:20.364 - 01:02:34.660, Speaker A: So I think if you send money to someone else no, that might be, that might be okay.
01:02:42.400 - 01:03:01.264, Speaker C: Basically you can always rely on some random server to give you the data and sort of pay for it. But even like receiving a transaction from somebody else, if you want to verify the transaction you need to know something about their state. You need to know their transaction counter.
01:03:01.312 - 01:03:01.910, Speaker A: And.
01:03:03.800 - 01:03:11.210, Speaker C: It'S a bit tricky to validate anything if you don't really have all the state. You either need all the state or you need to talk to someone about.
01:03:13.680 - 01:03:17.010, Speaker D: Even like just for you to send me money. You and I.
01:03:19.860 - 01:03:21.216, Speaker A: Receiving money is more.
01:03:21.238 - 01:03:32.380, Speaker C: Complicating verifying that someone gives you a transaction. Yeah, we should talk more about this.
01:03:36.610 - 01:04:11.798, Speaker A: It's a great question and yeah, this roll up part of the talk. I did my best to become well versed in the whole idea of roll ups, but this whole data availability stuff is not directly related to Snark. So keep that in mind. Let's put the two ideas together. Sort of. The first one was about saving computation, the second one was about keeping less data on chain or keeping it in call data instead of in the more expensive parts of on chain storage. So a first attempt at how a roll up would actually work is sort of at all time.
01:04:11.798 - 01:05:03.210, Speaker A: The l one is going to store a commitment to the state of the world that's like all of the data that used to live on chain and the roll up will take a batch of transactions and sort of apply it to the current State of the World to get an updated State of the World and then post a commitment to that updated state on the l one. And the problem is just how does the l one know that the new commitment is correct and basically the Snark is going to prove that. So the roll up will also post to L one a proof that sort of the updated commitment followed by applying a series of valid transactions to the old state of the world and everything was valid and the new state of the world produced this new commitment. And then note that the Snark proof would be verified on chain. So like every blockchain node in the world will have to run the Snark Verifier.
01:05:05.860 - 01:05:09.388, Speaker B: So the transactions executed are part of the witness?
01:05:09.564 - 01:05:21.140, Speaker A: Yes. As is any authentication information, like merkel authentication pass for any pieces of data that the transactions need to know to be applied.
01:05:22.120 - 01:05:30.760, Speaker B: You're not focusing on the current paradigm where transaction descriptions are posted as call data. You're actually thinking maybe that's off chain too, right?
01:05:30.830 - 01:06:17.928, Speaker A: Potentially, yeah. Or like you're agnostic and you can run into situations where if multiple transactions in batch hit the same account then you only need to provide authentication information kind of once for they're all hitting the same leaf of the merkel tree. So you only have to do one lookup to the initial account balance at the start state of the world. So you get savings. So the bigger these batches are, kind of the more that happens. And so you get extra savings because certain accounts might be hit many times. So there are incentives to make the batches big beyond the fact that we'll get into the main incentive to make batches big soon.
01:06:17.928 - 01:06:32.940, Speaker A: Okay, so if everyone is sort of clear enough on how roll ups work and what they're accomplishing, we can talk about their performance. Anyone want to ask other questions? Not too bad on time. I'm only slightly worried.
01:06:35.600 - 01:06:58.070, Speaker D: Sorry to go back here, say that you wanted this property that you and I can trade money without knowing the entire state. I don't know a lot about detach, but presumably that's feasible to do on detach because no one's supposed to know the entire state anyway. Is it like.
01:07:00.360 - 01:07:46.244, Speaker A: I'm trying to think of this in my head to provide authentication information for even just you and I, our accounts, there is information about other not people you need to know. Or you got this big hash tree. I haven't told you what merkle tree is, but it takes all the account balances and it pairs them up and hashes them and then pairs those up and hashes them. And the authentication information is like kind of all of the hash values on a path from your account leaf to the root and my account leaf to the root. So you need to know more than just that. You have x ethereum and I have y ethereum. It gets pretty hard to not know.
01:07:46.282 - 01:07:53.030, Speaker D: The state of is something fundamentally different than that or just also during.
01:07:53.880 - 01:07:55.364, Speaker A: Joe, do you want to yes.
01:07:55.402 - 01:08:22.556, Speaker C: I think the difference the definition of a transaction is like the proof that a transaction is valid relative to some kind of, like, state snapshot the whole thing's like wrapped in the dark. And in ethereum, people think of a transaction as just like a signed statement and then to verify that, you need to go check it against the state. But you could sort of expand the notion of a transaction ethereum to also include merkel path to prove that the.
01:08:22.578 - 01:08:23.150, Speaker A: Sign.
01:08:26.190 - 01:08:38.340, Speaker C: Coin in you could redefine what you mean by Ethereum transaction a little bit to make it as verifiable as a Zcash transaction, you just have to add some extra stuff.
01:08:43.610 - 01:09:08.014, Speaker A: And just because Zcash is zero knowledge, private, whatever, doesn't mean that necessarily that people don't need to store a lot of state. It's just the state isn't leaking anything about anyone's data. And I'm not saying that people in Zcash do have to store a lot of state, but it doesn't inherently mean.
01:09:08.052 - 01:09:26.580, Speaker C: That they in practice, they do compose transactions are not non zero knowledge transactions, which they do. If you forked Vcash and had only some shielded transactions, then like for this.
01:09:32.380 - 01:10:25.640, Speaker A: Okay, so let's talk about roll up performance and how we would even measure that. I'm an expert on Snarks, but I'm not specifically roll up. So as an outsider to the space, it can be a little challenging to sort of understand how people are measuring performance and kind of the states of various projects that are they're very actively developing their capabilities. So tracking it can be difficult. So I'll give you an example of just some language that I think can be confusing if you haven't seen it before. So here's a proof of a Tweet of Starkware. They said we had 3000 transactions per second, all 300,000 transactions in one proof, and it took us six minutes, 3 seconds.
01:10:25.640 - 01:10:42.030, Speaker A: So I don't know. Does anyone want to suggest what is a natural interpretation for that? Like what that means? So what does 3000 transactions per second and six minutes total mean?
01:10:44.620 - 01:10:52.100, Speaker D: They produce a proof in six minutes and 3 seconds. It works for 300,000 transactions.
01:10:52.280 - 01:11:16.164, Speaker A: Yeah. So that's what I thought as I was trying to kind of understand the space. Turns out that would be a little fast. Like, here's another example, this one from ZK Sync. They're talking about 20,000 transactions per second. So that's even faster than 3000 transactions per second. As far as I can tell.
01:11:16.164 - 01:11:44.216, Speaker A: This is not talking about proof generation. So there's a difference between latency and throughput. So this becomes very natural, kind of once I tell you what I'm pretty sure the role of companies are thinking. But if you haven't seen it before, it's not so natural, I think. So latency refers to the delay between a transaction posting to L Two and a proof capturing it, hitting L One. And this is determined by two things. So, like the time to compute that proof.
01:11:44.216 - 01:11:57.796, Speaker A: But also if your volume on L Two isn't that high, you're going to have to wait around until you have enough transactions that you're then comfortable generating a proof and posting that to l One. Otherwise you're going to have too many proofs on L One, and if your.
01:11:57.818 - 01:12:01.030, Speaker C: Volume is too high, have some congestion issue, which.
01:12:03.000 - 01:12:11.850, Speaker A: If your volume is too high yeah, I don't even know what you do sort of if your volume is way too high, like how you would keep up.
01:12:13.740 - 01:12:15.050, Speaker C: Maybe it's just like.
01:12:17.420 - 01:12:17.736, Speaker A: Yeah.
01:12:17.758 - 01:12:20.360, Speaker C: Over time, but it could add a lot of expected.
01:12:20.700 - 01:12:52.884, Speaker A: Yeah. So we maybe want to actually discuss this after we get to the other notion of performance, which is throughput, which is probably exactly what you have to think about with transaction volume. Okay. So throughput is the number of L two transactions that can be processed in a given amount of time. And this is what I think the Tweets I just put up are really talking about when they talk about transactions per second. And as I'm going to explain, technically this has nothing at all to do with provertime. So this is what I'm pretty sure they have in mind.
01:12:52.884 - 01:13:26.844, Speaker A: So suppose one can upload a proof capturing X transactions and it takes Y seconds to upload the proof. So forget about how long it takes to compute the proof. It's what you have. It takes Y seconds to upload. So you can do something called pipelining. So what's going to happen is every Y seconds you're going to have collected a batch in the last Y seconds, you ship it off to the prover to start chewing on, and at the same time, some proof is going to have just finished being proved. You now have this new proof.
01:13:26.844 - 01:13:49.176, Speaker A: You start uploading that to the L one. So kind of in steady state. This is getting kind of x transactions are kind of getting proved to the L one every Y seconds. Okay. But it says absolutely nothing about latency. Like the prover could have taken a year. Just after a year, this pipelining has started posting switch to the L one.
01:13:49.176 - 01:14:22.144, Speaker A: And from that point on, they're year old kind of transactions, but they're getting a very high rate on L one. Throughput is what determines blockchain scalability, meaning how many transactions are kind of getting summarized on the chain every second. But latency demands will ultimately constrain the batch sizes. Like people aren't going to wait a year. Just as an extreme example, and I think I find when some of these roll up projects are talking about their performance, they know throughput is blockchain scalability. That's what we care about. So they'll talk about it's very natural that's what they care about.
01:14:22.144 - 01:15:10.304, Speaker A: They talk about throughput, but there's just no mention latency at all. And that can be very confusing when you just see these things without that context. So this past week I've been trying to learn about this stuff and I had to puzzle this out. So hopefully I'm saying this correctly. On the part end side, if you have approvered the first ten transactions, can you pipeline that and add on new transactions, start all over again when you're adding new transactions? Yeah. So we were talking about this. If you have a certain kind of proof system that supports a functionality called incrementally verifiable computation, you can do something like this where you can kind of prove a bunch of transactions in the proverbs head.
01:15:10.304 - 01:16:01.650, Speaker A: Right. And then more transactions come in and it kind of updates the proof to incorporate them and kind of whenever it's ready you can kind of stop and post the running proof to the blockchain. But as far as I know the current blockchain projects are not supporting that functionality does that make sense? Yeah, but I mean we know how there's a lot of interest in that functionality like they could be supporting it. There are protocols known that support incrementally verifiable computation. Yeah, I do want to mention some counterpoints to this focus about on latency which is alternatives to these validity roll ups do have high latency. So optimistic roll ups my understanding is right now they're sort of waiting around seven days giving people a chance to identify fraud. So you don't really get finality on L one for a week.
01:16:01.650 - 01:16:41.870, Speaker A: The non blockchain solutions like Visa, it looks like your transaction went through right away but it takes like 48 hours or something to settle. So people are tolerating pretty high latencies now in some contexts they don't mind and even with the roll ups it might not be the end user that's taking on any risk of latency. It could be that a centralized exchange is the one interacting with the roll up and they're going to promise users finality and if something goes wrong it's the exchange that's going to be out a whole bunch of money or something like that. So if there is high latency, people might tolerate a really high amount of latency in certain contexts anyway. Yeah.
01:16:43.300 - 01:16:46.944, Speaker B: Can we close the loop on the Tweets like to 3001?
01:16:47.062 - 01:16:47.552, Speaker A: Yeah, sure.
01:16:47.606 - 01:17:14.552, Speaker B: I want to make sure I have points. So are you saying that what this take the one on the left. So is it implication here? That 3000 /second is what like the L two infrastructure could handle? Presumably. Are they claiming that actually if L two throughput was saturated they would be able to generate proofs sufficiently quickly to keep up with that?
01:17:14.686 - 01:17:50.150, Speaker A: I think that's right. So actually I emailed Starkware about this and some other things which I'll get to and my understanding of what they told me was that this is counting upload time. Six minutes is upload time. I don't know how to explain actually 300,000 over 300 seconds is 1000, not 3000. So I don't know about that. This is upload time is my understanding of what they said. And then I sort of have puzzled out this whole throughput thing as the reason that this makes sense to just count upload time and approver time.
01:17:50.150 - 01:18:16.056, Speaker A: Right. So I do not know how long the prover took here but the idea is that's going to determine latency assuming there's enough volume anyway that they're not waiting around for batches to fill. But this pipelining thing sort of means if you forget about latency sort of doesn't matter for throughput with this pipelining.
01:18:16.088 - 01:18:31.344, Speaker B: Thing the failure mode would be like 1000 transactions come in, you start generating proof for it. And then in the meantime, let's say you only have one machine generate proofs supposed to be, and then while you're generating proof for the first 1000, 3000.
01:18:31.382 - 01:18:32.752, Speaker A: New transactions come in.
01:18:32.886 - 01:18:37.520, Speaker B: Then you approve for them, but then 4000 transactions come in while you're doing that proof.
01:18:38.580 - 01:18:40.416, Speaker A: Yeah. You don't keep up. Yeah.
01:18:40.438 - 01:18:44.852, Speaker B: So I'm saying, is it an implication of this that they're claiming they can keep up?
01:18:44.906 - 01:19:19.330, Speaker A: I guess, yeah, I guess so. They were tweeting about some sort of proof of concept, if you will, for Reddit. Reddit had like a competition for Rolo projects. I don't know the details. So there's some context here of this tweet that I'm not getting into and don't necessarily know all the details of. And I will say I have a slide that we're probably going to run out of time for. But various kinds of composition of Snarks can help address some of these issues potentially, where maybe I should just jump to that real fast.
01:19:20.420 - 01:19:26.976, Speaker B: Prover time as a function of the number of transactions, as a function of the batch size, is it sublinear in the batch size?
01:19:27.078 - 01:19:37.050, Speaker A: Prover time, prover time. I mean, you have to at least read all the transactions in the batch. So linear in the prover time can't possibly be sublinear the.
01:19:40.300 - 01:19:57.180, Speaker B: Computational cost per transaction. You could imagine that being decreasing in the batch size. Otherwise I'm confused. Otherwise it seems like you're going to have a limit on how big your batches could possibly be. If you hope to keep up, you.
01:19:57.330 - 01:20:00.044, Speaker A: Might yeah, that limit on batch size.
01:20:00.082 - 01:20:06.130, Speaker B: Might be well less than the sort of throughput that the L Two infrastructure. You see what I'm saying.
01:20:08.900 - 01:20:55.120, Speaker A: Here? Let's jump to something here is how you might use kind of proof composition to address some of these issues that you're raising, I think. So imagine you had I'm calling these subblocks, but you could think of them as just multiple different blocks. Okay. So block one comes in. You ship it off to the proverb to compute a proof in the new commitment. As soon as you have the new commitments, you can start processing a new block. You don't have to wait for the proof to be done to start working on the next block of batch of transactions.
01:20:58.900 - 01:21:06.576, Speaker B: I think, actually, I mean, I didn't phrase my question better way. Imagine that the prover time was like a plus BX, where X is number.
01:21:06.598 - 01:21:07.956, Speaker A: Of transactions or something like that, where.
01:21:07.978 - 01:21:16.144, Speaker B: A is big and B is small. So the hope would be that B is so small that actually you could keep up with the L Two infrastructure.
01:21:16.192 - 01:21:16.596, Speaker A: Okay.
01:21:16.698 - 01:21:23.050, Speaker B: And then the point is just you need the batch size big enough so that the per transaction approval cost is approaching B.
01:21:24.780 - 01:22:17.970, Speaker A: So that's exactly right. And then the problem is that the Snarks people are using today, the approver time is like n log n. And so you're right, if you take the batch size big enough, I mean, it's super linear. Runtime for the per transaction. Prover time is getting worse, not better. But while I have the slide up, let me point out something that can help kind of improve latency, even if the provers are slow, is Snark composition. So you could imagine sort of breaking the block up into much smaller the batch up into much smaller batches, have the prover chew on each batch in parallel.
01:22:17.970 - 01:23:16.070, Speaker A: Now, because they're small batches, if you were to post each of these proofs to the blockchain, that'd be no good. That's a lot of gas. Okay, so instead you can aggregate the proofs. And this is very similar to the Snark composition thing I talked about for resolving the conflict, but the tension between proverb and verifier time. So this proof is going to prove knowledge of these proofs, each of which would have convinced the roll up verifier to accept, given the preceding don't touch the same thing. No, it's okay if they do. This proof would establish that Pi two would have convinced the verifier that commitment C two followed from commitment C one by applying some batch of transactions.
01:23:16.070 - 01:24:15.298, Speaker A: The proverb to start chewing on this batch needs to know C one, but does not need to know Pi one, and it'll know C one quickly because that's just like Merkel hashing. But Pi one is what takes like a long time to figure out. Yeah. And that lets you sort of rather than waiting for the whole giant thing to fill up and then having like a super linear time prover chew on like a giant batch. So that's kind of bad for two reasons. And this is getting similar to IVC, and it's not exactly IVC. With this incremental thing, you can get the provers going on smaller batches in parallel, and I think this will improve some of these issues in practice, but there's a limit to it because representing these Snark verifiers as a circuit is going to be nasty.
01:24:15.298 - 01:24:40.378, Speaker A: And that's going to mean that the number of sublocks really can't get that big. While I'm talking about this, it looks like I'm going to pick up where I left off. I'm going to finish this talk next time, but I might as well finish talking about this now. So several projects are doing this. ZK Sync has been doing this for a while, my understanding is. So they use plonk. I saw a blog post from 2020 that said they were doing this with plank.
01:24:40.378 - 01:25:01.974, Speaker A: So it's been a while. They were actually doing this initially, according to the blog post, not for through latency reasons, but they were using a structured reference string that was just not that big. And so if the blocks got too big, their circuit would need to be too big. They just couldn't prove. So that is Joe. I said I'd mentioned at least some issues with SRS later. That's an example of one.
01:25:01.974 - 01:25:42.340, Speaker A: They found a way around it, obviously. So just much more recently, Starkware and Polygon Zero are doing this with Fry based Snarks, and I don't really know what the performance is. I think it'll buy them some levels of speed ups due to the parallelism it's unlocking is my guess. But like the Fry verifier, like I said, the proofs are pretty big. So I think that the number of sublocks is going to have to not be super giant because you're just going to the statement being proved to aggregate the proofs is just going to get real nasty to my guess. Yeah.
01:25:42.810 - 01:25:54.310, Speaker C: Question about parallelism. Aren't there techniques to generate proofs using parallelism that goes short of actually like full recursion or aggregation?
01:25:54.810 - 01:26:29.662, Speaker A: Yeah. So a lot of proof systems can be parallelized. Most of these do a giant FFT, which is kind of nasty to parallelize. You can do it. So this is kind of a way to sort of more automatically do the parallelization. And then the other thing is, a lot of these Snarks, when applied to big statements in a non recursive fashion, very space intensive for the prover, and the proverb will just fall over because it's out of memory. Unless it's like on Starkware, at least old work, they were using terabytes of Ram.
01:26:29.662 - 01:27:06.400, Speaker A: I don't know what they're doing now. So they're just basically gross cost to having giant blocks. So this can help those costs in various ways. N log n is nicer if n is smaller because that log n factor is smaller. So, yeah, you can get parallel speed ups and people are without doing this. Some are also talking about I know Starcore is talking about now having an l three. So it's like these proofs are getting posted to l two, and then these proofs then get pi is getting posted to l one.
01:27:06.400 - 01:27:38.940, Speaker A: I don't know exactly the details of the benefits of that, rather than just kind of waiting around to post Pi to l one. I guess having these on l two is I don't know. So these are things people are thinking about. Yeah, exactly. Web three and l three, I've seen mention of l four, so I don't know why I think if they want ZK, maybe there's a reason to go to l four. So, yeah, the numbers will get bigger. I saw this as people talk about web five, but I think that was some kind of joke I didn't get the details of.
01:27:38.940 - 01:27:58.200, Speaker A: Okay, so I'll finish up the rest of this presentation next time and then tell you some more technical details of how these polynomial IOPS and polynomial commitments are designed. But yeah, thanks for all your questions. It's really good that I ran out of time because it means the questions were good and yeah, I'll say more next week.
