00:00:07.440 - 00:00:07.990, Speaker A: You.
00:00:10.200 - 00:00:40.430, Speaker B: Very pleased to introduce Justin Thaler, of course, one of our own research partner here at a 16 Z crypto, also a professor at Georgetown. He's going to be giving two talks. So today is going to be part one, which will be more of the kind of high level overview and kind of why you should care. And then tomorrow we'll have part two, which is more about how things actually work under the hood. It's very exciting work, sort of new proof systems that he's been working on with a number of collaborators. So lasso jolt and the lookup singularity. And so, Justin, I'll hand it to you.
00:00:41.280 - 00:01:04.048, Speaker A: Thanks, Tim. Yeah. So this joint work with Srinath SETI riyadh Wabi, Arasu Arun, Sam Ragazo and Michael Zhu. Arasu was an intern here last summer, and Sam and Michael are engineers here. Yeah. So this is a talk about something called Snarks. And I'll start by giving a brief survey of Snarks.
00:01:04.048 - 00:01:49.120, Speaker A: Kind of I start most of my talks on Snarks with this survey, kind of keep it updated as the ecosystem evolves. But then I'll dive into sort of a topic that's more specific to this talk, something called Lookup arguments. Because today I want to tell you about a new family of lookup arguments that we're calling Lasso. And then I'll describe something called a front end, a new front end technique, which we're calling Jolt. That, of course, builds on the new lookup argument, Lasso. So obviously I'll say in much more detail shortly what is a lookup argument? What is a front end? And all these terms will make sense. Okay, so let me start with what is a snark? So, in a snark, an untrusted prover claims to know a witness satisfying some property.
00:01:49.120 - 00:02:54.036, Speaker A: So just to have a really simple example in mind, the prover might claim to know, like a pre image under some cryptographic hash function of a designated output. So maybe, like, the proverb and verifier have agreed in advance that they care about, say, the Shaw Three hash function, and the prover is claiming to know just w that hashes to a certain string, y. Okay, so the trivial proof that the prover indeed knows the witness that it claims to know is just to provide the witness to the verifier, who can then just directly check that the witness satisfies the claim property. So in this example, the verifier would just evaluate Shaw Three on W and confirm that the output is whatever the prover says it should be. Okay, so a Snark is just a cryptographic protocol that achieves the same effect, but with better cost to the verifier. So ideally, in a Snark, the proofs will be short and fast to verify. So Snark stands for Succinct non Interactive Argument of Knowledge.
00:02:54.036 - 00:03:43.124, Speaker A: And Succinct is sort of just a technical term for short proofs. And at least I personally use Succinct to mean kind of any non trivial amount of shortness. So as long as the proof is shorter than the witness itself, like the trivial proof is the witness. So I call any proof shorter than the witness succinct, okay? And ideally checking the proof will be fast. Again, I would consider anything that's sort of faster than what's required to check the witness directly for validity to be nontrivially fast. And I'll use the term work saving to clarify that the proofs are not only short, they're fast to check. Okay, non interactive just means the proofs are static so they can be posted to a blockchain instead of requiring some kind of interaction between the prover and the verifier and argument of knowledge.
00:03:43.124 - 00:04:34.776, Speaker A: Just means that the prover can't actually find a convincing proof of a false statement unless the claim it's making is actually true. Or unless it can break some crypto system or something like that. Okay, so let me say a little bit about how Snarks are designed at least aspirationally, and then I'll say a little bit more about how they're kind of actually designed today. So ideally some kind of a developer would write a computer program in a high level language like Rust or C or whatever high level language you want to use. And I'll refer to this as kind of the witness checking program. So you think of this computer program as taking as input the purported witness and just checking that it satisfies the property the proverb claims it should. So in the little example from before, the witness would take as input W and evaluate Shaw Three on it and confirm that the output is Y.
00:04:34.776 - 00:05:39.592, Speaker A: Okay? So that's sort of the direct witness checking program. Okay? Now, in most Snark tool chains today there's sort of a two step procedure to actually apply a Snark. So first you apply something called a front end, which takes that computer program that the developer wrote and turns it into an equivalent representation in sort of a much lower level computational model. Typically some analog of a circuit, like an arithmetic circuit is typically what the Snarks like to use. So you can think of the circuit as sort of an implementation of the computer program on some very low level hardware, kind of even more simple and lower level than like the hardware run on your laptop. These circuits have very, very simple operations that they can do. And the reason to go through the circuit is because the circuit is so simple in a sense that it's then very easy to design Snarks to allow the prover to prove that it knows a satisfying assignment to the circuit.
00:05:39.592 - 00:06:04.740, Speaker A: Basic question but can every program be written as an arithmetic circuit? Yeah. So the question is can every computer program be turned into an equivalent arithmetic circuit? And the answer is yes. And asymptotically anyway the circuit doesn't need to be too much bigger than the computer programs like Runtime. And I'll get into more details about that's sort of the whole point of this talk is just how true is that statement concretely? Other questions.
00:06:04.810 - 00:06:12.100, Speaker B: We all learned the Cook Levin theorem, like to define empty, completeness and whatnot. But here it's really being used in a positive sense very strongly, right?
00:06:12.170 - 00:06:52.710, Speaker A: Yeah. And these reductions from computer programs to circuits, if you want them to be good enough for practical use, you can't just invoke like Cook Levin, which I guess typically just considers like three sat. The circuits that snark shoes tend to be sort of much deeper than depth two and not just have terms of width three or something. Yeah, sorry, quick follow up question to that. Sorry, I realized I'm suddenly speaking out of the screen or something. Should we think of these reductions as being general across context or are there going to be context specific ones for given types of programs? Yeah. Great.
00:06:52.710 - 00:07:46.310, Speaker A: So we do have general purpose transformations that can take any computer program and turn it into a circuit that in principle is not too much bigger than like the runtime of the computer program. Typically for any particular computer program, you could tailor the circuit to the program in a way that's going to get you something and that's always attention kind of generality of the transformation techniques versus efficiency in any particular application. So there's always a lot of work of kind of protocol designers very carefully like tailoring the circuits to the specific statement they want to prove. And yeah, not all systems. Some systems are completely general, others are sort of tailored to the very specific statements in the application of interest. Does that make sense? Yeah, super helpful. Thanks.
00:07:46.310 - 00:09:00.284, Speaker A: Other questions? Probably this is a sidetrack, but on the generality thing is that all NP programs can be arithmetic circuits or all programs? Yeah. Great. Okay. Whether you start with an NP program where there's some non deterministic input or just like a program with fixed input, if you want the circuit to be not much bigger than the runtime of the program, the instance that comes out is really like circuit satisfiability. And what happens is the transformations in general introduce a lot of what are called untrusted advice inputs, where you think of kind of the Snark prover as providing kind of help to the circuit to keep the circuit small. A concrete example is, let's say the computer program needs to divide two numbers together. It's pretty horrible in a circuit to actually implement a division algorithm, but you could have the prover sort of provide like the quotient and the remainder and the circuit just needs to check because it's untrusted advice is the quotient times whatever you're dividing by plus the remainder equal to the original thing.
00:09:00.284 - 00:09:01.550, Speaker A: Does that make sense?
00:09:02.480 - 00:09:16.560, Speaker B: Yeah, I think it's developed on that. Like the P part isn't relevant per se. The computation has whatever time it has poly. Maybe it's more the point is the circuit is going to be as big as the running time or a little bit bigger even than the program time, right?
00:09:16.710 - 00:10:09.590, Speaker A: That's right. Yeah. And you do get into situation some Snarks for circuitsat are really fast for the prover if there aren't many advice inputs, but are not so interesting if there are a lot of advice inputs. So it is worth noting that these general transformation for specific programs, you might not need many advice inputs to keep the circuit small, but in general, actually, the number of advice inputs will be a constant factor of the total size of the circuit. Okay, so just like taking a step back and summarizing the slide, most snarks are deployed in sort of a two step process where first you turn the witness checking procedure into an equivalent circuit and then the prover proves with something called a Snark backend that it knows a satisfying assignment to that circuit. Sort of put these two things together and the proverbs actually prove that it knows a witness that the computer program would have accepted. Okay.
00:10:09.590 - 00:10:39.800, Speaker A: All right. Yeah. So just a very brief summary of front ends today. So I mentioned that this paradigm I described is a little bit aspirational the aspirational part, largely being that I said we want our developer to write the witness checking procedure in a high level language. So there are a number of popular front ends today on this slide. I've sort of arranged them from low level language exposed to the developer to higher level languages. So, like, Bellman and Circom are very popular languages.
00:10:39.800 - 00:11:33.580, Speaker A: People actually really like using them, but they're almost having the developer specify the circuit gate by gate. Whereas this third category I have here are targeting letting the developer program in higher level languages, whether it's solidity or even like Cairo 1.0 refers to a rust like language. It's not literally rust, but sort of inspired by rust. And then there's a front end that will go through the whole compilation procedure I described to ultimately get out a circuit in the end, something like a circuit. I'll talk a little bit more about the tooling ecosystem today. Later in the talk, what is the middle range between the first one and the last one? Yeah, so Socrates and Xjsnarc, I just say they're domain specific languages that are a little bit higher level than Bellman and Circom.
00:11:33.580 - 00:12:19.192, Speaker A: Yeah, I guess there's not much more I want to say about that. There's sort of a step remove from having the programmer just write down the gates of the circuit directly. I'll just leave it at that. Yeah. And there's kind of naturally a potential trade off between the niceness of the developer experience and both the size of the circuits that might come out of the front end as well as sort of how big a surface area there is for bugs. Right. So kind of the more complicated the compilation procedure from the program the developer writes to the circuit that comes out, the more room there is for bugs.
00:12:19.192 - 00:13:17.870, Speaker A: Of course, the sort of less nice the developer procedure, the more likely it is the developer itself introduces a bug and doesn't write the circuit correctly. So it's kind of trade offs all around. Are there a lot of hybrid front ends that sort of allow general compilation but rely on a lot of gadgets or hand coded libraries and things? Yeah, a lot of these languages do have sort of built in gadgets for very specific functionalities that are done commonly. They're often called like built ins or something. So people are sort of trying to determine the best kind of trade off between performance and developer experience and generality and surface area for bugs to creep in. And it's a very sort of high dimensional optimization space. Other questions? Can I ask a question? Please, go ahead.
00:13:17.870 - 00:14:12.752, Speaker A: How do you make sure that the circuit actually corresponds to the software you're really interested in? Yeah. Great. So I guess I was kind of getting at that question when I said that. Kind of the more complicated the compiler from the program the developer writes down to the circuit the kind of more space there is for bugs to creep in. So people have attempted and are continuing to attempt to kind of formally verify as many parts of this tool chain as possible. But there are a lot of parts of the tool chain. So actually almost all of these projects that are aiming to expose high level languages to the developer, they all go through something called a zkvm, which kind of means the circuit actually directly runs a witness checking procedure kind of inside the circuit step by step.
00:14:12.752 - 00:15:35.070, Speaker A: Right. And that means they wind up using what people will call a virtual machine or CPU abstraction where they specify some primitive instructions and the circuit actually just for each step of the computer program figures out which instruction should be executed next and executes the instruction and just does this over and over again like in the circuit. And so there has been some work to formally verify that the step of going from a program in the assembly language for the virtual machine does get turned into an equivalent circuit. Okay, but then they have not formally verified that the compilation procedure from the high level language down to the assembly program for the virtual machine is correct. Right. So the best we can do is short of formally verifying the whole tool chain is to just be very careful about how we design these things so that what comes out of one end of the compiler is equivalent to what went into it. Does that make sense? Did that answer the question? Should I say? Yeah, but when I'm verifying I'm thinking as a user, don't I still need to trust the compiler? Yeah, you do.
00:15:35.070 - 00:16:19.128, Speaker A: So let me think how to answer that. Okay, so different front ends work in different ways. Some front ends will actually spit out a universal circuit where kind of the circuit actually takes as part of its input basically the computer program to be run. Okay. And then there's one verification procedure that's going to work for any computer program. You just have to make sure that the circuit is fed the correct computer program, kind of. But yeah, you could imagine there are other front ends that sort of the circuit itself is program specific.
00:16:19.128 - 00:16:57.190, Speaker A: It's not a universal circuit. And then every time you are interested in a different witness checking procedure, the Snark Verifier depends on the circuit it's getting applied to. So you're going to have to make sure every time you change the computer program that the equivalent circuit is indeed equivalent. Like the Verifier is ultimately just going to make sure the prover knows a satisfying assignment to that circuit. And if it's the wrong circuit, well, too bad. Does that make answer question? Yeah. Any more questions? Okay.
00:16:57.190 - 00:17:48.576, Speaker A: I think that the possibility of bugs in this tool chain is a major concern in sort of the long term usefulness of Snarks. And I think we can ultimately make sort of the surface area for bugs maybe no bigger than the current surface area for bugs in compilers from high level computer programs down to what actually runs on your laptop or something. But the stakes for bugs might be even higher than usual because one issue in the compiler and all of a sudden all of the money on an L two is just gone. Or something like that. Yeah, and I can say a little bit more about that if I don't run out of time by the end of the talk. Okay, cool. All right, so let me just say a little bit about how backends are designed.
00:17:48.576 - 00:18:18.716, Speaker A: So these are like Snarks for circuitsat. So typically these are now designed in a three step process. So first you give something called a polynomial IOP for circuit satisfiability. I'll say in a moment what that means, and then you combine it with a cryptographic primitive called a polynomial commitment scheme. And I'll say in a moment what that means. If you put these two together, you immediately get a succinct interactive argument. And then typically you can remove the interaction to get a Snark non interactive argument by applying something called the fiat shamir transformation.
00:18:18.716 - 00:19:11.856, Speaker A: Okay, so this is how almost all Snarks are designed. There are a couple of exceptions, say one of the two exceptions I actually don't even consider to really be an exception because it almost follows this paradigm. And then there is like a real exception, which is getting quite popular. Now, something called Nova or Nova is the most popular instantiation of this other version. This other approach inherently uses something called Snark recursion, whereas everything else on this slide, like, you can apply it recursively, but you don't have to. So this quote folding based Snarks that don't use polynomial commitment schemes and polynomial IOPS necessarily sort of are getting a lot of mileage through making heavy use of recursion. And if those words don't mean anything to you, just don't worry about it.
00:19:11.856 - 00:19:53.048, Speaker A: So the Snarks I want to tell you about today do fall into this paradigm. So let me briefly tell you what is a polynomial IOP? So it's a proof system where sort of the prover sends a sequence of messages to the verifier. And okay, this is actually a special case of a polynomial IOP, but it's all we're going to need today. So the first message the prover sends is kind of special. It specifies a polynomial. And by polynomial, I really mean sort of what you saw in maybe like 8th grade or high school algebra or something. I mean, one plus x plus three X squared plus four X cubed, like a sum of powers of x that's univariate polynomials.
00:19:53.048 - 00:20:54.460, Speaker A: We'll actually need multivariate ones today. And this polynomial, if you were to actually write down all of its coefficients or something in most Snarks, would be as big as the circuit the proverbs claiming to know a satisfying assignment to so it's too big for the verifier to learn in full. That would totally violate work saving for the verifier. So we're not going to let the verifier learn the whole polynomial. All we're going to let the verifier do is evaluate that polynomial at like one point of its choosing. Then, other than that one special message that specified this really big polynomial, a polynomial ILP is just sort of a standard, what's called an interactive proof, where the prover says some stuff and the verifier sends back a random challenge and the prover says some more stuff and the verifier sends back a random challenge and so forth. Okay, that's a polynomial IOP and a polynomial commitment scheme is just exactly what you would want to turn a polynomial IOP into a succinct argument.
00:20:54.460 - 00:21:53.280, Speaker A: Okay? So the problem with the polynomial IOP, the reason it doesn't just directly give you a succinct argument is because of this big first message specifying this big polynomial. So what the polynomial commitment scheme lets the prover do is cryptographically commit to the polynomial, and the commitment will be really small, like one Hash value or one group element or something. And any commitment scheme is binding, meaning sort of you think of the prover as like stuffing this big polynomial into a tiny box, and it hands the tiny box to the verifier. That's the commitment. The box is like locked, right? So once the verifier gets that box in its hands, the prover can no longer change what's inside of it. Okay? And so then later when the verifier says, hey, I need to evaluate the polynomial, at this one point, the prover can provide the evaluation as well as a proof that the provided evaluation is actually consistent with the polynomial that's inside the box. And sort of the verifier never has to open the box and take out the whole polynomial.
00:21:53.280 - 00:22:21.070, Speaker A: It can sort of just rely on this proof that the prover is being honest when it says here is the requested evaluation of the thing inside the box. So polynomial commitment scheme is just a Cryptographic protocol that allows exactly this functionality. And this just means we can sort of run the polynomial IOP without the prover ever sending this big polynomial. And that was the only thing keeping the polynomial IOP from sort of directly being a succinct argument on its own.
00:22:22.880 - 00:22:30.652, Speaker B: Question about that. So after the initial sort of send the polynomial commitment message, you need a protocol then that has a short transcript.
00:22:30.716 - 00:22:37.090, Speaker A: Right after that, right. So to get a Snark that's right. And so we make sure that that's always the case.
00:22:37.540 - 00:22:40.228, Speaker B: And that's generally going to just look like a bunch of small number of.
00:22:40.234 - 00:23:11.180, Speaker A: Random challenges basically from the verifier. And in the Snark, those challenges will be obtained via fiat Shimir, which means by hashing like earlier messages, so there's no interaction. And then the proverbs responses will typically be depends on Snark you're talking about the ones today. They'll just be really low degree univary polynomials. So just like a handful of field elements to actually write down all the coefficients. And other Snarks, they might involve more commitments to more polynomials and more evaluation proofs and things like that.
00:23:11.330 - 00:23:15.864, Speaker B: And so should I roughly think of it like there's going to be a logarithmic number of rounds with like constant.
00:23:15.912 - 00:23:52.120, Speaker A: Number of bits in each round. Every Snark I want to tell you about today, they're based on these sort of multivariate polynomial techniques and that's their characteristics. So there's logarithmically many rounds and there's like, say, three field elements sent per round. Questions? Yeah. How does linear PCP based methods differ from this paradigm? Like you mentioned, they are kind of similar. What is the main difference? Yeah, I'd say in the end the main difference is that the linear PCP based Snarks don't need to use fiat Chimere. And that can be nice.
00:23:52.120 - 00:24:34.344, Speaker A: It can give some nice properties of the Snarks that the ones that everything else uses fiat shamir might not have. But nonetheless, linear PCP based Snarks are very similar. Like a linear PCP is some sort of variant of a polynomial IOP. Replace low degree polynomial with linear function. And the Cryptography used to turn those into Snarks is very similar to something called KZG commitments. So here's just one slide about the different kinds of polynomial commitments out there. So there's sort of three or four main categories, and I've sort of highlighted it for each of the main categories in Green maybe what's most popular in terms of deployment today.
00:24:34.344 - 00:25:12.740, Speaker A: They tend to be the most popular because they have the shortest proofs in their category, at least asymptotically I actually think for some of these concretely, they might actually be bigger proofs than some things that are asymptotically worse. But anyway. Yeah. So KZG commitments are very popular because concretely, they definitely have the shortest proofs, evaluation proofs out of everything. And these linear PCP based Narcs use techniques very related to KZG. So there are trade offs like kind of right and left with polynomial commitment schemes out. You know, KZG commitments have the best verifier costs, but they require trusted setup.
00:25:12.740 - 00:25:30.730, Speaker A: IPA, also known as Bulletproofs, does not require trusted setup. But both of these use elliptic curves, so they're not plausibly post quantum secure. This third category is plausibly post quantum secure, but the verifier costs are bigger, things like that.
00:25:31.260 - 00:25:32.820, Speaker B: What did the green indicate?
00:25:32.980 - 00:25:59.376, Speaker A: The green indicates what kind of most practitioners should be aware of. The most popular from each category is sort of in deployment, and the reason is roughly because they have the shortest proofs amongst its category. Other questions? Cool. Okay. So, yeah, let me just briefly summarize Snark performance today. So I sort of told you, how.
00:25:59.398 - 00:26:07.590, Speaker B: Much does what happens after the polynomial commitment scheme depend on the polynomial commitment is the rest of the protocol tend to look kind of the same no matter which of these you use?
00:26:08.280 - 00:26:55.670, Speaker A: Yeah. So up to some technicalities, you can basically take any polynomial IOP and any polynomial commitment scheme and put them together and get a Snark. The main technicality is some polynomial IOPS want to send univariate polynomials, others multivariate, and the polynomial commitment scheme kind of has to match. And yeah, there's a variety of polynomial IOPS out there. There's a variety of polynomial commitment schemes. I'm going to start talking in a moment about how the concrete bottleneck in most Snarks is the polynomial commitment scheme, but the polynomial IOP can kind of help determine how big a polynomial needs to get committed and stuff like that. So maybe there'll be more information I'll cover in due course.
00:26:57.400 - 00:27:01.530, Speaker B: So you're saying, let's really focus on the commitment scheme and optimizing that.
00:27:04.060 - 00:27:48.980, Speaker A: Okay. The way I think of it is one should design a polynomial IOP to minimize the amount of stuff that needs to get committed, and then one can just choose any polynomial commitment scheme to plug in to turn that thing into a Snark. Just pick your favorite kind of trade offs from the polynomial commitment. And so the Snarks I want to tell you about today, they can use any polynomial commitment scheme for multivariate polynomials, and there are analogs from each of these categories for multivariate polynomials, and you literally can just pick your favorite one. So if you don't mind a trusted setup, you might pick one that's sort of based on KZG. If you do mind it, you might pick one based on Bulletproofs, based on this HIRAX thing. So we'll say more about this.
00:27:49.050 - 00:27:52.292, Speaker B: What's going to be novel is what follows the Commitments game, you're saying?
00:27:52.346 - 00:28:46.884, Speaker A: Yeah, so we're going to just blackbox the commitments today, which is actually one reason I wanted to talk about how you have a lot of choices of commitments. But now from now on, we're not going to talk about how the commitments work at all. Okay? Yeah. So in general today, I'd say the verifier in Snarks is generally cheap, certainly much cheaper than direct witness checking. There is a lot of variation in terms of the verifier costs, but nonetheless, it's still certainly like sublinear costs, right? So it can actually be multiple orders of magnitude differences in kind of how expensive is the verifier today. But my main focus in this talk is on the prover because I think this is really what sort of limits the applicability and scalability of Snarks. So in general, the prover today in a Snark will be at least six orders of magnitude.
00:28:46.884 - 00:29:41.780, Speaker A: So at least a million times slower than direct witness checking, than just like running the witness checking program directly on the witness. Now, this overhead can be heavily parallelized, so you can just throw a bunch of GPUs at it and that's going to help a lot. But still as you make one computation, like a million or 10 million or 100 million times slower, it's still going to limit what you can prove. Okay, question what is stark? That's a particular kind of Snark with everything's trade offs, right? So a pro is it's a one example of a plausibly post quantum Snark? A con is the proofs are much bigger than some of the others. Okay. So I think the overhead for the prover can generally be broken into two parts and they kind of multiply together. So I call it front end overhead and back end overhead.
00:29:41.780 - 00:30:23.210, Speaker A: And I'll put up a bunch of words on the slide, but maybe just listen to me speak. So sort of the front end overhead is like, how bad is it that we have to go through a circuit that we can't just kind of run the computer program directly, but we have to turn it into an equivalent circuit. And then the back end overhead is like, how much worse is it that we have to prove that the prover knows a satisfying assignment to the circuit versus just like evaluate the circuit on that satisfying assignment. Now, each of these overheads is at least like three orders of magnitude today, and they multiply together, which is where you get sort of a lower bound of a million fold slowdown for the proverb. Okay. Now the bottleneck in the resulting Snarks is typically the polynomial commitment scheme. Now, that's not always the case.
00:30:23.210 - 00:30:55.776, Speaker A: I think it's very rare that it's far from being the case. I would say if it's not the case, the right view is not that this Snark has an especially cheap polynomial commitment scheme. No, it's that there's too much work happening outside the polynomial commitment scheme. Okay. So certainly for the Snarcs I want to tell you about today, the polynomial commitment scheme will be the bottleneck. And so I'm going to very heavily. Focus on accounting for how expensive is this polynomial commitment scheme? And yeah, I think certainly the polynomial commitment scheme is always a lower bound on the prover time.
00:30:55.776 - 00:31:03.684, Speaker A: Say if it's not the bottleneck, it really means you should revisit the Snark because the proverbs doing too much work. Aside from the computing the commitments, you.
00:31:03.722 - 00:31:11.424, Speaker B: Feel like that's fundamental or just kind of an artifact of the current state of the art some reasons. Is there some kind of vague intuition.
00:31:11.472 - 00:32:02.196, Speaker A: About why you would expect yeah, well, the intuition is the bottleneck for the proverb. Cryptography is expensive, and the more stuff the prover needs to commit to the more cryptographic operations it's doing. Everything that happens outside of the commitment scheme is sort of non cryptographic in nature, and it should be lighter weight. Are these two overheads sort of independent, or could it happen that you generate a worse circuit in the front end, which is easier to generate a snark proof? Yeah, great question. Yeah, it can be very hard to keep these a clean line separating these two. And in fact, you run into these issues where you might have a snark that sort of supports a more general kind of circuit. So the circuit you apply it to can be smaller, but for a circuit of a given size, it's slower, but it still comes out ahead because the circuit is smaller and things like that.
00:32:02.196 - 00:32:36.544, Speaker A: And what I want to tell you about today and I'm finally getting to the new content, the questions have been great, though I hope people don't mind if I take a little extra time, is lookup arguments, and I think they really straddle front ends and back ends, and it's very difficult to draw a totally clean line. Nonetheless, I think this view of I'm pretending you can draw a clean line, I think it's a powerful view anyway, to the extent that it's accurate. Okay. Yeah. And if you're interested in more details about front end and back end overheads, last summer I wrote a blog post about this for a 16 z. So you could check that out. No.
00:32:36.544 - 00:33:09.228, Speaker A: So the Snarks I want to tell you about today, so again, they can use any commitments, any polynomial commitment scheme for multivariate polynomials. I think they're particularly interesting performance characteristics if the commitment scheme is based on what's called multi exponentiations. So this is the first two categories. Out of the three I had, or I guess I had four on the slide before. So things like KZG commitments, things like bulletproofs. Okay, so what is a multi exponentiation? So here we're dealing with a cryptographic group. I'm using multiplicative notation here.
00:33:09.228 - 00:33:29.804, Speaker A: That's just a convention. Sometimes people write the group operation as addition instead of multiplication. It doesn't matter. So multi exponentiation is just a product of a bunch of exponentiations. So these GIS are group elements, and these XIs are exponents. They're numbers. Right? So if you did use additive group notation.
00:33:29.804 - 00:34:04.616, Speaker A: It makes sense to call this a multiscaler multiplication because kind of exponentiation turns into multiplication and multiplication turns into addition. And so people abbreviate that MSMs. So I'll use the acronym MSM a bunch in this talk and yeah, a lot of polynomial commitment schemes that are popular are based on multi exponentiations. Basically to compute a commitment to a bunch of field elements, interpreted it as a polynomial in some way, you literally just do a multi exponentiation. And typically the committed field elements are the exponents in the multi exponentiation, GIS.
00:34:04.648 - 00:34:08.800, Speaker B: Are somehow I think the GIS is like fixed or should I?
00:34:08.870 - 00:34:32.490, Speaker A: Yeah, they're fixed. So with KZG, these GIS are elements of a structured reference string that was generated in some ceremony ethereum's running one right now. And for something like transparent, like bulletproofs, these would just be random group elements which everyone would obtain by applying a PRG to some natural seed of some sort.
00:34:33.900 - 00:34:36.244, Speaker B: The GIS are like magically fixed in advance.
00:34:36.372 - 00:34:48.910, Speaker A: Yeah. And then the XIs are like what you want to commit to. Okay? Yeah. And so it's compressing where you want to commit to Nxis. And the commitment is just one group element. That's what the commitments are. Nice and small.
00:34:48.910 - 00:35:27.370, Speaker A: Okay. Now the naive algorithm to compute a multi exponentiation would just do each exponentiation and multiply the results. Okay, I want to clarify. An exponentiation can be 400 times slower than a group operation, right? Basically these exponents can be as big as the size of the group. Typically these groups are size like two to the 256, which means one exponentiation is like 400 multiplications because that's how much it takes to raise a group element to the power like two to the 256 or something. Okay. Actually a lot of papers don't distinguish between exponentiations and group operations and they're ignoring a factor of 400 if they do that.
00:35:27.370 - 00:36:16.120, Speaker A: So there is a nontrivial algorithm because you don't actually care about the result of all N exponentiations, you only care about the one number that comes out of the multi exponentiation. There's something called Pippinger's algorithm that can kind of reuse work across the exponentiations and this will save you a factor in runtime of about log N. This can be significant in practice, it can be well over ten. Okay. On top of that, this is another underappreciated fact. If all the exponents of small are small, and I'll say what I mean by small later you can save another factor of ten in the commitment time. This is just analogous to how computing, like raising something to the power two to the 16 is much faster than raising it to the power two to the 160.
00:36:16.120 - 00:36:30.670, Speaker A: Like this can be computed by taking GI and squaring it and squaring that and squaring that. If you do 16 Squarings, you've raised GI to the power two to the 16. But you need 160 Squarings to raise it to the power two to the 160.
00:36:31.120 - 00:36:38.688, Speaker B: So your 400, you're getting that from 256 Squarings plus roughly half that to sort of sum the things together.
00:36:38.774 - 00:37:27.660, Speaker A: That's exactly it. That's where it comes from. Exactly. This 400 is if the exponents were like random as actually they often are, but they won't be today, they'll be small. It's one of the main points of today and that's why if it's actually like a worst case exponent, I guess it could be 512 times slower potentially or something like that. Okay so if all of the exponents are at most K, and I guess it should be N, the size of the multi exponentiation is more than the maximum exponent size. You can actually compute the multi exponentiation with just about one group operation per field element that you're committing.
00:37:27.660 - 00:37:59.930, Speaker A: So this is like 400 times faster than a naive algorithm when the exponents might be random. This is like a big difference in efficiency, which is why I'm highlighting it now at the start of the talk. So the summary is multi exponentiation based commitments when you're committing to small field elements are like way faster than general multi exponentiations if you're committing to not small field elements and if you're not using pippingers, everyone should use pippingers though.
00:38:01.340 - 00:38:04.712, Speaker B: And so in your construction you're going to take advantage of the small K thing.
00:38:04.766 - 00:38:14.030, Speaker A: Yeah. So the Snarks I want to tell you about today are faster for the prover even without the small K thing, but the small K thing buys an order of magnitude more on top of that.
00:38:17.140 - 00:38:20.690, Speaker B: Okay, so less than, so what should I think of as like little N for you?
00:38:21.860 - 00:39:17.990, Speaker A: You can think of it as like a million ish and you can think of K as also a million ish, whereas these groups have size like two to the two five six, which is way more than two to the 20 versus two to the two five six. Other questions? Okay. Yeah. All right, so let's talk about lookup arguments. So yeah, I said that these things sort of straddle front ends and back ends and now we're about to see why. So suppose as an example, I think it's easiest to present this notion with an example. Suppose Approver wants to prove that it said it ran a witness checking procedure but it wants to prove that none of the values arising during the execution of the witness checking procedure were bigger than say, two to the 128, say, because the checks it's doing sort of assume that their inputs are no bigger than that.
00:39:17.990 - 00:40:10.100, Speaker A: A cheating prover could try to feed the witness checking procedure like overly big numbers and it would mess up the checks. Maybe it causes some uncorrected overflow issues or something like that. So the prover is going to have to separately prove that none of the elements of the witness are too big. So this is called like a range. Okay. Now the traditional way to do this sort of inside a circuit is to use this untrusted advice I had mentioned in response to a question of Joe before, where the advice would just be sort of the binary decomposition of each number to be range checked. So if the prover wants to prove that some number x is between zero and two to the 128, it would just provide his untrusted advice like 128 bits and claim that x equals the appropriate weighted sum of those bits.
00:40:10.100 - 00:41:28.420, Speaker A: Those bits are X in binary. And the point is, if each of those bits are indeed in zero one they're bits, then this weighted sum here cannot be outside of the range zero up to two to the 128 minus one. So if the proverb provides these 128 bits, proves that each of them is in zero one, and proves that x equals this sum here, then it has indeed proved that X is in the appropriate range. Does that make sense? So this is kind of the old way of doing things and it's very expensive because something your computer could do in like one operation or maybe two if it has 64 bit instead of 128 bit data types, is requiring approver to cryptographically commit to 127 field elements. Now, fortunately, they're small field elements and as I said, with MSM based commitment schemes, those commitments are pretty fast to compute, but there's still 127 of them and there are certain other costs that also kind of grow with 127. So this is generally we'd like to do better than this if we can questions about this. Okay, so kind of the whole point of this talk is to explain how you can do better than this.
00:41:28.420 - 00:42:06.734, Speaker A: Yeah. So here why can't we just send Dei in the open? Do we also want this to be zero? If you did care about zero knowledge, that would be one reason, but it's really a succinctness issue. So imagine that you had every single value arising during the entire execution of the witness checking procedure needs to be range checked. Okay. So even sending like one bit per value is not succinct, basically. Other questions. Cool.
00:42:06.734 - 00:42:20.914, Speaker A: Yeah. So in the Snark applied to the circuit side instance, the prover is going to commit to all of these bits. And that's expensive. It's cryptographic operations for group operations or something for each, just to tie this all together.
00:42:20.952 - 00:42:25.374, Speaker B: So those are going to be like some of the XIs you're saying in the commitment?
00:42:25.422 - 00:42:28.994, Speaker A: Basically in the commitment, I guess x.
00:42:29.032 - 00:42:34.626, Speaker B: Is being used in multiple ways. Right. So like on the last slide, when you're talking about the MSMs, I'm just trying to connect all the dots.
00:42:34.658 - 00:42:35.558, Speaker A: Okay, great.
00:42:35.724 - 00:42:37.640, Speaker B: So you're committing to XIs there.
00:42:38.910 - 00:43:23.814, Speaker A: Yeah, that's a different Xi. But yeah, in the Snark applied to this range checking example, these exponents would be these bits like BIS, and the bits would all be committed. And fortunately, because they're like zeros and ones, the exponentiations are really cheap. It's just sort of one group operation for each because GI to the one is GI and GI to the zero is one or something. But nonetheless, that's still 127 group operations. And there are some things that happened inside these Snarks other than the commitments getting computed. And so 127 things getting committed is not great.
00:43:23.932 - 00:43:39.354, Speaker B: And then just to really connect the dots. So back when you're talking about the overall sort of IOP approach, send the Ponomail commitment, and then you get to do like, say one evaluation, and then you do this other step, which is cheap. All of this is like the one evaluation that you're talking about, right?
00:43:39.552 - 00:44:04.210, Speaker A: This is the commitment. And then later there'll be one evaluation. Like kind of at the end of the Snark, the verifier is going to say, hey, you committed to one or a handful of polynomials. And now I need to evaluate either all of those polynomial. There are ways to do batching. So now I need to evaluate some random linear combination of those polynomials at one point, and that also has a cost, and I'll address that cost shortly.
00:44:04.630 - 00:44:13.030, Speaker B: I just want to make sure I'm not confused. So you go back forward to where you left off. So talking about Pippitra as the algorithm. Yeah, the point is, this is all about fast evaluation.
00:44:13.450 - 00:44:49.950, Speaker A: This is all about fast commitments. Okay? So, yeah, polynomial commitment scheme has like two procedures. You commit and then later the verifier is like, I need to evaluate the committed polynomial at a point. The short version is the evaluation proofs. People tend to ignore their cost and they're not totally ignorable. And I'm going to say more about this later, but because of very nice batching procedures and also because a lot of polynomial commitment schemes, actually the evaluation procedure is faster than the commitment. It is reasonable in some cases to kind of ignore the cost of computing an evaluation proof.
00:44:49.950 - 00:45:19.146, Speaker A: In other cases you can't ignore it. And we're going to see both situations today. Does that make sense? So here I focused just on the cost of committing. Oftentimes that's the bottleneck. You can't always, always ignore the evaluation proof. Okay? So, yeah, this is sort of the naive baseline that we need to beat. If I'm not beating this, I'm not telling you anything interesting at all, okay? But lookup arguments, like their whole reason for existence is to try to do better than this.
00:45:19.146 - 00:46:05.622, Speaker A: Try to do better than bit decompositions. Okay? So what lookup arguments will let you do is imagine we just were able to somehow initialize like materialize, I'll call it a lookup table, just like an array of values that store all the valid values. So all the field elements zero up to two to the 128 minus one. No one can write down a table that big, but let's just ignore that for a minute. Okay? Then each range check every time the prover needs to prove that this x is in this range is equivalent to what's called a lookup into the table. All right, so let me give a little more details. So in this range check example, the table again would just be all of the valid range elements.
00:46:05.622 - 00:46:48.600, Speaker A: The number zero up to 228 minus one. And the prover will have ultimately committed to the vector of values that need to be checked. Okay, so the setting in a lookup argument is the verifier receives a commitment to a vector A, and the prover claims that every entry of A resides somewhere in the table. Okay, so it's in a little more detail. The prover claims to know an opening of the commitment so that for every element of the vector A that opens the commitment, there is some index in the table that equals that element of A.
00:46:49.530 - 00:46:57.222, Speaker B: So by opening A, you literally just mean like x one up to x n, such that if you plug that into the cheese, then you get back to CN.
00:46:57.286 - 00:47:59.310, Speaker A: Exactly. Yeah. And more conceptually, just think of the commitment as like a box with something inside of it and the proverbs claiming to know the thing inside of it, whatever that means. And moreover, that for that thing that it knows, every entry of it lives somewhere in the table. Does that make sense? Okay, so it's like in the range check example, you think of the Snark prover has committed to the entire execution trace of the witness checking procedure applied to some witness, but separately needs to prove that every element arising in that trace isn't too big, isn't just cooked up to mess up the witness checking procedures checks. And so it would use a lookup argument with this lookup table to convince the verifier that questions. Okay, so this is what I call it unindexed lookup arguments.
00:47:59.310 - 00:48:49.546, Speaker A: In some applications, you need indexed lookup arguments. So this means rather than just there being like, sort of one vector A with every entry of A needing to reside somewhere in the table, every entry A is paired with an index into the table. Right. And the provers and the index lookup argument is claiming that for every I, AI lives in the specific index bi in the table. So unindexed lookup arguments are like every element of A lives somewhere in the table. And I the verifier don't care where index lookups are like the elements of A are committed as well as indices. And the proverbs claiming that each entry of A lives in a specific index in the table BIS are also committed.
00:48:49.546 - 00:49:04.254, Speaker A: Yeah, I've drawn them as tuples, but you could think of like, there's one commitment to the AIS and one commitment to the BIS, and the proverbs claiming that this property holds. So for every I AI equals the bi table element.
00:49:04.302 - 00:49:07.960, Speaker B: So is there like, a very simple example where it's clear this is what you want?
00:49:08.330 - 00:49:47.730, Speaker A: Jolt. So if you just hold your horses. Okay, yeah, actually, most of the papers seem to consider the undenex case, but then the application we considered in this work definitely needs the index case. But fortunately there's a simple transformation like kind of if you can solve one, you can solve the other. So up to this point, we're still assuming that both the prover and the verifier have the full lookup table locally, right? Yeah. I will say in great detail later what the verifier actually needs to kind of know about the table. We are not going to want the verifier to necessarily have to write down the whole table.
00:49:47.730 - 00:50:23.950, Speaker A: Other questions? Cool. So now I'm going to summarize existing lookup arguments and I'll tell you about lasso. By the way, I'm not going to tell you anything about how Lasso works today. That's for the talk tomorrow. Okay, so let's say we want to do Mlookups into a table of size N. So there's now a long line of work on lookups and here are three prominent examples. So the paper that introduced the notion of lookup arguments gave a very good lookup argument when the number of lookups is much bigger than the size of the table.
00:50:23.950 - 00:51:36.670, Speaker A: So the bad thing about Aria is this table size times log number of lookups term. All right? We don't like that log being there, for example, but if the table size is pretty small, it doesn't really matter because there's already an M anyway. So actually, if you have like a really small table and you're doing a ton of lookups into it, I actually think reo is probably still what you would want to use today, but you really need the number of lookups to be much bigger than the size of the table for that to be best. Okay, then some later works sort of focused on the case where the number of lookups in the table size were about the same, say basically the case where you wouldn't want to use Aria. And so plokup is a very popular lookup argument where the prover commits to like five times the max of the two number of lookups and table size field elements. And in every existing lookup argument before the work I want to tell you about today, some honest party is always going to have to commit to the table in preprocessing, unless you want the verifier to sort of materialize the whole table when it comes time to verify the snark. So in all these lookup arguments, this is basically how the table gets specified to the verifier.
00:51:36.670 - 00:52:15.626, Speaker A: Is it's handed a commitment to the table and it better have been an honestly computed commitment or otherwise it's sort of going to bind the prover to the wrong table. Is this practical? Because if it's a lookup table for, let's say, two to the 128 elements, how can anyone do that? Yeah, so you can't for giant tables like that, and I'll say much more about this soon, but basically lookup arguments today are applied to tables typically with like two to the twelve ish entries, something like that. And one reason is because you just can't commit to giant tables efficiently.
00:52:15.818 - 00:52:27.700, Speaker B: One question would be do you feel like it was already in the air that people really wanted to use big lookup tables and there wasn't detect to do it properly? Or is even sort of that kind of a new idea to take advantage of?
00:52:28.070 - 00:53:33.850, Speaker A: I think it was in the air also, I think the giant lookup tables we wind up using in jolt actually turn out to you can kind of reduce a lookup into a giant table into several lookups into smaller tables, in which case you kind of could use existing techniques. Although I think that Lasso kind of makes it easier than the existing techniques would be to kind of decompose a lookup into a giant table into smaller ones. I think the perspective of doing one lookup into a giant table is powerful and we'll see why soon. So what is the sort of trade off point with current efficiency techniques of where you use a lookup table versus something naive like bit decomposition? Yeah, it's a great question. So it depends on what kind of snark you're going to sort of combine the lookup argument with because we're not at the point I'm going to say more about this is what the lookup singularity means is it's a notion of like just you only do lookups right, you don't do like anything else. And we're sort of not at the point. I'm not even sure if that's achievable.
00:53:33.850 - 00:54:56.370, Speaker A: What we do now is we turn in jolt and I'll get to jolt soon. Almost everything that happens is lookups. But there's still a little bit of work done to sort of put together the results of the lookups into like a full snark proof. The answer of when you should use bit decomposition versus a lookup argument depends on what snark you're going to use to prove everything that happens outside of the lookups. And some snarks like bitdecomposition play not nicely with because there are certain other costs I know this is a rambling answer there are certain other costs to bit decompositions besides just committing to the bits, which some snarks sort of pay a heavy price for and others don't necessarily, such as these opening proofs, these evaluation proofs for the polynomials. Some snarks only like one really small polynomial needs to be evaluated and other snarks like a much bigger polynomial needs to be evaluated anyway. So I guess kind of a rough rule would be you should think of lookup arguments as very good amortizers.
00:54:56.370 - 00:55:42.994, Speaker A: So bit decompositions, if you're only doing like one lookup, you should probably just use a bit decomposition, probably right. If you're doing m different lookups, then this can be better. And so what's happening is that if you ignore the n for for every lookup here the prover is committing the five field elements, okay? That's less than, like, 128 bit decomposition. On the other hand, as we'll see, a lot of these are, like, arbitrary field elements. They're not bits. Okay? So committing to, say, five arbitrary field elements can actually be more expensive than committing to 64 bits. But then there are these other costs, like opening proofs and stuff that also come in that might be higher if you commit to 64 things instead of five.
00:55:42.994 - 00:56:02.646, Speaker A: Anyway, so if you just sort of ignore those intricacies, just compare, like, the five times max of M and N to, like, 64 M or something like that, 128 m and pick which one is smaller. That's the precise answer.
00:56:02.828 - 00:56:06.774, Speaker B: Could you elaborate you had an interesting comment about amortization.
00:56:06.902 - 00:56:07.290, Speaker A: Yeah.
00:56:07.360 - 00:56:08.780, Speaker B: Could you elaborate on.
00:56:12.350 - 00:56:35.518, Speaker A: If okay, let's focus on Aria. Maybe that's because I've written Aria as, like, the sum of two different terms. Okay. So there's always this n log m term. N is the table size, and maybe just ignore the log m term. Okay? So if you do a lookup argument into a sizable table, you pay a term n. And if you ignore the log m, that term is sort of fixed.
00:56:35.518 - 00:57:02.940, Speaker A: It doesn't matter how many lookups you do, you pay it once it's done. And then you have this M term, and you think of that as, like, one field element per lookup. Okay? So if you do a bit decomposition, it's like, say it's 128 bit decomposition. That's 128 field elements committed per lookup. Now they're small, which makes them faster to commit to. But 128 field elements committed per lookup. Here it's just one.
00:57:02.940 - 00:57:23.374, Speaker A: But it still wouldn't make sense to do it if m was, like, just one because of the plus n. So basically, you pay some factor that depends on the table size. You pay that once. At that point, you've paid it doesn't matter how many lookups you do, and then you get a better per lookup cost. So that's the perspective you should have for lookup.
00:57:23.422 - 00:57:37.362, Speaker B: Yes, that makes a lot of sense, interpreting these expressions. I also wonder, just about conceptually, should I almost think about the lookup table is basically capturing some preprocessing, which you're then sort of reusing across multiple different commitments?
00:57:37.426 - 00:58:28.834, Speaker A: Or is that not I think that's the right way to think of some lookup arguments and maybe not of others. And I would actually say because an honest party has to commit to the table in preprocessing for all existing lookup arguments. That's a totally fine way to think about all existing lookup arguments. But we'll see if you view Lasso as really dealing with the table size, 228, no one's processing that table ever, so it's probably not quite the right way to think of it. So, yeah, all of these works, they have a table cost sorry, a cost for the prover that's sort of at least linear in the table size. There was a really nice line of work, starting with Caulk, which is only a 2022 paper. So it's really recent, this whole line of work that look to push all of the cost proportional to the table size into preprocessing doesn't get rid of it, moves it into preprocessing.
00:58:28.834 - 00:59:19.930, Speaker A: Okay, now Caulk, and it turns out all of them, this actually gets back to a question from before. They're tailored to KZG commitments, and it seems that they really heavily exploit that fiat shamir is not getting used in KZG commitments. So it's not clear that you can plug in other commitment schemes into the whole line of work from Caulk onward. So as a result, they all require a structured reference string of size, linear in the table size, because that's what's required to commit to a table with KZG. And that's really limiting in terms of how big the table you can use is. So like, even something like two to the 25 size table will be like many gigabytes sized SRS, I think. And anyone who wants to act as approver needs to download that structured reference string in order to use these lookup arguments.
00:59:19.930 - 00:59:58.230, Speaker A: CQ, short for cache quotients, is the latest version in this line of work. And after the pre processing, the CQ prover commits to sort of eight field elements per lookup. Like, that's really it. There's no dependence on the table size anymore. And there is some FFTs the prover does. So that's M log M, but that's field operations, not group cryptographic group operations. So what are some uses of these lookup arguments other than range proofs? Like, are they much more general? Can you use them for looking up advice to a circuit, for example? Yeah, I'd say they're used quite pervasively.
00:59:58.230 - 01:00:44.254, Speaker A: Basically, anytime it would be really difficult to turn something into a circuit, it's like something that doesn't look anything like addition or multiplication in a finite field. People will think about using these, is my impression. Yeah, you typically can always do a bit decomposition and things become pretty easy to implement in a circuit once you have individual bits to operate on. I mean, in principle, anything can be done as like, ends and ors right. And bits are just like true and false. People use it for hash evaluations and doing group operations inside a circuit and stuff. Just anything that's not nice to directly turn into finite field addition multiplication.
01:00:44.254 - 01:01:18.106, Speaker A: You might think of using lookup arguments. The whole point is they improve over bit decomposition pretty generically. As long as you're doing enough lookups into one table. Again, there's this cost that's sort of linear in the table size that you can't avoid paying, although Lasso is going to wind up paying sublinear in the table size fixed cost for some tables. Cool. Okay, so now I can tell you about the cost of Lassos. So, sort of a family of lookup arguments.
01:01:18.106 - 01:01:55.398, Speaker A: I'm going to start with what I call basic Lasso, which kind of the simplest. And cleanest and the most directly comparable to existing lookup arguments. Okay? And then there are two versions that I call Lasso and Generalized Lasso, which apply to gigantic tables, say a size like two to the 128. Now, if the tables are that big, they need to have some structure to them. No one can write down the trivial description of the table, which is just listing out all of its elements if it's size two to the 128. Right? So Lasso is the reason we have two. It's a little annoying, but Lasso is more efficient than Generalized Lasso when it applies, but it needs a stronger structural property out of the table.
01:01:55.398 - 01:02:33.110, Speaker A: We call this decomposability. And it's this thing I mentioned before of like one lookup into a giant table sort of being answerable by doing several lookups into smaller tables. And Generalized Lasso needs a weaker property than decomposability, which I call low degree extension structured. And you'll see by the time I'm done what that means. Okay, so let me just describe the cost of basic Lasso and I'm going to describe the cost for indexed lookups. It just sort of makes the accounting a little easier, but it also applies to unindexed lookups. In fact, tomorrow I will describe it when I tell you the details in the context of unindexed lookups.
01:02:33.110 - 01:02:59.870, Speaker A: Okay? So to do Mlookups into a table size n, the basic Lasso prover commits to n plus n field elements. And there's some other stuff, but they're low order terms. It's like little over M, little of n. Concretely, it will also be sort of a low order cost. I mean, it's not just like slightly less than m and n, it's significantly less. And moreover, all of them are small, so every one of them is in the set zero up to M. And I'll say more detail about just how small they are shortly.
01:03:00.690 - 01:03:05.582, Speaker B: This compares to like the four. You're losing a four from what you're talking about earlier.
01:03:05.646 - 01:03:31.240, Speaker A: Right, right. And I'll give comparisons shortly. Right. And so also no commitment to the table is needed if the table is low degree extension structured. And it turns out like there are a lot of tables that are low degree extension structured, and I'll give a few examples also. These are maybe sort of low order, the sorting thing is maybe low order. But actually I've been talking to some projects where this really does matter.
01:03:31.240 - 01:04:18.534, Speaker A: In all of the prior lookup arguments that I'm aware of, the prover would have to sort of take the lookups and sort of sort them or at least group them by what's being looked up. And you don't need that in Lasso. And also we can use any polynomial commitment scheme. If you don't use something like KZG, we don't need any SRS. Okay? All right, so how small are these m plus n field elements? So they're all, what's, multiplicity? So they're just sort of counting for each table elements, how many times was it accessed? How many times was it looked up? Which means like two things. So one is they sum the M, right? Because M things are accessed in total basically, right? So at most a fourth of them can be bigger than four, something like that. And also if n is bigger than m, then most of them are zero.
01:04:18.534 - 01:05:24.750, Speaker A: And committing the zeros is actually free in that there's no group operations at all required because like g to the zero is one. Right? Now this is where I'm going to discuss the caveat about opening evaluation proofs and how you can't totally ignore them. Okay, so in the lookup argument, one polynomial evaluation proof is needed for some polynomial commitment schemes. The evaluation proof requires like sublinear and n amount of cryptographic work. Okay? So sort of, at least asymptotically speaking, you can kind of treat it as the evaluation proof, as a lower order cost, because there's like, linear and n cryptographic work to commit and only like, square root n to open. And there's also like mice amortization where over the course of the lookup argument the prover commits to several polynomials and only has to evaluate one. But some polynomial commitment schemes actually have really extensive evaluation proofs and then you can't totally ignore this and I'll discuss that more in the context of experiments shortly.
01:05:24.750 - 01:06:43.430, Speaker A: Okay, so when I say that zeros cost nothing, like they don't really cost nothing, they cost almost nothing in the commitment phase, they do affect the cost of an evaluation proof. It's like maybe square root of n cryptographic cost and linear and n field work or something to produce an evaluation proof depending on the commitment scheme you use. Again all right, so yeah, I just sort of put up an extra term in the cost here, leveraging the fact that out of the M plus n field elements, if the n term is dominant, actually most are zeros. So they're sort of free to commit to and there are some other costs that then grow within, but they're sublinear in terms of the amount of cryptographic work done. So that just makes it a little easier to compare to some of the other commitment schemes. Okay, so yeah, I'm going to compare to Plokup and CQ because I think Plokup is maybe the most popular right now and then CQ is the best among the recent Caulk line of work. Remember, the Plokup proverb commits to like five times max of M n field elements and three times max of M n of them are like random field elements, they're arbitrary field elements.
01:06:43.430 - 01:07:11.834, Speaker A: There's also of course a table commitment even if the table is structured and so forth. And so we'll actually see some experimental comparisons here. But I mean, basically basic Lasso is committing to fewer field elements and they're smaller. Okay? Now with CQ, remember the prover commits to eight m field elements. Actually one of them is like a multiplicity, I think. So seven of them are random. Seven M of the eight M are random.
01:07:11.834 - 01:07:44.810, Speaker A: Okay. So I think if the number of lookups is much, much smaller than the size of the table, I think you'd want to use CQ as long as you can afford to have the table committed in preprocessing. Right? Because CQ pushes all costs that depend on the table size into preprocessing. And so you don't actually, in sort of the online phase, need this amortization property. Right? It just totally gets rid of it doesn't really go away. It goes into preprocessing. The cost that depends on the size of the table.
01:07:44.810 - 01:08:27.078, Speaker A: Now, one thing to remark on is, like, seven M arbitrary field elements is the same as committing to something like 70 M bits. So actually, in some context, bit decomposition could well be better than that. So it really depends on the context. It's how many bits would be in a bit decomposition or something like that. Okay, but if M is not vastly smaller than N, I think you would want to use Lasso. And I would actually expect it to be like 30 x faster. So basically there's something like three X coming from there just being fewer field elements committed, like seven versus two, something like that.
01:08:27.078 - 01:08:59.998, Speaker A: And then all of them being small is good for another factor of ten. And also the FFTs might actually be comparable to the cost of committing to very small field elements. Now, I do want to put in this caveat, which is this accounting just sort of didn't count for the time to commit to the lookups. Like the things being looked up, that's sort of baked into the definition of a lookup argument. You can't avoid it. Right there's not that that's literally free. It's just it's sort of a fixed cost for any lookup argument.
01:08:59.998 - 01:09:31.180, Speaker A: So should it be counted? Should it not be? So if you do count it, these numbers don't really change much if the looked up values are small. But if you do have a table which just contains random field elements, then you just can't avoid committing to M random field elements. Just by definition of a lookup argument. You have to commit to the lookups. Right. So in that case, I think this ten X speed up due to small elements will fall just to something much less than ten x. But there'll still be a nice speed up is my guess.
01:09:31.180 - 01:10:05.446, Speaker A: Okay, any questions about this? This comparison is not based on implementation. It's based on looking at commitment costs. Analytically, in basic loss, except M over K elements, everything was zero. What? Was the K there? Yeah. Okay, so the proverb commits to these M plus N elements, and they're all counts. So they just count, like, for, you know, this N term is saying, like, literally, for each of the N table elements, how many times was it looked up? Okay, now the counts sum to M just because they were M lookups. Right.
01:10:05.446 - 01:10:49.382, Speaker A: So you can have at most M over four things larger than four, although some to more than M, which means like if a fewer accounts are big, sort of just deal with them separately and everything else is small or something like that. And of course M just can't get that big relative to the size of the whole field because the fields have size like two to the 256 or something. Okay, I'm cognizant of being over time so I will try to move along but there's not too much more I have to say. I want to tell you what jolt is cover some experiments and a little discussion and that's. Yeah. So Lasso and generalized Lasso just apply to bigger tables. They sort of parameterize by an integer C.
01:10:49.382 - 01:11:23.502, Speaker A: So the protocol designer can choose the C and it kind of reduces the cost that depends on the table size from table size itself to table size of the one over C. Okay. And there is some some cost where if you set C bigger than one, no longer is it like the thing in front of the M, no longer is it one, but it goes actually up to three C and if anyone asks later, I'll tell you where the three comes from.
01:11:23.556 - 01:11:29.330, Speaker B: But yeah, what would you suggest thinking of C as canonically like two or something.
01:11:29.480 - 01:11:49.062, Speaker A: So in Jolt, the tables are going to have size like actually two of the 140 and so C might be like seven, six. Let me think if yeah, I think C will be like six but everything will be small. Everything committed will be small.
01:11:49.116 - 01:11:53.414, Speaker B: Right, but your point is like this one over C and the exponent is exactly why you can do big tables.
01:11:53.462 - 01:11:54.410, Speaker A: Yes, exactly.
01:11:54.480 - 01:11:55.900, Speaker B: Okay, exactly right.
01:11:58.270 - 01:12:02.940, Speaker A: Yeah. And Basic Lasso is just Lasso with SQL1.
01:12:03.950 - 01:12:04.314, Speaker B: Okay.
01:12:04.352 - 01:12:53.342, Speaker A: Let me give you an example of this decomposability property that's exploited by Lasso. And again, this is really not a new property. I mean our formal definition of it is but people were doing things like this before. So let's talk about the range check. So what you can have the prover do to do kind of 128 bit range check is have the prover provide as untrusted advice c limbs limbs is just digits but for base bigger than ten, I guess you got ten fingers and C limbs or something. So each of the limbs will consist of 128 over C bits. So for example, if you're breaking a number that's 128 bits into eight limbs, each limb would be 16 bits because 16 times eight is 128.
01:12:53.342 - 01:13:28.520, Speaker A: Right. So the prover commits to these limbs and just needs to prove that each committed limb is in the appropriate smaller range. And that the thing being range checked equals the appropriate weighted sum of the limbs. And so we've really just focus on the smaller range check. So we've reduced a range check for a giant range into like eight range checks for a much smaller range. Okay, so we've just decomposed one, lookup into this giant table into eight lookups into this table size two to the 16. Okay.
01:13:28.520 - 01:14:10.850, Speaker A: And sort of the whole point of the Jolt work is to show that this is like a pretty pervasive phenomenon, probably more pervasive than people realize. And I'll say more about that shortly. Okay. Again, people were sort of doing stuff like this already. I'll also point out, like okay, so in the Lasso protocol, the only information the verifier needs about the table is to evaluate a certain polynomial associated with the table at a random point. And for lots of tables, it turns out this polynomial can just be evaluated really quickly by the verifier directly. So for this particular small range table, this is the polynomial to evaluate it.
01:14:10.850 - 01:14:49.598, Speaker A: It's a 16 varied polynomial that just the evaluation is just like a weighted sum of the inputs. Okay, so that's why no one has to commit to the table. The verifier just does this evaluation itself. This is the only information the verifier needs to know about the table in Lasso. Yeah. So generalized Lasso is like basically Lasso, but it doesn't need decomposable tables. It really just needs that this polynomial is efficiently evaluatable and that I don't have very specific examples to give you, but in principle, that's like a much more general property than decomposing.
01:14:49.598 - 01:14:58.558, Speaker A: A lookup into the big table, into lookups into smaller tables. We'll see these polynomials can be for pretty complicated tables. These are very simple polynomials.
01:14:58.654 - 01:15:03.778, Speaker B: You're saying one reason that might be easy to evaluate would be because you have this decomposition.
01:15:03.794 - 01:15:07.686, Speaker A: Yeah. Decomposability implies maybe not, but there may.
01:15:07.708 - 01:15:08.614, Speaker B: Be other reasons why.
01:15:08.652 - 01:15:45.870, Speaker A: But there may be. Exactly right. I'm still looking for like a really killer example of something that is not decomposable, but is LDE structure. I think we're going to find them at some point, though. Okay, cool. One downside of generalized Lasso is that about a third ish of the committed field elements are not small, they're like arbitrary field elements, even if the table itself only contains small values. So that is the main reason to try to use Lasso itself and not generalize Lasso.
01:15:45.870 - 01:16:48.854, Speaker A: That's like the main cost of generalized Lasso is giving up on all of the field elements being small. Okay, so now let me describe Jolt. So Jolt is a new front end for virtual machine execution. So this is a pretty popular approach to Snark design, which came up earlier in the talk, where people basically will express a program in the assembly language of some simple virtual machine, whether it's the Ethereum virtual machine or like the RISC Five instruction set, which is a popular reduced instruction set from the computer architecture community. Or some projects will design their own simple instruction set, and then people will turn the program written in the assembly for that virtual machine into a circuit that literally like the circuit itself kind of runs the program for a designated number of steps. So, like, every step of the program, the circuit sort of figures out what instructions should be executed at that step and then it executes the instruction. It just keeps doing this until the time bound has been met.
01:16:48.854 - 01:17:34.200, Speaker A: Okay, so what Jolt does is it uses Lasso to replace the second step with a single lookup into a giant table. What that table is it's just the entire evaluation table of the instruction. Okay, so Tim, this was the answer to your question from before about why would you want an index lookup, here it is. So if you want to evaluate a primitive instruction on, like, 264 bit inputs X and Y, you just do one lookup into the table that for every possible input. X and Y spits out the evaluation of that instruction, and then jolt. The paper is going to work through all of the risk five instructions and show that the resulting table is decomposable. And I guess do I have an example? No, I'll tell you cost and at some point I'll give you one example.
01:17:35.290 - 01:17:42.220, Speaker B: So the index here is going to correspond to what?
01:17:43.150 - 01:18:48.240, Speaker A: To the input to the instruction. So if at step 1086 of your computer program, it's time to, I don't know, compute the bitwise end of X and Y, where X and Y are 264 bit values. The index into the table is x comma y and the value stored there. Okay, so, yeah, try to wrap this up in Jolt, we work through every step of the RISC five instruction set, which is, again, sort of a simple but popular instruction set, which has we didn't pick it out of thin air. There are other very nice projects targeting it. And for every one of those instructions or every step of the Risk Five CPU, the Joel prover commits to under 50 field elements, and all of them are pretty small. So all but seven are smaller than two to the 25 and none are bigger than two to the 64.
01:18:48.240 - 01:19:10.210, Speaker A: Okay? And so if you sort of do some crude math, sort of translating sort of committing to small elements into what it would equate if you were committing to sort of full size field elements, this is sort of like five arbitrary 256 bit field elements getting committed per step of the CPU. Okay, there are a couple of caveats.
01:19:10.370 - 01:19:21.142, Speaker B: That'S really just saying, like, sorry, like 43 times 25 plus seven times 64 divided by 250.
01:19:21.196 - 01:20:17.290, Speaker A: Yeah. The way you wind up thinking if you do this stuff for long enough is like, how many bits are in this field element? Like sum up all the bits committed and looking at the sum of the number of bits committed. And it's much cruder how the algorithms sort of wind up working, but that's roughly the just a couple of caveats. There are a couple of RISC five instructions that we handle via pseudo instructions, meaning like, if we want to, there's an instruction for division with remainder, and we sort of have the prover provide like the quotient and the remainder, and we check that with an addition and a multiplication that those were provided correctly. So each division with remainder instruction sort of turns into like two or three pseudo instructions, something like that. There are also some instructions we can actually handle more efficiently than I've listed on this slide. So some things are a little more expensive, some things are a little less expensive.
01:20:17.290 - 01:20:21.382, Speaker A: So I do want to put these costs in context of other approaches.
01:20:21.446 - 01:20:27.002, Speaker B: So can I ask one question about this? Because this is obviously kind of a key part of what's driving the performance.
01:20:27.066 - 01:20:27.342, Speaker A: Right?
01:20:27.396 - 01:20:40.258, Speaker B: So should I think of this as like, once you have this conceptual idea to use these kinds of lookup tables in this way, like, of course you can map all risk five instructions to tables of that form. Or should I be surprised by that?
01:20:40.424 - 01:21:33.666, Speaker A: There is a bunch of kind of gross work to be done, but you shouldn't be shocked. And if you see some, I'll verbally do the examples now, since that means I make sure I don't spend too much time on them. So, like bitwise end, so you have two field elements. You want to look at their representations in binary, take the bitwise end of those two representations and turn the result back into a field element. Right? So you can answer a bitwise end on 64 bits, 264 bit inputs by breaking each input up into like eight chunks of eight bits each, doing a bitwise end on each chunk and putting the results together, like concatenating them. The other instructions are more complicated, but it's the same flavor. So the instructions are just simple enough that you can kind of just break inputs into like they're bit representations into chunks, operate on each chunk, not completely independently as in bitwise end.
01:21:33.666 - 01:22:02.890, Speaker A: It is really independent and put them together in nice ways. Just work through it for everyone as instructions. Other questions? Yeah, so it turns out to be really natural. I think in the end it's sort of like in hindsight, oh, maybe, of course this is the way things should be done. I mean, I hope that some people's reactions I guess we'll see. Maybe I've been staring at this too long. Cool.
01:22:02.890 - 01:22:30.614, Speaker A: Okay. Yeah. So I want to compare this to some other approaches. Otherwise these are just numbers kind of hovering in the ether. So plank is a very popular back end for circuitsat, and it commits to eleven field elements per gate of the circuit. And I think seven of them are arbitrary. Even if, like, the gate values themselves are small, at least six are arbitrary, which means jolt is committing to about equivalent to five arbitrary field elements per step.
01:22:30.614 - 01:23:05.826, Speaker A: Of the CPU. This is like turning RISC Five program into a circuit with under one gate per step and applying Planck to it. Now, Planck is not tailored to these sorts of circuits that come from VM abstractions, which have a ton of structure. Planck is sort of, like, designed to handle arbitrary circuits. So this isn't really like, a fair comparison, but Planck is used today a lot, so it's meaningful to say the cost of jolt is like applying plank to a circuit with under one gate per step of the risk five CPU. Okay. All right.
01:23:05.826 - 01:23:42.938, Speaker A: So natural project to discuss is the other one that's already targeted the RISC Five instruction set, risk zero. Now, they're targeting today right now, I believe, 32 bit instructions. I gave you numbers, I guess, for Joel with 64 bit instructions, their cost would probably double if you went up to 64 bits, something like that. For the prover, our costs would not have if we went down to 32 bits. Anyway, that complicates the comparison a little bit. Okay. I don't know full details of what risk zero is doing.
01:23:42.938 - 01:24:22.330, Speaker A: I try to be conservative here. I think it's clear that their proverb is committing to at least 275 field elements per step of the risk five CPU. Now, these field elements are over a pretty small field. It's a 31 bit field called the baby bear field. So if you do the same kind of crude math I did before, it equates to about 35 field elements per CPU step. And, yeah, risk zero does not use MSM based commitment schemes. So it's like it's very hard to compare risk zero commits to this many elements with this commitment scheme that doesn't do MSMs to jolt, which commits to this many field elements per step, which does use MSMs.
01:24:22.330 - 01:24:51.220, Speaker A: We could avoid MSMs if we wanted to, but I think MSMs are, like, kind of the good properties of jolt are most evident. Yeah. Risk zero uses a Fry based back end, so they turn RISC Five into an error and apply Stark to error. Yeah. So this is really just meant to be very high level. And again, I might be missing some cost in risk five in risk zero. I don't know.
01:24:51.220 - 01:25:28.202, Speaker A: This is sort of the best I can do as a crude sense of how different approaches compare here. And then the other comparison point that I think makes sense to discuss is not targeting RISC Five. It's targeting sort of a custom virtual machine called the Cairo Virtual Machine from Starkware. This was a virtual machine that was designed to be Snark friendly. So it has just a handful of primitive instructions. It has what's called an immutable memory. The primitive data type is finite field elements, so it's all designed to play nicely with Snarks.
01:25:28.202 - 01:25:55.206, Speaker A: And they commit to about 51 field elements per step of the Cairo CPU, at least according to the Cairo White Paper. Now, Starware currently works over 251 bit field. I don't think they have to. I think they do this to match the field used by their elliptic curve digital signatures. Right. So they want that presumably to be really easy to quick to prove things about. So they work over a big field, even though they don't have to for other things.
01:25:55.206 - 01:26:42.120, Speaker A: But they can't work over something smaller than 63 bits because they use 63 numbers between zero and two to 63 to represent instructions in the Cairo CPU. So today, like, what's deployed? I guess they're committing to about 50 field elements, full field elements per step of the Cairo CPU. But I mean, it has to be at least like 13 Ish or something, just because the fields can't be smaller than about 63 bits. Okay, all right, so I already did some more examples of decomposable things. Apologies for going so over time. All right, so, yeah, we wanted to compare so we implemented basic Lasso. Actually, I think Lasso is implemented, but we only experimented on basic Lasso so far.
01:26:42.120 - 01:27:26.590, Speaker A: And this is Sam and Michael from a 16. Z have done amazing work to get this done. We compared it Halo two, which is a popular and highly engineered code base that implements basically the plank back end. And it sort of gives the choice of using the bulletproof polynomial commitment or KZG polynomial commitment. And it has a lookup argument built in, which is a variant of Pluckup. It's not literally Pluckup, but I'm pretty sure it's inspired by Pluckup, and I think its costs are comparable to Pluckups roughly. So, just as a reminder, the Pluckup prover commits to like five times max m and n field elements, where M is number of lookups, n is table size, and three fifths of them are like random field elements, whereas Lasso it's M plus n and all of them are small.
01:27:26.590 - 01:27:56.874, Speaker A: Okay, so Halo two, if you want to use KZG, you use the curve BN two five four, which is sort of the pre compile in ethereum. And with bulletproofs, it's the pasta curve. And there's a really nice implementation of the pasta curves from the folks at Zcash, I think. So, yeah. Nice engineering. Throughout Halo Two, the polynomial commitment we used in basic Lasso is something called HIRAX. So I do want to mention HIRAX has larger verifier costs.
01:27:56.874 - 01:28:34.714, Speaker A: I don't think they're astronomical, but they're larger. We used HIRAX because it's built into the Spartan library, which is we built on that code base. I think the verifier costs of Lasso will be attractive when everything is done, but I don't want to hide the fact that especially in these experiments, the verifier costs are bigger and my focus is on the prover cost. That's really where this shines. Also where I think the key kind of scalability bottlenecks are today. Okay, so this is Halo Two with Bulletproofs versus Lasso. And you can just focus on this ratio here.
01:28:34.714 - 01:28:46.046, Speaker A: This is the speed up offered by Lasso. So as the number of lookups is much smaller than the table size, which is two to the 16 in these experiments, we're seeing, this is now like.
01:28:46.068 - 01:28:48.814, Speaker B: Wall clock time of concrete implementation, right?
01:28:48.852 - 01:29:34.880, Speaker A: Yes. And also these are all single threaded time because we're not done parallelizing Lasso, but there's a lot of parallelization available in Lasso. And this is straight, just like how many milliseconds did it take for each? Okay, so remember the cost of pluckup sort of it depends on the max of M and N. Right. So you'll see that the costs don't really increase until the number of lookups is equal to the table size, because it's like table size dominates and then M goes above that and then it's M dominates. So the halo two sort of does best when they're the same. And in that case we're getting like a 19 x speed up, but elsewhere we're getting more like up to 50 x or something.
01:29:34.880 - 01:30:01.590, Speaker A: Okay? That's with bulletproofs. With KZG. With KZG it's faster, and I'll tell you why in a second, but you're going from a seven x speed up to maybe 2020 x. Okay. Why is it faster in KZG? It's because Bulletproofs has really slow evaluation proofs for the prover. I mean, that's basically the difference. So you can't ignore the cost of those evaluation proofs, especially in Bulletproofs.
01:30:01.590 - 01:30:09.480, Speaker A: But again, in the context of a larger snark, there's a lot of amortization available and things like that.
01:30:10.170 - 01:30:12.774, Speaker B: The last table was also bitwise end, by the way.
01:30:12.892 - 01:30:55.462, Speaker A: Yes, it's all bitwise end. So I did run not on the same system, but just on my laptop, just to get a sense of how much slower the prover would be in Lasso if the table was just filled with random field elements. It's about three x slower, which is actually what you would expect roughly, if you go through it's. Like a third of the committed elements are ten x slower, so you expect like a three to four x slowdown. So we would still get speed ups, like in sort of every setting over halo two, but they would be reduced by a factor of like three or four. Yeah. So I guess that's most of what I wanted to say.
01:30:55.462 - 01:31:41.014, Speaker A: I think there's a lot of engineering left to be done on basic Lasso. We just finished the implementation. Sam and Michael just finished the implementation, so I do anticipate maybe up to a two x further speed up for the prover in Lasso. But yeah, I mean, high level, it's just the commitment costs, I believe, are the bottleneck up to these issues about opening proofs as well. And if you commit to smaller and fewer field elements, the commitment costs are smaller and the field elements being small really is responsible for like an order magnitude, which I think is underappreciated today question back. It seemed like the workload you're simulating there is pretty synthetic. It's just.
01:31:41.014 - 01:32:47.834, Speaker A: Doing random table lookups? Have you thought about benchmarking this on some kind of real proof statement that's of interest? So in Jolt, the kind of table that will be used is exactly the bitwise end sort of table where the big table contains the results of 64 bit data type and the little tables are numbers. It's exactly this for all of the instructions. And then in terms of the cost of the prover in Lasso, the worst case is basically just big field elements. And so random field elements is within basically the worst case. Right, but it seems like that's how Lasso works. But if you're comparing it to plonk on a similar workload, is that a fair comparison? Yes, we're comparing to Halo Two, which has a variant to Pluck up, which, despite the name is not Planck. And three fifths of the committed field elements there are random no matter what the table has.
01:32:47.834 - 01:33:33.420, Speaker A: So you're talking about at most a factor of like 40% difference or something. And I think for the other stuff I don't know about lookups and some of the other lookup arguments, the stuff that isn't big is multiplicities and some of it it's table values anyway, they're also benefiting some from it. So the bitwise end is exactly what comes up in jolt and everyone kind of benefits somewhat from it. And the ones that benefit less from it, their dominant costs by far are the random field elements that are there no matter. What does that make?
01:33:36.510 - 01:33:53.070, Speaker B: Yeah. I mean, I don't know if this is part of Joe's question, but you could imagine trying to do, like, an end to end comparison where you really start with something written in a high level language, which then generates whatever pattern of lookups to whatever risk five operations it happens to generate and do the comparison on that basis.
01:33:53.570 - 01:33:56.446, Speaker A: And when jolt is built, that's exactly what we'll do.
01:33:56.628 - 01:34:10.246, Speaker B: What I'm hearing you're saying is that actually a lot of the forces that you're talking through that work here, you're actually saying are kind of, in a way, operation independent. Like you'd sort of expect these similar factors, whatever the workload is. That sort of right.
01:34:10.348 - 01:34:30.250, Speaker A: Yeah, I think that is right. I think the stuff the proverb commits to in Lasso is going to be just determined by what's looked up, sort of the smaller those values, the better for everyone. If they're using MSN, maybe one just very simple version.
01:34:32.110 - 01:34:37.178, Speaker B: If you look at the RISC Five sort of instruction set, are there certain operations where the speed up is more pronounced?
01:34:37.354 - 01:34:37.950, Speaker A: I see.
01:34:38.020 - 01:34:44.922, Speaker B: And if so, then the question would be like the programs that people care about, which are the operations that are being sort of used, the yeah.
01:34:44.996 - 01:35:16.490, Speaker A: Great, great. So in something like Jolt, actually jolt stands for just one lookup table. There's one lookup table with one caveat. And so every instruction is almost the same. The two caveats are the pseudo instructions thing like doing division by having the prover give a quotient and a remainder and checking them with a couple. So that turns into like two or three primitive instructions. Then the F caveat is some instructions we can actually do with a much smaller table.
01:35:16.490 - 01:35:48.646, Speaker A: So rather than size like two to the 128, we can do size like two to the 33, two to the 60, 65. It's because those instructions only depend on like x plus y. So x plus y, rather than having x NY, which is 64 bits each, you can operate on x plus y, which is 65 bits at most. Yeah. So there's not that much variation for different risk five programs in terms of what jolt would cost for your system. Right.
01:35:48.748 - 01:35:51.000, Speaker B: Maybe for the ones you're comparing to. Right.
01:35:51.690 - 01:36:39.954, Speaker A: There might be I think that there also won't be so much there. There's also probably pseudo instructions in other systems at least handling like risk five, I would guess. I don't know the details, but I think the way to do division is to do quotient and remainder. Also, they use different commitment schemes, so there's actually even less room for there to be variation if you use these Fry like commitment schemes. All that matters is, is the stuff being committed in the base field or an extension field and design. So everything's in the base field and I don't actually think that there should be too much difference between committing to a one and committing to a maximal value in the base field or something in that context. Yeah.
01:36:39.954 - 01:37:01.670, Speaker A: So there's much less an empirical variation that I expect than you would think. Everything kind of just costs what it costs, but obviously we'll see for sure when jolt is built. Okay, I went so far over, so apologies to everyone, but thanks for being such a great audience.
