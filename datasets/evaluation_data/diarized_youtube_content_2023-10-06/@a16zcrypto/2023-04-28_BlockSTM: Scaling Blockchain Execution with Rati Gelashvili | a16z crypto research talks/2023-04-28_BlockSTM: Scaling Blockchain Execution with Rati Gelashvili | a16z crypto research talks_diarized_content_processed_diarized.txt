00:00:06.390 - 00:00:06.940, Speaker A: You.
00:00:08.990 - 00:00:26.170, Speaker B: Welcome everyone to today's a 16 z crypto research seminar. Very happy to welcome Rati Gellisvili. He's going to be telling us about block STM. I'm very excited because I have heard so much about this paper over recent months. I really want to looking forward to learning more about the details. So rati over to you.
00:00:26.240 - 00:00:54.054, Speaker C: Thanks Tim. Very happy to be here. So, yeah, very happy to talk about our paper. This was a joint work with Sasha. Daniel george liquin daria you and Runtian. And the work started while we were all of us were working at Novi. And now this is basically the engine that powers the execution of the let.
00:00:54.092 - 00:00:54.680, Speaker A: Me.
00:00:56.430 - 00:01:37.880, Speaker C: This is just a very basic motivation slide. As we all know. Sort of blockchain is a huge system and usually it has this layered architecture. So we sort of have a consensus part of it where we agree on a sequence of blocks. Once we agree on the sequence of blocks, we sort of execute the they contain transactions like sort of this globally ordered sequence of transactions. We execute these transactions, we actually execute transactions in a block and then we sort of save them in storage. But all we are trying to say here is that everybody is trying to sort of scale up blockchains, make them as performant as possible.
00:01:37.880 - 00:02:49.450, Speaker C: But the weakest sort of this involves because it's like a layered architecture, if one of the components is very slow, it's just going to be the weakest link, it's going to be the bottleneck. So that's basically the motivation to look at all the aspects of it. And yeah, I believe Sasha talked about consensus, right? But yes, we're basically trying to tackle all of these challenges and today is going to be about execution. So in particular, what happens here, as I said, is exactly our input is a block. So we sort of receive this blocks, let's say like B two here. And then every validator this is very crucial sort of requirement, right? Basically every validator has to execute the transactions in B two and then the output of executing these transactions has to affect the global blockchain states. And in fact, it has to be a deterministic function sort of application that happens at the end of the sort of output of the block because there are many blockchains out there and there are a bunch of engineering approaches that we can take.
00:02:49.450 - 00:03:51.700, Speaker C: Of course, we can just take all the transactions in a block in the order they're given to us and just go and execute them one by one. This turns out to be really slow. I mean, especially as you start pushing the performance. If you get stuck with sequential execution, this will for sure be a violent so nothing sort of groundbreaking in the fact that at that moment you want to somehow use parallelism, right? Because you have multi processor architectures. Every computer has many cores. You want to utilize it to the fullest to try to sort of alleviate the spot on it. One of the approaches that's pretty common is to sort of ask the users to declare either the dependencies or some sort of hints for what parts of the state this transaction is going to write to.
00:03:51.700 - 00:04:46.782, Speaker C: We will talk more about this. This actually simplifies the problem quite a bit and allows some sort of efficient approaches but it's quite limiting. So first of all, it's not ergonomic if the user has to do all this work and moreover, what if you make a mistake or if you can't know exactly where the transaction is going to write then the transaction may need to be broken up, may need to be retried. So that's basically the limitation here. And so this brings us mainly to the design goals that we had in mind when we set out to sort of build this parallel execution algorithm. The goal was to make it basically transparent to the users. We have transactions, we want to execute them, but we don't want users to do any extra work from their perspective.
00:04:46.782 - 00:05:21.314, Speaker C: We want it to be the same experience as if we were running sequential execution engine. And on the other hand, because we actually want to be practically motivated and in blockchains you want to have considerations against adversarial behavior and also just all types of potential transaction loads. It's possible that every transaction will depend on something that the previous transaction writes. It's like unavoidable and we want to build something that in this case isn't much worse than the sequential execution.
00:05:21.382 - 00:05:21.710, Speaker A: Right.
00:05:21.780 - 00:05:38.786, Speaker C: And this is very critical design goal. But on the other hand, if there is parallelism, we want to exploit it to the fullest. So essentially it becomes like try to extract as much of the inherent parallelism as possible out of the transactions that we're given.
00:05:38.968 - 00:05:42.500, Speaker B: So what does transparent to users mean for you?
00:05:44.810 - 00:06:06.220, Speaker C: I remember us discussing about this word and what it's supposed to mean here is that they don't need to know how the sausage is made, they don't need to know that we are running parallel execution even if it was sequential execution. They don't need to give us any hints, they just submit the same transaction as it was before.
00:06:06.590 - 00:06:08.502, Speaker D: I would say invisible to users.
00:06:08.566 - 00:06:10.938, Speaker C: Invisible I think would be better.
00:06:11.104 - 00:06:21.440, Speaker D: And maybe another question I was thinking about malicious users like somebody trying to exploit this to break the system.
00:06:21.990 - 00:07:43.978, Speaker C: Yes, I mean in general, sort of the whole blockchain setting we assume sort of adversarial setting for this work. It manifests itself in a few ways and one of which is because usually if transaction is broken, we will see there is a whole execution environment for it and usually that's in charge of making sure that sequential or parallel, it has to make sure that a transaction cannot do something that's not allowed by specification. However, as it pertains to parallel execution for instance, if there was a way to construct some sequence of transactions that would break the algorithm or make it run for a century, we have to protect against that, right? And that's one of the motivations for saying no matter what happens, we want to design an algorithm that is not much slower than the sequential, right? So that's like we start there and we don't want to be worse than that. Of course there's going to be some overhead. Like if you literally give us transactions that are just sequential, we are dispatching all these threads. We're doing a lot of extra work, but as I will show you, we managed to keep the overhead pretty low. But great question.
00:07:43.978 - 00:08:19.126, Speaker C: Thanks. So there is actually a lot of academic work in a lot of related topics which I'll try to sort of briefly overview. One work is sort of called minor replay and this is actually exactly for the blockchain setting. So minor sort of kind of feels like proof of work, but it applies universally. Like basically the idea is that there is someone. So you can view this as a leader, for instance, right? So there is someone that creates the.
00:08:19.148 - 00:08:19.830, Speaker A: Block.
00:08:22.810 - 00:09:48.274, Speaker C: As I said, and we will dive deeper into that. But if we knew where each transaction was writing or if we knew which transaction had conflict with each other, we can be smart and the problem becomes significantly easier. The point is we don't want to assume it, but over here the idea is that let's say the leader that creates the block can execute the block, can see sort of what the behavior was, what read what, what wrote, what or what depended on what. And then there is all this work kind of deals with compressing that information, making that information as easy to transmit as possible, as easy to execute as possible. But the idea is that all the validators can replay the same block and they can be much more efficient because the data comes with this metadata that allows them to run it more efficiently, right? In fact, from their perspective, the way these algorithms are designed, they can really run a very simple fork join schedule based on this thing that the miner or the leader tells them. What are some of the sort of limitations of this approach? And this is also where the adversarial thing comes into play, right? Part of it. So there is for instance a huge trust issue here, right? Like why would they tell me the correct information? Like maybe they will give me adversarial information and then I will actually be slowed down.
00:09:48.274 - 00:10:47.666, Speaker C: I guess you can work around it and have a fallback that if it doesn't look like it's going very fast, I will just run it from scratch. But there is also an issue of latency, right? Because no matter what, we are reducing kind of the total work because the valve readers are faster. But someone still has to run it in blind, right? And then the latency still adds up and some of this extra work actually we're trying to still rely on some static analysis on precomputation to sort of make this dependency bag information as compact and efficient as possible. Actually, one of the most related work was from the databases literature. There is this famous paper called BOM. So what they did was they built a system that executes given set of transactions in a fixed preset order. So the order is set in stone.
00:10:47.666 - 00:11:16.298, Speaker C: You're just trying to execute it as fast as possible. This paper makes the following assumption. It assumes that we can have, and I will explain we can have a perfect overestimation of where the transactions are. Each transaction is going to write. And what I mean by perfect overestimation is again not the best phrase. Probably you are not allowed to underestimate. You are allowed to overestimate overestimation.
00:11:16.298 - 00:11:29.454, Speaker C: If you overestimate and not actually write there, the algorithm will deal with it. It will induce some overhead but not dramatic. But god forbid you underestimate. You have to run the whole algorithm from scratch.
00:11:29.502 - 00:11:30.098, Speaker A: Right?
00:11:30.264 - 00:11:50.570, Speaker C: And how do we get this sort of overestimates becomes the same problem. Either a static analysis pre execution to be fair to this paper, in fact they are in the databases setting and over there it's like stored procedures. It's much more limited setting. So it's much much more reasonable for them to make this assumption.
00:11:51.150 - 00:11:58.966, Speaker E: Sorry, probably to the question so we don't talk about reading at all, but is that not relevant?
00:11:59.158 - 00:12:37.100, Speaker C: So depending on an algorithm or approach it would be because at the end of the day actually how you design sometimes write conflict matters, sometimes it doesn't. But at the end of the day read write conflict is a key that you cannot run away from. All I mean is that this algorithm relies heavily on the it manages the reads but it really relies heavily on knowing the rights. It doesn't really care if you know the reads or not. And I think the engineering approach that I said is also similar. So it just turns out that if you know the writes there is a lot of sort of things that you can do. But I'm sure that knowing the reads would also help.
00:12:37.100 - 00:12:59.502, Speaker C: So the idea here is that because we have this perfect overestimation we basically can pre compute or pre build a data structure where we allocate a slot for every write that we estimate is going to happen. And then what happened exactly? Like very good question.
00:12:59.556 - 00:12:59.774, Speaker A: Right?
00:12:59.812 - 00:13:27.078, Speaker C: Because now every read I know exactly what it is supposed to read, right? So it can just go and either read it or it can wait there. So there is still because we are dealing with overestimates, it's possible that this location never gets written. So you have to deal with it in the algorithm. You have to actually mark it as like oh, I actually didn't end up writing here. And then you sort of do a traversal, et cetera. But we're not here to explain this algorithm but this is what it does. Right, it's a really good algorithm, it's highly efficient.
00:13:27.078 - 00:13:52.462, Speaker C: But again, it has again this drawback from our settings that we really don't want to require write estimates. We want to assume that we want to let people write arbitrary smart contracts where they have secondary indices. Like I read something and based on what I read, I do something completely different like auctions, whatever. And here any unestimated right is very costly.
00:13:52.526 - 00:13:53.140, Speaker A: Yes.
00:13:54.710 - 00:13:56.450, Speaker D: How do you think about locations?
00:13:57.590 - 00:14:21.850, Speaker C: Actually, good question. So let me sort of pick some terminology and I can use it for the rest of the talk. We can basically have this abstraction that the whole sort of state consists of these slots or locations. So let's call them locations and every write and read is just sort of on those.
00:14:21.920 - 00:14:22.298, Speaker A: Right.
00:14:22.384 - 00:14:33.882, Speaker C: And if you happen to write somewhere that some other transaction is reading and that transaction is serialized after you, it would have to read whatever the latest thing that got wrote to this location.
00:14:33.946 - 00:14:34.318, Speaker A: Right.
00:14:34.404 - 00:14:41.306, Speaker C: And if I ever use a path, it's the same thing. It's just something that refers to a particular memory location.
00:14:41.418 - 00:14:44.522, Speaker D: So an address would be a location.
00:14:44.666 - 00:14:49.842, Speaker C: Is that how we use it? Sure. Or memory is also I shouldn't use.
00:14:49.896 - 00:14:50.114, Speaker A: Right.
00:14:50.152 - 00:15:08.780, Speaker C: In shared memory that would be the but yes. So there is this storage and let's say storage consists of this locations and then we can identify every transaction, reads some of these locations and writes to some of these locations and can do like arbitrary logic expressed in a programming language to do so.
00:15:10.110 - 00:15:10.860, Speaker A: Cool.
00:15:11.630 - 00:15:31.206, Speaker C: And then there is this sort of prior work like tens of thousands of papers, I think award winning sort of papers from Ninety s, which is called software transactional memory. So this work is very relevant.
00:15:31.258 - 00:15:31.474, Speaker A: Right?
00:15:31.512 - 00:15:58.182, Speaker C: The setting is very relevant. The idea was that we are giving arbitrary transactions and we want to atomically execute them. And so it was basically designed or intended to replace locks and make this very easy abstraction for people to write programs and then if they have conflicts or not, this will be dealt by the framework.
00:15:58.246 - 00:15:58.860, Speaker A: Right.
00:15:59.710 - 00:16:58.154, Speaker C: And there are many as I said, it's a very well studied topic and there are many different approaches and in particular the one that's relevant to us is called Optimistic Concurrency Control. So in Optimistic Concurrency Control, you execute transaction and then you execute it optimistically you execute it and somehow you execute it based on some states that you currently have. Like the details might matter differ from an algorithm to an algorithm. But then at some point you have this extra validation step and what validation does is it basically checks whatever you read. If I read it now, would it still be the same. And then again, sometimes there is locking involved, sometimes not. But the idea is that if validation fails, it means well, I tried optimistically, but I didn't get the right input, so I have to discard it's useless, so I have to do it again.
00:16:58.154 - 00:17:45.206, Speaker C: But usually if you succeed, you can commit. So you say, okay, so I can use this execution result. Unfortunately, software transactional memory is rarely used in practice because these libraries sort of have limited performance. They have to do a lot of bookkeeping and whatnot. And for specific problems where you could apply software transactional memory, people usually just design handcrafted like specific solutions to that particular problem and it's more efficient. Also, generally outcome is not deterministic because all they care about is having some serialization order. But there is a line of research called deterministic software transactional memory where the extra requirement is that no matter where and how many times you run it, you arrive at the same output.
00:17:45.206 - 00:18:41.642, Speaker C: This would be applicable to us and we actually benchmark against one of the deterministic software transactional memory libraries. The thing here is that, as I said, even STMs, it's a known fact they're not used in practice that much because they're not very efficient. And the line of research that does deterministic software transactional memories views determinism as an extra constraint to satisfy. So they have even worse performance. They compete against each other, right? And we want better performance. STM performances like blackbox STM performance was for sure not good enough for us. The glimmer of hope here is that it has been shown in the past that for special, if you limit the scope to specific problems like linearizable queues, text, whatever, you could sometimes design software transactional memory libraries for those domain problems and make that efficient.
00:18:41.642 - 00:18:58.306, Speaker C: So there is a way to view this is why we call it SPM. There is a way to view our work as sort of blockchain specific software transactional memory library. Even though a lot of sort of I think some techniques that we come up with are more general.
00:18:58.488 - 00:19:04.740, Speaker B: It's like your canonical example. You're like suppose everyone's just pushing and popping on a common shared queue and that's all that's happening.
00:19:05.350 - 00:19:24.746, Speaker C: I think there was more than that, but yeah, Sasha had the paper about that and there was some other papers. The operations were limited, I guess. Yes, it was like I think it wasn't just used, it was sort of some generalization, some obstruction over this kind of data structures, but essentially I think.
00:19:24.768 - 00:19:26.790, Speaker B: You get deterministic and et cetera.
00:19:26.950 - 00:20:41.026, Speaker C: I'm not sure their thing was deterministic, but the only point I wanted to make here is to be fair, right? There are STMs that are pretty performant, but usually they've been shown in limited settings. There will be a few sort of places where I hopefully demonstrate that our blockchain use case allows us to do certain things that allow us to be efficient. So this is a good example of that and it certainly saves a lot of synchronization and makes our experiments more performant. I wouldn't say that this is like the make or break thing, but it's a good example of a specific thing that blockchains offer where it is very helpful for designing algorithms, which is in the blockchains, as we described before, right? You want to execute a block, then you want to execute the next block. In software transactional memory you have this long potentially infinite list of transactions that you have to process. You have to deal with garbage collection, you have to deal with knowing what's committed at any given time. We can just avoid worrying about it.
00:20:41.026 - 00:20:49.954, Speaker C: We can garbage collect at block boundaries and we can also just track when the block is committed. We don't need to know when transaction five is committed.
00:20:50.002 - 00:20:54.070, Speaker E: Do you think there's a discretization of time because of these blocks?
00:20:54.810 - 00:22:38.470, Speaker C: And it comes naturally, right, as a part of the problem. And this saves a lot of synchronization. Actually the other one, which might be a lot more powerful, I think, in terms of designing algorithms for us is that what especially optimistically concurrency controlled algorithms have to contend with software transactional memory algorithms is that they running some state. So here is an example. Like a user might have some invariants which are always satisfied in sort of sequential executions. But unless you take some extra precautions, you might violate sort of this invariance while doing optimistic execution. So for instance, let's convince ourselves that no matter in which order you run these transactions, assuming also in the beginning x is bigger than x, is always going to be bigger than y, right? If you execute these transactions like atomically but now if you're running them optimistically and just sort of just doing instructions concurrently with each other, this can happen, right? Like the green thread might write x two, then the blue thread might read it, purple thread might write sort of do two more writes and write x equals three, y equals two and then blue will divide by zero and make everything explode, right? So you have to have a way to deal with this because you cannot just build an end from a sort of software transactional memory point of view and there are properties called opacity and so on that deal with it and then they have to make sure that this holds.
00:22:38.470 - 00:23:17.010, Speaker C: And that also adds a lot of synchronization and work and complication. For us. Again, our setting is much simpler because we know that we have a smart contract programming language and it's running inside a virtual machine that executes that smart contract. And in fact, any behavior that is expressible in the language is an allowed behavior. It's VM's responsibility to catch errors, right? So for instance, division by zero will just be like a transaction will fail and it will return an output that I divided by zero.
00:23:17.080 - 00:23:17.518, Speaker A: Right.
00:23:17.624 - 00:23:52.382, Speaker C: And if it happened during speculative execution and this is not the real, like it was just read the wrong thing, it's fine because we're going to reexecute it later and it will give us the right answer. Right. But we don't have to worry about sort of catching panic and I don't know, making sure this never happens. Yes. So we can continue running. And then the last one is really key to sort of everything we do. I would love to say that sort of we made this observation first, but it's not true.
00:23:52.382 - 00:24:22.790, Speaker C: Bomb very explicitly made the same observation. I guess we were inspired by Bomb. We were working on it, we actually did realize it. And then we were reading Bomb and we were like, wow, they literally explicitly say the same thing. Which is great. So the point is that there is this intuition that if I'm allowed to choose from all the possible serializations, I will have more freedom. I should be able to run more efficiently.
00:24:22.870 - 00:24:23.306, Speaker A: Right.
00:24:23.408 - 00:25:27.358, Speaker C: And there is certain truth to that, right? Like if there might be specific blocks where depending on the ordering, there will be more conflicts or less conflicts. However, notice just this very sort of simple thought experiment, right? Let's say we're just some software transactional memory library and the purple thread is trying to run transaction X. The green thread is trying to run transaction Y. Basically, if green thread never runs and just purple thread runs, eventually sort of purple thread will commit X and X will be serialized before Y. If green goes first, y will be serialized before X. And if you've seen sort of this Fisher Lynch Patterson kind of impossibility results for consensus, it's very similar. Essentially what we're saying here is that if serialization has to be determined during runtime, we are solving these very consensus like tasks for basically every transaction that's running concurrently because we are also trying to decide which one gets serialized first.
00:25:27.358 - 00:25:35.358, Speaker C: And consensus is not an easy problem to solve. There are ways to solve it, but it inherently needs like synchronization.
00:25:35.454 - 00:25:35.954, Speaker A: Right?
00:25:36.072 - 00:25:54.402, Speaker C: And this observation that Bomb made, and I believe we really pushed it to the limit, is that I just fixed the order. I avoid having to worry about what goes before what. And I can do a lot of things now that will make my problem significantly easier.
00:25:54.546 - 00:26:01.610, Speaker D: You mean given that you want deterministic? If I didn't want deterministic, well, the preset order will also be a blessing.
00:26:02.350 - 00:26:07.914, Speaker C: This is even stronger. Exactly. This is even stronger. So if you just want deterministic well.
00:26:07.952 - 00:26:23.806, Speaker D: I guess if I don't care about determinism if I care about determinism I understand all your arguments can make total sense, but if you're saying that's a blessing of it, if you don't need determinism, then I'm confused.
00:26:23.838 - 00:27:15.730, Speaker C: I would even go that far except with an asterisk that of course there is choosing the order element of it right. So if you choose the order, it might be possible that I pre commit to this preset order that's just bad, and you happen to get lucky and somehow run it in an order that allowed you to run it parallel. And maybe you beat me. But let's say other than that, right? If somehow you made everything equal or if your preset order was, like, reasonable representation of all the orders available yes. This will destroy any possible STM library out there. That will not even if they don't need to be deterministic.
00:27:16.550 - 00:27:20.706, Speaker D: But if you don't want to be deterministic, why do you tell? Like, blue goes first.
00:27:20.808 - 00:27:21.554, Speaker A: Fine.
00:27:21.752 - 00:27:42.486, Speaker C: Because you have to decide, right? You have to decide. The algorithm is not explicitly solving consensus, right? The algorithm is sort of running things and validating and running the game, but inherently it's trying to decide who goes first, because if you abort me, you won consensus. And I have to agree with you that you were first or some sort of leader election.
00:27:42.518 - 00:27:42.714, Speaker A: Right.
00:27:42.752 - 00:28:07.694, Speaker C: And Pestin said, if my right aborts you, then somehow I beat you. And we are paying all this overhead to decide who goes before who. That's the intuition. But as I said, this is something that we exploit a lot. And I'll show you at least two ways how this is very beneficial. But the observation was made very explicitly by bomb exactly this observation.
00:28:07.822 - 00:28:22.018, Speaker E: So just making sure I understand that discussion. So this preset notion of the order, it's not about deterministic or randomized. It's about the fact that this order is pre committed to before. You look at the transactions that you're ordering.
00:28:22.194 - 00:28:50.800, Speaker C: You can even look at the transactions. But one point that you make is crucial is it is even stronger than deterministic. It's like deterministic would be like a deterministic. STM would be, just give me the same serialization every time. And this says, give me a serialization that I pick. Somehow we pick ahead of time. Your output has to be the same as if you executed this transaction sequentially in this order that is committed to ahead of time.
00:28:51.650 - 00:28:58.100, Speaker E: So the fact that it's ahead of time means it can't depend on this specific transaction that you're ordering, or it can.
00:28:58.950 - 00:29:09.318, Speaker C: So you could have a different component that looks at transactions and determines an order, and then we all execute it in that order.
00:29:09.404 - 00:29:10.040, Speaker A: Right.
00:29:11.050 - 00:29:24.970, Speaker C: That would be a way to use this system. But what we do right now is what you're saying basically, we just say, let's take the order that the transactions are in the block, and let's just do our best.
00:29:25.040 - 00:29:26.234, Speaker A: Right. Okay.
00:29:26.272 - 00:29:34.014, Speaker E: So we're all committing to a recipe that results in us all reaching consensus over the ordering we're aiming for.
00:29:34.132 - 00:29:38.510, Speaker C: Yeah, we just agree on an order.
00:29:38.580 - 00:29:43.330, Speaker E: Transaction, but we agree what the function is beforehand.
00:29:45.270 - 00:30:39.540, Speaker C: It's a bit confusing to phrase it that way, right. Because at the end of the day, the order means nothing. It's still the transactions that will determine everything, right? But you could say it's always the order and you can use it in any way. As long as we have a deterministic way to agree on the ordering, it can be some function that we can all run and gives us the order. And now that's the order, right? As I said, for us right now, it's very naive. We just take like the validator proposes a block, it contains transactions ordered somehow we will just take that, right? But actually it's a really interesting future direction to either try to quickly guess what order would be good and use that as long as you have a deterministic way or have unpredictability to something like this.
00:30:40.870 - 00:30:51.000, Speaker B: So the block producer fixes the serialization and then it's just now your job to speed that up as much as possible, right?
00:30:51.370 - 00:31:52.874, Speaker C: And actually another so the paper just got accepted at PPOP and in fact one of the very useful feedback we got from the reviewers was I already mentioned three almost separate lines of research that had connections, right? And there is also this thing called thread level speculation that threads doing computer architectures where you are trying to execute sequential code, but you're trying to guess, right? Exactly. And there also the order is fixed and you're just trying to do your best. So there is also basically some similarities there. So maybe I'm going too slow, but yes. So this is the high level overview of sort of how the system looks. We have executor basically where all these threads leave and we have a scheduler that is the brains behind it. Schedules, tasks and tasks are either execute this or validate this.
00:31:52.874 - 00:32:38.266, Speaker C: And when you get the task. So the way it's designed is basically threads just constantly do the repeat a loop where they try to get a task. Let's say it gets an execution task. Then the thread goes to the VM with a particular transaction that was given and says execute this for me. While in order to execute that, the VM needs to make reads. Reads will be from this MultiVersion data structure that I'll explain that basically wraps storage, right? But it also includes some speculative things that other transactions might have written. So it will run it in this weird, whatever optimistic sort of view of the world.
00:32:38.266 - 00:33:17.618, Speaker C: It will never write. It will just produce the right set and the output and will return it back and then we can apply the writes to the MultiVersion data structure. And sometimes actually everything needs to be validated. It's still like an optimistic concurrency controlled algorithm. So a validation simply just goes and checks that reads are still valid. And then every time you do a task, you just have to go back and update the scheduler based on what happened with this task. Like did validation fail, did it succeed, et cetera.
00:33:17.618 - 00:33:28.858, Speaker C: And at the end of the day when. We're done, we can basically apply what we have in this MultiVersion data structure back to storage and we continue.
00:33:28.944 - 00:33:29.580, Speaker A: Right?
00:33:30.110 - 00:33:54.190, Speaker C: And we can also, if there is more to the output than just writes, we can just take the sort of last re execution output. But for simplicity, we can assume that it's just a global state. It's only reads and writes. And basically what you need to at the end of the day, the MultiVersion data structure will contain the final writes, which are consistent with the writes that would happen if you executed everything sequentially.
00:33:54.530 - 00:33:57.934, Speaker D: What does the validate do? How do you validate?
00:33:58.062 - 00:34:35.722, Speaker C: So there is an implementation detail to make it more efficient by stamping everything. But essentially we're validating that whatever you read during the optimistic execution, you have your reset. Which means, like, I read this memory location and this is what I read, I read this location, this is what I read. We reverse in them so that we don't actually have to check the whole data. Right, but that's a detail. And then you just go back and you reread the same locations and you make sure you just check whether it's the same or not. If everything is the same, you say validation succeeded.
00:34:35.722 - 00:34:38.890, Speaker C: If anything fails, you say validation failed.
00:34:39.050 - 00:34:41.230, Speaker D: And what happens after the failure?
00:34:41.670 - 00:34:43.342, Speaker E: What if multiple of them fail?
00:34:43.406 - 00:34:45.220, Speaker D: Do you kind of go the same?
00:34:47.910 - 00:35:07.986, Speaker C: What is similar to what I've described before is that when a validation fails, it means this transaction needs to be re executed. So when you go back to the scheduler, you tell validation failed. It will internally update the state and mark this transaction abstractly as it needs reexecution.
00:35:08.098 - 00:35:12.346, Speaker E: Does this mean that the commits that happen, the writes that happen in the.
00:35:12.368 - 00:35:24.362, Speaker C: Execution of that transaction, yeah, we will clean the rights and actually we'll do something more than that, which is one of the key pieces actually. But yeah, we will actually mark them as estimates.
00:35:24.426 - 00:35:31.760, Speaker D: But I'm getting ahead of failure is a bad thing or it's just like an overly broad check.
00:35:37.510 - 00:35:41.890, Speaker C: Failure is obviously bad because we have to re execute this transaction.
00:35:42.550 - 00:35:58.554, Speaker D: I'm not sure I understand this part, sorry, background. But it may be that in the sequence, something should change after this transaction and maybe this transaction should still go first. Why do you need to push it later?
00:35:58.752 - 00:36:02.266, Speaker C: We don't push it later, but there.
00:36:02.288 - 00:36:04.090, Speaker D: Is just one dependencies.
00:36:04.590 - 00:36:21.840, Speaker C: So two things. First, because we have a fixed order, things are a bit different than just saying basically for us, a failed validation means more than just re execute this transaction. And I will show an example.
00:36:22.710 - 00:36:23.650, Speaker D: I'll wait.
00:36:23.800 - 00:37:09.882, Speaker C: Yeah, because actually we cannot just take this transaction and serialize it somewhere else, right? It has a fixed order. So if some other transaction was successfully validated, but now this one is before and I have to re execute it, it also means that the next transaction, I also have to revalidate. Right, but then we do actually that's exactly what I was going to get, actually. Yeah. I'm going to talk about one piece here that I hope will be it's relatively simple because especially I also mentioned that Bomb uses a similar structure. But BOM assumes that basically you have this overestimate so you can statically create the data structure for every estimated rate. We have a similar data structure.
00:37:09.882 - 00:37:40.054, Speaker C: It's a multi version data structure. This is also a very common thing in general, right, where the idea is that different transactions do not override each other's data. They just write to the same data structure and they just say, I'm transaction five. This is what I wrote, right? And this is a general technique that lets you avoid write conflict. And then what you do is like, okay, so transaction write by transaction six happened. You just put it there and it's ordered because we have an order. So that's convenient.
00:37:40.054 - 00:38:11.140, Speaker C: And then a read will just have to basically find the highest transaction that's lower than it in order to read that, right. And if it doesn't find anything, it has to read from storage. Okay, so this I also mentioned, but it's relevant for the technically noble things that we do to understand. So the way the scheduler works, it has this global queue obstruction. Of course, it's not implemented as a queue of tasks, but it's basically.
00:38:13.590 - 00:38:13.906, Speaker A: The.
00:38:13.928 - 00:38:34.394, Speaker C: Interface is that every thread just keeps going. Eventually the scheduler will tell it you're done. But before that, it's just like, give me the next steps, give me the next steps, give me the next steps. And the scheduler is in charge of deciding whether it has to do execution or validation. And if it has and which transaction, right. If you have to do validation, it will tell you, here is a reset, go validate. It's still valid, right.
00:38:34.394 - 00:39:03.934, Speaker C: If you have to do an execution, it will tell you go to the VM execute transaction file. But internally, it has to have all this logic to dispatch these things, right. And so it has to essentially do two things. One, or it's very desirable that we do two things because we have preset order, we want to make sure that we run the lowest task that there is available to us, right. Early transaction.
00:39:03.982 - 00:39:04.302, Speaker A: Yes.
00:39:04.376 - 00:40:09.850, Speaker C: Because if like transaction five kind of needs reexecution or validation, it's useless for us to do 100 because most likely this is not going to be useful work. And the validation must logically occur in sequence. Meaning if we executed these three transactions and then we validated them and validation of two failed, we know that two has to be reexecuted, but actually three needs to be revalidated because two might change something and because we fixed the order, three will have to reread this. So this is just like we can go through this quickly. Essentially, this is what scheduler does, right, and an important thing exactly from the previous example is that successful validation for us doesn't mean it's safe to commit. There is like a predicates that we use in our proofs to show that eventually everything's committed to the state that it needs to be when the block is committed. But we don't even track what's safe to commit, right? There is a predicate that lets us know when everything is ready.
00:40:09.850 - 00:40:47.202, Speaker C: But a successful validation is just nothing but a failed validation means that you need to re execute and possibly mark other things to require revalidation. And the scheduler also manages dependencies. In fact, if a transaction tries to read a location where another transaction is estimated to write, we suspend it, we mark it as a dependency, and when that transaction reexecutes, we continue. And at this point, you must be asking me how the hell do you you just told me you don't have estimates. You don't want anybody to give you estimates. Like why? How are you cheating?
00:40:47.266 - 00:40:47.830, Speaker A: Right?
00:40:47.980 - 00:41:51.082, Speaker C: And so this is actually one of the key ideas in the paper, which is, okay, so where do the estimates come from? And it's actually really simple idea. And as I will show you, it improves something that people have been doing for many years. So essentially all we do is when a transaction fails, we know what it wrote before, so we just go and mark all of them as estimates, right? We're saying, look, you ran into some states that wasn't quite the state you wanted, but this is what you produced. I'm going to assume that the next time you might it's not for say, like we're just assuming that you might write to the same location. So if another transaction so let's say transaction three aborted, we mark the previous thing as an estimate. Now, if a transaction five comes, I see an estimate. Essentially, the thought process here is I know I have to re execute three.
00:41:51.082 - 00:42:51.086, Speaker C: Previously three wrote here. So there must be likelihood that three writes here. So let me not go ahead, let me wait until three runs again and then run, right? And this is really important because one of the things that kill STM performance is this cascading of words where something changes and just cascades out of control. And let me say why this is actually important, right? It's a very simple idea, but why is it powerful? There is this concept called so there is all this static analysis work, right, to guess or user provided things. But a lot of systems, what they do when they need some sort of estimates for performance is they do like pre execution. So you take all your transactions, you run them from the initial state, embrace and link parallel, and then you take whatever they write and whatever they read as estimates. It's some sort of estimate, right? And this is from the initial state, you see from the initial state.
00:42:51.086 - 00:42:59.642, Speaker C: And the idea is, okay, this is not perfect, but we just waste this one thing. But it's literally embarrassingly paralyzable.
00:42:59.706 - 00:43:00.320, Speaker A: Right.
00:43:01.250 - 00:43:41.106, Speaker C: And this is a very known paradigm, right? It's called pre execution. And you can view what we do as just a strict improvement over preexcution. Why is that? Because in a good case where we didn't have any aborts, we did not need to execute everything one more time, we just didn't. And when there are aborts, your meaning preexcution. So pre execution estimates were from the initial state, but here, every time a board, I am running in a more fresh state and my quality of my so basically I generate estimates when I need them and the quality of them keep improving if I keep aboarding.
00:43:41.158 - 00:43:41.760, Speaker A: Right.
00:43:42.130 - 00:44:03.474, Speaker C: So this is basically the idea here and contrasted with bomb, our estimates are just best efforts, right? Like if they're good, they're good, but if we miss something, it's not a disaster. And in the other approaches, if you underestimate, you have to run from the beginning. I was just wondering, so this other.
00:44:03.512 - 00:44:08.566, Speaker E: Task that sees an estimate stops and wait until this is so could it.
00:44:08.588 - 00:44:10.130, Speaker D: Happen that all of them got stuck.
00:44:10.210 - 00:44:52.738, Speaker C: Waiting somehow in a circle? We prove that that's not possible. The idea there is simple because there is essentially boils down to the fixed order and just a descent, right? Because you can only get stuck on someone that's lower than you, so you cannot infinitely descend. Makes sense because you are only going to wait for something that you would have read, which would be some estimated write by a lower, strictly lower transaction. And then if that transaction is also weighted for someone, then again and you cannot go should I roughly think of.
00:44:52.744 - 00:45:06.520, Speaker B: It like the old solution? Unestimated, right? You have to go back to the very beginning, but here you just have to go back to the sort of deepest unestimated right and go from there instead of the beginning. Is that how you think about it?
00:45:07.710 - 00:45:12.860, Speaker C: Yes. At the end of the day.
00:45:14.670 - 00:45:15.034, Speaker A: All.
00:45:15.072 - 00:45:35.022, Speaker C: You are using the estimates from is to like the worst that a conflict can do is to trigger you to re execute. Because execution is much more expensive than validation or anything else that we do. So any knowledge that you know that someone might write here or read here, all you're trying to use it for is to sort of avoid this mistake.
00:45:35.086 - 00:45:35.602, Speaker A: Right.
00:45:35.736 - 00:46:11.342, Speaker C: And this is also usually how pre execution is used, but it can also be used in other contexts, in other situations. But here you're essentially saying I will start blind. This is another good point where if someone gives our algorithm some estimates, we can start from there, we can just mark them as an estimate, right, but we just start blind. But if we happen to abort now, I'm going to use these as estimates so I have at least a set of things that will avoid me making the same mistake and then we will keep going.
00:46:11.396 - 00:46:12.000, Speaker A: Right?
00:46:13.330 - 00:47:03.102, Speaker C: And there are also two ways to deal when an estimate happens. One was to just crash and do it again. I think what we do currently is just really just wait at that moment and then restart. And funnily enough, we were really surprised. Like sometimes we would run sequential load faster than sequential execution and we were like, what the hell is this? What did we mess up? And then we realized what was happening. Because if you do it this way, it's possible that some prefix was useful and you got lucky and you waited on its dependency. Even though it sequential load usually just means that the transaction has a read that depends on a write from here, right? But it might be like in the middle or in the end of this transaction and then you could have done useful work here and then reused it.
00:47:03.102 - 00:48:06.734, Speaker C: And this actually happened way more often than we expected. And every time we forget and then we see it again, we're like, what? This was one thing that I think allows our sort of system to be really fast and I think it's kind of novel. And the other thing is also a novel thing, which is, again, very heavily dependent on the preset order. So the idea is how do we pick the tasks with lower transaction index? Because this is also really crucial for system performance. So in fact, my advisor has a bunch of papers about it and this is also a very well studied topic, right? Concurrent data structures, concurrent priority queues, concurrent ordered sets. The big problem there usually is that if you do a priority queue, you want to extract the lowest index, like lowest element or highest element, highest priority element. Everybody's kind of contending on the same element and it's hard to parallelize, but they have ways like they usually relax the semantics.
00:48:06.734 - 00:48:32.170, Speaker C: So you can take one of the highest priority ones and you can kind of scale it. It's pretty decent. But actually for the performance that we were intending given that was not good enough or we wanted to push it farther. And what we do is we again rely on the fact that our priorities are not arbitrary, they're indices. And they're like very well compact indices from one to the zero to block size.
00:48:32.240 - 00:48:32.714, Speaker A: Right?
00:48:32.832 - 00:48:53.242, Speaker C: So what we do is essentially counting based sort. So we do basically a counting based concurrent order test, right? So essentially the idea is that even though we don't track what's committed, usually some prefixes, usually the threads are working on some dense interval they're executing and validating.
00:48:53.306 - 00:48:53.534, Speaker A: Right?
00:48:53.572 - 00:49:03.570, Speaker C: Like everything here, even if we don't track it from God's view, is committed. And everything here, we haven't even started yet. And this is a very dense set of indices.
00:49:04.070 - 00:49:08.226, Speaker D: My assumptions can only depend on things before you, right?
00:49:08.408 - 00:49:20.834, Speaker C: Yes. And the reason right. But for here, even that's not like as long as we just keep a status per index, just looking for the next one, it becomes the traversal.
00:49:20.882 - 00:49:21.046, Speaker A: Right?
00:49:21.068 - 00:49:44.862, Speaker C: You are just traversing the indices and you're looking like, oh, does it need validation? Doesn't need execution. So you just track an index, you reset that index. The worst thing that happens is you have to walk over a bunch of empty slots, right? And then we optimize it a little bit. Like you can have either one index or you can have one index for validation tasks, one index for execution tasks. But these are all details.
00:49:44.926 - 00:49:57.560, Speaker D: So if I have K and everything before K got executed and validated already, and I validate K, then I can commit K?
00:49:58.010 - 00:50:06.940, Speaker C: If it's committed, yes. But we are experimenting with these things. But in the current version of the system, no, that's true.
00:50:10.110 - 00:50:14.620, Speaker D: Are you using this property? No, you're not using this data.
00:50:15.150 - 00:50:54.850, Speaker C: But it's true. So if the K things are committed and you happen to successfully validate K plus one, it means that K plus one can also be considered committed. But we don't track it. But that property is technically true. The reason we don't even track it, it allows us sometimes to even not do that sequentially. You can have five validations that all succeed and that all happen concurrently. And now technically all five of them are essentially committed because if one of them failed, it would mark all of them for revalidation.
00:50:54.930 - 00:50:55.560, Speaker A: Right?
00:50:56.430 - 00:51:23.082, Speaker C: But yeah, what you said is for sure correct. This is just an example. So we are at this index and essentially you can do fetching, increments and some light synchronization and other things that we observed is essentially what this index does. It just tricks the minimum of all things that we know requires validation or execution and we just have to maintain that environment.
00:51:23.146 - 00:51:23.760, Speaker A: Right.
00:51:24.450 - 00:51:49.874, Speaker C: But the funny thing that happened also is that so fetch and increment, everybody touches the same counter. So that counter is kind of contended, but it also gets amortized because in between you do the task and whatnot. But once you do that, fetch an increment and these instructions are implemented in hardware, they're relatively like they're very optimized. But once you do that, it also acts as a load balancer on individual statuses.
00:51:49.922 - 00:51:50.134, Speaker A: Right.
00:51:50.172 - 00:52:16.994, Speaker C: Because everybody gets a different index. Like if I do a fetch and increment and you do a fetch and increment, it's a problem. We get two different indices. So in the beginning we were doing some crazy log free things to sort of do build these statuses. And then in the process of productionization, we were just replaced everything with locks, same performance. Because fetch and increment is basically a load balancer. I will not go into this.
00:52:16.994 - 00:53:58.350, Speaker C: It's just some sort of details that you have to deal in implementations and proofs. We have like twelve pages of proofs to actually prove that the algorithm is correct in terms of what it will like, it will terminate and it will produce the same output as the sequential execution of the fixed order. And there are some optimizations that are really useful but I will not go into them because they're important for performance but it's not very fundamental. The only one I can mention is sometimes it all becomes a question of how early can we start validation? Because the earlier you can start validation the earlier you catch potential of words and you avoid the cascading and also still how can I avoid doing useless work? Because when a validation fails you re execute that transaction but you dispatch everything else for validation. Validations are cheap but if you sort of do this wave after wave after wave it will add up. So we have ways to sort of like if you execute it before and you have your set of where you wrote and you get reexecuted and you write to a strict subset of it we can prove that you don't need to revalidate anything because they would have seen the estimates and things like this, right? But they're neat but they're not like some foundational thing, right? They're in the context of the algorithm and lazy commit I mentioned. It basically has to track how many tasks are ongoing and where the indices are has to do that atomically.
00:53:58.350 - 00:55:29.726, Speaker C: So we do it by essentially doing a double collect. You read something, you read again if it's the same, we can prove that the block can be safely committed. We prove correctness and finally I can show you the graphs. So the way we benchmarked this is we benchmarked it we also know that it helps the system end to end performance, it's running in production. But to demonstrate academically what this thing does we had to isolate the execution component and that means we get a block that's ordered, right? We don't do consensus and we don't go to actual persisted storage because that's expensive. So we have some in memory representation of all the states and we just execute it in memory and we have sort of implementation on the DM code base and also on Aptos and we compared it to BOM in order to be able to compare to bomb bomb needs estimates, right? So we just gave it perfect estimates for free because sometimes it will beat us, right? But it's because it knows this thing that it would have to work really hard for or would be impossible to get. And then there is LITM, which is this recent deterministic SPM algorithm which basically came out a few years ago and has comparison with other deterministic algorithms and outperforms them.
00:55:29.726 - 00:56:15.742, Speaker C: So we chose it as a good candidate and we tried it on peer to peer transactions. I will show you usually people might think that this is just a very straightforward transaction. It's not like this transaction is very complicated but it's also not very trivial. It does a bunch of reads and writes. And I think at the DM days it was for a lot of checks that it had to do about accounts and whatnot. And even at Aptos it has to do a few extra reads like sequence number, whatnot. So it's non trivial, but what's a good thing about this kind of load is because it's peer to peer, we randomly choose who transfers to who.
00:56:15.742 - 00:56:42.306, Speaker C: And by controlling the number of accounts, we have a very good control on the number of conflicts that we have to benchmark on. So for instance, if we do take two accounts, it's going to be a fully sequential load because every transaction will modify the same balance. It's right read and write and then we try incremental. Like we also try different block sizes.
00:56:42.498 - 00:57:00.682, Speaker D: So one way which it seems favorable for you to take those transactions is like, it seems like the worst Cooper algorithm is something that changes outputs depending on the reads. Because you said suppose you wrote to some address ax, I'm going to guess that you write to address X. And the worst kind of algorithm maybe.
00:57:00.736 - 00:57:03.086, Speaker C: Is like if yeah, some condition, write.
00:57:03.108 - 00:57:04.766, Speaker D: To X, otherwise write to Y. I.
00:57:04.788 - 00:57:06.942, Speaker C: Agree, that's a very good point.
00:57:06.996 - 00:57:07.600, Speaker A: Yes.
00:57:09.410 - 00:57:31.766, Speaker C: That'S a very good point. I think it would be great to also benchmark on that, although it would also demonstrate this adaptivity of getting pressure estimates. So I would be actually very curious. That's a really good point. So this was also the transactions that were like yeah, but we can write our own transactions. So that's a really good point. Yes, we can definitely benchmark that.
00:57:31.766 - 00:57:54.762, Speaker C: Just about no, thanks a lot. So let me show you though, for these ones, what we observe. So first, interestingly, for block size one k, the first thing to notice is we scale as we increase the number of threads, which is always a good sign if you're designing sort of a concurrent algorithm.
00:57:54.906 - 00:57:58.014, Speaker B: The y axis is thousands of transactions per second.
00:57:58.212 - 00:58:56.798, Speaker C: Yes, the number of threads and I guess these are the lines themselves are like sort of it's tried for 10,000 accounts here and the black line is a sequential baseline. So a few things to notice actually. So the lithium, it scales with sort of ten to the four, but I don't know if it's that's order of magnitude, but it's still significantly slower than us. As you create more conflicts, it starts being worse than the sequential right. And this was something that we really wanted to avoid. Now against Bomb, what's really interesting is on the left hand side we actually big Bomb, which should not be possible because it has perfect right estimates and for ten k it actually bids us. So this might also be our implementation artifact, even though we tried to do a good implementation.
00:58:56.798 - 00:59:14.482, Speaker C: But the point there is that Bomb has to pre create these data structures and that's not for free. Even though we so I think sort of that overhead was why for smaller block size, we could bid it for larger block size. Of course it performed better, but it had this perfect estimate.
00:59:14.546 - 00:59:17.980, Speaker D: Why does it matter whether the block size is one k or ten k.
00:59:19.390 - 01:00:14.250, Speaker C: Only for Bomb and also as an Asterisk, it's like our implementation of Bomb. But no matter what you do, you have to create this data structure ahead of time that contains like a slot for every estimated write. And I believe the cost of that was somehow yeah, because for ten k the rest of the thing is faster and it makes up for the cost that you pay even though for ten k I guess the data structure will also be larger but they don't grow at the same rate. Right. The amount of time you spend some amount of time to build the data structure, but for one k it was in such a way that this was actually significant compared to how much time it took to execute that many transactions. But once you go to ten k, the execution time became large enough such that it wasn't important anymore. But anyway, that's like an implementation artifact.
01:00:14.250 - 01:00:20.714, Speaker C: The more important thing is that we are basically in the same ballpark and that algorithm has perfect right estimates and we don't.
01:00:20.762 - 01:00:20.926, Speaker A: Right.
01:00:20.948 - 01:00:27.950, Speaker E: I'm curious, what growth rate should I have in mind for the number of conflicts as block size increases.
01:00:29.890 - 01:01:19.022, Speaker C: For real load? Yeah, that is an open question. But this is a really good question because I think there are at least probably it's still early days for blockchain adoption, right? So I believe it happened on many blockchains when some project came alive and it dominated the load for extended period of time and then it can lead to slowness or whatever. So this is also the case when things might become more sequential. But you would hope that in a case where there are lots of different applications running and lots of different users submitting transactions, there would be a lot of transactions that would just be completely independent of each other.
01:01:19.076 - 01:01:25.570, Speaker E: Like square root maybe sorry? Oh, I was saying maybe square root in a very uniform random.
01:01:28.950 - 01:01:34.180, Speaker C: Like NFT drops very bad spending more money to each other.
01:01:35.910 - 01:01:47.690, Speaker D: Can you give us a benchmark of suppose we're not in blockchain settings. Suppose you were doing this for DM or Visa or something like that. Would the same technology apply? Would those numbers be comparable?
01:01:49.710 - 01:02:08.686, Speaker C: I think if you are in the setting where you can whatever the assumptions we made about the blockchain, right, we have blocks, we can take a preset order, we have a virtual machine that executes the smart contracts. Under these assumptions, we can sort of.
01:02:08.868 - 01:02:17.410, Speaker D: I'm kind of thinking Visa probably needs to do with more than that. No? Is this Visa scale or other systems.
01:02:17.910 - 01:02:36.454, Speaker C: Numbers that I've heard is like I think it is Visa scale because I've heard 60, I've heard 100. I think that's the order of magnitude that transactions that they're doing. So theoretically, this engine can do that. Now, again, the whole blockchain will not have that throughput, right? Because it will be public.
01:02:36.492 - 01:02:38.730, Speaker B: Single server, right? Just doing execution.
01:02:39.070 - 01:03:11.570, Speaker C: Yeah, this is a single server doing execution. And we use a lot of threads, right? So it's also nontrivial. I think requiring 32 cores is probably a bit too much, but maybe like 16, eight is quite reasonable. And I think we currently run with default configuration parameters. Eight, I think. So the things that I wanted to demonstrate here so aptos, peer to peer transactions are a bit lighter. So it has like eight reads, five writes.
01:03:11.570 - 01:04:05.160, Speaker C: Again, one thing is like showing just the scalability with the number of threads. But also, I think the important point here is the line here, right? So even fully sequential load, it's like 1020 percent, if I remember correctly. Like slower than the sequential execution. And actually, to answer your question, I think it's this world where you hope for the best. So you want to be really performant when things are nice and distributed and very few conflicts, but you want to be really safe and also not worse than sequential when, let's say, someone adversarially submits to you, like a fully sequential load, right? So you want to interpolate it as smoothly as possible. But squared is a good number. I like it.
01:04:06.330 - 01:04:19.770, Speaker B: But there's like some kind of instance, optimality type conjecture that he's formulated here. You'd love to say, like workload by workload, you're like constant competitive with scheduler tailored to that workload, right?
01:04:19.840 - 01:04:23.462, Speaker C: Yeah, it would have to be competitive analysis.
01:04:23.526 - 01:04:24.842, Speaker A: Exactly. Good point.
01:04:24.976 - 01:04:37.790, Speaker D: I would guess that's false, though. Once you have a guess, wouldn't something that just maliciously adversarily just violates all your guesses be making you worse in that particular instance?
01:04:40.290 - 01:04:42.980, Speaker B: It's a big design space. You have to think about it.
01:04:46.070 - 01:05:34.270, Speaker C: This is my last slide, I guess. There is a bunch of extensions. This was what we wrote long time ago. We now know many more things that can be combined with experimenting, with actually tracking the commit with as little overhead as possible, for instance, and many other aspects. But these are some immediate things that one could think about, like sort of a concept of nested transactions, because right now the failure granularity is a whole transaction. And maybe you want to have sort of sub blocks where you just have to avoid something smaller. Like you have a loop that internally mints many things and one of them fails, you know, that it's fine, you don't have to re execute the whole loop.
01:05:34.270 - 01:06:03.258, Speaker C: You could combine this with minor replay, maybe. Right, because this is just the way that the miner could do it. And then it can extract stuff from the MultiVersion data structure and send it to others and the numbers that are reported there and. It's very believable. Once you know the dependencies, you can be faster than us, right? It's not a question. But again, you would have to overcome this trust and other assumptions. We didn't optimize for non uniform memory access.
01:06:03.258 - 01:06:42.290, Speaker C: That means like machines where you have multiple sockets. So you can have like 32 cores here and 32 cores here. In total, you have 64. But then data locality becomes the main thing in those algorithms and you have to sort of design your algorithm specifically to scale beyond out of socket. But first, because it's the blockchain setting and we're not necessarily trying to just require these very expensive machines but it would be interesting research space to generalize it. It should be possible. But what we have right now doesn't scale out of socket.
01:06:42.290 - 01:07:58.280, Speaker C: And the same about hyperthreading because usually some architectures has two threads that run on the same core but only one of them makes progress at any given time. But you can amortize some sort of I O and things and benefit from but again, we didn't design our algorithm in such a way to exploit these things. And it's an interesting direction, actually, even in our setting because the experiments here in memory but in reality, things are coming from storage, right? And in general, I guess we have many other problems. Storage is a huge problem which I don't know, authenticated storage, obviously, for blockchains is like a huge source of performance bottleneck. And in general, I think the biggest open problem out there in the blockchain space is Sharding, which we have to solve if blockchains have to get to sort of the promised land, which we believe. And we're working hard on it. But yeah, I think some of the open thanks.
