00:00:10.410 - 00:00:43.082, Speaker A: Everybody. This talk is called Protocol design. Winehow and I'd love to talk about despite how early it is and despite how new our tools are, we can design protocols that resist centralization and that can remain economically sustainable in a repeated way as long as we follow some specific guidelines. But let me also say, please interrupt with questions. I'm a very conversational reasoner. It's super hard for me to just go for 2 hours. This is not 2 hours, this is like 45 minutes.
00:00:43.082 - 00:01:17.240, Speaker A: But to go straight for a long time. I love questions, there's so many things I'd love to talk about and they won't come out unless somebody asks something. So please, I encourage you interrupt at any point. So first I'd like to talk a little bit about why the Internet is a network of protocols. Some are charmingly, compact and simple. This is a state diagram of Http and some are dizzying complex, like this infamous interaction diagram for Maker protocol. That's a great one.
00:01:17.240 - 00:02:09.190, Speaker A: There are all kinds of protocols out there internet protocols, physical protocols, political protocols and more. On the left we have an intersection interaction diagram which is really cool. Obviously we're familiar with how those work and the thing that unites them is there are formal systems for interaction that facilitate complex group behaviors. That's kind of the key underlying component of a protocol. And what makes an internet protocol so powerful is that you can interact with it, not just with people, but with software. And as we know, software is incredibly malleable and performant and has all kinds of interesting other kinds of mechanisms you can build in. So internet protocols end up being maybe one of the most important, if not the most important category of protocols that we have at our disposal.
00:02:09.190 - 00:03:20.118, Speaker A: This diagram, I think I will try to unite all of the subcomponents of my talk with this diagram. On the x axis you have decentralization and centralization in terms of control, like how is the protocol controlled. On the y axis we have the economic model of the protocol and specifically how explicit or unspecified it is. Now, this is a little bit of a subtlety, but what I mean is that you can design a protocol that does all kinds of things but is totally and utterly unappeinionated about where value flows, what ownership means, what access rights are, how payments work. Those are totally optional components of protocols. In fact, I think if you look at a lot of the web one protocols that we're going to look at like these, you will find that they are conspicuously unappinionated in terms of how they set up their economic systems. I don't know how familiar you guys are with each of these.
00:03:20.118 - 00:03:58.650, Speaker A: These are some of my favorites growing up. But usenet was a protocol for exchanging posts and files in categories, kind of like reddit. You'll see, in the early days of the Internet, IRC is a totally universal protocol for chat. I'll say a little bit more about SMTP RSS. These are great protocols that are very well specified but totally punt on how they handle their economic components. So I'll say a little bit about Usenet. This is someone using Usenet a couple decades ago.
00:03:58.650 - 00:04:37.400, Speaker A: I'm kidding. This is a usenet interface. Usenet is organized by categories where there are specific subservers that allow people to post content relating to that specific category. It was the birthplace of terms like FAQ spam. Very early internet culture had a strong place in Usenet, and very interestingly because it existed outside of Http. You had to have a specific client and you had to have an ISP that supported it typically. So this is a description from Wikipedia about how Usenet worked.
00:04:37.400 - 00:05:23.150, Speaker A: It's distributed across a large number of constantly changing news servers that can be operated by anybody and posts will be automatically forwarded to other news servers. That sounds like a pretty decentralized system. There's no specific privileged actor. If anybody can run a server, then that's it. How Usenet ended up actually working is that people gained access through their universities research institutions. ISPs users rarely paid directly for Usenet access, but later in the 2000s people might pay for a commercial Usenet server. In other words, you had to broker your own deal to use Usenet and it was decentralized.
00:05:23.150 - 00:06:00.560, Speaker A: So it was decentralized, but also lacked an in protocol economic model. So if you look at each of these systems, I think that you'll find they're all very similar architecturally. They all kind of come from the same ethos. In fact, I was talking with Sam Ragsdale from my team yesterday and I was saying a few words about IRC. Funny for me, he had never heard of it. But despite that, Sam was able to guess exactly how it worked just based on my saying it was a Web One chat protocol. That's how legible it is and how clean I think the template for what a Web One protocol is.
00:06:00.560 - 00:07:01.010, Speaker A: So why did these fail or change over time? And I think if you go through each of these, I will highlight some examples. If you look at how each of these failed, it was all because of one of two reasons, or sometimes both. They either lost directly to Web Two competitors because of a lack of specific features or they struggled to fund themselves. So it all comes down to specific features that they could embody by being decentralized and the ways that they were able to handle their economics. So Web One protocols go into this bottom left corner, decentralized and unspecified. Now, each of these as Web Two came by, there's this really interesting pattern. If you look at Usenet, it was replaced very cleanly by Reddit or similar sort of discussion forums.
00:07:01.010 - 00:08:07.882, Speaker A: IRC was replaced by WhatsApp Imessage, any of the sort of data, centralized messaging systems, email, although it hasn't been replaced, and I'll say more about this later has ended up being de facto centralized. RSS was beaten basically by Twitter, and if you go item by item, there are specific features. So for usenet usenet's failed features have to do with how difficult it was to set up a new category in its namespace and how difficult it was to modify the underlying protocol. It also had no spam mitigation. Email also struggled from this spam mitigation issue. It's a common issue in decentralized protocols and that's because they don't have an inbuilt concept to measure whether a message is legitimate, whether it's from a legitimate person, or whether the sender has specific permissions, or whether there's a real economic relationship between them. There's all kinds of angles you could think of, especially given some of the things we have in Web Three.
00:08:07.882 - 00:09:32.478, Speaker A: But in a naive decentralized system, those lack of features mean that the centralized competitors that come later have a critical kind of thing that they can offer uniquely by being centralized that allows them to surpass their Web One predecessor. So we don't need to go into detail, but obviously Web Two protocols are in the top right corner and what defines them is that the systems are entirely controlled by their owners who can make unilateral decisions, limited only by business strategy and the law to modify their networks. And they do that. But very interestingly, if you were running a Web One protocol or you were interested in facilitating the development of one, you would feel a natural pressure to move up this gradient to make the economic model more and more explicit. Like you would feel this pressure and that's because you need to fund it, or you need to align the participants of your ecosystem. And making these things more explicit and more legible gives you the type of control that the protocol needs to enforce new features, pay for itself, all those things. But very interestingly, that corner is not available without decentralized consensus or without verifiable computation and all the other kinds of tools and properties we have in crypto.
00:09:32.478 - 00:10:55.806, Speaker A: So it's not actually possible to maintain decentralization while making the economic model explicit without what we have in crypto. So what ends up happening is this slide from the bottom left to the top right, where either the protocol adapts and becomes de facto centralized, which is what happened with email, which I'll show in a second, or the protocol simply gets leapfrogged by a web two protocol that places itself in the correct area of the design space at the outset. This is email today, although it certainly exists, it's become centralized. It's a little hard to make other details here, but you can see that over 50% of email, depending on domain and depending on the specialization area, is handled by centralized email service providers. And I think there's very clear reasons for this. There are centralizing forces in email and decentralizing forces, and this is where we'll start to get a little bit into the components of protocol design. Centralizing forces include spam, a lack of economic model, amortizing DNS registration costs, high switching costs, right? No economic model for email means it's only viable at scale as a strategic side project for large tech companies.
00:10:55.806 - 00:12:31.150, Speaker A: And spam is only partially able to be mitigated by a combination of economies of scale and data where companies hosting millions of email accounts can identify anomalies easier because they have more data to do this kind of work or de facto reputation systems between email service providers. So a specific example I invite you to try. If you try to run your own email server, which you can do, right? You can run your own email server, you can get a machine, you can point the DNS certificate for mail to the IP address of your machine, and you can send your own emails, but you'll find that they'll usually end up in the spam box. And that's because there's a de facto reputation system between the top corporate email service providers and they don't do that because they are trying to mess with anybody, they're doing that because it's necessary to solve what is lacking in the email protocol, in the SMTP protocol. So SMTP is under specified with regard to how to ensure this is a legitimate email. Another key thing to note is switching costs, which I'll say a little bit more about in a minute. But a switching cost well, let me kind of zoom out a little bit here and say two key centralizing forces that apply to different components of a protocol that will come into play constantly over and over and over again at every turn when you're designing a protocol are network effects and switching costs.
00:12:31.150 - 00:14:12.142, Speaker A: Network effects are where power accumulates as the result of a system becoming bigger and more used. And switching costs are just the economic cost or cognitive cost or time cost for users of the system to exit and use a different system, right? If you go back here and look at each of these forces in email, you can apply those two forces to each of these components. The DNS registration cost is maybe an example of an economy of scale, but high switching costs if you don't own the domain and low switching costs if you do, are examples of how users can take advantage of a design in a system to mitigate those switching costs and resist centralizing effects. So when we look at email, it becomes clear there's kind of three components that rise to the top where switching costs and network effects are at play. There's namespaces, specifically the DNS system and email addresses within a domain like for Google, there's payment systems which has something to do with how we think about spam and what a legitimate email is and paying for email service, which obviously few are willing to do so and reputation, which is how we think about the spam problem with regard to email. So before I go to Web Three, are there any questions or thoughts? So the question was, say a little bit more about how high switching costs are a centralizing force. So let me zoom in a little bit here.
00:14:12.142 - 00:14:43.830, Speaker A: Really interestingly. You can see low switching costs, and high switching costs are forces at play in email, right? The difference here is in Gmail, if you're using Gmail and you don't own the domain, like you don't own@gmail.com the switching costs are very high. And that's because if you want to use a different email service provider, forget an email client. I mean, email service provider like the actual machine that receives and sends the mail. If you don't own the@gmail.com domain, you cannot actually switch to a different provider.
00:14:43.830 - 00:15:49.900, Speaker A: Whereas if you do own the domain, like you buy your own web domain, you can not only move your contacts, but you can continue to receive emails at that domain using any email service provider, whether it's one you run in your garage or not. And the ability of a company to inflict high switching costs by forcing you to use or encouraging you to use specific components in their protocol's design changes those costs. The higher the switching costs, the less likely a user is to leave, because they have to do whether or not they're doing it explicitly, they're doing a calculation. It can be a subconscious sort of behavioral calculation, it can be an explicit business calculation, but they have to understand exactly the cost that they're bearing to leave. And high switching costs makes things sticky, which makes it easier to collect rents, which consolidates power around that system. Hope that answers your question. So where would we put Slack, discord, and then systems that generally have powerful moderators? Is that the question? Yeah.
00:15:49.900 - 00:16:55.018, Speaker A: So I'll think about slack and discord in a second. But when it comes to allowing moderators, like take, for example, take Reddit, where there are subreddits that are unilaterally pardon me, unilaterally controlled by the moderators, I think that it's hard to say whether that is a centralizing or decentralizing force specifically. I mean, I think allowing anyone to become a moderator feels like a potentially slightly decentralizing force. But if the power is still accumulating to the admin, above all admins, right, which is like the Reddit team, then they may be taking advantage of an innovative force where people can experiment and try different ways of running a subreddit. But it's still coalescing power to the Reddit domain, to the Reddit servers where everything is stored. It's still ultimately concentrating toward Reddit. So I'd say that I think with Reddit, my first reaction is always that it's kind of crazy that there's no way to remove a moderator, right, or to continue the development and maturation of a subreddit.
00:16:55.018 - 00:17:46.482, Speaker A: And there's also no in protocol way for a moderator to monetize. So you end up with a lot of strange pathologies where moderators get paid out of protocol in ways that violate reddit's terms of service or are consistent with them. It's a really interesting topic. As far as Slack and Discord, that's tough. I think those are still very centralizing. Because with slack, even though your company has control over the Slack instance or you control the discord server, whether or not you can integrate specific tools with those systems, whether or not you have control over the data, whether or not you can receive payments or add features like token gating, things like whatever, all those types of features. Those are still at the discretion of the company because they control the client and API interface that you can use.
00:17:46.482 - 00:18:30.746, Speaker A: So those are still totally centralized systems. They're taking advantage of the ecosystem benefits of allowing for an open system, but they're trying to get the benefits of decentralization without the costs. Right. Still ends up being centralized. Thanks for that question. Yeah, the question was is high quality user experience or is user experience a centralizing force in general? I'd say no, but it is a force that requires funding. High quality user experience requires funding, which is why it's not a coincidence that the decentralized protocols in Web One typically have a pretty low quality user experience given what is possible because of funding.
00:18:30.746 - 00:19:20.190, Speaker A: If you can solve funding, I think you can solve user experience. I don't think it's inherently centralizing or not. And in fact, I'd argue maybe this is a little bit of a roundabout path, but if you allow the system to be totally open, so you allow people to compete at the level of the user experience, and you allow them to monetize independently, like at the periphery of the protocol, then you probably end up with the best user experience there. That type of ecosystem is where you'd end up with the best user experience because not only can each of the individual projects fund themselves, but they can experiment and compete. Whereas if you look at like a Twitter or Facebook or Instagram or like there is a specific user interface that you have to use, you have to use the interface that comes integrated with the platform. And that means that you are kind of at their whim in terms of how they upgrade the interface. Right.
00:19:20.190 - 00:19:58.586, Speaker A: So it seems to me like you won't achieve the global maximum interface, the global best interface, if there's only one agent that can climb the hill. Right. So how does introducing all the components of Web Three change protocols? Well, we can incorporate cryptography and blockchains to minimize trust assumptions. We can make economics explicit, all while supporting decentralization. That is a really interesting corner of the space. And in fact, I'd say that that unblocks. That top left corner of the space and parks Web Three protocols comfortably in there.
00:19:58.586 - 00:21:00.750, Speaker A: So, talking a little bit about value, how do you answer? I would say this is not necessarily a statement for builders. This is kind of a statement for it is for builders, but it's also for the whole space in general. How do you think about the economic outcomes for all the participants of these systems and for the systems themselves? The problem with web one protocols in the bottom left corner is that they unwillingly externalize all the value that they create because their economic models are unspecified in the protocol. That means all the interesting economics happens outside of the protocol and there's no lever that the protocol can use to collect value. Web Two protocols, centralized and explicit in terms of their economics, internalize all possible value. Their goal is to internalize all the surpluses of the system. And in the cases where they end up being de facto monopolies, the way that they revenue maximize is by absorbing every individual's surplus.
00:21:00.750 - 00:22:25.290, Speaker A: Which means that the people building on top of them cannot be as big as they otherwise would be. If you build a very successful or very powerful business on top of a social network's API, they will limit you, they will change the fees, they will cut you off, they may build a competitor and vertically integrate in that direction. The point is that you are building on someone else's territory, which means you cannot fully internalize all the value that you are creating. But what's very interesting about the web3 corner is that these types of systems, because they're open, interoperable, open source, have known economics in advance, can commit to how they operate. They can find ways to be sustainable by including their own funding in their protocols while also avoiding capturing all the value. Which means if you're a builder and you're going to build on a system, the best type of system to build on would be one that is decentralized and also has explicit economic models. That is the best corner of the tradeoff space because you know it will last and you know your economic relationship with it as opposed to allowing the economic relationship to develop outside of the protocol, then in terms of stability, it's an entirely different axis as opposed to value capture.
00:22:25.290 - 00:23:22.846, Speaker A: It is always better as a builder to build well, if you want a very it's not always better if you want to build the biggest possible system that has the greatest chance of surviving, the longest, the most ambitious project, you should build on a decentralized system. And that's because if you don't, you're subjecting your system to potential risks. Single points of failure, platform risks. No one thinks it's crazy to build on the internet anymore because it's totally decentralized and it's never going to go away. No one thinks it's crazy to use an open source programming language or rely on web browsers because these systems are decentralized, they're the most solid ground to build the most ambitious projects. And if you build on centralized systems that have strong network effects or switching costs that accumulate to them you're building on someone else's territory and could be limiting the size and scope of your project. So this is the pattern of how they're developing and I think that it's possible.
00:23:22.846 - 00:24:37.560, Speaker A: As a result of the fact that web3 systems can attract better builders that can build bigger, more ambitious projects, I think it's possible that we see some of the web two networks find legitimate and serious competitors. So any last questions about that before we go to an example? Protocol design? Yeah, that's a great point. Yeah. So the observation is that if web two networks capture as much value as possible, then doesn't that give them the most resources to resist potential competitors? It's totally true, but I'd say there's a couple key dynamics with how we see institutions unfold that make this not an existential threat. The first is that incumbents are very brittle. In fact, there's a slide I removed that would have been like a perfect slide for this. But if you think about how this is an analogy I'm borrowing from seeing like a State, which is a great book about ways in the 20th century that large states have tried to build optimized and legible systems to solve social problems.
00:24:37.560 - 00:25:23.394, Speaker A: If you analogize a Web Two network with a farm, analogize it with like a modern scientific farm or scientific forest, they use monocropping, fertilizers, pesticides, they make legible the rows and organization of all their crops. What they're trying to do is they're trying to optimize along a specific dimension. That is always their goal. They have specific metrics that they are hell bent at optimizing at all costs. And anything that does not play directly into the metrics that they are trying to optimize is either a risk or a distraction and is often a second class priority for them. There's a reason why they have failed so far to develop all these monopolies. Well, scratch that.
00:25:23.394 - 00:26:10.786, Speaker A: They're not really, really monopolies, they just have incredible network effects. There's a reason why they've failed to innovate and create the same types of new products. And it also makes them more brittle, because it means if someone finds their kryptonite, so to speak, the specific pathogen that works against them, their business model is so optimized to a specific pattern of behavior that it's easier to disrupt if you can find the exact location to undermine it. The way they mitigate this is by acquiring companies and trying to buy them things like this. And they can certainly extend their time horizon. But at the limit I think that the innovative types of projects are not going to take place and to be built within these companies. So I think it's kind of an innovator's dilemma story.
00:26:10.786 - 00:27:08.406, Speaker A: Going back to the farm analogy though, the most productive and interesting type of space that is most ecologically valuable is a space that is fertile soil, that has a fantastic ecological equilibrium where all types of different species and all types of different things can intermingle and rely on each other. Like think of a rich multispecies rainforest. This is like the natural state of nature. This is where all the interesting types of things develop. And if you build the right type of substrate, if you build the right underlying protocol, when all the interesting things develop, they will develop on top of that richer fertile soil. They will not even be able to take off in the highly sterilized, purpose built specific industrial farm. So the longer time passes, the more interesting things will pop up.
00:27:08.406 - 00:28:19.850, Speaker A: Because of interesting entrepreneurs and developers in the fertile soil, the fewer things will accumulate to the single purpose field. So although they have the most economic resources, those aren't the most valuable things. The most economic resources are the willingness to build and willingness to take risks by the potential participants of the system. So on top of that, and this is a whole other path, crypto, because it can create explicit economic models, can do creative things like take helium as an example. You can create a token that pays for the development of the network and helps bootstrap up to that large critical mass where the system becomes economically viable by creating a token that represents the future control over the network and distributing it across the widest set of participants so that it's totally authentically, organically, decentralized. In other words, there are specific ways that you can use an explicit economic model that crypto system can commit to in order to guarantee that participants will get the reward for their efforts. And in that case you have ways to kind of go up the hill and solve the bootstrapping problem.
00:28:19.850 - 00:29:01.622, Speaker A: So thanks for that question. So I'd like to say a little bit about how and in talking about how I will use a toy example for a really cool little system like a free so this is called Stablehoard. It is a free system for inference, image generation kind of can't see that super well, but this is one of the many free clients you can use to use Stablehorde. If you know stable diffusion, you know that stable diffusion model weights are very small, they're only like 5GB, which means anybody can run them pretty quickly if they have a GPU. And that's about all you need. All the code is open source to run the model and once you have the weights, you can execute them. But not everybody has a great GPU.
00:29:01.622 - 00:29:39.334, Speaker A: So what stablehorde is, is it's a very simple coordination layer where someone can request an image and specify a prompt which is kind of at the bottom of this image and request that this prompt be generated by a horde of people who are essentially donating their compute time. And it is live. You can go check it out. I think they actually recently renamed it to AI horde but they've already served 38 million images through Stable horde despite the fact that it's totally free and it's really cool. I recommend you try it yourself. You can just specify an image and it'll generate it. This is a process diagram for how Stablehorde works.
00:29:39.334 - 00:30:45.846, Speaker A: So there is a database in the middle and what happens is a client, which can be anybody, it just has to satisfy the Stablehorde API, submits a job. That job goes into a work queue for model, a worker can receive a job and process the inference, send the result back to, like, a results store where clients can retrieve it and also pay kudos to a worker who receives those kudos. So kudos are a free little point system that accumulates in Stablehorde. When you come up to Stablehorde and you make a new API key, you start with zero kudos and you can actually go negative if you spend. But the way that the queue works is that the workers automatically prioritize clients that have a higher number of kudos. So if you have a lot of kudos, you will be processed first after you pay. And if you have a decreasing amount of kudos, your request will take longer and longer to process by the horde.
00:30:45.846 - 00:31:08.898, Speaker A: So this is a cool thing. It works today, but it's fundamentally limited in terms of how many people are willing to donate. High quality free compute. That's it. The demand, I think, is limitless. They've made many millions of images, but because demand is self limiting, the longer the queue is, the longer it takes to generate an image. You get longer and longer wait times.
00:31:08.898 - 00:32:07.202, Speaker A: So I think a really interesting question is how do we scale the system so that it can be much, much bigger and professionalized a little bit, but while still preserving its open and interoperable character and also avoiding the risk that it centralizes in a way that undermines the original spirit of the project? I think that's, like, an interesting task. So does anybody have any immediate thoughts, like anything that would work or not work? Yeah, so the suggestion was, why don't we just make kudos an, ERC, 20 token and then maybe, just to add a little more detail, maybe we would move the kudos scores onto a blockchain. Something like that? Right, something like that. That's like an immediate thought. But if we go down this path, well, I'll skip here for a second and then I'll come back. If we skip right here at the point we send the inference result and wait for a payment, it's pretty clear someone could attack this system by providing fake results. They could provide fake results and receive payment.
00:32:07.202 - 00:32:51.330, Speaker A: And that's the most negative possible user experience because you're just not getting what you paid for. So I think it's clear that we can't sort of naively strap a blockchain onto this and make it work. So let's revisit a design process. This is a design process that I've talked a little bit about before and I'll just. Revisit it very briefly. You should always start a protocol design with a goal, then constraints and then mechanisms. So a goal should be unmistakable, it should be concise, and it should be very clear whether or not your goal is achieved through means you can measure.
00:32:51.330 - 00:34:01.100, Speaker A: We'll go into a specific example, but even if we're not exactly sure how to measure, it should be something measurable. And also, ideally, it sounds like the design of a system, like it sounds like a type of thing that could be a protocol, a coordination of complex behaviors to facilitate a specific end constraints. Constraints come in many forms, there's endogenous constraints and exogenous constraints. Endogenous constraints are things that we choose to accept that come from inside our design, from inside our group, and exogenous constraints come from outside. They're imposed by nature the state of technology, regulations, the resources we have available, things like that. I'll give some examples, but the purpose of these constraints is to constrain and focus our design so that it's clear what exact mechanisms are viable and which ones are not. So you want to limit the design space, you want to constrain the possibilities of design so you can focus extremely clearly and not be distracted by all the different possible forks in the road.
00:34:01.100 - 00:35:02.220, Speaker A: Mechanisms are the actual substance of the protocol. Like these are the subsystems that put together, satisfy the goal according to the constraints. So we have our goal, we have our mechanisms, and we check that the mechanisms satisfy the goal according to the constraints. Examples of different we'll go through some examples of different mechanisms, but these might be things like liquidation systems, pricing systems, staking systems, incentives, systems, payments verification, all these types of things. Like all the different types of things, when you look at well known crypto protocols or web two or web one protocols are used. So going back to our new protocol here, which let's call unstable confusion for stable diffusion inferences, we already talked about the issue where someone sends fake inferences. So it's pretty clear that we need something to guarantee that consumers get what they ask for.
00:35:02.220 - 00:35:38.060, Speaker A: So let's call that verified inference. So one of the constraints is that we need inference to be verified. We need to be able to ensure that this is the intended result. Another issue is that in stablehoard workers ask the database for the next job in the queue, but the job goes to the first worker who requests it. So this works fine. But in a system with money on the line, incentives could develop to claim lots of jobs that workers don't intend to complete. They could come in and compete on latency, start clamoring up jobs and congest the system.
00:35:38.060 - 00:36:30.810, Speaker A: Sometimes this family of concerns is related to mev, right? But it depends on how the winner is selected. So it's not totally clear out the gate what exact constraint we want. But the ideal worker experience is probably one where workers don't need to compete in some way that isn't helpful to the network. If workers are going to compete for jobs, it should be productive to the network. So let's just say that workers should be paid according to their contribution and see where that gets us. So we can call this proportionate rewards, right? So while we're at it, it's pretty clear that it would be useful if workers could drop in or out with minimal costs. Like this is a common behavior in other types of distributed systems and makes it easier for us to get to solicit more participation.
00:36:30.810 - 00:37:11.638, Speaker A: So let's add that and let's call that variable participation. And then finally, from a user experience point of view, it's clear the app should be responsive and quick in order to remain fun. So let's add that and let's call that low latency. So if we go back to our template, we have a goal, which is to make a decentralized interoperable image generation marketplace. We have a couple of key constraints which is probably not complete and we can definitely add more or modify them or become more specific later. And now we're in a place to evaluate the viability of different mechanisms. So this is where I'm going to go through a couple and talk about them.
00:37:11.638 - 00:38:03.442, Speaker A: But I'd love your thoughts as we consider them. So let's look at first we agree we need some verification mechanism because we need to make sure that the inferences are correct. A classic place to go in crypto world is game theoretic mechanisms, right? Where maybe we use a dispute resolution system. So this would be where users receive a result, they're unhappy with it, they escalate, and some sort of in protocol party evaluates the result and litigates it and says, yeah, this is correct or this is wrong. This inference was given the model, given the parameters. This model hash disagrees with what I got, so this is wrong. Another in the game theoretic category would be continuous or sampled auditing.
00:38:03.442 - 00:39:29.134, Speaker A: This is where there's an in protocol agent who has something at stake, who goes around and plucks from the underlying worker set and makes sure that different jobs are being handed out to different workers and updates some sort of representation of which workers are passing audits that can provide information to the protocol that certain workers are high quality. Another interesting path is to use cryptography to use zero knowledge proof. Maybe if we could generate an efficient proof that an inference was correct, then we could use that within the system and allow workers to generate a proof that their inference was correct and exactly as specified in a way that's very succinct and low cost. This, of course, without going too long into this, you might think zero knowledge protocols have a lot of overhead. But actually there's an interesting and appealing way to do this that one of the researchers from our team, Justin Thaler, has spent a lot of time advocating for. That involves making very very quick and easy low overhead proofs for matrix multiplications, which are the key components in inference. Some traditional methods that are used in similar sort of marketplaces for verification are trusted third parties.
00:39:29.134 - 00:40:39.914, Speaker A: So having someone who vouches for this worker or vouches for this type of job, something like that. Or you can use user reviews right, where users say yeah, this looks right, this is what I wanted, here's a thumbs up, here's a five star and allows that information to accumulate. The reason I have it in red is because these are pretty obviously potential centralization vectors because network effects could accumulate to third. Well, there's both network effects and switching costs involved in both of these. User reviews create network effects because when users use user reviews, they are more willing to use workers that have high quality user reviews and those workers will then get more work from users, which means they're more likely to receive more user reviews. So depending on where those user reviews are stored, that could accumulate network effects either to whoever controls the review system or directly to the workers themselves. For trusted third parties, that's a little bit weaker where that network effect comes from.
00:40:39.914 - 00:41:55.082, Speaker A: But a trusted third party can accumulate the trust and reputation to themselves, which means that when you're asking for the next inference, you can request that the trusted third party just generates it for you, right? If you already trust them, you're already building trust into the system they could potentially undercut because they don't have to double check anyone's work or do something like that, offer an out of protocol special deal, any other interesting verification mechanism candidates, anybody can think of? The suggestion was maybe there's a system where multiple workers complete a job and the user basically has their choice among the completed results. And of course, if there's obvious disagreement, that would be obvious to the user, so that's a good one. But it does have high fixed costs because the network then has to produce every inference x number of extra times. But if it's sufficiently cheap, it may be worth it, right, it may be worth it. So we have a couple more potential avenues to consider or potential mechanisms to consider for our design. There's pricing strategies. You could have an on chain order book, you could have an onchain verified proxy measure for compute like Gas.
00:41:55.082 - 00:43:01.800, Speaker A: So this is where instead of just simply a free market where you post how much you're willing to pay for the inference and workers can accept it or they can bid for work, instead you create a proxy representation like Gas where specific inferences cost some amount of compute and that quantity of compute is what's priced directly. That can simplify the mechanism. We could also use an off chain order book, right? And that would be really cheap to run and potentially really performant the issue is, of course, is whoever owns that order book could potentially coalesce network effects to themselves too. That's some pricing strategies. There's also ways to manage storage. Storage although we didn't go I didn't touch this part in the diagram here. Storage is really important because you need to ensure that the result was delivered to the client and it's very difficult to prove that work was delivered properly in a way that mitigates trust.
00:43:01.800 - 00:44:05.580, Speaker A: You could imagine a pathological case where whether or not the item was delivered happened becomes a source of dispute, right? Like a user basically complaining I didn't receive the ended good. Or you could imagine a system where auditors, in order to verify the compute, need to be able to check the output specifically to ensure that it was done correctly. So outputs need to be visible to the protocol, so they need to be stored somewhere that's protocol accessible. You could put it on chain, but that's very expensive. You could use a separate storage specific crypto network. It's also possible peer to peer is probably a little bit more difficult, but maybe there are some ways around that. You could also store it off chain, but obviously if it's stored off chain and whoever's controlling that system has the ability to influence many other components of the system like the verification, the final transmission of payment, things like that.
00:44:05.580 - 00:44:49.610, Speaker A: And maybe the last component is how we schedule work. This is kind of a complicated category and I'm not going to spend much time on it. But you can imagine cases where the worker chooses the task once it's been submitted to the system, where the protocol allocates tasks once they're submitted to the system, or allowing the client the end user to choose which worker does the work. And each of these have potential vulnerabilities and potential strengths. Maybe there's some combinations between a few. Like you could imagine a system where the protocol decides which workers are able to request which jobs or things like that. But you'll notice there's all kinds of interesting subtleties here.
00:44:49.610 - 00:46:19.618, Speaker A: For example, if in a protocol chooses system, you need to have a notion of whether a worker is online available to the system so that it can know whether to schedule to it. And you also need to have a sense of the capacity of each worker as an example. So you start to see how all kinds of extra facts need to be visible to the protocol and our naive implementation might not have had them in there in the first place. So now that we've done like kind of a rapid fire tour of some potential paths, interesting paths in the IDMAs about Web Threeizing, this Web Two protocol, we can see a couple of other key design components that end up being potential vectors for centralization. The first three came from email, but these last four, like storage, matching pricing systems, verification systems, these are all potential centralization vectors and of course this is just a reminder that the key ways that these objects can be influenced, it can become centralizing, is as a result of network effects or high switching costs. So certain types of systems, you can mitigate the accumulation of network effects or you can drive the network effects to accumulate to the protocol and in the ideal case, network effects accumulate to the protocol. And additionally there is a decentralized control layer for the protocol.
00:46:19.618 - 00:47:20.406, Speaker A: This is personally where I think tokens make the most sense, like volatile tokens that allow control over decentralized systems because a system often needs a steward who is responsible for the long term health of the system. And a token, if sufficiently decentralized and sufficiently distributed, is a great thing to point a control requirement toward. How else do you distribute control over many, many people? I think there's actually some really interesting governance designs that don't involve volatile tokens. You could imagine sort of reputational systems being used for governance or a rotating cast of elected people through some civil resistant mechanism. There's some really cool design patterns. But either way you need to accumulate the network effect to a decentralized entity that is incentive aligned with the protocol. If that's the case, and the protocol is potent enough and has a strong enough network effect and is creating enough value, it's very, very likely to me.
00:47:20.406 - 00:48:10.394, Speaker A: And this kind of depends on the protocol that you will be. Able to find a path for the protocol to extract value in a way that is not rent collecting, is not excessive, and does not capture all of the surpluses built on top of it. So in my mind, that is the kind of tightest way to explain how protocols can end up capturing value in a positive way. And the same thing is true for switching costs. I think interoperability and allowing for low switching costs is really important for encouraging entrepreneurs to build on a system. I think we've all learned over the last couple of decades on the internet that if you build on a system that you can't leave with anything, you're taking a lot of risk. So I think that's probably table stakes is to minimize switching costs wherever possible.
00:48:10.394 - 00:49:34.210, Speaker A: But either way we can inadvertently introduce high switching costs into naive systems like in stablehorde by for example, having an off chain order book or having a verification system that relies on third party that we really come to rely on. Yeah, so the observation is just that in web Two, the largest systems tend to the largest protocols, largest players tend to either in house or verticalize or acquire or limit the people building on top of them. And that obvious pattern is something that maybe we can ameliorate with some of the tech in web Three. So if you kind of sum everything up, I think each of the if we want the system that we're building to be a building block for the future of the Internet that others can use and that will stand the test of time, we need to endeavor to steal people's resolve with all these cool web. Three tools that unlock this very valuable corner of the design space. Protocols are going to end up bigger, last longer, and have more vibrant system ecosystems than web two protocols because that's where all the safe and interesting innovation will continue to be possible beyond the tolerances of the largest companies. Anyway.
00:49:34.210 - 00:50:12.382, Speaker A: So I'm going to open it up to questions. Yeah. So the question was, is there an optimal strategy for this decentralized compute marketplace? We've been thinking about it a lot. I think that a few of these areas are much more promising than others. But ultimately which mechanisms you adopt is a complicated research task where you have to plumb all the depths. I don't know that there's an exact one. I think for verification, it's almost certainly going to be the case that the cryptographic solution is the best.
00:50:12.382 - 00:50:58.414, Speaker A: That seems clear to me. For the pricing, I think having a proxy for compute is the best because it's the most generalizable. Because once you have a decent enough proxy for compute, then you can have all kinds of different inferences or machine learning tasks that are represented by this one measurable thing like gas. Right. In terms of pricing or sorry, in terms of scheduling, my gut is that a mixture of protocol and worker chosen is the best. Where the protocol has some continuously updating concept of what the capacity of different workers is and whether they're on or off and hands tasks to them in an equitable way where the workers can choose to accept them or not. I think that there's something interesting there.
00:50:58.414 - 00:51:31.162, Speaker A: And then the last dimension was storage. This is an interesting but tough 148 44 like proto. Dank Sharding actually plays into this a little bit, this similar design path, because you actually only need the image there for as long as you need to dispute it and guarantee that it was received. Right. So you don't actually need to permanently store the image. So if you can get away with that short time window, you can probably use some type of temporary storage that is guaranteed to be held by consensus, but is ephemeral. So if I had to kind of like pick and choose, that's how I'd pick and choose.
00:51:31.162 - 00:52:11.298, Speaker A: But I haven't designed this whole thing and I'm sure that there's many entrepreneurs who are going deeper down that. Thanks for that question. Yeah. Hey, it's Nikita from Chain patrol. I wanted to ask, when developing any software, I think centralized sometimes feels almost like easier to develop or maybe faster. And so what are your thoughts on starting off centralized and progressively decentralizing? And also if that's a good approach or if you should start it off that way. And then if you do start off centralized, what is the stage when you decide, okay, now we're going to start progressively decentralizing.
00:52:11.298 - 00:52:48.206, Speaker A: Yeah, that's a great question and could be a very long conversation for sure. I personally think it's a really good strategy and I think it's important for a couple of key reasons. It is easier to iterate and to move on a centralized system, right? Not because of some grand political fact, but because a centralized system is under your control and something you can control, you can manipulate faster. That's just incredibly straightforward. As far as the right time, I think that it depends on what you're building. But there needs to be a credible path to decentralization. You shouldn't just start building something totally centralized and just kind of wave your hands and hope that it's going to be decentralized, be decentralizable.
00:52:48.206 - 00:53:21.030, Speaker A: Decentralization is incredibly, incredibly difficult. We've already seen kind of how every path has its risks. So I think you need to start the process of decentralizing thinking about decentralizing immediately. It needs to be at the extreme beginning. But if you had to pick an exact right time to choose to actually take the steps of decentralizing, I think it would be right around the time that you get product market fit. Right around the time that the need for iteration and the need for experimentation starts to taper off. But there's other forces that can drive you to need to decentralize.
00:53:21.030 - 00:53:47.400, Speaker A: What if you're not attracting enough builders because it's just not credible enough yet? Right at that point you're still asking people to trust you. I think it depends a lot on the thing, but I'd say start decentralization planning immediately, like from the beginning, and have a credible path there and tactically choose which components you will tolerate centralization for at the outset. Thanks for that question. Thank you. Yeah.
00:53:49.370 - 00:54:06.330, Speaker B: Hi, Jay from Etherid. Thank you for the brilliant talk. There's a lot to think about. This is more of a philosophical question, but would you say the fact that branding exists for humans means that there's a natural human tendency towards centralization?
00:54:08.990 - 00:55:00.490, Speaker A: I think it's a good observation. I would modify it a little bit is that there is a natural need by human beings when dealing with a complicated system to reduce things into a single synoptic lens, right? Human beings want to minimize the cognitive burdens of considering choices and things like that and being able to say like, this is a good brand, and I trust this brand is a great instrument to simplify the mind. It's a great groove to grease, so to speak, in your behavior. So there is an incredibly important role for brands for that reason. And it is to some degree centralizing. Brand and reputation is a centralizing thing. But I think that if I had to sort of quantify how centralizing it is compared to network effects, compared to incredibly high switching costs, I'd say it's much, much smaller.
00:55:00.490 - 00:55:08.880, Speaker A: So I think it's not like a huge concern, but it is a very fascinating effect. So thanks for mentioning it. Yeah.
00:55:10.530 - 00:55:39.698, Speaker B: Heidi. My name is Murat from Primev. We're building a block builder communication network. You gave a great framework on kind of how to go about mechanism design, essentially, or for protocol, the three steps. And when we think about protocols that kind of stack on top of each other, for example, on Ethereum, there's a compute protocol, there's a consensus protocol that has some slashing mechanism in it. How do you kind of bring together that framework with kind of these stacking protocols? In a sense?
00:55:39.804 - 00:55:49.546, Speaker A: Yeah, it's interesting. So when you're building one protocol on top of another right. So you're taking advantage of composability, which your project certainly is totally preoccupied with. Right?
00:55:49.648 - 00:55:50.010, Speaker B: Yeah.
00:55:50.080 - 00:56:26.310, Speaker A: It's an important piece. You should think of the thing that you're building on as introducing additional constraints. In certain cases, it also supplies mechanisms that you can take advantage of, but these are constraints that you're limited by. So I just dump them in and say, like, look, this is the gas environment we're in. This is the way that we communicate, blocks to relays right. And so on and so forth. So I would introduce that into the constraints side, and I think that would simplify the view.
00:56:26.310 - 00:56:30.198, Speaker A: Is there another way that you mean? I mean, it certainly makes it more complex.
00:56:30.294 - 00:56:49.818, Speaker B: That answers it, for sure. That helps kind of think through how to apply it on multiple levels. So thank you. Can you share your thoughts about all of this at the application layer or how the decentralization of the protocols and the economic model being explicit?
00:56:49.914 - 00:56:55.442, Speaker A: Yeah. What do you think about the application layer? Say a little more about what you mean by application layer. Like an example, maybe.
00:56:55.496 - 00:56:57.806, Speaker B: So you have clients building on top of protocols.
00:56:57.838 - 00:57:02.002, Speaker A: Like, you have user interfaces. Assume Twitter is a protocol, and anyone.
00:57:02.056 - 00:57:17.778, Speaker B: Can now build a client. Right. Does that trend tend towards centralization? Is centralization of the clients and the applications a bad thing? How do they capture value? Yeah, just general thoughts.
00:57:17.794 - 00:57:44.602, Speaker A: Yeah, that's an interesting question. I think what's interesting is that we have not really seen the full array of potential business models by clients. Right. We've actually not seen that full space explored because typically it's too powerful a move for a client to try to vertically integrate downward into the data side. Right. They all want to control their own graph. They all want to control their own network.
00:57:44.602 - 00:58:30.030, Speaker A: And the reason they want to do that is because there's a nearby viable business model there. If you control the client and you control the underlying the back end, then you can do ads and things like that. Right. So there's a very nearby sort of local maxima that's like high quality. I think if we had a space where if we had a space where the underlying protocol has explicit economic conditions, unlike Usenet and a usenet client trying to monetize on top of usenet. Right. Where the underlying protocol actually has economic components, I think that gives a lot more options and a lot more maneuverability to a client on top.
00:58:30.030 - 00:59:21.322, Speaker A: Right. The reason for that is that if the protocol is well specified enough, the client can try to take credit for behaviors and contributions to the underlying protocol. And if the underlying protocol is monetized and is such that it wants to encourage the widest possible set of clients, then there's a positive relationship between them, not a competing relationship. So there's a really interesting design space there. In general, whether it's centralizing for there to be clients at all. Is the relationship between users and clients centralizing? I think it kind of is for brand like reasons, but depending on switching costs and depending where the network effects are accumulating, it can be relatively minor such that a client won't be able to exert economic power over the underlying protocol. But again, it depends on the specifics.
00:59:21.322 - 00:59:31.460, Speaker A: I'd like to spend more time talking about it because there's a lot of nuance. I'm trying to have a general answer but not meander too much. Yeah, that's interesting. Thanks. Sure. Thanks.
00:59:32.630 - 00:59:43.254, Speaker B: Hey, at the application level, I just want to think about what if we flip the script a little bit? So in web One, if you're building on top of a protocol and you're building an application, we sort of knew.
00:59:43.292 - 00:59:45.318, Speaker A: What the risks were. In web two, we saw that the.
00:59:45.324 - 01:00:05.502, Speaker B: Risks were like, hey, they could capture the value, they could limit your growth. How do you think about application risk when you're choosing to build on top of a protocol? For web3, what's different compared to web two and web one that we should look out for? Like, for example, I know some friends who built companies at certain protocols that well, those are no longer companies.
01:00:05.556 - 01:00:05.774, Speaker A: Right.
01:00:05.812 - 01:00:08.400, Speaker B: So how do you think about.
01:00:10.290 - 01:00:10.606, Speaker A: What.
01:00:10.628 - 01:00:14.810, Speaker B: The risk profile and exposure is for applications building on top of protocols?
01:00:14.890 - 01:00:40.540, Speaker A: Yeah, that's a really interesting question. Yeah. What's the risk profile for an application building on top? Because at the limit, like I said in the talk, my genuine belief is that the risk profile should be the least of all potential risk profiles. Like building on top of take for example, like building on ethereum. Right. There's no sense in which building on ethereum you should feel like you are competing against ethereum. That doesn't make sense.
01:00:40.540 - 01:01:13.490, Speaker A: You may be competing with other types of agents in the ecosystem, but not with the base layer that you're building on. And on top of that ethereum's economic model is totally legible. You know exactly what it's trying to do and how it operates. Even though it's not run by or controlled by any specific group. You kind of have a reason for how you can reason about how this protocol itself operates. So I'd say that at the limit, it should be the least risky thing you can do because it's all totally legible and it's not going to shift underneath your feet. The decentralization and the economic explicitness.
01:01:13.490 - 01:02:03.890, Speaker A: That being said, the riskiest component is probably that because the protocol itself is interoperable, you can assume that the switching costs between your application and other applications are very low. That's kind of one of the promises of relying on a system that just uses permissionlessly, generatable, public, private keys. So I think that's more of the risk, but I think that just means more competition in terms of features and quality and things like that. And finding different directions to expand, like vertical expansion into the protocol will probably be a little bit more difficult. It's kind of a big strategic lift. Yeah, it's hard to say, but thanks for the question. Cool, thanks.
01:02:03.890 - 01:02:04.740, Speaker A: Yeah.
01:02:06.630 - 01:02:28.410, Speaker C: Eddie, Brandon, I thought that was a really fascinating talk, so thank you. I thought your analogy with the farm and the rainforest was particularly inspiring. Do you have concerns that long term, the optimal corporate strategy would be to wait for the rainforest to be sufficiently developed and then to come in and extract all the value by building a vertically integrated application that just competes on UX through funding?
01:02:29.070 - 01:02:57.758, Speaker A: It's a good question. I'd say that I think this sort of really complex ecosystem is really resilient. If there's genuinely a lot of players and a lot of different systems are interlocking and intercomposing, it's going to be really difficult to come in and then just straight vertically integrate from the bottom to the top. If it's as robust and complex as it should be, then there's an incredible number of participants that you need to displace. So I think it's actually really challenging.
01:02:57.934 - 01:03:25.082, Speaker C: Yeah, I hope so too. I just wonder because you did mention, like, funding can solve UX, right? And so if UX becomes the fundamental factor in switching cost, then at some juncture you just have UX. That's so incredible from one player that they know take advantage of the innovation. I think one example would be, like Apple, and like, Unix is a very innovative kernel and Apple comes in and has all the money and that becomes the de facto way to interoperate with Unix.
01:03:25.146 - 01:04:14.302, Speaker A: Yeah, that's true. Although Apple's strategic position comes because they fundamentally control the hardware and it's incredibly difficult to make extremely high quality hardware. Apple's lead is there's a bit of a brand, but they actually have different network effects and different economies of scale all down the stack. That took an incredibly long time to generate. I think if anybody wanted to compete even with one of those layers, that would be extremely difficult. And so it's hard to locate exactly if there was a sufficiently developed rainforest. What is exactly the thing that a competitor would pry into? You could say, like transaction fees or something.
01:04:14.302 - 01:04:47.320, Speaker A: Like some totally centralized system is cheaper. Yeah, but then it doesn't interoperate necessarily with all the. Other participants in the system that have built all the software that connects with each other and composes all the different types of tokens and protocols, they would have to replicate an increasingly large thing and the larger and more complex their vertically integrated system would need to replicate, the more difficult and also the less credible for people building on it. Why build on this thing when you can build on this? Why build on this? Someone else's liard. Yeah, cool. Thanks so much.
01:04:47.770 - 01:05:15.662, Speaker B: Thanks so much for the talk. I really liked what you mentioned around network effects and switching costs. And I guess in a system like Encrypted where everything's open source and there are sort of reducing or tending towards zero switching costs, what are other options for creating strong network effects? And in particular we've seen vampire attacks or different types of attempts to sort of subvert existing networks.
01:05:15.726 - 01:06:14.978, Speaker A: Yeah, that's a great question. A little point I'd make about the vampire attacks is that none of them have worked, like at the limit. They fail. I think they fail partly for legitimacy reasons because they just seem less legitimate, because they're kind of obviously fast following and they don't necessarily have the credibility of the original who, if you had to choose which team you're going to work with, you know, you want to deal with the people on the original. Right? And another reason is that I think once the control plane goes out, once the incentive plane goes out, like once all the tokens go out, that can catalyze a community. A community can be a very strong source of defense because you have a bunch of people who are inherently aligned even though they're not part of the same legal entity or may not even know each other, they're all aligned around defending a specific thing and expanding it and pushing it together. And we're at the very earliest days of that.
01:06:14.978 - 01:06:47.818, Speaker A: But I think that that is a kind of defensibility that's not even possible, really, in Web Two world. Right? Because you'd have to give everyone some ownership of your company, which is at ODS inherently with its being centrally controlled. Right. So I think that's a potentially interesting one. I also think people may undervalue the network effects that are possible with composability. Composability has to be limited by centralized companies. They have to limit it because they want to focus the ways people use their system in ways that serve their end goal.
01:06:47.818 - 01:08:06.290, Speaker A: But if it's an open ecosystem where anyone can remix and recombine the constituent parts, then you can emerge with new types of composability and interoperability that are inherently very sticky, that contribute to network effects. Think about the incredibly powerful, like crazily powerful network effects of programming languages, for example, or like low level pieces of software that people check out, like open source software, massive network effects around those things. If you try to build a system with a new programming language you have a major uphill battle like some new esoteric language, yet the languages have almost no means of collecting on that network effect. Right. They cannot internalize the value that they're creating, which is why I don't know if you've seen the Python Software Foundation struggles for funding all the time, even though it's like, what, ubiquitous, one of the most powerful popular programming languages in the world. So my point is to say is that if the components of the systems you're building are truly composable in such a way that they can achieve network effects like open source software or programming languages, that is a kind of network effect that even a centralized system cannot obtain. So I genuinely think there is a much higher global maximum.
01:08:06.290 - 01:08:20.140, Speaker A: And I think once you have a potent network effect and a way to incentivize all the contributors and participants in that system, then it's very likely, although it depends on the system, that you'll be able to discover some way to capture value.
01:08:20.670 - 01:08:22.138, Speaker B: Awesome. Thanks so much.
01:08:22.304 - 01:08:23.020, Speaker A: Sure.
01:08:23.630 - 01:08:35.786, Speaker B: Hey, Adi, thank you very much for your talk. I have a question. So what's your definition for application? So I'm thinking like ethereum. It's a protocol, right? And then Uniswap on top of it is also a protocol.
01:08:35.898 - 01:08:37.330, Speaker A: And it's all protocols.
01:08:38.070 - 01:08:42.100, Speaker B: Maybe like the front end of Uniswap. It's a product.
01:08:42.470 - 01:09:07.900, Speaker A: Yeah. I mean, it's an admitted weakness and over generality of the definition. I think the right way to think about applications is like it's the thing that has some sort of meaning to its end user. Like has a specific meaning. If you're using some corporate API, in some sense, that's an application that you're using. Right. That's an application.
01:09:07.900 - 01:09:24.638, Speaker A: Uniswap is an application for some people, but it's also infrastructure for others. So I admit it's a little bit blurry, but I think that maybe the key defining line is whether the application has a direct relationship with the end user, whether the software has an direct relationship with the end user.
01:09:24.814 - 01:09:25.586, Speaker B: Okay.
01:09:25.768 - 01:09:29.294, Speaker A: But it's not a strict definition. Weekly held.
01:09:29.422 - 01:09:38.694, Speaker B: Yeah. And by which most protocol, the team runs their own front end, but it's not like the protocol itself has a relationship.
01:09:38.892 - 01:10:06.462, Speaker A: Yeah. So in that framing, when we're talking about, for example, our regulatory team's idea, which I totally agree with, regulate apps and app protocols, what they mean by that is exactly that. It's like the application in that case is the interface, the front end. The front end, right. Which many people could run. And then the protocol is the sort of credibly neutral system that's running directly on the blockchain, that has some either governance system or it's immutable or something like that. Right.
01:10:06.462 - 01:10:13.774, Speaker A: So in that case, I'd say, yeah, the application is the interface, the protocol is the back end, so to speak.
01:10:13.812 - 01:10:16.446, Speaker B: Like a public good type of could be, yeah.
01:10:16.468 - 01:10:39.222, Speaker A: I mean, it's a little bit of a play on public good, because when people talk about public good. They typically think of something because of how few options we've had in the past. When they say public good, they typically mean something like something that is free or does not capture value. Right. But I would argue there's, like this third space. You don't have to be trying to capture every externality you create. You also don't need to be totally free.
01:10:39.222 - 01:10:56.958, Speaker A: In fact, I think that the best space to be in for the longest lasting infrastructure can feed itself, can grow itself, can defend itself, can pay for itself, can internalize value, but doesn't have to do so in a way that is rent seeking, that is net deleterious to the ecosystem that uses it.
01:10:57.044 - 01:10:58.862, Speaker B: Yes. Thank you.
01:10:58.996 - 01:11:02.620, Speaker A: Sure. Cool. Okay, well, that's all the questions, so thank you again.
