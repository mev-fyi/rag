00:00:09.690 - 00:00:18.702, Speaker A: If you're feeling happy. XOR your private key is an OD number. Clap your hands if you're happy.
00:00:18.702 - 00:00:21.760, Speaker A: XOR. The second bit of your private key is OD. Clap your hands.
00:00:21.760 - 00:00:30.678, Speaker A: Okay. Maybe there's not enough time to extract private keys. Okay, so great.
00:00:30.678 - 00:01:15.490, Speaker A: So what I'll going to talk about today is basically an overview of how upcoming changes to the Ethereum ecosystem. So both Ethereum layer one and ethereum layer two are going to end up affecting the mev landscape, what differences they'll create between mev as it exists in Ethereum today, and the way that the mev ecosystem will just have to be structured in the years to come. And basically, what can we do to make the best out of the world that we're going into? So start with the first thing on the timeline, right? The merge.
00:01:15.490 - 00:01:30.742, Speaker A: The merge is happening. I mean, I hear the core devoter saying like 2027 or so, but it's happening, it'll be here eventually. And proof of stake is going to significantly affect mev.
00:01:30.742 - 00:01:51.006, Speaker A: Right? So one of the kind of interesting and very subtle ways in which proof of stake affects mev is kind of this fact that basically proof of work style SGX protection can't be done in the same way. Right. So just to kind of get into the weeds here, this is a technique that's theoretically possible.
00:01:51.006 - 00:02:34.106, Speaker A: I think there's a version of this that's being developed or has been developed for proof of work. But figuring out how to translate this to a proof of stake context is not very trivial. But the basic idea is how do we ensure transaction privacy up until the point where those transactions actually do get included into a block? Right? And so the basic technique is that miners could have an SGX module and they would receive bundles that get encrypted with a key, where the private key only exists inside of the SGX module.
00:02:34.106 - 00:02:59.970, Speaker A: So anyone can encrypt you with a public key, but only computation inside of the SGX can decrypt, and basically they will get these encrypted bundles. Then the Miner inside of the SGX would be able to build a block and construct the block body, construct the block header. If you assume SGX is safe, big if, then the miner has no way of kind of extracting the contents of the bundles out so they can't do mev stealing.
00:02:59.970 - 00:03:13.658, Speaker A: And then the SGX enclave would only release the header, and then the header would go to the Miner, the miner would mine an ons. And only if the miner actually creates a valid block would the enclave actually release the body.
00:03:13.744 - 00:03:14.090, Speaker B: Right?
00:03:14.160 - 00:03:34.270, Speaker A: So the core idea here is that the body in clear text only becomes visible at the moment when a valid nonce is already created. And so if the Miner does anything other than just immediately publish the block exactly as it is, then the Miner pays a really huge opportunity cost, right. Because they basically lose that block.
00:03:34.270 - 00:04:13.590, Speaker A: So this does not translate that easily to a proof of stake context, right? The reason basically is that in proof of work the main form of authentication on a block is the nonce that gets checked with a proof of work function. In proof of stake the main thing that you check is the signature with the proposer's private key. But the problem is that you can't just make releasing the body conditional on a signature with the proposer's private key because while the proposer can make that signature, then they can get the block and then they could use that steal the mev, generate a different block.
00:04:13.590 - 00:04:29.166, Speaker A: They would double sign, they would create actually have to sign two different blocks. But because they never actually published the first block, they would never get slashed for it. Right? So this is kind of the key reason why the proof of work approach doesn't translate into proof of stake directly.
00:04:29.166 - 00:04:59.318, Speaker A: Now there are kind of some solutions to this. So one of them is basically what ABV Boost is doing, which is if you rely on kind of doubly trusted intermediaries. So intermediaries that are trusted both by the Validators and by builders, so we call these relays, then they can basically be trusted as oracles to kind of store the block and not actually release the block until they see that there's a signature from the proposer.
00:04:59.318 - 00:05:38.790, Speaker A: And then at that point they're the ones in charge of releasing and so the proposer can't really trick them. And then the second approach is if you have some kind of in protocol PBS, then you could design the PBS in such a way that basically after the proposer signs there's a round of signatures and then after that round of signatures the body gets published. And then this kind of release body check basically instead of depending on just a proof of work or on just one signature, it would depend on the entire committee signature, right? So basically the body only becomes visible once lots and lots of attesters have actually agreed on the header.
00:05:38.790 - 00:06:01.646, Speaker A: So this is like one possible way to make it work if you don't want to have any trust assumptions except the majority of validators. So that's kind of one way in which proof of work and proof of stake are just different, right? And you have to design differently around them. They have these different properties where they're not exactly equivalent to each other.
00:06:01.646 - 00:06:24.514, Speaker A: Another really important one here is like the next proposer is known, right. So in proof of work the next proposer is not known and this has interesting consequences. So like for example, if you want to do cross domain mev between multiple proof of work chains, then it's harder because there's no possibility of an actor knowing that they're going to be the proposer the next proposer on both chains.
00:06:24.514 - 00:06:44.750, Speaker A: But in a proof of stake context it actually is possible, right, because you know ahead of time that you're going to be the proposer. And so let's say you have 20% of all the stake on chain A and you have 20% of all the stake on chain B, 4% of the time you're going to be the next one on both. And so 4% of the time you're going to be able to extract the cross domain mev, right.
00:06:44.750 - 00:07:00.434, Speaker A: So this is both potentially useful, but it's also a centralization vector because while if you have 10% of the stake, you can do it 1% of the time. If you have 20% of the stake, you can do it 40% of the time. If you have 50% of the stake on both sides, you can do it a quarter of the time.
00:07:00.434 - 00:07:33.894, Speaker A: So it is a centralization factor. And this is one of the reasons why I think kind of thinking about the cross domain mev problem and kind of creating ways of getting those gains without actually needing to be the proposer is so important. Another interesting issue is that the proposer has this kind of game where they can either publish at the normal time and then be guaranteed that if the network is not totally broken, the Attesters will see everything in time and they'll publish.
00:07:33.894 - 00:07:42.080, Speaker A: Or they can wait a little longer. They can wait and try to get some more mev. Instead of publishing at T equals zero, they would publish at like T equals 3.7
00:07:42.080 - 00:08:17.658, Speaker A: and basically kind of live on the edge and try and get to the point where if more than 51% of Attesters see your block on time and sign off on it, then in some ways you didn't publish late enough, right? So you can kind of play the edge award game if you really want to. And this is something that doesn't really exist in proof of work, but it exists in proof of stake, which is also interesting. Now maybe if we somehow change how proof of stake works, you could use VDFS to kind of simulate more properties of proof of work.
00:08:17.658 - 00:08:45.730, Speaker A: But this is an issue also the possibility of making side deals with the proposer, right? If you know ahead of time that you're going to be the proposer, you could send transactions directly to them. Like I even know there are chains way back, like I think NXT back in 2013, I forget, maybe this was what they called transparent proof of stake or something, but where you send transactions straight to the next proposer and it's supposed to be more efficient. But of course it also has Dos risks.
00:08:45.730 - 00:09:29.310, Speaker A: And then if you use single secret leader election, then well, the proposer doesn't actually reveal who they are until they make a block, but then they could still reveal who they are to a few people if they want to make some side deals. So there's some issues that you have to think about there too, right? So the question of how do we minimize trust in the builder market is interesting, right? So this is starting to get a little further beyond proof of stake, right? Proof of stake does kind of change the game in a lot of ways. It basically means that these designs for markets have to be designed differently in a whole bunch of subtle ways that are not very easy to summarize in one sentence.
00:09:29.310 - 00:10:02.634, Speaker A: But it's just a different system, it has different properties. So Mev Burst is this kind of shorter term technique, right? So unfortunately the images are a bit small, but if you search for both of these on e three search, you can find big versions of the images. So feel free to basically Mev Boost says you have builders, they send their blocks to the relays, relayers send the headers to the proposers, and then the relayer publishes the block after they see a valid signature.
00:10:02.634 - 00:10:42.326, Speaker A: And then in Protocol PBS is this more complicated thing where it tried to remove all of the trust assumptions involved in relays. And this particular thing is the two slot version of proposer builder separation, where basically what we do here is instead of requiring the proposer to directly or instead of expecting the proposer to directly create the block contents, you have an explicit in protocol auction, right? So it's like even slots are basically auctions for either who's going to create the next block or what the hash of the next block is going to be. And then OD swats are just there for the winner of the auction to be able to publish.
00:10:42.326 - 00:11:14.254, Speaker A: Right? So it's a different architecture, it's an interesting architecture and it does have the benefit that it lets you basically separate out the proposer from the builder and keep proposers very decentralized, insulate proposers from the economies of scale in the Mev ecosystem and kind of leave all of that to the builder. And then of course there's the question of like, can you make builders more well behaved internally?
00:11:14.302 - 00:11:14.466, Speaker B: Right?
00:11:14.488 - 00:12:01.262, Speaker A: So can you make builders more decentralized internally? So a lot of the thinking in Dank Sharding, for example, is about can we make builders more decentralized them internally? Actually, one of the benefits of using KZG is that you can do that much more easily than in a lot of other systems. And there's the question of, well, can you use SGX type stuff so that these decentralized builders can give people more kind of pre transaction privacy, right? So users can't get exploited. So in Protocol PBS markets basically, you know, designed the Ethereum protocol around the assumption that just random validators and builders are going to be these kind of separate classes of actors where builders are just likely to be much more sophisticated.
00:12:01.262 - 00:12:27.370, Speaker A: And I guess the hope of many people in this room is that builders don't even have to be like monolithic kind of individuals or organizations. Builders themselves can be fairly decentralized ecosystems, right? So there are going to be kind of more assumptions possibly governance layers, trusted hardware, whatever on the builder side. But then the proposer side is kind of even further insulated from all of that.
00:12:27.440 - 00:12:28.060, Speaker C: Right?
00:12:28.910 - 00:12:47.810, Speaker A: So basically the goal here is to keep on reducing the trust assumptions. And the reason why you reduce the trust assumptions is because that increases the possibility of decentralization. Right? So today builders have to trust miners, which is horrible because only the big miners can be trusted.
00:12:47.810 - 00:13:00.434, Speaker A: Tomorrow builders and stakers and validators need to trust relayers. So nobody needs to trust the validators, you don't need to trust the builders. And so both of those can be fairly decentralized.
00:13:00.434 - 00:13:07.970, Speaker A: And then relayers are kind of this relatively dumb functionary. In principle, you can have a whole bunch of them. In principle.
00:13:07.970 - 00:13:20.726, Speaker A: Some builders might merge with relayers, but they don't have to. And there probably will be at least some relayers that are willing to work with anyone. So still some trust, but it's kind of better, at least creates the possibility of more open markets.
00:13:20.726 - 00:13:46.710, Speaker A: And then in protocol, PBS is like no trust. And it allows validators to come much closer to this role of basically just being dumb functionaries who execute code and who don't have to make decisions of hey, which relayers do I trust? So dank sharding. So this is happening in parallel with PBS, but is one of these other big things on the horizon.
00:13:46.710 - 00:14:03.350, Speaker A: Basically the idea behind Dank sharding is this new form of sharding that tries to be really ultra minimal on how much it actually shards, right? It doesn't shard execution, it only shards data. It doesn't even shard proposing. It basically says there's still one proposal.
00:14:03.350 - 00:14:32.660, Speaker A: Like there aren't actual shards. It's all kind of sharded in this very sort of continuous way where it's still technically sharded because every node only needs to download and check a little bit of the data. So it fits the definition, it fits the goal of giving much more scalability, but otherwise it tries to pretend to be a simple monolithic chain as much as possible, right? And that reduces the conceptual load of having to deal with the system and has a bunch of benefits.
00:14:32.660 - 00:14:39.506, Speaker A: So what are the consequences of Dank sharding?
00:14:39.538 - 00:14:39.926, Speaker C: Right?
00:14:40.028 - 00:15:13.380, Speaker A: So one of them is that it is probably going to make some kind of proposal builder separation mandatory. The reason why is because you have these 32 megabyte blocks and the proposer of a block, we can't realistically expect them to actually be able to download and check 32 megabytes and potentially more like potentially going up to 128 megabytes or more in the future. Right? It's like Ethereum Satoshi's vision here, except it actually works.
00:15:13.380 - 00:15:42.210, Speaker A: But basically you can't expect individual validators to be able to have that, right? You are going to need to have some separation between the builders who actually will needs to kind of handle bigger amounts of data and validators, which only needs to actually verify a tiny portion of the data individually. So there's actually a few different options for how to handle this. So one of them is to go the route of PBS.
00:15:42.210 - 00:16:04.554, Speaker A: Basically you have proposed builder separation, you have options and then okay fine, you have one builder and the builder has to handle how the heck am I going to make 128 megawide block? And if they have amazing data connections they'll be able to do it just fine. And everyone else only needs to download and check a little bit of the data. Option two, status quo plus availability Oracle.
00:16:04.554 - 00:16:26.966, Speaker A: So this is like an interesting intermediate option, right? This is like if we're allergic to PBS then how could we make it work? And the idea basically here is that with these blob transactions where in Dank Sharding, basically blob transactions, they have a header and the header is fairly small, a couple of hundred bytes. And then they also have a body. And the body is currently one hundred and twenty eight k.
00:16:26.966 - 00:16:37.622, Speaker A: It might go up a bit in the future but what you can do is you can basically have the headers and the bodies be broadcasted on different subnets.
00:16:37.686 - 00:16:38.058, Speaker B: Right?
00:16:38.144 - 00:17:15.490, Speaker A: And so then as a builder what you can do is you can listen to the mempool that contains the headers, you can grab transactions that have the headers and then for every header, simple and dumb solution. You ask a chain link Oracle like hey, is the body available? And if it tells you the body is available you accept it and you don't even try to figure out whether or not the body is actually available yourself. And then you publish the block and then you basically just rely on the distributed self healing and two dimensional data availability sampling blah blah blah to make sure the network kind of gets all of the data and the contents that it expects.
00:17:15.490 - 00:18:10.460, Speaker A: So also an option, but this would require proposers to know which Oracle is to trust and it requires proposers to make these more active decisions which is not very nice. Option three is like builders plus availability Oracles, right? So this is what I mean by decentralized builders, right? Basically you have a builder that just looks at these headers and that tries to just make an optimal block based on the headers but then they listen to various availability Oracles and they just kind of outsourced a job of figuring out whether or not the data is actually there to other people. So within the context of Dank Sharding, if we want to have PBS, if we want to have kind of mev optimization, be compatible with decentralization, you have to start looking into some of these issues.
00:18:10.460 - 00:18:26.190, Speaker A: Layer two protocols. So we were talking about layer one for now. Layer two is like a whole other thing, right? And layer two is interesting because it does very significantly impact how the mev landscape works.
00:18:26.190 - 00:19:22.730, Speaker A: Basically the kind of impact that it has really depends on how the yellier two is structured. Right? So basically, how is roll up sequencing controlled? What is the mechanism that decides who actually has the right to publish the next optimism block, or the next Arbitrum block, or the next ZK sync block, or the next StarkNet or A block or whatever else? So the key question is how is role sequencing controlled? One very simple and naive way to do it is like anyone can submit, right? Anyone can submit the next block. Historically, I think anyone can submit has been rejected because anyone can submit would lead to a lot of overhead from basically multiple people trying to submit the block at the same time and most of them not getting in, but then still having to pay because their transaction still pays gas.
00:19:22.730 - 00:19:37.294, Speaker A: And so you just have a lot of junk on the Ethereum chain and it's not very good. But with a much stronger in healthier mev ecosystem, that problem doesn't exist anymore. So you could just do anyone could submit.
00:19:37.294 - 00:20:21.314, Speaker A: I think the reason why I don't expect Anyone could submit to be popular is because ultimately roll up teams are going to want to capture the mev value for themselves, right? And so roll up teams are going to want to kind of front run the implied Ethereum auction and create their own auction to try to auction off sequencing rights. So if the roll up controls submission, then the situation is kind of similar to cross chain mev, though it's also different in some ways too, right? It actually depends on the details of how the roll up controls submission. Do you have auctions for every SWAT? Do you have auctions for periods of 30 minutes? Is it something else? I don't know.
00:20:21.314 - 00:20:56.526, Speaker A: So I think there's two major categories of possible worlds here and the question is to what extent do people seriously try to create or capture cross domain mev between multiple roll ups or between roll ups and the base chain? So one of the worlds is kind of no merging, right? Basically roll up sequencers. So whoever wins the auction to be a sequencer in the published blocks in the roll up, they just submit their Blob transactions and that's the only thing that the proposers can or the builders can include. And so they just get included in the merging world.
00:20:56.526 - 00:21:37.946, Speaker A: The question is like, well, so you have this kind of roll up auction and so the auctions do kind of get separated, but then you have to sort of bring the auction back together if you want one person to be able to optimize over both at the same time. Right? And so basically one possibility is like if the auctions happen out of step with each other, so the roll up auction happens first. Whoever wins the roll up sequencing, they have an incentive to try much harder to win the auction for creating a block on the Ethereum side or the other option is they could kind of sell their sequencing right to the highest bidder or just basically sell their sequencing right.
00:21:37.946 - 00:22:00.002, Speaker A: Lots of ways to kind of pre commit to this. You could even do it off chain and try to just make side deals to make sure that whoever has the ability to make the optimal block actually kind of gets the power to choose there. Right, so how this would work, I mean, I think it's still one of the topics that we need to do a lot more thinking about.
00:22:00.002 - 00:22:49.662, Speaker A: But it is an important issue. So these are things, issues that are going to pop up over the next few years and some of the ways in which the ecosystem might respond. The question that I want to end with is what do we want? Right? What are even the goals of an mev landscape and what are even the goals of what are we trying to achieve when we design the mev ecosystem in response to these kinds of pressures and in response to the desires to achieve more scalability and other properties of the protocol? So the way that I think about my goal is basically insulating the Ethereum base layer from centralizing tendencies of complicated stuff happening on top of or around Ethereum.
00:22:49.662 - 00:23:15.818, Speaker A: Right? So one way that I think about this is one of the kind of reasons why a lot of people in bitcoin land are upset or do not want to see bitcoin have. What I consider real smart contracts is basically because it's not because they're afraid of technical complexity of the bitcoin base layer. Because the amount of technical complexity you need to enable arbitrary applications is actually not that high.
00:23:15.818 - 00:23:29.962, Speaker A: The thing that they're more concerned about is basically applications existing around the bitcoin, interfering with the base layer in all sorts of ways. Mev is actually one good example. Basically mev.
00:23:29.962 - 00:24:15.370, Speaker A: Just like if mev on a chain is possible, if a chain is powerful enough to support applications that have mev, then mev itself kind of feeds back and creates a centralizing risk to the base chain. Right? There's also political risks. Like if you support applications, then are those applications going to start pushing for changes to the protocol to support them? So all of these kind of unavoidable systemic interactions between layer one and layer two, right? And I think the best that we can do in Ethereum if we do both, value the goal of having this very performance layer two that actually supports people's desire to do all of these amazing, awesome blockchain things on it, but at the same time still have this layer one that is robust.
00:24:15.370 - 00:24:21.562, Speaker A: Is ultrasound money? Is Justin still here? Yep. Yay. Is ultrasound money is robust.
00:24:21.562 - 00:24:57.142, Speaker A: It kind of does this job of being this really stable and sturdy thing. Then the goal is you want the interaction between the two to be structured so that the layer one gets insulated from the crazy stuff happening around it, right? You want some layer of separation. So in meme format this is basically it, right? So you have your economies of scale and we want like layer separation between layer two and layer one, where you have layer two side auctions that try to kind of absorb all of these mev issues, proposer builder separation.
00:24:57.142 - 00:25:20.898, Speaker A: So builders absorb the economies of scale and proposers aren't as affected by economies of scale. All of these things kind of make sure that the ethereum base layer can kind of sleep soundly and have as few economies of scale causing centralization risk as possible. So layer twos might help, proposer builder separation might help.
00:25:20.898 - 00:26:06.510, Speaker A: But then there's also this open question of like, well, how can we make these kind of separations even more robust, right? If in the case that this stuff is not enough, then what other kinds of protocol level reform, what kinds of ecosystem level reform could actually get us all the way there and create both mev utopia and base layer utopia? Thank you. Questions?
00:26:13.200 - 00:26:29.972, Speaker D: Hi, thanks for the talk. It was really interesting. The threat to the base layer that I'm not sure is addressed by any of those things is the variability in proposer rewards because most proposers will only if you're a solo staker, you'll propose a few times a year at most.
00:26:29.972 - 00:26:38.776, Speaker D: So there's a massive variability there. So that's going to be a centralization pressure because if you can socialize those rewards such as Lido is doing, then you get a big advantage there.
00:26:38.878 - 00:27:00.610, Speaker A: Is there anything we can do to share that? I mean, this is definitely a good point. I mean, Justin over there has been a fan of mev smoothing for a long time, which basically forces most of the bid to be split among all of the validators instead of going to the specific one that happens to be the proposer. So that's one option.
00:27:00.610 - 00:27:34.310, Speaker A: Aside from that, there's obviously the strategy of mev minimization, which is like an ecosystem layer thing and we should probably do some of that too. So combination of both of those, oh, one third technique I should mention is the more crazier idea is sell block building slots further ahead of time so that you have less knowledge of whether or not there's going to be crazy mev then.
00:27:34.840 - 00:27:45.210, Speaker B: Hey, Vitalik over here. Quick question, just to follow up on your comment about kind of layer twos. Likely looking to take some of that mev sequencing value.
00:27:45.210 - 00:27:58.350, Speaker B: Could you just double click on that and talk about ways that we could design the layer one going forward to be more resistant to that? Or is that something that's kind of inevitable and you're relying on layer two, competition among each other to maybe.
00:27:59.120 - 00:28:11.356, Speaker A: I think one question is, like, do we want to take away layer two's, ability to extract NAV? Right? Because layer twos do have more political room to do certain things that layer ones can't.
00:28:11.388 - 00:28:11.584, Speaker B: Right?
00:28:11.622 - 00:28:40.296, Speaker A: So if, for example, layer one started printing ETH to PayCore developers, then that could easily turn into a political shit show. But if optimism and Arbitrum started to doing that which optimism has signaled, and already started doing all of these retro efforts to do that themselves, then people tend to be more comfortable with that and the ecosystem can benefit a lot from having these diverse funding sources. So that would be a reason not to care.
00:28:40.296 - 00:28:57.970, Speaker A: I mean, if we do care, then obviously you don't want layer twos to just be to completely extractive and make users actually have to pay more than they need to. And in that competition, I think pretty much has to be the answer. At that point, we can take your last question.
00:29:07.440 - 00:29:17.600, Speaker C: Hi, thank you. At this point, do you think it's inevitable that it's going to become an mev dystopia or are you hopeful for the utopia side of the argument?
00:29:18.020 - 00:29:40.452, Speaker A: I'm definitely hopeful. I mean, I do think that realistically, we don't want to try to kind of overextend ourselves and it's important to design in such a way that if central, some centralization is inevitable, then don't just think about minimizing, but also think about directing it and containing it. So kind of pushing it into places where it's less harmful.
00:29:40.452 - 00:30:03.536, Speaker A: I think at this point, with all of this stuff, it definitely gets to the point where mev is far from the largest factor in staking centralization, for example. Right. If we already get to the point where the bigger concern in staking centralization is like say, liquidity of staking tokens, for example, then in some ways we kind of won.
00:30:03.558 - 00:30:03.744, Speaker B: Right?
00:30:03.782 - 00:30:19.380, Speaker A: Because the bigger problem is something else and we have to focus on it and we've achieved what we could. But yeah, in general, yes, I am hopeful that we will be able to achieve many of the goals.
00:30:19.800 - 00:30:22.260, Speaker C: And second question, what are your private keys?
00:30:24.120 - 00:30:35.930, Speaker A: I think it contains a 14. It.
