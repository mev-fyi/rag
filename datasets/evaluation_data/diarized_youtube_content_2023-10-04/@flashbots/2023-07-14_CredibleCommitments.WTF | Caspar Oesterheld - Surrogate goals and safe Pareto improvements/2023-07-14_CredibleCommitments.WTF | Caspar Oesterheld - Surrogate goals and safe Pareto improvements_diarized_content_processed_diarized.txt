00:00:00.330 - 00:00:09.102, Speaker A: Great. Yeah. Hi everyone, and thanks for letting me present remotely here.
00:00:09.102 - 00:00:30.120, Speaker A: So I'll present on surrogate goals and safe paratro improvements. Just quickly, can you see the whole screen? Is there some issue if I move around? Because I have have like small zoom windows in front of stuff. But I guess you can see the whole slide and right.
00:00:30.810 - 00:00:31.560, Speaker B: Yeah.
00:00:32.010 - 00:00:32.518, Speaker C: Good.
00:00:32.604 - 00:00:36.246, Speaker A: Okay, just making sure. Okay. Yeah.
00:00:36.246 - 00:01:02.942, Speaker A: My name is Casper Osterhald and the work I'm presenting is kind of joint work with some people, like the paper is with Vince Conitzer. Also worked on this stuff with some others. And I want to in particular highlight Nathaniel and Anthony because they're actually in London.
00:01:02.942 - 00:01:37.562, Speaker A: So if later, over dinner or the like, you want to discuss this with these kind of topics, I think they'd be great to talk to. I think Anthony will give the talk right after this one on some similar ideas. Maybe Nathaniel, maybe I guess maybe it's a small group, probably know each other's names at this point, but maybe Nathaniel, you can raise your hand or so here's the plan.
00:01:37.562 - 00:02:04.590, Speaker A: I think we have an hour in the schedule and I have slides for 20 to 30 minutes. And the idea is for you to ask questions during the during the presentations if you have any. And then we can use the remaining time for discussion and kind of brainstorming.
00:02:04.590 - 00:02:29.050, Speaker A: The presentation will mostly not be talking about any kind of cryptography ideas, even though yeah, I'm actually currently with Nathaniel working on a project that does use kind of cryptography to implement some of these ideas. But I'll mostly keep it out of the presentation part and then maybe you can discuss it during the brainstorming.
00:02:31.010 - 00:02:31.760, Speaker C: Okay.
00:02:32.610 - 00:03:18.342, Speaker A: So the general motivation for this work is that sometimes in the real world there's kind of some kind of bargaining going on and people make kind of conflicting demands for some good, like I don't know, countries making conflicting demands about pieces of land or something. And then if the demands are inconsistent, then normally what happens is some kind of conflict. At least that's one of the things that can happen is some kind of conflict in which utility is basically burned.
00:03:18.342 - 00:03:33.460, Speaker A: So players kind of actively pay to hurt the other side. You can think of war as being like this at least some kind of wars, I guess.
00:03:34.470 - 00:03:35.220, Speaker C: Okay.
00:03:38.150 - 00:03:55.858, Speaker A: So here's an example of a normal form gain where something like this could happen. So for now, ignore these kind of eliminated parts of the game. So those are actions that can be eliminated by strict dominance.
00:03:55.858 - 00:04:04.220, Speaker A: So we would not expect them to be paid played. Yeah, just ignore them for now. They'll become relevant later.
00:04:04.220 - 00:04:27.726, Speaker A: So we basically have this two by two game in the top and it's kind of like a game of chicken. The two players can make demands, which is like this DM, which is kind of like I demand this item or something like that. And the problem is if both demand the item then well there's conflict.
00:04:27.726 - 00:05:08.602, Speaker A: So the story in the paper is that it's exactly this kind of territory that two countries both kind of demands to annex and there are multiple Nash equilibria. So there's this Nash equilibrium selection problem similar to a game of chicken. And we might imagine that in the real world sometimes they won't be able to kind of, you might say, coordinate on the same natural equilibrium or they might disagree on what the appropriate Nash equilibrium is.
00:05:08.602 - 00:05:27.230, Speaker A: One of them might think that, they might both think that rightfully that the thing that they're bargaining over should belong to them. And so sometimes it might happen that they both play DN and that conflict occurs.
00:05:27.670 - 00:05:28.420, Speaker C: Okay.
00:05:32.150 - 00:05:53.000, Speaker A: Now let's imagine that this game is not played kind of directly by the players, but it's played via delegation. So let's say both kind of original players or principals are humans and.
00:05:55.290 - 00:05:55.654, Speaker C: They.
00:05:55.692 - 00:06:27.870, Speaker A: Each recruit a robot or kind of more generally some kind of representative to play the game on their behalf. We might imagine that one way they can do this is to write some algorithm or like, I mean if it's just a simple game like this, they could just explicitly specify what they want to happen, what they want their representative to play in the game. So they could just explicitly say play DM or play RM.
00:06:27.950 - 00:06:28.386, Speaker C: Okay?
00:06:28.488 - 00:06:49.610, Speaker A: If they do that then of course doesn't really change that much about the game. But now we're going to make kind of two kind of additional assumptions. The first is that they can make some kind of joint credible commitments in how they instruct their representatives.
00:06:49.610 - 00:07:12.574, Speaker A: So one version of this is this kind of program equilibrium idea. But you can also imagine that it's kind of more like mediators or there's some joint contract that they both signed which says they're going to tell their representative certain things. Okay, so this alone still doesn't really solve this game because there's still many equilibria.
00:07:12.574 - 00:07:27.814, Speaker A: They might still disagree about what should happen in the game. Each of them might still say, okay, well I want the Dmrn outcome and.
00:07:27.852 - 00:07:28.440, Speaker C: Then.
00:07:30.810 - 00:07:34.620, Speaker A: Both being able to make these credible commitments doesn't really help.
00:07:36.990 - 00:07:37.402, Speaker C: Okay.
00:07:37.456 - 00:08:16.854, Speaker A: But now an extra assumption that we're going to make is that the representatives are actually kind of, they have some intrinsic intelligence or abilities and in particular they're able to play games really well. So instead of telling them what action to take or giving them some algorithms, we assume that the humans can just tell them to play the game in whatever way they think is reasonable. And in fact we are going to assume that they are kind of so good or like the humans trust them so much to do this, to do this.
00:08:16.854 - 00:08:44.880, Speaker A: Well that kind of by default if they weren't able to make credible commitments or if they're kind of unable to agree on some joint bilateral commitment, then the default is for them to just do that, to just say, okay, just play the game with these utilities as best as you can.
00:08:47.900 - 00:08:48.650, Speaker C: Okay?
00:08:48.960 - 00:09:08.720, Speaker A: Even so the equilibrium selection problem remains. If they do this aligned delegation, if they tell the robots, here's my utility function, do your best, the robots might still end up in the DMDM outcome.
00:09:09.300 - 00:09:10.050, Speaker C: Okay?
00:09:10.980 - 00:09:38.090, Speaker A: Now comes the idea for how to decrease basically eliminate the loss of potentially playing this top left DMDM outcome. Just a quick question. Can you see my cursor? Probably not.
00:09:38.090 - 00:09:39.324, Speaker A: You can?
00:09:39.362 - 00:09:39.900, Speaker C: Okay.
00:09:40.050 - 00:09:51.424, Speaker A: Can you see it reasonably well? So like if I point at things negative five. Okay, so this is kind of the bad outcome, right?
00:09:51.462 - 00:09:51.664, Speaker C: Okay.
00:09:51.702 - 00:10:05.696, Speaker A: And now these kind of other parts of the game they become relevant. So again, normally by default we expect that only this top left two x, two quadrant would be played.
00:10:05.888 - 00:10:06.292, Speaker C: Okay?
00:10:06.346 - 00:10:19.160, Speaker A: But now the idea is the following. The two original players give the following instruction. So first they tell them to not play DM and RM.
00:10:19.160 - 00:10:49.996, Speaker A: They both tell this to their representative and all of this conditional on the other one giving the analogous instruction. So they tell them to only play DL and RL which weren't originally relevant to how the game is played. And now these DL and RL, they can be seen as kind of harmless versions of DM and RM.
00:10:49.996 - 00:11:02.150, Speaker A: So in particular DL is kind of like DM. It's kind of like demanding the thing. But then in case of DLDL there's no conflict, there's just kind of nothing happens or something like that.
00:11:02.150 - 00:11:21.532, Speaker A: And the idea is that the original players tell the representatives to value this DLDL outcome in the same way as they would value the DMDM outcome. So minus five. Minus five.
00:11:21.532 - 00:11:35.170, Speaker A: So these blue numbers are the utilities that the original players tell the representative they should assign to outcomes in the game. Okay, so this is now minus five. Minus five.
00:11:35.170 - 00:11:45.590, Speaker A: And these are they're kept as in the original game which happens to be the same as this part of the game.
00:11:47.320 - 00:11:47.972, Speaker C: Okay?
00:11:48.106 - 00:12:30.800, Speaker A: Now why might one do this by default? So the left is now the game that effectively would have been played by default and the right is the game that is now going to be played with the blue utilities being the utilities that the representatives are supposed to assign to the outcomes. And the black utilities are still the ones that the original players, the humans, assign to the outcomes. And now the point is that the blue utilities game on the right hand side is isomorphic to this original game that would have been played by default.
00:12:30.800 - 00:13:07.676, Speaker A: So we might imagine that plausibly this right hand game is going to be played kind of in an analogous way to the left hand game. So this game is going to be played in the same way as kind of the game would have been played if it wasn't for this possibility of making commitments. And the important difference between the two games is that one of the outcomes is better for both players than in the original game.
00:13:07.676 - 00:13:46.840, Speaker A: So if DMDM had been played in the original game, then in this new way of playing the game, we would expect DLDL to be played, which of course to the representatives, to them that's the same. But to the principals, to the original players, this one one outcome is much better than the minus five, minus five outcome. So in the case that this conflict outcome occurs in the original game, it is better for the representative, for both of them, if this new game is played where this harmless outcome occurs.
00:13:47.580 - 00:13:48.148, Speaker C: Okay?
00:13:48.254 - 00:14:40.830, Speaker A: And the remaining outcomes, if in the original game dMRM had been played, then we might expect DLRL to be played in the new game. And that has the same utility for both players and that's the case for the other ones as well. And that is basically an example of a safe paratro improvements which is kind of like an instruction to the representatives to play a game in a new way such that it's kind of guaranteed to be pareto better for the original players even in the face of equilibrium selection and so on.
00:14:40.830 - 00:15:10.950, Speaker A: So the important thing is that the original players, to improve on the default, the original players don't have to resolve in any way the equilibrium selection problem in this original game. Okay, I'm now going to move to a second example. Maybe people have questions before I get to the second example.
00:15:10.950 - 00:15:17.944, Speaker A: I still don't get why the players would do that.
00:15:17.982 - 00:15:22.584, Speaker D: Because they now want, I mean, those actions are dominated, right?
00:15:22.782 - 00:15:26.040, Speaker A: So why would they give these instructions.
00:15:28.080 - 00:15:31.820, Speaker D: To dominated actions or dominated strategies?
00:15:35.760 - 00:15:52.880, Speaker A: That's a good question. They only do this kind of conditional on the other player giving the analogous instructions. Like one player wouldn't ever unilaterally say don't play dMRM, play DLRL.
00:15:52.880 - 00:16:54.740, Speaker A: That would definitely be bad. I guess one kind of very simple case where you would tell your representative to play a dominated action conditional on other people making analogous commitments would be the prisoners Dilemma, right? In the prisoner Dilemma, cooperate is strictly dominated. But if you tell your representative to, or if you kind of write, I don't know, make a commitment to play cooperate, if the other player makes an analogous commitment, that's reasonable, right? And in some sense what's going on here, at least with respect to this kind of commitment to only play these dominated actions, in some sense that is analogous to this commitment in the prisoner slamma.
00:16:54.740 - 00:17:47.350, Speaker A: Does that make sense? I'll continue then. Okay, so here's a second example which is, which is, which is actually older than the first example. And yeah, people sometimes call this a surrogate goal, like the example that I'm now going to give or threaten a neutral surrogate goal.
00:17:47.350 - 00:17:50.058, Speaker A: And yeah, I view it as kind.
00:17:50.064 - 00:17:50.620, Speaker C: Of.
00:17:52.670 - 00:18:03.630, Speaker A: Special case of a unilateral safe crater improvement where unilateral just means that only one player has to make commitments.
00:18:05.890 - 00:18:06.640, Speaker C: Okay.
00:18:10.770 - 00:18:25.710, Speaker A: So here we have a blackmail game. So we have two agents. The right agent has this item, the star.
00:18:25.710 - 00:18:48.140, Speaker A: And the left agent wants the star. And the only thing that the left agent can do is that she has this kind of lightning thing which represents something bad that she can do for player two. So she can hurt player two in some way.
00:18:48.140 - 00:19:07.700, Speaker A: And so if she wants to get the item, the natural thing for her to try is to blackmail player two into handing over the item so she could say I'm going to blackmail you if you don't give me the star, I'll do this lightning thing against you.
00:19:08.390 - 00:19:08.898, Speaker C: Okay?
00:19:08.984 - 00:19:40.918, Speaker A: And of course this has to be credible. If she just says this then there's no, it's just cheat talk, right? So somehow this has to be a credible commitment or at least there has to be some kind of credibility. And then also we assume that this lightning, like using this lightning against player two is not something that she intrinsically wants to do, it's something that she otherwise wouldn't want to do.
00:19:41.104 - 00:19:41.840, Speaker C: Okay?
00:19:45.160 - 00:19:55.290, Speaker A: And now if player one decides to make this threat, player two can decide whether to hand over the star.
00:19:57.100 - 00:19:57.416, Speaker C: In.
00:19:57.438 - 00:20:36.260, Speaker A: Which case he's unhappy because he lost the star. But at least no threat was carried out and player 2 may also decide to not give in to this blackmail attempt and then kind of everyone is unhappy because player one doesn't get the item and she has to execute the threat which she doesn't want to do. And player two, okay, kept the item but the threat was carried out, which has to be like I mean for the threat to make sense it has to be worse than losing the item.
00:20:36.260 - 00:20:58.860, Speaker A: So this is kind of like, again, this kind of utility being burned outcome as a result of two people making conflicting demands for the star. So here player one kind of insists on getting the star. Player two also insists on getting the star and the result is that utility is being burned.
00:21:00.560 - 00:21:01.404, Speaker C: Okay?
00:21:01.602 - 00:21:41.080, Speaker A: And here this is given as a normal form game. So this game is a bit weird because it's because giving in conditional on being threatened weakly dominates in this game the strategy of not giving in. So one has to tell some kind of story for why player two would ever not give in to threats or to blackmail in this scenario.
00:21:41.080 - 00:22:18.900, Speaker A: There are lots of stories that we can imagine here like reputation or maybe player two can kind of commit beforehand and there's some chance that player one will see whether player two has committed beforehand or the like. In any case, it seems like kind of just giving in to threats whenever they're made against you is kind of like agents that behave this way probably end up not having all that many resources.
00:22:20.920 - 00:22:21.284, Speaker C: Okay?
00:22:21.322 - 00:22:34.650, Speaker A: But I'm not going to kind of model this. I'm just going to assume that sometimes this kind of weak equilibrium in which player two doesn't give in that this sometimes can happen.
00:22:37.280 - 00:22:38.030, Speaker C: Okay.
00:22:42.880 - 00:22:58.960, Speaker A: Now here's the proposed, the proposed solution for this. So again we have this game. But now we imagine that player two, and only player two this time plays this game through a representative.
00:22:58.960 - 00:23:20.456, Speaker A: So this time they're all smiley faces. So I guess they're all humans. And basically what player two does is player two kind of hands over the star to his representative and kind of says like, yeah, you have to manage this now.
00:23:20.456 - 00:23:55.540, Speaker A: And again player two can give some kind of instructions for what to do. And one might imagine that kind of by default if there was no credible commitment, if credible commitment wasn't possible, then we might imagine that we would have this default of a line delegation which is like player two telling his representatives here's how much I like this star. Here's how much I dislike getting hit by lightning.
00:23:55.540 - 00:24:47.060, Speaker A: And do the best that you can given this information for me, okay? Now of course now if player two can credibly give instructions to his representative then there's another thing that is very natural for him to do which is to say just never give in to Fred. And then it kind of becomes like a stackerberg game where player two kind of wins by being the first to choose. And it's kind of in some ways kind of the opposite of the other direction of the other ordering.
00:24:47.060 - 00:25:16.124, Speaker A: Just two minutes ago I talked about how maybe for player two it makes sense to always give into threats. Like similarly maybe for player one it makes sense to then always not make threats if player two commits first. But I think again one can make all of these or come up with these kind of different real world stories for why this doesn't work.
00:25:16.124 - 00:25:29.280, Speaker A: And it seems like in the real world, just like someone committing first doesn't always resolve these kind of disputes.
00:25:34.420 - 00:25:35.170, Speaker C: Okay.
00:25:37.700 - 00:26:15.896, Speaker A: But now what if player two doesn't want to just give this like never give in instructions? What if player two wants to do something kind of soft that maybe player one wouldn't mind that much? Okay, so here's the idea. The idea is that player one doesn't only have this threat that she can make against player one. She also has some other thing that she can do that player one doesn't actually care about but that is equally costly to her as the original threat.
00:26:15.896 - 00:26:38.644, Speaker A: So if the original threat is throwing paint at player one's house, then maybe the new threat could be throwing paint at player one's neighbor's house or something like that. I guess it's tough for player one's sorry, player two's, player two's neighbor's house which I guess is tough for player two's neighbor's. Neighbor.
00:26:38.644 - 00:26:40.650, Speaker A: But maybe player two didn't care about.
00:26:41.900 - 00:26:42.264, Speaker C: Okay?
00:26:42.302 - 00:27:23.556, Speaker A: And now the idea is that player two could tell his representative to not. So there are different versions of this but one version is like player two tells his representative to not care about the original threat but care about this new threat that player one doesn't actually care about. And again we assume that this is credible and then given that this is made that this instruction is given player one will player one can again decide whether to blackmail or not.
00:27:23.556 - 00:27:52.540, Speaker A: But player one will. Now for player one it now only really makes sense to threaten this new threat target that player one doesn't actually care about. So what might happen? So player one could blackmail and then well it could be that player two's representative decides to give into this threat because player two's representative actually is told to care about this new kind of threat.
00:27:52.540 - 00:28:23.032, Speaker A: So then the star is transferred and then basically everything is asked before but it could also be that player two's representative decides to resist. Then player two's representative is kind of hit with this new type of lightning, this new threat. And now for player one this is just as in the original game.
00:28:23.032 - 00:28:40.300, Speaker A: For player two this is also just as sorry. For player two's representative this is also just like in the original game. But for player two, the original player two this is now fine because the original player two doesn't actually care about this lightning.
00:28:44.110 - 00:28:44.570, Speaker C: Okay?
00:28:44.640 - 00:29:29.014, Speaker A: And yeah we can look at this in the payoff matrix. We can look at how the game is played by default, then we can look at how this game is played with this instruction. So the utilities need to kind of be exactly isomorphic to the utilities in the original game and then we again get this safe parameter improvement that if we assume that the games are played isomorphically it's guaranteed like this new game is guaranteed to be at least as good for both players as the original game.
00:29:29.014 - 00:29:51.520, Speaker A: And there's one outcome, see if we can quickly find it. I guess this outcome, the outcome where the threat is made and player two doesn't give in or player two's representative doesn't give in. In that outcome player two, the original player two is now much better off than in the original game.
00:29:51.520 - 00:30:03.220, Speaker A: Okay, basically I've just given examples, right? So yeah, there's this paper which.
00:30:04.950 - 00:30:05.218, Speaker C: Kind.
00:30:05.224 - 00:30:19.240, Speaker A: Of defines what a safe parity of improvements is and shows how to find these in general and gives complexity results and these kind of things.
00:30:20.010 - 00:30:20.760, Speaker C: Okay?
00:30:21.770 - 00:30:50.450, Speaker A: So now kind of moving a bit towards the more discussiony part in my mind the biggest obstacle to implementing safe pressure improvements or surrogate goals is that one has to make one, one has to make these, these instructions credible.
00:30:54.390 - 00:30:54.706, Speaker C: And.
00:30:54.728 - 00:31:30.430, Speaker A: There are in fact two things that one has to make credible. So the first thing is that in the surrogate goal language, the player two has to make it credible that player two's representative actually cares about the new kind of threat. Player one has to kind of really believe that player two's representative actually cares about this new threat about as much as player two's representative cares about the sorry.
00:31:30.430 - 00:32:04.130, Speaker A: About as much as player two cares about the original threat. And also you have to make it credible that you didn't then kind of mess with their representatives behavior in other ways. So one thing that you might try to do is you tell your representative, okay, you care about this surrogate goal, you make this credible, but then also kind of sneakily.
00:32:04.130 - 00:32:31.330, Speaker A: You kind of tell them like, oh, and by the way, don't give in as much. And then it's not I mean, maybe that's like a sensible strategy, but then if that's a possibility, then it's not a safe paratro improvement anymore because now it is kind of a strategy. For example, in this blackmail game, it is a strategy for player two to kind of screw over player one to make player one worse off than in the game that would have been played by default.
00:32:31.330 - 00:33:19.620, Speaker A: And this seems relatively difficult. I think it's a relatively difficult kind of credible commitment because it's not just about what action you take in the end, but it's also about the process at which you arrive at these actions or about maybe it's about counterfactuals, like what would have happened in the original game in the end. If we observe that, for example, in the blackmail game, player one makes a threat and then player two's representative doesn't give in, we can't tell whether everything was done reasonably or whether there was some funny thing going on where player two told his representative to never give in.
00:33:24.390 - 00:33:25.140, Speaker C: Okay.
00:33:29.110 - 00:33:49.050, Speaker A: I think I want to just very briefly give one of these kind of stories for how this could work in the real world. And it's like not using cryptography. Leaving that to the discussion.
00:33:49.050 - 00:34:20.420, Speaker A: So let's say that I get fired and there's this kind of typical kind of dispute between my kind of now former employer and me about, I don't know what my severance package has to look like and things like that. So I hire a lawyer, my employer hires a lawyer. And now these lawyers kind of argue with each other about what should happen.
00:34:20.420 - 00:34:33.298, Speaker A: I don't know anything about law, by the way. It's a very naive view of what happens probably. But that's kind of how I imagine this happened.
00:34:33.298 - 00:35:08.180, Speaker A: You then have these lawyers and they have these debates. And now let's say that my employer could use various kinds of threats to get my lawyer to accept a lower chevrons package. So for example, my employer could say, well, if you don't settle for this low severance package, we're going to put on our website a page that says that Casper is very unreasonable or something like that.
00:35:08.180 - 00:35:32.300, Speaker A: And they have no interest in doing this. The only reason why they might do this is to force me into accepting a lower or force my lawyer or get my lawyer to accept a lower severance package. And if that in the end actually happens, then that would be like burning utility again.
00:35:32.300 - 00:36:18.374, Speaker A: And now one thing that I might try to do is put in my contract with my lawyer, incentives for my lawyer. I'm pretty sure that this is not exactly how it works in the real world, but in principle, the way I could set up the contract with my lawyer is that I say you get paid in proportion to how valuable the outcome of this negotiation is for me. And now if I make this contract public or if I show it to the university, if I make it transparent to the university what my lawyer's incentives are, then I could use a surrogate goal for my lawyer.
00:36:18.374 - 00:37:18.462, Speaker A: So, for example, I could say that I could put in the contract that my lawyer gets a low pay, not if the university puts my former employer, which I guess in my case, my employer is a university if the employer puts up a page that says that I'm unreasonable. Instead, I could tell my lawyer that I'm not going to pay her or pay her as much if the university puts up some random page that just, I don't know, consumes some random ones and zeros, because then essentially the university could use that threat that is completely unimportant to me in the negotiation with my lawyer. Okay, so that's one story.
00:37:18.462 - 00:37:33.010, Speaker A: I think to keep it short, I'll skip the other ones and yeah, thanks for listening and I hope we have some interesting discussion and brainstorming.
00:37:41.610 - 00:37:50.182, Speaker E: Great. I think I will open the camera here on this computer for you to maybe see the audience better.
00:37:50.316 - 00:37:52.486, Speaker A: Yeah, that would be good.
00:37:52.668 - 00:37:53.122, Speaker C: Okay.
00:37:53.196 - 00:37:55.222, Speaker E: And you can indeed see the audience.
00:37:55.286 - 00:37:59.900, Speaker A: Right, okay, nice.
00:38:01.470 - 00:38:02.220, Speaker C: Okay.
00:38:04.350 - 00:38:04.714, Speaker A: Yeah.
00:38:04.752 - 00:38:33.190, Speaker E: I want to start with a question. So you mentioned we need to make sure that there are no further secret contracts. And so is it possible for or have you thought about does there exist some ways for us to make an agent to commit, to lose the ability of further commitment? So if the agent is able to make such a commitment, then it cannot enter a further secret contract?
00:38:35.210 - 00:38:44.634, Speaker A: Yeah, that's a good question. Yeah. I would imagine that this is very dependent on the exact application.
00:38:44.634 - 00:39:37.450, Speaker A: So, for example, in this lawyer story well, I don't really know that much about law, but I don't know, I would assume that it's possible for a contract to specify that no further payments are going to be made between the parties or something like that. In. Yeah, I think in the case of AI, it's very tricky because I guess the application that I usually think about is we build AI and we install some surrogate goal in it, so that if people want to blackmail the AI or have some conflict with the AI, the surrogate goal is targeted rather than our real goals.
00:39:37.450 - 00:39:58.480, Speaker A: And there are all of these different ways in which we can kind of sneakily tell the AI to do some other things and one has to make it credible that one doesn't do that. One doesn't use any of these ways to tell the AI to.
00:40:01.990 - 00:40:02.514, Speaker C: What are.
00:40:02.552 - 00:40:05.330, Speaker E: Some examples of telling AIS to sneakily?
00:40:09.190 - 00:40:19.350, Speaker A: The simplest is just to train the AI a bit differently. If you use some ML techniques.
00:40:22.570 - 00:40:22.934, Speaker C: To.
00:40:22.972 - 00:41:13.480, Speaker A: Train your system on to behave reasonably in bargaining situations or something like that, then if you know that a surrogate goal is going to be used right, you'll want to train the AI in a way that is less likely to give in. And there might be relatively kind of yeah, there might be various ways in which you might be able to train the AI to be less likely to give in. You just change the way that the system is trained or something like that in such a way that it is more likely to learn to not give in.
00:41:13.480 - 00:41:46.500, Speaker A: And yeah, I would imagine that many of these ways are difficult to detect. Like, even if even if the even if the opponent could look at the source code of my training scheme, it might be hard for them to tell whether this training scheme is set up specifically to make it less likely that the resulting model is going to give in.
00:41:48.150 - 00:42:09.980, Speaker E: I see. Any work the assumption was that the person delegating the play to the agent playing the SPI trust the agent to play it similar to what the human or the delegator would have played the game in the isomorphic whatever case.
00:42:10.350 - 00:42:43.894, Speaker A: Yeah, or that even I guess I think the way the paper is written is that the human kind of has no idea how to play the game. The human sees this equilibrium selection like the original player, the human in the slides sees this equilibrium selection problem and says like, okay, I have no idea what is supposed to be done here. And then just says, okay, I'll just want to do whatever this system decides is best for me.
00:42:43.894 - 00:42:52.360, Speaker A: Or this representative who might be an AI. Might be might be an experienced negotiator or something like that.
00:42:54.010 - 00:42:54.678, Speaker E: I see.
00:42:54.764 - 00:43:10.200, Speaker A: Makes sense. Have a question.
00:43:12.570 - 00:43:14.070, Speaker B: So if you're imagining.
00:43:17.130 - 00:43:21.082, Speaker D: If you're imagining I'm just kind of off camera, but I have if you're imagining a situation.
00:43:21.136 - 00:43:23.526, Speaker A: In which you have pretty sophisticated AI.
00:43:23.558 - 00:43:58.850, Speaker D: Systems that are kind of acting on behalf of humans to do maybe tasks that we can't do ourselves, then that situation sort of makes sense. But I wonder in that case, whether any sufficiently capable AI system that can do that will also be able to just infer the preferences, the true preferences of the agent. Of the human or person that we care about, the principal, who is delegating to the agent, and in which case they can, in fact, just maybe threaten that human.
00:43:58.850 - 00:44:16.460, Speaker D: I suppose in that case, if the AI system is the only person taking actions, then maybe that doesn't buy you anything. But I wonder, is this a concern or does this just get kind of washed away by the fact that the human is totally in some sense out of the picture after the agent is kind of delegated to.
00:44:20.750 - 00:44:45.634, Speaker A: I'm not sure, I'm not sure I fully understand the question. So, I mean, one thing is that normally the idea is that everyone knows what's going on, or at least potentially everyone knows exactly what's going on. So if I tell my lawyer to disvalue, if I say, okay, lawyer, I'm not going to pay you all that much.
00:44:45.634 - 00:45:26.030, Speaker A: If the university, after firing me, puts up a page that says, then everyone will know that what I'm doing. No one will think, maybe Casper actually cares about this page. My lawyer will know, the university will know, everyone will know that what I'm doing is deploying a surrogate goal, but they don't really care, right? Like the lawyer will want to maximize their payments, so they're going to act on whatever payments I set for them.
00:45:26.030 - 00:45:47.080, Speaker A: So they're going to try to avoid this from happening and the university just sees this thing. Okay, I guess now instead of threatening to put up a page about how Casper is unreasonable, we're going to threaten to put up this other page. Well, to us, it's like this also is kind of the same as the it's kind of the same as playing the original game, so we don't really care.
00:45:47.080 - 00:46:16.210, Speaker A: And that's kind of the idea. Even if everyone knows what my original preferences are, the idea is that they still go along with this because well, the, the representative goes along with this because that's what they're told to do, my representative. And the other side goes along with it because it doesn't hurt them.
00:46:16.210 - 00:46:33.462, Speaker A: They have no reason to say, actually, we're going to punish you for using this scheme because for them, they're going to be equally well off as in the original game. Okay, cool.
00:46:33.516 - 00:46:33.974, Speaker B: Yeah, sorry.
00:46:34.012 - 00:46:44.806, Speaker D: Some of the stories in my head were about kind of things where the principals preferences might have been kind of hidden or kind of whatever, but I guess that's not actually necessarily what needs to be going on here. Seems fine.
00:46:44.828 - 00:46:48.360, Speaker A: If it's not that yeah, cool.
00:47:05.150 - 00:47:13.200, Speaker E: Yeah. I think does anybody has other questions? Otherwise, I think we can also win.
00:47:13.650 - 00:47:22.180, Speaker B: Yeah. Okay, I have one. Did you hear the end of the discussion at last, the last talk?
00:47:23.510 - 00:47:37.080, Speaker A: I heard a bit of it, yeah. I'm not sure I heard the part that you're going to reference. Now, if it was the discussion between.
00:47:40.250 - 00:48:27.050, Speaker B: Okay, so there's this distinction between delegating to a representative and just telling them exactly what to do, like specifying a strategy for them, which is like a particular form of commitment, which is sometimes useful, essentially equal one to something or to behave in a particular way or to, just as in your paper, delegate to a representative and change their incentives like by changing their utility function. And yeah, we heard some reasons in the last talk why the former is preferred.
00:48:27.230 - 00:48:27.960, Speaker C: Sorry.
00:48:29.770 - 00:48:58.538, Speaker B: Yeah, there's also this idea of like, committing yourself to transfer utility, which the previous talk was kind of advocating for and like saying that it works in their setting. It's basically like as powerful as anything else you would want to do, and it's kind of good for other reasons, like it's more credible and more like realistic or something. Sorry, I forgot some of the reasons.
00:48:58.538 - 00:48:59.870, Speaker B: But they were good reasons.
00:49:00.610 - 00:49:00.974, Speaker A: Yeah.
00:49:01.012 - 00:49:14.390, Speaker B: And do you have a perspective on this? Do you kind of agree with the idea that just committing yourself to transfer a utility would be as powerful as, for instance, what you want to do with safe crate improvements?
00:49:23.290 - 00:49:35.856, Speaker A: I mean, I guess. Okay. I mean, I guess in a sense you can so one point, if you.
00:49:35.878 - 00:49:51.030, Speaker B: Look at the first example you showed, you can't just by committing to transfer utility, you can't transform the upper left of the quadrant into the lower right.
00:49:51.560 - 00:49:52.310, Speaker A: Yeah.
00:49:53.480 - 00:50:11.980, Speaker B: You can do it in different like if you have the ability to burn utility, then you can kind of reverse like there's like this the four quadrants of the game are kind of like prisoner sola and you can reverse that prisoner, maybe if you commit to burn utility.
00:50:14.000 - 00:50:32.032, Speaker A: Yeah. So that's this thing right. Are you saying you could use burning utility to commit against DM and RM? Basically, yeah.
00:50:32.032 - 00:50:33.296, Speaker A: So I guess the point of this.
00:50:33.318 - 00:50:43.380, Speaker B: Is that if you pursue this strategy, you also have to burn six utility in DLRL to make it is more and then there's no point because now you're just playing the original game.
00:50:43.530 - 00:50:44.230, Speaker A: Yeah.
00:50:44.940 - 00:50:54.570, Speaker B: You can delegate to somebody who where your representative imagines that they're burning utility, but really not then it is again.
00:50:56.140 - 00:51:38.090, Speaker A: Yeah, um, yeah, I think the yeah, I think the the answer does kind of depend on whether these kind of schemes count. Yeah. I mean, yeah, like depending on what things you allow, it might enable to basically do exactly the safe Parato improvements idea, which is, as you say, to transform this game into the bottom right game.
00:51:38.090 - 00:52:12.010, Speaker A: But yeah, I guess normally I wouldn't think of these ideas as kind of being allowed when you say like, yeah, you can commit to transfers and yeah, I would think that normally.
00:52:16.430 - 00:52:16.870, Speaker C: If you.
00:52:16.880 - 00:53:21.490, Speaker A: Can just commit to transfers, there are problems that safe paratro improvements can help with, that committing to transfers doesn't help with. But all of this is partly because Fake Improvements takes this somewhat unusual perspective of assuming that there are these equilibrium selection problems and assuming that you don't just want to resolve them. If you just analyze this as like a normal game if you analyze this top left in the normal way of saying, well, there's east equilibria right, then in some sense there's not really a problem even in this game, right? You could just well, for example, dMRM, it's like a perfectly fine outcome, right? It's Pareto optimal and so on.
00:53:21.490 - 00:54:05.610, Speaker A: I would imagine that the normal way of thinking about committing to these transfers doesn't always resolve equilibrium selection or enables dealing with it in the ways that safe parade improvements can deal with. Of course, there are lots of other cases where it can resolve the problem in a way that's maybe better than safe parameter improvements. Yeah, but I haven't thought about this that much.
00:54:07.820 - 00:54:26.240, Speaker E: Okay, yeah, I think it's time for next next session. But before that we're going to have, let's say 1520 minutes break your session kind of eating took the end of the brainstorming session, so that's good. And Casper, thanks for zeroing the troll.
00:54:26.980 - 00:54:31.900, Speaker C: Okay, thanks.
00:54:34.910 - 00:54:38.710, Speaker E: Yeah. Look forward to talk to you more. Bye.
